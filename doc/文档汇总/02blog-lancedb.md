

---

## ðŸ“„ æ–‡ä»¶: a-practical-guide-to-fine-tuning-embedding-models.md

---

```md
---
title: "A Practical Guide to Fine-Tuning Embedding Models"
date: 2024-03-25
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/a-practical-guide-to-fine-tuning-embedding-models/a-practical-guide-to-fine-tuning-embedding-models.png
description: "Explore a practical guide to fine-tuning embedding models with practical insights and expert guidance from the LanceDB team."
author: Ayush Chaurasia
author_avatar: "/assets/authors/ayush-chaurasia.jpg"
author_bio: "ML Engineer and researcher focused on multi-modal AI systems and efficient retrieval methods."
author_twitter: "ayushchaurasia"
author_github: "ayushchaurasia"
author_linkedin: "ayushchaurasia"
---

This is a follow up to the following report that deals with improving retrievers by training and fine-tuning reranker models

[A Practical Guide to Training Custom Rerankers](/a-practical-guide-to-training-custom-rerankers/)

In this report, we try to answer questions like - *If/when should you fine-tune embedding models, and what are the qualities of a good fine-tuning dataset*

We'll deal with embedding part of the retrieval pipeline, which means any changes or updates will require re-ingestion of the data, unlike reranking.

![Retrieval Pipeline](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-20-at-7.49.44-PM.png)

## Tuning Embedding Models

For this report, the sentence-transformers Python library used to fine-tune embedding models, the same as the previous reranking report.

![Sentence Transformers](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-19-at-9.45.35-PM.png)

You can refer to the guide to fine-tuning and training embedding models using sentence-transformers [here](https://huggingface.co/blog/train-sentence-transformers). In brief, the training process happens in the following steps:

- Choose a dataset. We're using GooQA dataset's  `question` and `context` columns
- Mine hard negatives to transform it into a similarity dataset of triplets formats (anchor, positive, negative)
- Repeat the same process for evaluation set
- Choose a loss function for your dataset format. Here we'll use `MultipleNegativesLoss`
- Choose a suitable evaluator based on your dataset format. Here we'll use `TripletEvaluator`
- Finally, run the training loop for the desired iterations/epochs.

**Base model -** The base model used is `all-MiniLM-L6-v2` as it was the model used for embedding generation in the Rerankers report as well.

### Fine-tuning Results

Here are the results of fine-tuning the model as described in the previous section.

**Hit rate @5**

![Hit Rate @5](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-16-at-7.22.41-PM.png)

**Hit rate @10**

![Hit Rate @10](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-16-at-7.23.28-PM.png)

- Overall, we see improvements of **roughly 10% across both top 5 and top 10**
- The main thing to notice here is that **each fine-tuned model performs significantly better than the baseline**
- Increasing the number of iterations generally tends to improve the results further, which is expected. 
- Towards the end, where the model is trained for larger epochs (10), it starts to show some signs of unstable training or overfitting, which, again, is expected.

### Should You Always Fine-tune Embedding Models?

The most important question at this point is if we can generalize these results, i.e, **should you always fine-tune your embedding models if you can?**

Fine-tuning is like training. The only difference is that when training a model, you start with random or close-to-random weights that output gibberish. When fine-tuning, you start with a model already trained on some different dataset or combination of datasets.

Let's look at another example.

**Experiment setup**

- Model -  all-MiniLM-L6-v2 ( same as before)
- Dataset - SQuAD. The dataset has ~90K rows. For this experiment, I used 45K rows for the training set and 5K for the evaluation set. The format and loss functions used were the same as the ones used in GooQA dataset fine-tuning.
- In contrast to previous experiment, the dataset is much smaller (2M rows vs 45K rows)

**Ideal scenario for fine-tuning**

Another thing to note about the dataset is that both of these datasets are general QA datasets, i.e., they're not specialized domain-specific query-context pairs. SQuAD is a popular dataset that most embedding models use as a part of their larger training set. 

Here's the visualization of the training dataset and fine-tuning dataset.

![Training Dataset Visualization](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-17-at-2.59.17-PM.png)

An educated guess would be to associate fine-tuning on

-   SQuAD dataset as the left representation.
- In this case, **you're fine-tuning the model on the subset of data it has already seen** without adding any new information. 
- This can result in unstable training, overfitting, catastrophic forgetting or just some sudden performance degradation depending on the training process
- It is **generally not recommended to fine-tune** in such scenarios. Evaluation **might initially show good results due to overfitting**.

- A large domain-specific dataset as the right representation. 
- You can think of legal docs or financial reports/filings, etc., as a large domain-specific corpus that might have a small intersection with the larger general training data.
- **It is recommended to fine-tune models on such datasets** as it allows the model to adapt to a new domain, specializing in answering those questions with better context or more confidence.

- GooQA dataset as somewhere between the left and the right representation.
- GooQA is much larger than SQuAD, yet it is a general QA dataset without any domain specifications.
- Training on this dataset should be much more stable than SQuAD, **and you can fine-tune on such datasets if you expect future user queries to be limited to the same distribution**. In the real world, the chance of this happening is quite low.

### SQuAD Fine-tuning Results

Here's what happens if you fine-tune on a dataset that's similar to the representation on the left

**hit rate @5**

![SQuAD Hit Rate @5](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-20-at-7.09.35-PM.png)

**Hit rate @ 10**

![SQuAD Hit Rate @10](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-20-at-7.10.18-PM.png)

The results seem aligned with our analysis in the previous section:

- The delta in improvement is much smaller.
- **The model starts to overfit much faster**. In fact, in most cases with a high number of epochs, the results get worse. In the case of hit-rate@10, only 3 models perform better than the baseline.

Let's run another experiment that fine-tunes a much larger model on this dataset, to confirm our hypothesis. Here's the top@5 and top@10 hit-rates on both `All-miniLM-L6-v2` and `bge-1.5-en-base` with all results sorted for easier comparison

![Model Comparison](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-20-at-7.14.51-PM.png)

The results** in both experiments seem aligned. Both models overfit at higher epochs, and training is pretty unstable.**

**What about augmentation and synthetic data generation?**

LLMs have lately been used to generate synthetic questions and answers from a given passage/context to fine-tune embedding models. Can't we just use those techniques to augment the dataset in this scenario? 

There are 2 major things to keep in mind before attempting synthetic data generation.

- Augmentation improves the dataset by creating slightly different versions of existing data points. It doesn't necessarily add a lot of *new* context to your dataset distribution. For example, when training vision models, data augmentation takes an image and randomly crops parts of it or rotates it by certain angles to create new samples. It basically prevents the model from overfitting to a specific angle, position, etc. It doesn't really add any new information about a new class object. So, the rule of thumb to follow in data augmentation is: **Bad data, on augmentation, will most likely result in bad or worse augmented data.**
- **LLMs hallucinate.** When generating synthetic datasets (questions from context) from existing dataset, it has been found that a large portion of generated questions can be hallucinations or simply of bad quality questions. In fact, there are many tools that are used to filter out hallucinations from the synthetic data generation process. 

So although you can experiment with synthetic data generation, and it also is a very powerful tool to augment good quality datasets, it is not a magical solution if you don't have a decent base dataset.

![Synthetic Data Generation](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-20-at-7.40.07-PM.png)

### Fine-tuned Embedder + Reranker? 

Now coming back to the GooQA dataset experiment results. We saw significant improvement in the results even though the dataset was pretty general, which isn't the best case for fine-tuning. But can we do better? Let's revisit our experiment results from the previous report that dealt with training and fine-tuning reranker models. Here's the best result that we got after fine-tuning an existing reranker.

![Reranker Results](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/image-5.png)

The **best results were 70% @top10 and 62.35% @top5** respectively

You can read the entire report here for more details on reranking process and tradeoffs:

[A Practical Guide to Training Custom Rerankers](/a-practical-guide-to-training-custom-rerankers/)

Can we improve the results further by combining our fine-tuned embedding models with the best rerankers @top 5 and top 10? Here are the results (FTS column is irrelevant as FTS doesn't involve embedding models)

![Combined Results @5](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-16-at-7.25.46-PM.png)

![Combined Results @10](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-16-at-7.26.06-PM.png)

On combining fine-tuning with reranking, we get new best results:

![Best Combined Results](/assets/blog/a-practical-guide-to-fine-tuning-embedding-models/Screenshot-2025-04-23-at-2.27.51-PM.png)

Top 5 -** 62.34 -> 64.50**

Top 10 - **70.00 -> 71.85**

## Using Embedding Functions with LanceDB

LanceDB has integrations with all popular embedding model providers. With the embedding API, you can simply define the embedding model of your choice when initializing the table, and it'll automatically take care of generating embeddings for both source and queries. Here's an example

```python
import lancedb
from lancedb.embeddings import get_registry
from lancedb.pydantic import LanceModel, Vector

model = get_registry().get("sentence-transformers").create() # use default ST model

# define schema with embedding API
class Schema(LanceModel):
    text: str = model.SourceField() # All entries of sourcefield are vectorized
    vector: Vector(model.ndims()) = model.VectorField

db = lancedb.connect("~/lancedb")
table = db.create_table(schema=Schema, name="tbl")

table.add(
 [
  {"text": "some random text"},
  {"text": "some random text again"}
 ]
 ) # Source field automatically gets converted to their vector embeddings

table.search("search text") 
# You can directly pass string as it'll be 
# converted to embeddings as we've initialized 
# the schema with embedding API
```

Using a custom fine-tuned embeddings is as simple as passing the model name when initializing model from embedding registry

```python
tuned_model = get_registry().get("sentence-transformers").create(
                                name="path/to/tuned_model")
```

## Reproducibility

All code to reproduce the above experiments are available [here](https://github.com/lancedb/research/tree/main/embedding-fine-tuning)

All trained models used in this experiment are available of [HF hub](https://huggingface.co/ayushexel)
```

---

## ðŸ“„ æ–‡ä»¶: a-practical-guide-to-training-custom-rerankers.md

---

```md
---
title: A Practical Guide to Training Custom Rerankers
date: 2025-04-10
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/a-practical-guide-to-training-custom-rerankers/a-practical-guide-to-training-custom-rerankers.jpg
description: Explore a practical guide to training custom rerankers with practical insights and expert guidance from the LanceDB team.
author: Ayush Chaurasia
author_avatar: "/assets/authors/ayush-chaurasia.jpg"
author_bio: "ML Engineer and researcher specializing in retrieval systems, reranking models, and practical AI applications."
author_twitter: "ayushexel"
author_github: "ayushexel"
author_linkedin: "ayushchaurasia"
---

### A report on reranking, training, & fine-tuning rerankers for retrieval

---

This report offers practical insights for improving a retriever by reranking results. We'll tackle the important questions, like: *When should you implement a reranker? Should you opt for a pre-trained solution, fine-tune an existing model, or build one from scratch?*

The retrieval process forms the backbone of RAG or agentic workflows. Chatbots and question-answer systems rely on retrievers to fetch context for every response during a conversation.
Most retrievers default to using vector search. It makes intuitive sense that improved embedding representations should yield better search results.

![Retrieval Process Overview](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-09-at-4.44.51-PM.png)

In this blog series, we'll work backwards from the last component of the retrieval process. This blog deals with the reranking aspect of RAG or retrievers.

### So, where does reranking fit in?

To understand the importance of reranking models, let's first look at the difference between embedding and ranking models and try to answer this question:

If enhancing embedding models can yield better search results, why not focus on refining (by fine-tuning) existing models or experimenting with new ones?

## Embedding vs Ranking models

The fundamental difference between embedding and ranking models is

- Embedding models convert any given data point to a vector representation in a given vector space.
- Ranking models rank the data points (or documents in case of text ranking ), based on their relevance to the query

![Reranking Process](/assets/blog/a-practical-guide-to-training-custom-rerankers/reranking-scaled.jpg)
*source - [https://unfoldai.com/rag-rerankers/](https://unfoldai.com/rag-rerankers/)*

So the** job of an embedding model is to keep semantically similar docs or queries close in a given vector space**. During vector search these similar docs are fetched as probable responses to the query. 

A **reranking model re-arranges this list such that the most relevant answers appear at the top**. Another scenario where reranking is useful is in cases where you need to combine multiple independent search result sets like from both vector search and full-text search.

**Why not use a ranking model for search?**

Ranking is typically a much more expensive operation than vector search. You can't run your entire database row-by-row through a reranker model. Unlike embedding models, you can't create vector representations of docs offline, as ranking models need both the query and docs to calculate the rank.

Here are some more differences between embedding and ranking models.

| Feature | Embedding Models | Reranking Models |
|---------|------------------|------------------|
| **Primary Goal** | Generate vector representations capturing semantics. | Calculate precise relevance scores for query-item(s) pairs. |
| **Input** | Single item (query *or* document/data point). | A pair: (query *and* a candidate item). |
| **Output** | Dense numerical vector (embedding). | A single relevance score per input pair. |
| **Typical Use** | Initial candidate retrieval, semantic search, clustering. | Refining/reordering top candidates from retrieval. |
| **Stage in Pipeline** | Early stage (candidate generation/retrieval). | Later stage (refinement/reordering). |
| **Compute Cost** | Generally lower per-item (embeddings often pre-computed). | Generally higher per-pair (evaluates interaction). |
| **Scalability** | High (efficient for large datasets via vector search). | Lower (applied only to a small subset of candidates). |
| **Architecture** | Encodes items independently. | Processes query and item together for interaction. |
| **Quality vs. Speed** | Optimized for speed & broad recall on large datasets. | Optimized for precision/quality on a smaller set. |
| **Example** | Encoding docs into vectors for a vector DB. | Re-ordering top 50 results based on relevance scores. |

*In case of cross-encoders, each query-doc pair's similarity is calculated individually. But late interaction based models, like ColBert, calculates doc representations as a batch and reranks them based on MaxSim operation with query representations.*

## Fine-tuning tradeoffs 

### Embedding models

Upgrading your embedding model, by either switching to a new model or fine-tuning a base model on your data, *can* significantly improve the base retrieval performance. However, fine-tuning the embedding model changes the entire embedding space, which may end up harming the overall performance of the model. In addition, it is disruptive operationally because you need to regenerate all of the embeddings. And with many vector databases that only allows one vector field per record, it means managing many different tables for experimentation. For more info on this, you can read up on catastrophic forgetting in LLMs.

### Rerankers

However, because rerankers are used in the later stage of the retrieval pipeline, you can switch them without disrupting the existing workflow. You can switch models, train new ones, or even get rid of them completely without having to re-ingest your data or update query/source embedders. This tends to be a much easier way to squeeze more quality out of your retrieval pipeline.

---

## Training rerankers from scratch

This blog deals with reranker models and how you can start off with base LLMs (not base rerankers) and train them on your private data in a few minutes to get significant retrieval performance gains. Of course you can start off with a trained reranker and fine-tune it on your dataset too. 

**Dataset**

The dataset used is [gooaq](https://huggingface.co/datasets/sentence-transformers/gooaq) which has over 3M `query and answer` pairs. Here's it is used for training and eval:

- First, 2M rows are used for training.
- Next 100_000 rows are ingested in LanceDB
- From that 100_000, we take 2000 samples and evaluate the hit rate

### The baselines
![Baseline Performance](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-07-at-1.55.57-AM.png)

**Base Model(s) used:**

For training cross-encoder and ColBert models, we'll use the same two base models:

1. MiniLM-v6 - A very small base model with only** ~6Million params**
2. ModernBert-base - A relatively larger base model with **~150Million params** (yet much smaller than SOTA reranker models)

This allows us to look at the tradeoff between model size (and latency) vs quality improvement.

### Training Cross encoders 

A cross-encoder is a neural network architecture designed for comparing pairs of text sequences. Unlike bi-encoders, which encode texts separately, cross-encoders process both texts simultaneously for more accurate comparisons.

![Cross Encoder Architecture](/assets/blog/a-practical-guide-to-training-custom-rerankers/cross_encoder.png)

- **Joint Processing**: Both the query and document/passage are concatenated and fed together into a transformer model (like BERT or RoBERTa).
- **Cross-Attention**: The transformer applies self-attention across the entire concatenated text, allowing each token to attend to all other tokens in both sequences

**Dataset preprocessing:**

- We'll format the train subset of the dataset for training a cross-encoder model using BinaryCrossEntropy loss. To use that loss function, we'll need the dataset is this format: `query/anchor, positive, label `
- Similarly, for evaluating, we'll create the eval such such that it has many negatives like `anchor, positive, neg_1, neg_2 ..`

Finally, to create the train and test split from the dataset, **we'll mine "hard negatives" from the given dataset**. 

Hard negatives are negatives that have very similar semantic meaning to the query's positive answer. **We use an independent embedding model to find negatives are semantically close to the query**, also known as anchors.

To read more in-depth about the training recipe, you can refer to sentence-transformer's cross encoder training [guide](https://huggingface.co/blog/train-reranker#training-components).

In brief, this is how the training pipeline looks:

![Cross Encoder Training Pipeline](/assets/blog/a-practical-guide-to-training-custom-rerankers/ce.png)

### Training ColBERT reranker

![ColBERT Architecture](/assets/blog/a-practical-guide-to-training-custom-rerankers/1_0vzi15Gl7kbiYunawMKggw--1-.png)

For training late interaction models like Colbert, we'll use Pylate, a sentence transformer-based library that implements late interaction utils. The only difference in the training script would be using

- **Contrastive loss** is used for training late interaction models. Here's how it works
- **Token Embeddings**: Extract and normalize token-level representations for query and documents
- **MaxSim Scoring**: For each query token, find maximum similarity with any document token, then sum
- **Label Assignment**: Generate labels identifying which documents match each query. We'll get 2 scores, one for -ve sample and the other for +ve. Now this becomes a classification problem. 
- **Cross-Entropy**: Apply cross-entropy loss between similarity scores and labels. 

- **ColbertTripletEvaluator** similar to cross-encoder reranker, but using late interaction multi-vector embeddings

We'll use `PyLate` python package for fine-tuning late interaction ColBert models.

**The same pipeline will be used for training models from scratch and fine-tuning**

### Using fine-tuned Reranker models with LanceDB

LanceDB has integrations with all popular reranking models. Switching to a new model is as simple as changing a parameter.

![LanceDB Integration](/assets/blog/a-practical-guide-to-training-custom-rerankers/image-3.png)

For example here's how you can use ColBERT model as a reranker in LanceDB, with hybrid search query type.

```python
from lancedb.rerankers import ColbertReranker

reranker = ColbertReranker("my_trained_model/") # local or hf hub

rs = tbl.search(query_type="hybrid")
        .vector([...])
        .text(query)
        .rerank(reranker)
        .to_pandas()
```

LanceDB also integrated with many other rerankers including Cohere, Cross-encoder, AnswerdotaiRerankers etc. Answerdotai reranker integration allows you to use multiple various types of reranker architectures. Alternatively, Here's how you would use ColberReranker using Answerdotai integration.

```python
from lancedb.rerankers import AnswerdotaiRerankers

reranker = ColbertReranker(
            "colbert",    # Model type
            "trained_colbert_model/" # local of HF hub
            )

rs = tbl.search(query_type="hybrid")
        .vector([...])
        .text(query)
        .rerank(reranker)
        .to_pandas()
```

### Performance comparison

The experiment dashboard contains all combinations of model architectures, base models, and **top_k** combinations. Here, I'll present some summary numbers making it simpler to compare the performance and judge the speed-accuracy tradeoffs.

**Control experiments** - These are baseline runs with no reranking. Same as covered above

![Control Experiments](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-04-at-11.54.23-PM.png)

**Base model reranking performance**

This shows that any base model without being trained as reranker model will output rubbish.

**All reranking results in this experiment are first overfetched by a factor of 4**, then reranked and finally top_k results are chosen. This allows reranker to do its job. Without overfetching there will be no performance difference.

![Base Model Performance](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-05-at-1.06.04-AM.png)

**Comparison with a dedicated reranker model**

Similarly, a dedicated reranker model, trained with many optimization tricks on a huge corpus of diverse data improves search performance significantly.

![Dedicated Reranker Comparison](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-05-at-1.57.18-AM-2.png)

### Training models from scratch

All models except the first rows in both the tables are trained from scratch. The naming convention used is `{reranker_arch}*{*base _ arch}*{*num_ negatives*}*_{num epochs}`

![Training Results 1](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-07-at-5.39.47-AM.png)

![Training Results 2](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-07-at-5.40.49-AM.png)

## Key Findings

1. **Reranking Impact Analysis**:
- Vector search performance improved significantly with reranking, showing up to **12.3%** improvement at top-5 and **6.6%** at top-10
- FTS results saw mixed improvements from reranking, with some models showing up to **20.65%** gains and others suffering performance degradation

2. **Model Architecture Comparison**:
- ModernBERT-based models demonstrated the strongest performance, particularly the 2-neg-4-epoch variants which achieved near-baseline performance at top-10 (-0.51%)
- MiniLM-based models consistently underperformed compared to their ModernBERT counterparts. This is expected as these models are tiny. They still end up improving base performance by a lotâ€”**which highlights that reranking is a low-hanging fruit.**
- Cross-encoder models showed competitive performance in FTS reranking, outperforming colbert-based approaches

3. **Training Parameter Effects**:
- Increasing negative samples generally improved model performance across architectures
- The 2-neg-4-epoch combination provided the best balance of performance to training cost
- Epoch count showed diminishing returns beyond 2 epochs for most models. This could also highlight overfitting or catastrophic forgetting.

{{< admonition tip "ðŸ’¡ Training Note" >}}
Note: This isn't a primer on "how to train best models". With slight tuning using standard random/grid search, the same models with the same configurations should show much better performance. 

But that's beyond the scope of this blog. This blog primarily deals with how reranking works, and if it is right for your use-case
{{< /admonition >}}

4. **Best Non-Finetuned Models**:
- For top-5: cross-encoder_Modernbert_tuned_1_epoch_2M_unseen_5 (Vector: 61.05, FTS: 53.95, Hybrid: **62.30**) 
- For top-10: cross-encoder_Modernbert_tuned_1epoch_2M_unseen_10 (Vector: 66.15, FTS: 60.70, Hybrid: **67.20**)

This shows models trained only on a few million data points (including augmentation) can perform similar to a dedicated reranker model trained on a much larger corpus and also tuned carefully. 

**Hybrid Search Variability**:

- Hybrid search performance varied based on individual result quality from component searches. 
- In cases where an individual result perform poorly, hybrid seach performs even worse than vector search. **But in cases where both FTS and vector search result sets are decent, hybrid search performs better than any of the individual search types**.

### Fine-tuning existing rerankers

With enough high-quality data, you can also improve the performance of an existing reranker model by fine-tuning as demonstrated by the finetuned answerai-colbert models which consistently outperformed all other configurations

**Baseline performance of dedicated reranker model**

![Baseline Reranker Performance](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-05-at-2.03.34-AM.png)

**Finetuned performance**

![Finetuned Performance](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-05-at-2.03.47-AM.png)

### Should you train a reranker from scratch or fine-tune them?

It depends on the size and quality of the dataset. Just like other tasks, the rules of thumb for training from scratch vs fine-tuning apply to rerankers as well.

**You should train from scratch if:**

If there are no rerankers available for the base model of your choice. For example, `ModernBert` itself is not a reranker model, we use it as a base for creating cross-encoder and colbert architecture reranker models. Hence, we train these architectures from scratch.

**You should fine-tune a reranker if:**

A reranker of your choice already exists, but you suspect your dataset might have become specialised enough, or the distribution might have drifted from the original datasets used to train these rerankers.

A general rule to remember is that fine-tuning an existing architecture converges much faster. Another thing to be careful about is Catastrophic forgetting in Large Language Models (LLMs). It refers to the phenomenon where a model, after being trained on a new task, forgets or significantly degrades its performance on previously learned tasks

## Tradeoffs

As the 'no free lunch theorem' states, no single optimization algorithm is universally superior across all possible problems; there are tradeoffs involved when reranking retrieval results.

**Latency vs Performance**

For reference, here's **the performance improvements (from above tables)**

![Performance Improvements](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-07-at-1.52.49-AM.png)

This chart shows ablation table comparing latency at various top_k

![Latency Comparison](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-05-at-2.21.26-AM.png)

These numbers have been calculated on **L4 GPU, which is comparable to a T4**. The reasoning is that these are the cheapest and most readily available GPUs.

{{< admonition tip "ðŸ’¡ Optimization Tips" >}}
These numbers can be further optimized using various techniques, which can cause major gains in performance. Some simple optimizations:
* torch.compile
* Quantization
* Using better GPUs
{{< /admonition >}}

**Who should and should NOT use rerankers?**

- As expected, rerankers end up adding *some* latency to the query time. Depending on the use-case this latency might be significant. 
- If your goal is to **always have latencies less than, say, 100ms, you'll need to carefully choose if a reranker is worth the penalty**. 
- All other more **general cases where an added latency of at most ~50ms doesn't do much damage, should definitely exploit reranking** before thinking about more disruptive techniques like fine-tuning or switching to a new embedding model.

**If your use-case does not have a hard latency limit, you can stick to using CPUs,** as even without acceleration, reranking models only add a few 100 ms.

![CPU Performance](/assets/blog/a-practical-guide-to-training-custom-rerankers/Screenshot-2025-04-07-at-1.54.03-AM.png)

**API based rerankers**

This blog covers only OSS models, but you can also choose to use an API based reranker. The best way to access them is by evaluating manually for accuracy and speed. There are various unknowns in API-based approach that won't allow direct comparison in this experiment:

1. The latency varies a lot. 
2. The architecture of these models is unknown
3. You generally can't train or fine-tune them on your own data

In some of our experiments we've seen very high accuracies using API-based rerankers, but with unstable latencies. To find more details you can check out this [example](https://lancedb.github.io/lancedb/notebooks/lancedb_reranking/) on our docs

### Reproducibility

- The code and instructions to reproduce this can be found [here](https://github.com/lancedb/research/tree/main/reranking). 
- All of the trained artifacts can be found [here](https://huggingface.co/ayushexel).
```

---

## ðŸ“„ æ–‡ä»¶: a-primer-on-text-chunking-and-its-types-a420efc96a13.md

---

```md
---
title: "A Primer on Text Chunking and Its Types"
date: 2023-10-24
author: Prashant Kumar
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/a-primer-on-text-chunking-and-its-types-a420efc96a13/preview-image.png
meta_image: /assets/blog/a-primer-on-text-chunking-and-its-types-a420efc96a13/preview-image.png
description: "Text chunking is a technique in natural language processing that divides text into smaller segments, usually based on the parts of speech and grammatical meanings of the words."
---

Text chunking is a technique in natural language processing that divides text into smaller segments, usually based on the parts of speech and grammatical meanings of the words. Text chunking can help extract important information from a text, such as noun phrases, verb phrases, or other semantic units.

In this blog, Weâ€™ll see some Text Chunking strategies, why they are important for building LLM-based systems like RAG, and How to use them.

## Why is Text Chunking Important?

There are various reasons why text chunking becomes important when working with LLMs, and in this post, weâ€™ll share a few examples that have a significant impact on the results.

Say you have a document of 15 pages full of text and you want to perform summarization and question-answering on the document. The first and foremost step is to extract embeddings of the full document. It's from here that all the problems related to text processing begin.

1. When you extract embeddings for an entire document at once, you may capture the overall context, but you risk losing valuable information about specific topics. This can lead to imprecise results and missing details when working with large language models (LLMs).
2. Regardless of the embedding model provider, you need to be aware of their context window limit, to pick the right chunk size. Although OpenAI GPT-4 has a 32K token window, which sounds reasonably large, it's still a good practice to think about the right chunk size from the outset.

Not using text chunking correctly when it's needed can cause problems that impact retrieval quality, which further affects the downstream task. If the chunk size is too large, the relevant piece of information in the retrieved chunk may be buried under a mountain of other irrelevant text. If the chunk size is too small, it can scatter the relevant information across several pieces of retrieved information, in a way that order isn't preserved (which affects semantics).

## Text Chunking Strategies

There are different Text Chunking Strategies, Here weâ€™ll discuss them with their strengths and weaknesses and the right scenarios where they can be applied.

### Sentence Splitting Using Classical Methods

#### 1. Naive method
The most naive method of sentence splitting is using a _split function_ that splits text on a given character.

```python
text = "content.of.document." #input text
chunks = text.split(".")
print(chunks)
```

This gives:

```
['content', 'of', 'document', '']
```

#### 2. NLTK text splitter
NLTK is a library used for working with Language data. It provides a sentence tokenizer that can split the text into sentences, helping to create more meaningful chunks.

```python
import nltk

input_text ="Much of refactoring is devoted to correctly composing methods. In most cases, excessively long methods are the root of all evil. The vagaries of code inside these methods conceal the execution logic and make the method extremely hard to understand"

sentences = nltk.sent_tokenize(input_text)
print(sentences)
```

Here, we have not used any character to split sentences. Further, many other chunking techniques can be used, like tokenizing, POS tagging, etc.

Output:
```
['Much of refactoring is devoted to correctly composing methods.',
    'In most cases, excessively long methods are the root of all evil.',
    'The vagaries of code inside these methods conceal the execution logic and make the method extremely hard to understand']
```

#### 3. SpaCy text splitter
spaCy is a popular NLP library is used for performing various tasks of NLP. The text splitter in spaCy creates text splits, preserving contexts of resultant splits.

```python
import spacy
input_text ="Much of refactoring is devoted to correctly composing methods. In most cases, excessively long methods are the root of all evil. The vagaries of code inside these methods conceal the execution logic and make the method extremely hard to understand"

nlp = spacy.load("en_core_web_sm")
doc = nlp(input_text)
for s in doc.sents:
    print(s)
```

Output:
```
Much of refactoring is devoted to correctly composing methods.
In most cases, excessively long methods are the root of all evil.
The vagaries of code inside these methods conceal the execution logic and make the method extremely hard to understand
```

spaCy uses tokenization and a dependency parser under the hood and offers more sophisticated techniques for detecting sentence boundaries for splitting.

### Recursive Splitting

Recursive Splitting splits the input text into small chunks in iterative matter using a set of separators. If, in the starting steps, chunks are not created of the desired size, it will recursively try different separators or criteria to achieve the desired size of the chunk.

Here is an example of using Recursive Splitting using LangChain.

```python
# input text
input_text ="Much of refactoring is devoted to correctly composing methods. In most cases, excessively long methods are the root of all evil. The vagaries of code inside these methods conceal the execution logic and make the method extremely hard to understand"

from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 100, #set desired text size
    chunk_overlap  = 20 )

chunks = text_splitter.create_documents([input_text])
print(chunks)
```

Output:

```
[Document(page_content='Much of refactoring is devoted to correctly composing methods. In most cases, excessively long'),
Document(page_content='excessively long methods are the root of all evil. The vagaries of code inside these methods'),
Document(page_content='these methods conceal the execution logic and make the method extremely hard to understand')]
```

### Specialized Structured Splitting

#### 1. HTML Text Splitter
HTML Splitter is a *structure-aware* chunker that splits text at the HTML element level and adds metadata for each header relevant to any given chunk.

Given this input HTML string:
```
html_string ="""
<!DOCTYPE html>
<html>
<body>
    <div>
        <h1>Foo</h1>
        <p>Some intro text about Foo.</p>
        <div>
            <h2>Bar main section</h2>
            <p>Some intro text about Bar.</p>
            <h3>Bar subsection 1</h3>
            <p>Some text about the first subtopic of Bar.</p>
            <h3>Bar subsection 2</h3>
            <p>Some text about the second subtopic of Bar.</p>
        </div>
        <div>
            <h2>Baz</h2>
            <p>Some text about Baz</p>
        </div>
        <br>
        <p>Some concluding text about Foo</p>
    </div>
</body>
</html>
"""
```
We can process it as follows:
```python
from langchain.text_splitter import HTMLHeaderTextSplitter

headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"),
    ("h3", "Header 3"),
]

html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

html_header_splits = html_splitter.split_text(html_string)
print(html_header_split)
```

The HTML header will extract only the headers mentioned in *header_to_split_on*.

```
[Document(page_content='Foo'),
Document(page_content='Some intro text about Foo.  \nBar main section Bar subsection 1 Bar subsection 2', metadata={'Header 1': 'Foo'}),
Document(page_content='Some intro text about Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}),
Document(page_content='Some text about the first subtopic of Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}),
Document(page_content='Some text about the second subtopic of Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}),
Document(page_content='Baz', metadata={'Header 1': 'Foo'}),
Document(page_content='Some text about Baz', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}),
Document(page_content='Some concluding text about Foo', metadata={'Header 1': 'Foo'})]
```

#### 2. Markdown text splitting

Markdown Splitting is used to chunk based on Markdown syntax like heading, bash code, images, and lists. It can also structure-aware chunker.

```python
#input markdown string
markdown_text = '# Foo\n\n ## Bar\n\nHi this is Jim  \nHi this is Joe\n\n ## Baz\n\n Hi this is Molly'

from langchain.text_splitter import MarkdownHeaderTextSplitter
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
md_header_splits = markdown_splitter.split_text(markdown_text)
print(md_header_splits)
```

MarkdownHeaderTextSplitter splits markdown text based on *headers_to_split_on*.

Output:
```
[Document(page_content='Hi this is Jim\nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),
Document(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]
```

#### 3. LaTex text splitting 

LaTex text splitting is another *code-splitting* chunker that parses LaTex commands to create chunks that are the logical organization, like sections and subsections, leading to more accurate and contextually relevant results.

```python
#input latex string
latex_text = """
\documentclass{article}

\begin{document}

\maketitle

\section{Introduction}
Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis.

\subsection{History of LLMs}
The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.

\subsection{Applications of LLMs}
LLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.

\end{document}
"""

from langchain.text_splitter import LatexTextSplitter
latex_splitter = LatexTextSplitter(chunk_size=100, chunk_overlap=0)

latex_splits = latex_splitter.create_documents([latex_text])
print(latex_splits)
```

In the above example you can see that *overlap is 0, *it is because when we are working with code splits at that time overlapping codes totally change the meaning of it. So overlapping should be 0.

```
[Document(page_content='\\documentclass{article}\n\n\x08egin{document}\n\n\\maketitle\n\n\\section{Introduction}\nLarge language models'),
    Document(page_content='(LLMs) are a type of machine learning model that can be trained on vast amounts of text data to'),
    Document(page_content='generate human-like language. In recent years, LLMs have made significant advances in a variety of'),
    Document(page_content='natural language processing tasks, including language translation, text generation, and sentiment'),
    Document(page_content='analysis.\n\n\\subsection{History of LLMs}\nThe earliest LLMs were developed in the 1980s and 1990s,'),
    Document(page_content='but they were limited by the amount of data that could be processed and the computational power'),
    Document(page_content='available at the time. In the past decade, however, advances in hardware and software have made it'),
    Document(page_content='possible to train LLMs on massive datasets, leading to significant improvements in'),
    Document(page_content='performance.\n\n\\subsection{Applications of LLMs}\nLLMs have many applications in industry, including'),
    Document(page_content='chatbots, content creation, and virtual assistants. They can also be used in academia for research'),
    Document(page_content='in linguistics, psychology, and computational linguistics.\n\n\\end{document}')]
```

Now when you choose which chunker to use for your data, extract embeddings for them and store them in Vector DB.

In this post, we've seen numerous examples of using chunking with LanceDB to store text chunks and their respective embeddings. LanceDB is an easy-to-setup, open source vector database that persists your data and vector index on disk, allowing you to scale without breaking the bank. It is also well-integrated with the Python data ecosystem so you can use it with your existing data tools like pandas, pyarrow, and more.

See [this notebook](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/tutorials/different-types-text-chunking-in-RAG/Text_Chunking_on_RAG_application_with_LanceDB.ipynb) for the code that performs these tasks.

## Conclusion

Text chunking is a relatively straightforward task, but it's also important to study how it works, because it presents certain challenges and tradeoffs. There's no single strategy that works universally, nor a chunk size that suits every scenario. What works for one type of data or solution might not be suitable for others.

Hopefully, this post reinforced the importance of text chunking and showed you some new approaches!

Learn more about LanceDB or applied GenAI applications from [vectordb-recipes](https://github.com/lancedb/vectordb-recipes) . Donâ€™t forget to drop us a ðŸŒŸ!
```

---

## ðŸ“„ æ–‡ä»¶: accelerate-vector-search-applications-using-openvino-lancedb.md

---

```md
---
title: "Accelerate Vector Search Applications Using OpenVINO & LanceDB"
date: 2023-12-06
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/accelerate-vector-search-applications-using-openvino-lancedb/preview-image.png
meta_image: /assets/blog/accelerate-vector-search-applications-using-openvino-lancedb/preview-image.png
description: "We show how to use the CLIP from OpenAI for Text-to-Image and Image-to-Image searching. Weâ€™ll also do a comparative analysis of the PyTorch model, FP16 OpenVINO format, and INT8 OpenVINO format in terms of speedup."
---

In this article, we'll show how to use the CLIP model from OpenAI for Text-to-Image and Image-to-Image searching. Weâ€™ll also do a comparative analysis of the PyTorch model, FP16 OpenVINO format, and INT8 OpenVINO format in terms of speedup.

Here's a summary of what's covered:
1. ***Using the PyTorch model***
2. ***Using OpenVINO conversion to speed up by 70%***
3. ***Using Quantization with OpenVINO NNCF to speed up by 400%***

All results reported below are from a *13th Gen Intel(R) Core(TM) i5â€“13420H** using OpenVINO=2023.2 and NNCF=2.7.0 version.***

If you'd like to code along, here's a [Colab notebook](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Accelerate-Vector-Search-Applications-Using-OpenVINO/clip_text_image_search.ipynb) with all the code you need to get started!

## CLIP from OpenAI

***CLIP (Contrastive Languageâ€“Image Pre-training) is a neural network capable of processing both images and text.***

CLIP is a multimodal model, which means it can process both text and images. This capability allows it to embed different types of inputs in a shared multimodal space, where the positions of images and text have semantic meaning, regardless of their format.

The following image presents a visualization of the pre-training procedure
![Combining Image and Text Embeddings (Source: OpenAI)](/assets/blog/accelerate-vector-search-applications-using-openvino-lancedb/1*oKfC-Vyc3r85W06yLH2y_g.png)

## OpenVINO by Intel

OpenVINO toolkit is a free toolkit facilitating the optimization of a deep learning model from a framework and deploying an inference engine onto Intel hardware. Weâ€™ll use the FP16 and INT8 formats using the OpenVINO CLIP model.
This post demonstrates how to use OpenVINO to accelerate an embedding pipeline in LanceDB.

## Implementation

In the implementation section, we see the comparative implementation of the CLIP model from Hugging Face and OpenVINO formats, using the conceptual caption dataset.
We start with the first step of loading the conceptual caption dataset from Hugging Face.

```python
# https://huggingface.co/datasets/conceptual_captions
image_data = load_dataset(
    "conceptual_captions", split="train",
)
```

We will select a sample of 100 images from this large number of images

```python
# taking first 100 images
image_data_df = pd.DataFrame(image_data[:100])
```

Helper ***functions to validate image URLs*** and get images and captions from image URL

```python
def check_valid_URLs(image_URL):
    """
    Not all the URLs are valid. This function returns True if the URL is valid. False otherwise.
    """
    try:
        response = requests.get(image_URL)
        Image.open(BytesIO(response.content))
        return True
    except Exception:
        return False

def get_image(image_URL):
    response = requests.get(image_URL)
    image = Image.open(BytesIO(response.content)).convert("RGB")
    return image

def get_image_caption(image_ID):
    return image_data[image_ID]["caption"]

# Transform dataframe
image_data_df["is_valid"] = image_data_df["image_url"].apply(check_valid_URLs)

# removing all the invalid URLs
image_data_df = image_data_df[image_data_df["is_valid"] == True]
image_data_df.head()
```

Now we have prepared the dataset and we are ready to start with CLIP using Hugging Face and OpenVINO and their performance comparative analysis in terms of speed.

### PyTorch CLIP using Hugging Face

Weâ€™ll start with CLIP using Hugging Face and report the time taken to extract embeddings and search using LanceDB.

```python
def get_model_info(model_ID, device):
    """
    Loading CLIP from HuggingFace
    """
    # Save the model to device
    model = CLIPModel.from_pretrained(model_ID).to(device)

    # Get the processor
    processor = CLIPProcessor.from_pretrained(model_ID)

    # Get the tokenizer
    tokenizer = CLIPTokenizer.from_pretrained(model_ID)

    # Return model, processor & tokenizer
    return model, processor, tokenizer

# Set the device
device = "cuda" if torch.cuda.is_available() else "cpu"
model_ID = "openai/clip-vit-base-patch16"

model, processor, tokenizer = get_model_info(model_ID, device)
```

Letâ€™s write a helper function to extract text and image embeddings:

```python
def get_single_text_embedding(text):
    # Get single text embeddings
    inputs = tokenizer(text, return_tensors="pt").to(device)

    text_embeddings = model.get_text_features(**inputs)

    # convert the embeddings to numpy array
    embedding_as_np = text_embeddings.cpu().detach().numpy()
    return embedding_as_np

def get_all_text_embeddings(df, text_col):
    # Get all the text embeddings
    df["text_embeddings"] = df[str(text_col)].apply(get_single_text_embedding)
    return df

def get_single_image_embedding(my_image):
    # Get single image embeddings
    image = processor(text=None, images=my_image, return_tensors="pt")["pixel_values"].to(device)
    embedding = model.get_image_features(image)
    # convert the embeddings to numpy array
    embedding_as_np = embedding.cpu().detach().numpy()
    return embedding_as_np

def get_all_images_embedding(df, img_column):
    # Get all image embeddings
    df["img_embeddings"] = df[str(img_column)].apply(get_single_image_embedding)
    return df
```

### Use LanceDB for storing the embeddings & search

```python
import lancedb
db = lancedb.connect("./.lancedb")
```
    Extracting Embeddings of 83 images using CLIP Hugging faces model and time taken to extract embeddings.
```python
import time
# extracting embeddings using Hugging Face
start_time = time.time()
image_data_df = get_all_images_embedding(image_data_df, "image")
print(f"Time Taken to extract Embeddings of {len(image_data_df)} Images(in seconds): ", time.time()-start_time)
```
This pipeline to extract embeddings of 83 images took **55.79 sec**.

### Data ingestion and creating embeddings in LanceDB

Next, we show how to create the embeddings and ingest them into LanceDB.

```python
def create_and_ingest(image_data_df, table_name):
    """
    Create and Ingest Extracted Embeddings using Hugging Face
    """
    image_url = image_data_df.image_url.tolist()
    image_embeddings = [arr.astype(np.float32).tolist() for arr in image_data_df.img_embeddings.tolist()]

    data = []
    for i in range(len(image_url)):
        temp = {}
        temp['vector'] = image_embeddings[i][0]
        temp['image'] = image_url[i]
        data.append(temp)

    # Create a Table
    tbl = db.create_table(name=table_name, data=data, mode="overwrite")
    return tbl

# Create and ingest embeddings for pt model
pt_tbl = create_and_ingest(image_data_df, "pt_table")
```

### Query the embeddings

You can easily query the embeddings via similarity in LanceDB as follows:
```python
# Get the image embedding and query for each caption
pt_img_results = {}

start_time = time.time()
for i in range(len(image_data_df)):
    img_query = image_data_df.iloc[i].image
    query_embedding = get_single_image_embedding(img_query).tolist()

    # querying with image
    result = pt_tbl.search(query_embedding[0]).limit(4).to_list()
    pt_img_results[str(i)] = result
```

## CLIP model using FP16 OpenVINO format

Next, weâ€™ll show the results from the same pipeline with the CLIP F16 OpenVINO format.

```python
import openvino as ov

# saving openvino model
model.config.torchscript = True
ov_model = ov.convert_model(model, example_input=dict(inputs))
ov.save_model(ov_model, 'clip-vit-base-patch16.xml')
```

Compiling the CLIP OpenVINO model

```python
import numpy as np
from scipy.special import softmax
from openvino.runtime import Core

"""
  Compiling CLIP in Openvino FP16 format
"""
# create OpenVINO core object instance
core = Core()
# compile model for loading on device
compiled_model = core.compile_model(ov_model, device_name="AUTO", config={"PERFORMANCE_HINT": "CUMULATIVE_THROUGHPUT"})
# obtain output tensor for getting predictions
```

Extracting the embeddings of 83 images using CLIP FP16 OpenVINO model now takes **31.79** seconds -- this is a 43% reduction!

```python
import time

# time taken to extract embeddings using CLIP OpenVINO format
start_time = time.time()
image_embeddings = extract_openvino_embeddings(image_data_df)
print(f"Time Taken to extract Embeddings of {len(image_data_df)} Images(in seconds): ", time.time()-start_time)
```

The embeddings can be ingested to LanceDB the same as before:

```python
def create_and_ingest_openvino(image_url, image_embeddings, table_name):
    """
    Create and Ingest Extracted Embeddings using OpenVINO FP16 format
    """
    data = []
    for i in range(len(image_url)):
        temp = {}
        temp['vector'] = image_embeddings[i]
        temp['image'] = image_url[i]
        data.append(temp)

    # Create a Table
    tbl = db.create_table(name=table_name, data=data, mode="overwrite")
    return tbl

# create and ingest embeddings for OpenVINO fp16 model
ov_tbl = create_and_ingest_openvino(image_url, image_embeddings, "ov_tbl")
```
We query the embeddings and run search just like before:
```python
# Get the image embedding and query for each caption
ov_img_results = {}
start_time = time.time()
for i in range(len(image_data_df)):
    img_query = image_data_df.iloc[i].image
    image = image_data_df.iloc[i].image
    inputs = processor(images=[image], return_tensors="np", padding=True)
    query_embedding = compiled_model(dict(inputs))["image_embeds"][0]

    # querying with query image
    result = ov_tbl.search(query_embedding).limit(4).to_list()
    ov_img_results[str(i)] = result
```

## NNCF INT 8-bit Quantization

You can also use 8-bit Post Training Optimization from NNCF (Neural Network Compression Framework) and run inference on the quantized model via OpenVINO Toolkit.

```bash
pip install -q datasets
pip install -q "nncf>=2.7.0"
```

```python
import os
from transformers import CLIPProcessor, CLIPModel

fp16_model_path = 'clip-vit-base-patch16.xml'

#inputs preparation for conversion and creating processor
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch16")
max_length = model.config.text_config.max_position_embeddings
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch16")
```
Here's a helper function to convert into `Int8` format using NNCF:

```python
import requests
from io import BytesIO
import numpy as np
from PIL import Image
from requests.packages.urllib3.exceptions import InsecureRequestWarning
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

def check_text_data(data):
    """
    Check if the given data is text-based.
    """
    if isinstance(data, str):
        return True
    if isinstance(data, list):
        return all(isinstance(x, str) for x in data)
    return False

def get_pil_from_url(url):
    """
    Downloads and converts an image from a URL to a PIL Image object.
    """
    response = requests.get(url, verify=False, timeout=20)
    image = Image.open(BytesIO(response.content))
    return image.convert("RGB")

def collate_fn(example, image_column="image_url", text_column="caption"):
    """
    Preprocesses an example by loading and transforming image and text data.
    Checks if the text data in the example is valid by calling the `check_text_data` function.
    Downloads the image specified by the URL in the image_column by calling the `get_pil_from_url` function.
    If there is any error during the download process, returns None.
    Returns the preprocessed inputs with transformed image and text data.
    """
    assert len(example) == 1
    example = example[0]

    if not check_text_data(example[text_column]):
        raise ValueError("Text data is not valid")

    url = example[image_column]
    try:
        image = get_pil_from_url(url)
        h, w = image.size
        if h == 1 or w == 1:
            return None
    except Exception:
        return None

    #preparing inputs for processor
    inputs = processor(text=example[text_column], images=[image], return_tensors="pt", padding=True)
    if inputs['input_ids'].shape[1] > max_length:
        return None
    return inputs
```

### Initializing NNCF and Saving the Quantized Model

```python
import logging
import nncf
from openvino.runtime import Core, serialize

core = Core()

# Initialize NNCF
nncf.set_log_level(logging.ERROR)

int8_model_path = 'clip-vit-base-patch16_int8.xml'
calibration_data = prepare_dataset()
ov_model = core.read_model(fp16_model_path)

if len(calibration_data) == 0:
    raise RuntimeError(
        'Calibration dataset is empty. Please check internet connection and try to download images manually.'
    )

#Quantize CLIP fp16 model using NNCF
calibration_dataset = nncf.Dataset(calibration_data)
quantized_model = nncf.quantize(
    model=ov_model,
    calibration_dataset=calibration_dataset,
    model_type=nncf.ModelType.TRANSFORMER,
)

#Saving Quantized model
serialize(quantized_model, int8_model_path)
```

### Compiling the INT8 model and Helper function for extracting features

```python
import numpy as np
from scipy.special import softmax
from openvino.runtime import Core

# create OpenVINO core object instance
core = Core()
# compile model for loading on device
compiled_model = core.compile_model(quantized_model, device_name="AUTO", config={"PERFORMANCE_HINT": "CUMULATIVE_THROUGHPUT"})

# obtain output tensor for getting predictions
image_embeds = compiled_model.output(0)
logits_per_image_out = compiled_model.output(0)

image_url = image_data_df.image_url.tolist()
image_embeddings = []

def extract_quantized_openvino_embeddings(image_data_df, compiled_model):
    """
    Extract embeddings of Images using CLIP Quantized model
    """
    for i in range(len(image_data_df)):
        image = image_data_df.iloc[i].image
        inputs = processor(images=[image], return_tensors="np", padding=True)
        image_embeddings.append(compiled_model(dict(inputs))["image_embeds"][0])

    return image_embeddings

import time

start_time = time.time()

# time taken to extract embeddings using CLIP OpenVINO format
image_embeddings = extract_quantized_openvino_embeddings(image_data_df, compiled_model)

print(f"Time Taken to extract Embeddings of {len(image_data_df)} Images(in seconds): ", time.time()-start_time)
```

With the updated pipeline using CLIP OpenVINO format, the time taken to extract embeddings of 83 images is brought down to just 13.70 sec! That's a 75.4% reduction from
the original CLIP model!

We can ingest the embeddings into LanceDB as follows:

```python
# create and ingest embeddings for OpenVINO int8 model format
qov_tbl = create_and_ingest_openvino(image_url, image_embeddings, "qov_tbl")
```

```python
# Get the image embedding and query for each caption
ov_img_results = {}
start_time = time.time()
for i in range(len(image_data_df)):
    img_query = image_data_df.iloc[i].image
    image = image_data_df.iloc[i].image
    inputs = processor(images=[image], return_tensors="np", padding=True)
    query_embedding = compiled_model(dict(inputs))["image_embeds"][0]

    # querying with query image
    result = qov_tbl.search(query_embedding).limit(4).to_list()
    ov_img_results[str(i)] = result
```

We've now shown the performance improvement using all the CLIP model formats PyTorch from Hugging Face, FP16 OpenVINO, and INT8 OpenVINO.

## Conclusions

All these results are on CPU for comparison of the PyTorch model with the OpenVINO model formats(FP16/ INT8)

| Format | Time (s) |
| --- | --- |
| PyTorch model from Hugging Face | 55.26 |
| OpenVINO FP16 format | 31.79 |
| OpenVINO INT8 format | 13.70 |

The performance acceleration achieved with an `FP16` model is **1.73** times the PyTorch model, which is a relatively modest (yet decent) increase in speed.
However, when switching to the `INT8` OpenVINO format, there is a **4.03** times increase in speed compared to the PyTorch model.

Visit the LanceDB [GitHub](https://github.com/lancedb) to learn more about how to
work with vector search at scale, and for more such tutorials and demo applications, visit the [vectordb-recipes](https://github.com/lancedb/vectordb-recipes/tree/main) repo.
For the latest updates from LanceDB, follow our [LinkedIn](https://www.linkedin.com/company/lancedb/) and [X](https://twitter.com/lancedb) pages.
```

---

## ðŸ“„ æ–‡ä»¶: advanced-rag-precise-zero-shot-dense-retrieval-with-hyde-0946c54dfdcb.md

---

```md
---
title: "Advanced RAG: Precise Zero-Shot Dense Retrieval with HyDE"
date: 2023-11-27
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/advanced-rag-precise-zero-shot-dense-retrieval-with-hyde-0946c54dfdcb/preview-image.png
meta_image: /assets/blog/advanced-rag-precise-zero-shot-dense-retrieval-with-hyde-0946c54dfdcb/preview-image.png
description: "In the world of search engines, the quest to find the most relevant information is a constant challenge.  Researchers are always on the lookout for innovative ways to improve the effectiveness of search results."
---

In the world of search engines, the quest to find the most relevant information is a constant challenge. Researchers are always on the lookout for innovative ways to improve the effectiveness of search results. One such innovation is [HyDE](https://arxiv.org/pdf/2212.10496.pdf), which stands for Hypothetical Document Embeddings, a novel approach to dense retrieval that promises to make searching for information even more efficient and accurate.

## The Challenge of Dense Retrieval

Dense retrieval, a method used by search engines to find relevant documents by comparing their semantic similarities, has shown great promise across various tasks and languages. However, building fully zero-shot dense retrieval systems without any relevant labels has been a significant challenge. Traditional methods rely on supervised learning, which requires a large dataset of labeled examples to train the model effectively.
![HyDE overview](/assets/blog/advanced-rag-precise-zero-shot-dense-retrieval-with-hyde-0946c54dfdcb/1*mCjvp4YeeGn-T6XTfDcwgw.png)
## Introducing HyDE

The HyDE approach recognizes the difficulty of zero-shot learning and encoding relevance without labeled data. Instead, it leverages the power of language models and hypothetical documents. Hereâ€™s how it works:

1. **Generating Hypothetical Documents**: When a user enters a query, HyDE instructs a language model, like GPT-3, to generate a hypothetical document. This document is designed to capture relevance patterns but may contain inaccuracies.
2. **Unsupervised Encoding**: The generated hypothetical document is then encoded into an embedding vector using an unsupervised contrastive encoder. This vector identifies a neighbourhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity.
3. **Retrieval Process**: HyDE searches for real documents in the corpus that are most similar to the encoded hypothetical document. The retrieved documents are then presented as search results.

## The Benefits of HyDE

What makes HyDE intriguing is its ability to perform effectively without the need for relevant labels. It offloads the task of modeling relevance from traditional retrieval models to a language model that can generalize to a wide range of queries and tasks. This approach has several advantages:

- Zero-Shot Retrieval: HyDE can work â€œout of the boxâ€ without relying on a large dataset of labeled examples.
- Cross-Lingual: It performs well across various languages, making it suitable for multilingual search applications.
- Flexibility: HyDEâ€™s approach allows it to adapt to different tasks without extensive fine-tuning.

## Implementing HyDE in Langchain

To utilize HyDE effectively, one needs to provide a base embedding model and an LLMChain for generating documents. The HyDE class comes with default prompts, but thereâ€™s also the flexibility to create custom prompts.

> Follow along with the Colab notebook:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Advance-RAG-with-HyDE/main.ipynb)

```python
from langchain.llms import OpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import LLMChain, HypotheticalDocumentEmbedder
from langchain.prompts import PromptTemplate
```

Initialize the LLM & embedding model

```python
# instantiate llm
llm = OpenAI()
emebeddings = OpenAIEmbeddings()

embeddings = HypotheticalDocumentEmbedder.from_llm(llm, emebeddings, "web_search")
# Now we can use it as any embedding class!
result = embeddings.embed_query("What bhagavad gita tell us?")
```

We can also generate multiple documents and then combine the embeddings for those. By default, we combine those by taking the average. We can do this by changing the LLM we use to generate documents to return multiple things.

```python
multi_llm = OpenAI(n=3, best_of=3)

embeddings = HypotheticalDocumentEmbedder.from_llm(
    multi_llm, embeddings, "web_search"
)

result = embeddings.embed_query("What bhagavad gita tell us?")
```

The `HypotheticalDocumentEmbedder` does not actually create full hypothetical documents. It only generates an embedding vector representing a hypothetical document. This `HypotheticalDocumentEmbedder` is used to generate "dummy" embeddings that can be inserted into a vector store index.

This allows you to reserve space for documents that donâ€™t exist yet so that you can incrementally add new real documents later.

## Use your own prompts

You can also make and use your own prompts when creating documents with LLMChain. This is helpful if you know what topic youâ€™re asking about. With a custom prompt, you can get a text that fits your topic better.

Letâ€™s try this out. Weâ€™ll make a prompt which weâ€™ll use in the next example.

```python
prompt_template = """
As a knowledgeable and helpful research assistant, your task is to provide informative answers based on the given context. Use your extensive knowledge base to offer clear, concise, and accurate responses to the user's inquiries.

Question: {question}

Answer:
"""

prompt = PromptTemplate(input_variables=["question"], template=prompt_template)

llm_chain = LLMChain(llm=llm, prompt=prompt)

embeddings = HypotheticalDocumentEmbedder(
    llm_chain=llm_chain,
    base_embeddings=embeddings
)
```

loading the pdf we are using

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader

# Load the multiple PDFs
pdf_folder_path = '/content/book'

from langchain.document_loaders import PyPDFDirectoryLoader
loader = PyPDFDirectoryLoader(pdf_folder_path)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
)
documents = text_splitter.split_documents(docs)
```

letâ€™s create a vector store for retrieving information

```python
from langchain.vectorstores import LanceDB
import lancedb  # lancedb as vectorstore

db = lancedb.connect('/tmp/lancedb')
table = db.create_table("documentsai", data=[
    {"vector": embeddings.embed_query("Hello World"), "text": "Hello World", "id": "1"}
], mode="overwrite")
vector_store = LanceDB.from_documents(documents, embeddings, connection=table)
```

the result of vector_store getting some relevant information from the doc

```
    [Document(page_content='gaged in work, such a Karma -yogi is not bound by Karma. (4.22) \nThe one who is free f rom attachment, whose mind is fixed in Self -\nknowledge, who does work as a service (Sev a) to the Lord, all K ar-\nmic bonds of such a philanthropic person ( Karma -yogi) dissolve \naway. (4.23) God shall be realized by the one who considers eve-\nrything as a manifest ation or an act of God. (Also see 9.16) (4.24)  \nDifferent types of spiritual practices', metadata={'vector': array([-0.00890432, -0.01419295,  0.00024622, ..., -0.0255662 ,
             0.01837529, -0.0352935 ], dtype=float32), 'id': '849b3475-6bf5-4a6a-955c-aa9c1426cdbb', '_distance': 0.2407873421907425}),
     Document(page_content='renunciation (Samny asa) is also known as Karma -yoga . No one \nbecomes a Karma -yogi who has not renounced the selfish motive \nbehind an action. (6.02)  \nA definition of yoga and yogi  \nFor the wise who seeks to attain yoga of meditation or calm-\nness of mind, Karma -yoga  is said to be the means. For the one \nwho has attained yoga, the calmness becomes the means of Self -\nrealization. A person is said to have attained yogic perfection when', metadata={'vector': array([ 0.00463139, -0.02188308,  0.01836756, ...,  0.00026087,
             0.01343005, -0.02467442], dtype=float32), 'id': 'f560dd78-48b8-419b-8576-978e6afee272', '_distance': 0.24962666630744934}),
     Document(page_content='one should know the nature of attached or selfish action, the nature \nof detached or selfless action, and also the nature of forbidden ac-\ntion. (4.17)  \nA Karma -yogi is not subject  to the K armic laws  \nThe one who sees inaction in action, and action in inaction, is \na wise person. Such a person is a yogi and has accomplished eve-\nrything. (4.18)  \nTo see inaction in action and vice versa is to understand that \nthe Lord does  all the work indirectly through His power by using us.', metadata={'vector': array([-0.01086397, -0.01465061,  0.00732531, ..., -0.00368611,
             0.01414126, -0.0371828 ], dtype=float32), 'id': 'a2088f52-eb0e-43bc-a93d-1023541dff9d', '_distance': 0.26249048113822937}),
     Document(page_content='the best of your ability, O Arjuna, with your mind attached to the \nLord, aba ndoning worry and attachment  to the results, and remain-\ning calm in both success and failure. The calmness of mind  is \ncalled Karma -yoga . (2.48) Work done with selfish motives is infe-\nrior by far to selfless service or Karma -yoga . Therefore, be a \nKarma -yogi, O Arjuna. Those who work only to enjoy the fruits of \ntheir labor are, in truth, unhappy. Because , one has no control over \nthe results. (2.49)', metadata={'vector': array([ 0.00598168, -0.01145132,  0.01744962, ..., -0.01556102,
             0.00799331, -0.03753265], dtype=float32), 'id': 'b3e30fff-f3a5-4665-9569-b285f8cf9c76', '_distance': 0.2726559340953827})]
```

below is a screenshot of a PDF file that has all the information related to a query.

![PDF screenshot example](/assets/blog/advanced-rag-precise-zero-shot-dense-retrieval-with-hyde-0946c54dfdcb/1*YtYoBsurIhNd24xDWmtFVg.png)

passing the string query to get some reference

```python
# passing in the string query to get some reference
query = "which factors appear to be the major nutritional limitations of fast-food meals"

vector_store.similarity_search(query)

llm_chain.run("which factors appear to be the major nutritional limitations of fast-food meals")

"""
The major nutritional limitations of fast-food meals are typically high levels of saturated fat, trans fat, sodium, and added sugar. These ingredients can lead to an increased risk of obesity, type 2 diabetes, cardiovascular disease, and other health issues. Additionally, fast-food meals often lack essential vitamins, minerals, and fiber, which are important for optimal nutrition.
"""
```

HyDE response: here we can see that we are getting the output. which is very good.

![HyDE results 1](/assets/blog/advanced-rag-precise-zero-shot-dense-retrieval-with-hyde-0946c54dfdcb/1*VskgpwscCiFdhKeLHNCx8g.png)

![HyDE results 2](/assets/blog/advanced-rag-precise-zero-shot-dense-retrieval-with-hyde-0946c54dfdcb/1*ICbqfWSAFNmWLGni7MOF7A.png)

A vanilla RAG is not able to get the correct answer because it's directly searching similar keywords in the database.

## *Normal RAG System:*

- In a typical RAG system, the retrieval phase involves searching for relevant information from a corpus using traditional keyword-based or semantic matching methods.
- The retrieved documents are then used to augment the generation process, providing context and information for generating responses or answers.
- The quality of the retrieved documents heavily depends on the effectiveness of the retrieval methods, which may not always capture highly relevant information.

*Example Output:*
![Normal RAG result 1](/assets/blog/advanced-rag-precise-zero-shot-dense-retrieval-with-hyde-0946c54dfdcb/1*EZpfcbs4ssfVJ8tuXWlqIw.png)
![Normal RAG result 2](/assets/blog/advanced-rag-precise-zero-shot-dense-retrieval-with-hyde-0946c54dfdcb/1*VZfhPM515sQq70WAXU0hog.png)
you can check our blog for a [vanilla RAG](https://github.com/lancedb/vectordb-recipes/tree/main/tutorials/chatbot_using_Llama2_&amp;_lanceDB)

Colab to reproduce the results:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Advance-RAG-with-HyDE/main.ipynb)

- HyDE uses Language Models (LLMs) to generate hypothetical documents, making information retrieval more precise.
- It doesnâ€™t rely on extensive labeled data, making it a valuable tool when training data is limited.
- HyDE can assist in building the training data needed for specific retrieval tasks.
- It excels at retrieving relevant chunks of information in the RAG pipeline.

For even more exciting applications of vector databases and Large Language Models (LLMs), be sure to explore the** **[**LanceDB**](https://github.com/lancedb/lancedb) repository.

Explore more GenAI and LLM applications by visiting the [vector-recipes](https://github.com/lancedb/vectordb-recipes). Itâ€™s filled with real-world examples, use cases, and recipes to inspire your next project. We hope you found this journey both informative and inspiring. Cheers!
```

---

## ðŸ“„ æ–‡ä»¶: agentic-rag-using-langgraph-building-a-simple-customer-support-autonomous-agent.md

---

```md
---
title: "Agentic RAG Using LangGraph: Build an Autonomous Customer Support Agent"
date: 2025-01-26
author: LanceDB
author_avatar: "/assets/authors/lancedb.jpg"
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/agentic-rag-using-langgraph-building-a-simple-customer-support-autonomous-agent/langgraph-1.png
meta_image: /assets/blog/agentic-rag-using-langgraph-building-a-simple-customer-support-autonomous-agent/langgraph-1.png
description: "Build an autonomous customer support agent using LangGraph and LanceDB that automatically fetches, classifies, drafts, and responds to emails with RAG-powered policy retrieval."
---

## What is an Agent?

In the current world where everything is running with and for AI, retrieval-augmented generation (RAG) systems have become essential for handling simple queries and generating contextually relevant responses. However, as ever evolving humans, we need complex, autonomous problem-solving methods. In this post, we present, behold: **AI agents** â€” autonomous entities that change how we as humans interact with technology.

#### What are they good for?

- **Autonomous Problem-Solving**: AI agents operate independently, driven by goals rather than specific inputs, and adapt dynamically to new information and environments.
- **Multi-Step Task Execution**: They perform complex, multi-step tasks, maintain state across interactions, and utilize tools like machine learning and rule-based systems to achieve the intended goals.
- **Versatile Capabilities**: From browsing the internet and managing apps to conducting financial transactions and controlling devices, AI agents are reshaping intelligent automation.

### What's LangGraph?

There are many tools available in the market to build agents and among the more popular, are [LangGraph](https://www.langchain.com/langgraph), [AutoGen](https://github.com/microsoft/autogen), [Swarm](https://github.com/openai/swarm), [CrewAI](https://github.com/crewAIInc/crewAI) etc. You can choose any of these but in this post, we'll cover LangGraph, because it offers more granular control and is 100% open source. LangGraph basically creates a "graph" for your workflow, with the following internal components:

1. `State` : Pydantic models or `TypedDict` objects to hold your variables and used for message passing
2. `Node` : It is just a function that does some work. It accepts a `State` object and modifies that state
3. `Tools` : There are pure Python functions or Pydantic models which your agents can call. You use the tools to do retrieval, web Search, calculator or custom API calls, etc. You just have to write the definition of what it does and a rule-based router or model will decide which tool can (but not _should_) be used at any point in time.
4. `Edge` : You have pre-defined flows that define the execution order of functions (i.e., nodes, in our case)
5. `Conditional Edges`  or `Routers`: Instead of fixing the workflow, we can make it conditional. So if you are at node `N`, you decide based on a condition which node to go to next (e.g. `N_i` ... `N_x`).

### Where does RAG Come in?

Remember our `tools` and `Nodes` above? We can define the RAG component of the pipeline either as a tool OR a `Node`. You'll see most tutorials online using RAG as a tool. However, in this post, we'll show how can you use it as a `Node` in a directed graph, and that too, a conditional one.

### Let's build a use case: Email Agent

What this pipeline does is:

1. Fetch the unread emails from your inbox
2. Look at the type of email
3. If it is a policy-related email, it'll use RAG to refer to the policies to draft the email -- otherwise, it just creates a normal draft. If it's spam or something else, it just discards it..
4. Proof-read the draft. If it's good to send, send it, otherwise send it to redraft again. Ideally, you'd let the proof reader node know what are the criteria and then you'd send the reasoning why it was rejected so that the drafting model improves it. That would be out of scope of this blog (wait for a future post ðŸ˜„)
5. Once you get an "OK" from proofreader, send a reply. Ideally, you want an `interrupt` so that the human-in-the-loop (HITL) can review and THEN it gets sent. But again, HITL is a topic for another post...
6. For sending, we just `print` for now

**Yes, all of it is autonomous!**

Let's get some policies and build our RAG on top of this dataset.
You can follow along using the notebook [here](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/customer_support_agent_langgraph/LangGraph_LanceDB.ipynb)


```
pip install -U colorama langgraph langchain-community langchain-openai langchain-anthropic tavily-python pandas openai lancedb sentence-transformers
```

```python
from langchain_core.messages import ToolMessage, SystemMessage, AIMessage, HumanMessage
from langchain_core.runnables import RunnableLambda
from langgraph.prebuilt import ToolNode
from langchain_core.runnables import Runnable, RunnableConfig
from typing import TypedDict, Annotated
from langgraph.graph.message import AnyMessage, add_messages
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, StateGraph, START
from langgraph.prebuilt import tools_condition
import torch

# ------------ Vector Search ----------------

import lancedb, re, requests
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import get_registry
import numpy as np
from langchain_core.tools import tool

# ------- Vector DB using LanceDB ------------
model = get_registry().get("sentence-transformers").create(name="BAAI/bge-small-en-v1.5", device="cuda" if torch.cuda.is_available() else "cpu")

class Policy(LanceModel):
    text: str = model.SourceField()
    vector: Vector(model.ndims()) = model.VectorField()

response = requests.get(
    "https://storage.googleapis.com/benchmarks-artifacts/travel-db/swiss_faq.md"
)
response.raise_for_status()
faq_text = response.text

class VectorStoreRetriever:
    def __init__(self, db_path:str, table_name:str, model, docs: list, schema, ):
        self.db = lancedb.connect(db_path)
        self.table = self.db.create_table(table_name, schema = schema)
        self.table.add([{"text": txt} for txt in re.split(r"(?=\n##)", faq_text)])

    def query(self, query: str, k: int = 5) -> list[dict]:
        result = self.table.search(query).limit(k).to_list()
        return [{"page_content": item["text"], "similarity": 1- item["_distance"]} for item in result]

retriever = VectorStoreRetriever("./lancedb", "company_policy", model, faq_text, Policy)
```

Now that we have our documents ready, let's build some helpers including a dummy function to fetch your email. In the real world, you'd replace it with your own logic and APIs:

```python
from typing import Optional, List
from pydantic import BaseModel
from langchain_core.prompts import PromptTemplate
from langchain_openai import AzureChatOpenAI
from langgraph.graph import END, StateGraph, START
import os
from dotenv import load_dotenv
import random
from typing import Annotated
from langgraph.graph.message import AnyMessage, add_messages
from typing_extensions import TypedDict
from langchain_core.messages import ToolMessage, SystemMessage, AIMessage, HumanMessage
from langchain_core.runnables import RunnableLambda
from langgraph.prebuilt import ToolNode
from langchain_core.runnables import Runnable, RunnableConfig
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import tools_condition
import lancedb, re, requests
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import get_registry
import numpy as np
from langchain_core.tools import tool
from google.colab import userdata # use os.environ.get()
import os
from colorama import Fore, Style

memory = MemorySaver() # It'll save all the states and history corresponding to a `thread_id`. We can get previous conversations if we use memory

# llm = ChatOpenAI(model="gpt-3.5-turbo") # use any
def setup_llm():
    return AzureChatOpenAI(
        api_key=userdata.get("AZURE_OPENAI_API_KEY"),
        api_version=userdata.get("AZURE_OPENAI_API_MODEL_VERSION"),
        azure_endpoint=userdata.get("AZURE_OPENAI_API_ENDPOINT"),
        azure_deployment=userdata.get("AZURE_OPENAI_API_DEPLOYMENT_NAME"),
        temperature=0.7
    )

def create_dummy_random_emails():
    items = [
        {
            "subject": "Invoice Request for Recent Flight Booking",
            "body": "Dear SWISS Team, I recently booked a flight with SWISS (Booking Reference: LX123456) and would like to request an invoice for my records. Could you please guide me on how to obtain it? Thank you, Anna MÃ¼ller"
        },
        {
            "subject": "Rebooking Inquiry for Upcoming Flight",
            "body": "Hello, I need to change the travel dates for my flight (Booking Reference: LX789012). Can you confirm if this is possible and what fees might apply? Best regards, John Smith"
        },
        {
            "subject": "Cancellation of Flight LX345678",
            "body": "Hi SWISS Customer Service, I need to cancel my flight (Booking Reference: LX345678) due to unforeseen circumstances. Could you please explain the cancellation process and any associated fees? Sincerely, Maria Gonzalez"
        },
        {
            "subject": "Request for Special Invoice for Italy",
            "body": "Dear SWISS, I booked a flight originating in Italy and require a special invoice for tax purposes. Can you assist me with this request? Kind regards, Luca Rossi"
        },
        {
            "subject": "Payment Issue with Credit Card",
            "body": "Hello, I tried to pay for my booking using my Visa card, but the payment failed. Can you confirm if the issue is with my card or the payment system? Thanks, Emily Brown"
        },
        {
            "subject": "Refund Status for Cancelled Flight",
            "body": "Dear SWISS, I cancelled my flight (Booking Reference: LX456789) two weeks ago and was told I would receive a refund. Could you provide an update on the status? Best, David Johnson"
        },
        {
            "subject": "Seat Reservation Inquiry",
            "body": "Hi, I have a booking (Reference: LX567890) and would like to confirm if my seat reservation will be retained after a rebooking. Please advise. Regards, Sophie Lee"
        },
        {
            "subject": "Upgrade Request for Economy Flex Fare",
            "body": "Dear SWISS, I booked an Economy Flex fare and would like to upgrade to Business Class. Can you guide me on how to proceed? Thank you, Michael Chen"
        },
        {
            "subject": "Group Booking Inquiry",
            "body": "Hello, I am planning to book flights for a group of 12 passengers. Can you provide details on group booking options and any discounts? Best, Sarah Wilson"
        },
        {
            "subject": "Issue with Online Booking Platform",
            "body": "Hi SWISS, I am unable to see my recent booking in my profile on the SWISS website. Can you help resolve this issue? Regards, Thomas Anderson"
        }
    ]
    chosen_items = [random.choice(items) for _ in range(random.randint(0,2))]
    return [Email(id=str(i), sender="some_user@example.mail", subject=item["subject"], body=item["body"]) for i, item in enumerate(chosen_items)]
```

#### Let's set up our Email Agent

First is our `Email` object which defines what an email is. The second one is the `State` which will be used inside the graph.

```python
class Email(BaseModel):
    id: str
    sender: str
    subject: str
    body: str
    final_reply: str = ""
    status: str = "pending"  # pending, sent, failed, skipped
    failure_reason: str = ""

class EmailState(BaseModel):
    emails: List[Email] = [] # List of the Unread above Email class
    processed_emails: List[Email] = [] # Final emails with the replies and denial reason
    current_email: Optional[Email] = None # Pop one everytime from the above list
    policy_context: Optional[str] = "" # Rag context for CURRENT email
    draft: str = "" # Current Draft of the Current Email
    trials: int = 0 # Trails done for Draft <-> Proof Read for current email
    allowed_trials: int = 3 # do Drft <-> Proof Read a max of 3 times
    sendable: bool = False # send the current email if True
    exit:bool = False # There are no unread emails left
```

Let's setup our Agent Classes and simple functions. Names and Prompts are self explanatory.  Our LanceDB-based RAG system is used in the function `lookup_policy` to fetch policies if the query requires searching the internal policies.

```python
class EmailAgent:
    def __init__(self):
        self.llm = setup_llm() # Replace with your LLM you want

    def fetch_unread_emails(self) -> List[Email]:
        """
        Replace this with your Email LOGIC
        """
        return create_dummy_random_emails()

    def lookup_policy(self, subject: str, body:str) -> str:
        """Always Consult the company policies to answer the queries.
        Use this for drafting the emails"""
        prompt = PromptTemplate(template="Identify whether the given email is policy related or not. Identify if the email requires info which might be in the policy documents.\n\nSubject: {subject}\n\nBody:\n{body}\n\n. Do not output any reasoning etc. Strictly reply with Yes/No", input_variables=["email"])
        chain = prompt | self.llm
        response = chain.invoke({"subject": subject, "body": body})
        policy_related = response.content.strip().lower() == "yes"
        if policy_related:
            docs = retriever.query(f"Email Subject: {subject}\n\nEmail Body:\n{body}", k=2)
            return "\nPolicy Context:" + "\n\n".join([doc["page_content"] for doc in docs])
        return ""

    def draft_email(self, email_subject:str, email_body: str, email_context:str = "") -> str:
        if not email_context:
            prompt = PromptTemplate(template="You are a specialised chat agent named Saleem Shady' working for SWISS Airline. Write a well professional response to this user email:\n\nEmail Subject: {email_subject}\n\nEmail Body:\n{email_body}\n\nResponse:", input_variables=["email"])
        else:
            prompt = PromptTemplate(template="You are a specialised chat agent named Saleem Shady' working for SWISS Airline. Write a well professional response to this user email given the Context (which may or may not be required in answering)\n\n{email_context}\n\nEmail Subject: {email_subject}\n\nEmail Body:\n{email_body}\n\nResponse:", input_variables=["email"])

        chain = prompt | self.llm
        response = chain.invoke({"email_subject":email_subject,"email_body": email_body, "email_context": email_context})
        return response.content

    def validate_draft(self, initial_email: str, draft_email: str) -> bool:
        prompt = PromptTemplate(template="You are a Email Proofreader. Review this response:\n\nOriginal Email:\n{initial_email}\n\nDraft Response:\n{draft_email}\n\nIs this mail ready to send? Do not give your reasoning or views. Reply only with (Yes/No):", input_variables=["initial_email", "draft_email"])
        chain = prompt | self.llm
        response = chain.invoke({"initial_email": initial_email, "draft_email": draft_email})
        return response.content.strip().lower() == "yes"
```

Now, let's setup our main `Workflow`, which is what you've likely been waiting for. The below functions are Either `Nodes` or `Routers`, which we'll get to know when we build the nodes and define edges.

```python
agent = EmailAgent()

def fetch_emails(state: EmailState) -> EmailState:
    emails = agent.fetch_unread_emails()
    state.emails = emails
    return state

def process_next_email(state: EmailState) -> EmailState:
    if state.emails:
        state.current_email = state.emails.pop(0)
        state.policy_context = agent.lookup_policy(state.current_email.subject, state.current_email.body)
    else:
        state.exit = True
    return state

def draft_email(state: EmailState) -> EmailState:
    if state.current_email:
        state.draft = agent.draft_email(state.current_email.subject, state.current_email.body, state.policy_context)
        state.trials += 1
    return state

def validate_draft(state: EmailState) -> EmailState:
    if state.current_email and state.draft:
        state.sendable = agent.validate_draft(state.current_email.body, state.draft)
    return state

def decide_next_step(state: EmailState) -> str:
    if state.sendable:
        print("\n\n-----------------------Sending Email ---------------\n\n")
        return "send"
    elif state.trials >= state.allowed_trials:
        state.current_email.status = "failed"
        state.current_email.failure_reason = "Failed after 3 attempts"
        print("\n\n*********************** Draft Failed after Max Tries ******************** \n\n")
        return "stop"
    else:
        return "rewrite"

def send_or_skip_email(state: EmailState) -> EmailState:
    if state.current_email.status != "failed":
        print(f"\n\nSending email: {state.draft}")
        state.current_email.final_reply = state.draft
        state.current_email.status = "sent"
        state.processed_emails.append(state.current_email)

    # Reset state for the next email
    state.current_email = None
    state.draft = ""
    state.trials = 0
    state.policy_context = ""
    return state
```

Let's Define the `Nodes` and `Edges / Conditional Edges`

```python
workflow.add_node("fetch_emails", fetch_emails)
workflow.add_node("process_next_email", process_next_email)
workflow.add_node("draft_email", draft_email)
workflow.add_node("validate_draft", validate_draft)
workflow.add_node("send_or_skip_email", send_or_skip_email)

workflow.add_edge(START, "fetch_emails")
workflow.add_edge("fetch_emails", "process_next_email")

workflow.add_conditional_edges(
    "process_next_email",
    lambda state: END if state.exit else "draft_email" ,
    {"draft_email": "draft_email", END: END}
    )

workflow.add_edge("draft_email", "validate_draft")

workflow.add_conditional_edges(
    "validate_draft",
    decide_next_step,
    {"send": "send_or_skip_email", "rewrite": "draft_email", "stop": "send_or_skip_email"}
)

workflow.add_edge("send_or_skip_email", "process_next_email")

compiled_email_subgraph = workflow.compile()
```

Want to see how our graph looks?

```python
initial_state = EmailState()

from IPython.display import Image, display
try:
    display(Image(compiled_email_subgraph.get_graph(xray=True).draw_mermaid_png()))
except Exception:
    pass
```

![](/assets/blog/agentic-rag-using-langgraph-building-a-simple-customer-support-autonomous-agent/langgraph-1.png)

You can match the graph working with what we discussed the in the workflow. Let's put it to work. (Uncomment the commented lines if you want to see the `State` at each point)

```python
print(Fore.GREEN + "Starting workflow..." + Style.RESET_ALL)
for output in compiled_email_subgraph.stream(initial_state):
    for key, value in output.items():
        print(Fore.CYAN + f"Finished running: {key}" + Style.RESET_ALL)
        # print(Fore.YELLOW + f"State after {key}:" + Style.RESET_ALL)
        # print(value)
```

![](/assets/blog/agentic-rag-using-langgraph-building-a-simple-customer-support-autonomous-agent/langgraph-2.png)

You can see that one case failed after 3 attempts, and one produced a `successfulAgentic` RAG message. Having failure modes means that you may need to tweak your prompts, and/or add validations, reasoning and guidance, depending on the data and use case at hand.

The full code for this workflow is [here](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/customer_support_agent_langgraph/LangGraph_LanceDB.ipynb).


### Final Notes

So now that we've built a working agent that takes care of things for you, one thing to notice is that it relies heavily on fetching the right context. This is where LanceDB stands out as a powerful tool, because of its ability to efficiently handle large-scale vector search thus making it an invaluable tool, whether you're doing a quick PoC or taking your AI workflows to production.

Stay tuned to this blog to learn some advanced Agentic concepts (like human-in-the-loop, memory, multi-agents etc.), each of which can be further enhanced by LanceDB's powerful capabilities. Till next time!
```

---

## ðŸ“„ æ–‡ä»¶: anythingllms-competitive-edge-lancedb-for-seamless-rag-and-agent-workflows.md

---

```md
---
title: "AnythingLLM's Competitive Edge: LanceDB for Seamless RAG and Agent Workflows"
date: 2025-04-02
draft: false
featured: false
categories: ["Case Study"]
image: /assets/blog/anythingllms-competitive-edge-lancedb-for-seamless-rag-and-agent-workflows/anythingllms-competitive-edge-lancedb-for-seamless-rag-and-agent-workflows.png
description: "Discover how AnythingLLM leveraged LanceDB's serverless architecture to eliminate vector database setup complexity, enabling seamless cross-platform RAG and agent workflows with zero configuration required."
author: Ayush Chaurasia
author_avatar: "/assets/authors/ayush-chaurasia.jpg"
author_bio: "AI Engineer and technical writer specializing in RAG systems, vector databases, and enterprise AI applications."
author_twitter: "ayushchaurasia"
author_github: "ayushchaurasia"
author_linkedin: "ayushchaurasia"
---

AnythingLLM chose LanceDB as their vector database backbone to create a frictionless experience for developers and end-users alike. By leveraging LanceDB's serverless, setup-free architecture, the AnythingLLM team slashed engineering time previously spent on troubleshooting infrastructure issues and redirected it toward building innovative features. The result? An application that works seamlessly across all platforms with zero configuration or setup, empowering users to quickly deploy document chat and agentic workflows while maintaining complete data privacy and control.

## Introduction

In an AI landscape crowded with fragmented open-source tools requiring significant technical expertise, AnythingLLM provides a solution that simplifies the deployment of powerful LLM applications. It provides a standard interface that allows users to chat with their documents and create agentic workflows to improve productivityâ€”all while maintaining privacy through offline, local setup.

*AnythingLLM's intuitive interface demonstrating seamless document chat and agentic workflow capabilities powered by LanceDB's vector storage.*

![AnythingLLM Interface](/assets/blog/anythingllms-competitive-edge-lancedb-for-seamless-rag-and-agent-workflows/294273127-cfc5f47c-bd91-4067-986c-f3f49621a859--1-.gif)

AnythingLLM has seamlessly integrated [LanceDB](https://lancedb.com/) as their vector database of choice for all applications involving context retrieval for document chat and agentic workflows. By choosing LanceDB's open-source, serverless, and setup-free architecture, AnythingLLM delivers a smooth user experience across all platforms, including Windows, which has traditionally been a pain point for many vector database solutions.

{{< admonition tip "Unique Advantage" >}}
LanceDB is the only embedded vector database option in the Node.js ecosystem, making it the perfect choice for JavaScript-based applications like AnythingLLM.
{{< /admonition >}}

## The Challenge

### Making AI Accessible and Private

Despite the growing power of open-source LLMs and frameworks, two significant challenges stood in the way of wider adoption:

#### 1. Technical Fragmentation and Setup Complexity

- Open-source AI tools, while powerful, are fragmented and require significant technical effort to configure correctly
- Cross-platform compatibility issues lead to frustrating experiences, particularly on Windows

#### 2. Vector Database Infrastructure Hurdles

- Any useful contextual chat or agentic workflow depends on efficient vector storage and retrieval
- Most vector databases require separate infrastructure setup and maintenance
- Setup instructions vary across platforms, consuming significant engineering time to solve user issues
- Infrastructure challenges lead to user abandonment before experiencing the actual value of the application

{{< admonition warning "Critical Problem" >}}
AnythingLLM needed a solution that would eliminate vector database configuration headaches while delivering high performance across all operating systems and hardware configurations.
{{< /admonition >}}

## The Solution

### LanceDB: The Zero-Configuration Vector Database Backbone

To overcome these challenges, AnythingLLM integrated LanceDB as their default vector database, providing users with a truly hassle-free experience. The decision was strategicâ€”LanceDB's architecture aligned perfectly with AnythingLLM's vision for simplicity and privacy.

LanceDB delivers critical advantages that support AnythingLLM's mission:

- **Serverless and Setup-Free**: Removes all friction from the setup stage, allowing users to get started immediately
- **Cross-Platform Compatibility**: Works seamlessly across all platforms including Windows ARM, enabling full functionality on CoPilot AI PCs
- **Incredible Retrieval Speed**: Provides fast context retrieval while being persisted on disk, scaling to significant workloads locally without memory limitations
- **Native Multimodal Support**: Well-suited for VLM-based applications with advanced capability to store and retrieve various data types

> "With support for Windows ARM, LanceDB is the only VectorDB with seamless experience across platforms and able to run fully on CoPilot AI PCs - something no other vector databases can do at this time. This only affirmed our choice that LanceDB is the best VectorDB provider for on-device AI with AnythingLLM."
> 
> â€” Timothy Carambat, Founder & CEO @ AnythingLLM, Mintplex Labs

### Implementation Architecture

AnythingLLM leverages LanceDB for both its core RAG (Retrieval Augmented Generation) architecture and its agentic workflows:

#### 1. RAG Implementation
Documents are broken down into smaller chunks, embedded, and stored in LanceDB. These are retrieved as contexts based on user queries to assist LLMs in generating final responses.

#### 2. Agentic Flows
Unlike RAG, AI agents in AnythingLLM can take actions on APIs or local devices. The memory component of these agents relies on LanceDB's vector store for efficient information retrieval.

*RAG architecture showing how documents flow through chunking, embedding, and storage in LanceDB for efficient retrieval.*

![RAG Architecture](/assets/blog/anythingllms-competitive-edge-lancedb-for-seamless-rag-and-agent-workflows/rag_from_scratch.png)

## Results & Impact

### Transformative Impact on User Experience and Engineering Efficiency

By integrating LanceDB as their vector database, AnythingLLM has achieved remarkable improvements:

### End-User Benefits

> "I can't even begin to describe how much time LanceDB saves us. Nearly 100% of users use our LanceDB VectorDB database as it seamlessly operates in the background managing their vectors for RAG and agents. It is blazing fast on even the lowest end hardware we target."
> 
> â€” Timothy Carambat, Founder & CEO @ AnythingLLM, Mintplex Labs

- **Zero Configuration**: Users can get started immediately without any vector database setup
- **Enhanced Cross-Platform Experience**: Seamless operation across all platforms, including Windows ARM and CoPilot AI PCs
- **Improved Performance**: Blazing fast retrieval even on low-end hardware
- **Complete Data Privacy**: Fully local operation with no data leaving the user's device

### Engineering Productivity

> "Relying on LanceDB allows us to focus on building the applications and not spend any engineering or debugging time on one of the most critical pieces of infra, the vectorDB - even at millions of vectors."
> 
> â€” Timothy Carambat, Founder & CEO @ AnythingLLM, Mintplex Labs

- **Redirected Engineering Focus**: Freed from solving infrastructure issues, the team can concentrate on core feature development
- **Reduced Support Load**: Significantly fewer user issues related to vector database setup
- **Accelerated Development Cycle**: More time spent on product roadmap rather than troubleshooting
- **Scalability Without Concerns**: LanceDB handles millions of vectors efficiently without additional engineering effort

## Learn More

{{< admonition resources "Additional Resources" >}}
**AnythingLLM Resources:**
- [Website](https://anythingllm.com/) - Official AnythingLLM homepage
- [GitHub Repository](https://github.com/Mintplex-Labs/anything-llm) - Open-source codebase

**LanceDB Resources:**
- [Website](https://lancedb.com/) - LanceDB platform overview
- [Documentation](https://lancedb.github.io/lancedb/) - Technical documentation and API references
- [GitHub Repository](https://github.com/lancedb/lancedb) - Open-source vector database
- [Discord Community](https://discord.gg/G5DcmnZWKB) - Join our developer community
- [Example Recipes](https://github.com/lancedb/vectordb-recipes) - Get started with LanceDB examples
{{< /admonition >}}
```

---

## ðŸ“„ æ–‡ä»¶: benchmarking-cohere-reranker-with-lancedb.md

---

```md
---
title: "Benchmarking Cohere Rerankers with LanceDB"
date: 2024-05-07
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/benchmarking-cohere-reranker-with-lancedb/preview-image.png
meta_image: /assets/blog/benchmarking-cohere-reranker-with-lancedb/preview-image.png
description: "Improve retrieval quality by reranking LanceDB results with Cohere and ColBERT. Youâ€™ll plug rerankers into vector, FTS, and hybrid search and compare accuracy on real datasets."
---

Reranking is a process of re-arranging the results of a retriever based on some metric that can be independent of the retrieval scores. In this blog, we'll see how you can use LanceDB to use any of the popular reranking techniques including the new cohere reranker-V3 along with some results.

### Reranking in LanceDB

LanceDB comes with built-in support for reranking across all search types - vector, full-text and hybrid search. Using a reranker is as simple as initializing a reranker object and passing that on to the query builder. Here's a simple example using the cohere reranker

```python
import lancedb
from lancedb.rerankers import CohereReranker
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import get_registry

db = lancedb.connect("~/tmp/db")
embedding_fcn = get_registry().get("huggingface").create()

class Schema(LanceModel):
    text: str = embedding_fcn.SourceField()
    vector: Vector(embedding_fcn.ndims()) = embedding_fcn.VectorField()

tbl = db.create_table("tbl_example", schema=Schema)
tbl.add([{ "text": "hey" }, { "text": "bye" }, { "text": "hello" }, { "text": "goodbye" }])

reranker = CohereReranker(api_key="...")  # Requires Cohere API key
```

With this setup, you can now use this reranker for reranking results of any search type.

```python
# Reranking Vector Search
tbl.search("hey").rerank(reranker=reranker).to_pandas()

# Reranking Full-Text Search
tbl.create_fts_index("text")  # Create FTS index
tbl.search("bye", query_type="fts").rerank(reranker=reranker).to_pandas()

# Reranking Hybrid Search
tbl.search("bye", query_type="hybrid").rerank(reranker=reranker).to_pandas()
```

## Benchmarking Cohere Reranker

In this test, we want to test the retrieval performance of rerankers. The baseline used will be the vector only search. We'll also use `colbert-v2` model as reranker to add to the comparison. Note, this is different from `colbert` toolkit that implements its own searching and indexing algorithm.

Full code: [vectordb-recipes/tutorials/cohere-reranker](https://github.com/lancedb/vectordb-recipes/tree/main/tutorials/cohere-reranker)

### Evaluation metric

Here we are not evaluating a RAG. Our tests are limited to testing the quality of the retriever. So, we'll devise a simple metric that just measures the hit-rate of a retriever, which is a simplified version of recall@k or mAP.

```python
# Contains only relevant snippets
def evaluate(
    docs,
    dataset,
    embed_model=None,
    reranker=None,
    top_k=5,
    query_type="auto",
):
    # Create vector store from docs
    tbl = vector_store._connection.open_table(vector_store.table_name)

    eval_results = []
    for idx in tqdm(range(len(datasets))):
        query = ds['query'][idx]
        reference_context = ds['reference_contexts'][idx]
        try:
            if reranker is None:  # vector only search
                rs = tbl.search(query, query_type="vector").to_pandas()
            elif query_type == "auto":  # Hybrid search
                rs = tbl.search(query, query_type="hybrid").rerank(reranker=reranker).to_pandas()
            elif query_type == "vector":  # vector search with reranker
                # Overfetch for vector-only reranking
                rs = (
                    tbl.search(query, query_type="vector")
                       .rerank(reranker=reranker)
                       .limit(top_k * 2)
                       .to_pandas()
                )
        except Exception as e:
            print(f'Error with query: {idx} {e}')
            continue

        retrieved_texts = rs['text'].tolist()[:top_k]
        expected_text = reference_context[0]
        is_hit = expected_text in retrieved_texts  # assume 1 relevant doc
        eval_result = {
            'is_hit': is_hit,
            'retrieved': retrieved_texts,
            'expected': expected_text,
            'query': query,
        }
        eval_results.append(eval_result)
    return pd.DataFrame(eval_results)['is_hit'].mean()
```

Here, we are over-fetching results in case of vector only search results if a reranker is applied, i.e, we fetch 2*K results in this case. This is done to make sure reranker has some effect as we take the top K results after reranking. In this experiment, `k` is 5.

With this evaluation function, let's test rerankers across different datasets and embedding functions.

### Benchmarking on real datasets

First dataset that we'll be using is Uber 10K dataset that contains more than 800 query-and-context pairs based on Uber's 2021 SEC filing. This dataset can be downloaded and loaded from LlamaIndex as follows:

```bash
llamaindex-cli download-llamadataset Uber10KDataset2021 --download-dir ./data
```

```python
from llama_index.core import SimpleDirectoryReader
from llama_index.core.llama_dataset import LabelledRagDataset

rag_dataset = LabelledRagDataset.from_json("./data/rag_dataset.json")
documents = SimpleDirectoryReader(input_dir="./data/source_files").load_data()
```

Here `documents` contains the text to be ingested in the vector dataset and `rag_dataset` contains query and context pairs.

The 2nd dataset used is [Evaluating LLM Survey Paper Dataset](https://llamahub.ai/l/llama_datasets/Evaluating%20LLM%20Survey%20Paper%20Dataset?from=llama_datasets)

The script calculates the evaluation score for each of these rerankers with other params remaining constant:

```python
datasets = ["Uber10Kdataset2021", "LLMevalData"]

embed_models = {
    "bge": HuggingFaceEmbedding(model_name="BAAI/bge-large-en-v1.5"),
    "colbert": HuggingFaceEmbedding(model_name="colbert-ir/colbertv2.0"),
}

rerankers = {
    "Vector-baseline": None,
    "cohere-v2": CohereReranker(),
    "cohere-v3": CohereReranker(model_name="rerank-english-v3.0"),
    "ColbertReranker": ColbertReranker(),
}
```

### Results

Overall the results are similar in case of both embedders, except ColBERT model used both as embedder and reranker seems to work slightly better when compared with the baseline. Cohere rerankers stand out here in terms of accuracy as they get the highest score. **Simply by reranking over-fetched vector search results, it gets around 8% and 11% performance increase **for BGE and ColBERT embedding function respectively.

Next up is hybrid search with ColBERT reranker which is expected. But hybrid search with **Cohere reranker again stands out in both cases as it achieves more than 90%**. That is approximately 23% and 49% improvement on the w.r.t the 2 embedding functions.

One thing that can be noticed here is that in terms of accuracy, the performance improvement of Cohere reranker-v3 over v2 is negligible. But because both the numbers are around 90%, this might not be the best dataset to measure Cohere v3 and v2  rerankers.

![Benchmark results 1](/assets/blog/benchmarking-cohere-reranker-with-lancedb/Screenshot-2024-05-06-at-6.06.30-PM.png)

### LLM Survey Paper Dataset

The results on this dataset are pretty much the same as the previous one. Cohere Reranker performs the best in case of both ColBERT and BGE embedding models. And ColBERT Reranker used with ColBERT embedding function performs slightly better than the baseline.

![Benchmark results 2](/assets/blog/benchmarking-cohere-reranker-with-lancedb/Screenshot-2024-05-06-at-7.58.15-PM.png)

Cohere reranker v2 itself seems to reach the upper limit of retrieval accuracy in both the datasets so the improvement with v3 isn't all that noticeable. This might start to matter more on specific types of datasets. For example, on [cohere's announcement blog](https://cohere.com/blog/rerank-3), rerankers are compared on semi-structured JSON data and searching algorithm used is BM25 (full-text search in LanceDB) and not vector search. In those settings, rerank-v3 model shows significant improvements when compared with v2. These benchmarks are beyond the scope of this blog, but something that we plan to explore in the future.

Apart from accuracy, rerank-v3 model API is also supposed to be much faster v2. Here's a comparison chart from Cohere's blog linked above:

![Cohere v3 performance chart](/assets/blog/benchmarking-cohere-reranker-with-lancedb/Screenshot-2024-05-06-at-8.14.10-PM.png)
Illustration taken from - [https://cohere.com/blog/rerank-3](https://cohere.com/blog/rerank-3)

### Conclusion

This blog covers  LanceDB's reranker API that allows you to plug in popular or custom reranking methods in your retrieval pipelines across all query types. In this part, we show the results of a couple of datasets but based on other similar datasets, cohere reranker performed the best across all of them. It beats every reranker including cross-encoders, linear combination, etc. across different embedding models. In the next parts, we'll cover more advanced datasets like structured and semi-structured retrieval to see if the the results change.

Drop us a ðŸŒŸ on GitHub: [vectordb-recipes](https://github.com/lancedb/vectordb-recipes) â€¢ [lancedb](https://github.com/lancedb/lancedb)
```

---

## ðŸ“„ æ–‡ä»¶: benchmarking-lancedb-92b01032874a-2.md

---

```md
---
title: "Inverted File Product Quantization (IVF_PQ): Accelerate Vector Search by Creating Indices"
date: 2023-12-17
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/benchmarking-lancedb-92b01032874a-2/preview-image.png
meta_image: /assets/blog/benchmarking-lancedb-92b01032874a-2/preview-image.png
description: "Compress vectors with PQ and accelerate retrieval with IVF_PQ in LanceDB. The tutorial explains the concepts, memory savings, and a minimal implementation with search tuning knobs."
---

Vector similarity search is finding similar vectors from a list of given vectors in a particular embedding space. It plays a vital role in various fields and applications because it efficiently retrieves relevant information from large datasets.

Vector similarity search requires excessive memory resources for efficient search, especially when dealing with dense vector datasets. Here comes the role of compressing high-dimensional vectors for optimizing memory storage. In this blog, weâ€™ll discuss

1. **Product Quantization(PQ) & How it works**
2. **Inverted File Product Quantization(IVFPQ) Index**
3. **Implementation of IVFPQ using LanceDB**

Weâ€™ll also see the performance of PQ and IVFPQ in terms of memory and cover an implementation of the IVFPQ Index using LanceDB.

Quantization is a process used for dimensional reduction without losing important information.

![Quantization: Dimensionality Reduction](/assets/blog/benchmarking-lancedb-92b01032874a-2/1*9ibrbi3Gox6nMDda8oL7yg.png)
## How does Product Quantization work?

Product Quantization can be broken down into steps listed below:

1. Divide a large, high-dimensional vector into equally sized chunks, creating subvectors.
2. Identify the nearest centroid for each subvector, referring to it as reproduction or reconstruction values.
3. Replace these reproduction values with unique IDs that represent the corresponding centroids.

![Product Quantization](/assets/blog/benchmarking-lancedb-92b01032874a-2/1*5EyBpJ2H0jQkKFpIDIPhAg.png)
Let's see how it works in the implementation; for that weâ€™ll create a random array of size 12 and keep the chunk size as 3.

```python
import random

# consider this as a high dimensional vector
vec = [random.randint(1, 20) for i in range(12)]
chunk_count = 4
vector_size = len(vec)

# vector_size must be divisible by chunk_count
assert vector_size % chunk_count == 0

# length of each subvector will be vector_size / chunk_count
subvector_size = int(vector_size / chunk_count)

# subvectors
sub_vectors = [vec[row: row + subvector_size] for row in range(0, vector_size, subvector_size)]
sub_vectors
```

The output looks like this:

```python
[[13, 3, 2], [5, 13, 5], [17, 8, 5], [3, 12, 9]]
```

These subvectors are substituted with a designated centroid vector called Reproduction Value because it helps identify each subvector. Subsequently, this centroid vector can be substituted with a distinct ID that is unique to it.

```python
k = 2 ** 5
assert k % chunk_count == 0
k_ = int(k / chunk_count)

from random import randint

# reproduction values
c = []
for j in range(chunk_count):
    # each j represents a subvector position
    c_j = []
    for i in range(k_):
        # each i represents a cluster/reproduction value position
        c_ji = [randint(0, 9) for _ in range(subvector_size)]
        c_j.append(c_ji)  # add cluster centroid to subspace list
    # add subspace list of centroids
    c.append(c_j)

# helper function to calculate euclidean distance
def euclidean(v, u):
    distance = sum((x - y) ** 2 for x, y in zip(v, u)) ** 0.5
    return distance

# helper function to create unique ids
def nearest(c_j, chunk_j):
    distance = 9e9
    for i in range(k_):
        new_dist = euclidean(c_j[i], chunk_j)
        if new_dist < distance:
            nearest_idx = i
            distance = new_dist
    return nearest_idx
```

Now, let's see how we can get unique centroid IDs using the nearest helper function.

```python
ids = []
# unique centroid IDs for each subvector
for j in range(chunk_count):
    i = nearest(c[j], sub_vectors[j])
    ids.append(i)
print(ids)
```

Output shows unique centroid IDs for each subvector:

```python
[5, 6, 7, 7]
```

When utilizing PQ to handle a vector, we divide it into subvectors. These subvectors are then processed and linked to their closest centroids, also known as reproduction values, within the respective subclusters.

**Instead of saving our Quantized Vector using the centroids, we substitute it with a unique Centroid ID. Each centroid has its specific ID, allowing us to later map these ID values back to the complete centroids.**

```python
quantized = []
for j in range(chunk_count):
    c_ji = c[j][ids[j]]
    quantized.extend(c_ji)

print(quantized)
```

Here is the reconstructed vector using Centroid IDs:

```python
[9, 9, 2, 5, 7, 6, 8, 3, 5, 2, 9, 4]
```

In doing so, **weâ€™ve condensed a 12-dimensional vector into a 4-dimensional vector represented by IDs**. We opted for a smaller dimensionality for simplicity, which might make the advantages of this technique less immediately apparent.

***Itâ€™s important to highlight that the reconstructed vector is not identical to the original vector. This discrepancy arises due to inherent losses during the compression and reconstruction process in all compression algorithms.***

Letâ€™s change our starting 12-dimensional vector made of 8-bit integers to a more practical 128-dimensional vector of 32-bit floats. By compressing it to an 8-bit integer vector with only eight dimensions, we strike a good balance in performance.

**Original: 128Ã—32 = 4096   Quantized: 8Ã—8 = 64**

**This marks a substantial difference â€” a 64x reduction in memory.**

## How does IVFPQ Index help in speeding things up?

In IVFPQ, an Inverted File index (IVF) is integrated with Product Quantization (PQ) to facilitate a rapid and effective approximate nearest neighbor search by initial broad-stroke that narrows down the scope of vectors in our search.

After this, we continue our PQ search as we did before â€” but with a significantly reduced number of vectors. By minimizing our Search Scope, it is anticipated to achieve significantly improved search speeds.

**IVFPQ can be very easily implemented in just a few lines of code using LanceDB**

Creating an IVF_PQ Index

```python
import lancedb
import numpy as np
uri = "./lancedb"
db = lancedb.connect(uri)

# Create 10,000 sample vectors
data = [{"vector": row, "item": f"item {i}"}
   for i, row in enumerate(np.random.random((10_000, 1536)).astype('float32'))]

# Add the vectors to a table
tbl = db.create_table("my_vectors", data=data)

# Create and train the index - you need enough data in the table for an effective training step
tbl.create_index(num_partitions=256, num_sub_vectors=96)
```

- ***metric*** (default: â€œL2â€): The distance metric to use. By default, it uses Euclidean distance â€œ`L2`". We also support "cosine" and "dot" distance as well.
- ***num_partitions*** (default: 256): The number of partitions of the index.
- ***num_sub_vectors*** (default: 96): The number of sub-vectors (M) that will be created during Product Quantization (PQ).

Now let's see what this IVF Index does to reduce the scope of vectors. An inverted file is an index structure that is used to map database vectors to their respective partitions where these vectors reside.

![PQ vectors](/assets/blog/benchmarking-lancedb-92b01032874a-2/1*sqiTUncKKCVBOTks7XtvIw.png)

![Vectors assigned to Voronoi cells via IVF](/assets/blog/benchmarking-lancedb-92b01032874a-2/1*dolXJIJ4YVubaREDPSxFww.png)
This is Voronoi's Representation of vectors using IVF, theyâ€™re simply a set of partitions each containing vectors close to each other, and when it comes to search â€” When we introduce our query vector, it restricts our search to the nearest cells only because of which searching becomes way faster compared to PQ.
![Query Vector searches closest cell](/assets/blog/benchmarking-lancedb-92b01032874a-2/1*07pM49ui5dtkCeRYQLbmlQ.png)
Afterwards, PQ needs to be applied as we have seen above.

All of this can be applied using the IVF+PQ Index using [LanceDB](https://github.com/lancedb/vectordb-recipes) in minimal lines of code

```python
tbl.search(np.random.random((1536))) \
    .limit(2) \
    .nprobes(20) \
    .refine_factor(10) \
    .to_pandas()
```

- **limit** (default: 10): The number of results that will be returned
- **n-probes** (default: 20): The quantity of probes (sections) determines the distribution of vector space. While a higher number enhances search accuracy, it also results in slower performance. Typically, setting the number of probes (n-probes) to cover 5â€“10% of the dataset proves effective in achieving high recall with minimal latency.
- **refine_factor** (default: None): Refine the results by reading extra elements and re-ranking them in memory.
A higher number makes the search more accurate but also slower.

## Conclusion

In summary, Product Quantization helps reduce memory usage when storing high-dimensional vectors. Along with the IVF index, it significantly speeds up the search process by focusing only on the nearest vectors.

Visit the [**LanceDB**](https://www.linkedin.com/feed/?trk=sem-ga_campid.14650114791_asid.148989926143_crid.662526548043_kw.www+linkedin_d.c_tid.kwd-296759856208_n.g_mt.p_geo.9062111#)**repo to learn more about LanceDB Python and Typescript library**

To discover more such applied GenAI and vectorDB applications, examples and tutorials visit [**vectordb-recipes**](https://github.com/lancedb/vectordb-recipes)
```

---

## ðŸ“„ æ–‡ä»¶: benchmarking-random-access-in-lance.md

---

```md
---
title: "Benchmarking Random Access in Lance"
date: 2023-03-14
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/benchmarking-random-access-in-lance/preview-image.png
meta_image: /assets/blog/benchmarking-random-access-in-lance/preview-image.png
description: "In this short blog post weâ€™ll take you through some simple benchmarks to show the random access performance of Lance format. Get practical steps and examples from 'Benchmarking random access in Lance'."
---

In this short blog post weâ€™ll take you through some simple benchmarks to show the random access performance of Lance format.

Lance delivers comparable scan performance to parquet but supports fast random access, making it perfect for:

1. search engines
2. real-time feature retrieval, and
3. speeding up shuffling performance for deep learning training

What makes Lance interesting is that in the existing tooling ecosystem you either have to deal with the complexity of putting together multiple systems OR dealing with the expense of all in-memory stores. Moreover, Lance doesnâ€™t require extra servers or complicated setup. `pip install pylance` is all you need.

[GitHub - eto-ai/lance: Modern columnar data format for ML implemented in Rust. Convert from parquetâ€¦Modern columnar data format for ML implemented in Rust. Convert from parquet in 2 lines of code for 100x faster randomâ€¦github.com](https://github.com/eto-ai/lance?source=post_page-----ed690757a826--------------------------------)

## Test setup

Here weâ€™re going to compare the random access performance of Lance vs parquet. Weâ€™ll create 100 million records where each value is a 1000-character long randomly generated string. We then run a benchmark of 1000 queries that fetch a random set of 20â€“50 rows across the dataset. Both tests are done on the same Ubuntu 22.04 system:

```bash
sudo lshw -short
Class          Description
=============================================================
system         20M9CTO1WW (LENOVO_MT_20M9_BU_Think_FM_ThinkPad P52)
memory         128GiB System Memory
memory         32GiB SODIMM DDR4 Synchronous 2667 MHz (0.4 ns)
memory         32GiB SODIMM DDR4 Synchronous 2667 MHz (0.4 ns)
memory         32GiB SODIMM DDR4 Synchronous 2667 MHz (0.4 ns)
memory         32GiB SODIMM DDR4 Synchronous 2667 MHz (0.4 ns)
memory         384KiB L1 cache
memory         1536KiB L2 cache
memory         12MiB L3 cache
processor      Intel(R) Xeon(R) E-2176M  CPU @ 2.70GHz
storage        Samsung SSD 980 PRO 2TB
```

## Creating dataset

To run this benchmark we first generate 100 million entries, each of which is a 1000 character long string.

```python
import lance
import pyarrow as pa
import random
import string

batch_size = 1_000_000

for i in range(100):
    print(f"Creating batch {i}")
    string_arr = pa.array([''.join(random.choices(string.ascii_letters, k=1_000))
                           for _ in range(batch_size)])
    tbl = pa.Table.from_arrays([string_arr], names=["value"])
    print(f"Writing batch {i} to lance")
    if i == 0:
        params = {"mode": "create"}
    else:
        params = {"mode": "append"}
    lance.write_dataset(tbl, "take.lance", **params)
```

Converting from Lance to parquet is just one line:

```python
import lance
import pyarrow as pa

ds = lance.dataset("take.lance")
pa.dataset.write_dataset(ds.scanner().to_reader(),
                         "take.parquet",
                         format="parquet")
```

## Benchmarking Take

For both datasets, we run 1000 queries each. For each query, we generate 20â€“50 row idâ€™s randomly and then retrieve those rows and record the run time. We then compute the average time per key.

The API we use is `Dataset.take`:

```python
import time
import numpy as np
import lance

tot_time = 0
tot_keys = 0
nruns = 1000
ds = lance.dataset("take.lance")

for _ in range(nruns):
    nrows = np.random.randint(20, 50, 1)
    row_ids = np.random.randint(0, 100_000_000, nrows)
    start = time.time()
    tbl = ds.take(row_ids)
    end = time.time()
    tot_time += end - start
    tot_keys += len(row_ids)

print(f"Lance: mean time per key is {tot_time / tot_keys}")
```

The parquet snippet is almost identical, so itâ€™s omitted.

Hereâ€™s the output (in seconds):

    Lance: mean time per key is 0.0006225714343229975
    Parquet: mean time per key is 1.246656603929473

I also benchmarked a similar setup using LMDB and plotted all on the same chart for comparison:
![](/assets/blog/benchmarking-random-access-in-lance/1*CgLqW9c8Q8UMEBWgvBI17Q.png)throughput is computed as â€œ1 / mean time per keyâ€
## Key lookup

If youâ€™ve noticed weâ€™ve only benchmarked `Dataset::Take` on row ids. On the roadmap is to make this more generic so you can lookup arbitrary keys in any column.

Part of the limitation is in Lance itself. Currently looking up a particular key is done using a pyarrow Compute Expression, like `dataset.to_table(columns=["value"], filter=pa.Field("key") == <key>)` . Currently this requires scanning through the `key` column to find the right row ids, which adds more than 10ms to the query time. To solve this problem, we plan to 1) calculate batch stats so we can 2) implement batch pruning. And for super heavily queried key columns, 3) adding a secondary index would make arbitrary key lookups much faster.

## Duckdb integration

In Python, Lance is already queryable by DuckDB via the Arrow integration. However, one major shortcoming of DuckDBâ€™s Arrow integration is the extremely limited filter pushdown. For example, `pa.Field("key") == <key>` is pushed down across the pyarrow interface, but multiple key lookups is not. This can be the difference between <10ms response time vs >500ms response time. In Lance OSS, weâ€™re working on a native duckdb extension so that we donâ€™t have to be subject to these limitations.

## Conclusion

Weâ€™ve been claiming 100x faster random access performance than parquet, but as this benchmark shows, itâ€™s really more like 2000x. Lance brings fast random access performance to the OSS data ecosystem needed by important ML workflows. This is critical for search, feature hydration, and shuffling for training deep learning models. While Lanceâ€™s performance is already very valuable for these use cases, weâ€™ll be working to implement generalized key lookups, better duckdb integration, and hooks to distribute large Lance datasets across Spark/Ray nodes.

If any of these use cases apply to you, please give Lance a shot. Weâ€™d love to hear your feedback. If you like us, please give us a â­ on [ï¸Github](http://github.com/eto-ai/lance)!
```

---

## ðŸ“„ æ–‡ä»¶: better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f.md

---

```md
---
title: "Better RAG with Active Retrieval Augmented Generation FLARE"
date: 2023-11-17
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f/preview-image.png
meta_image: /assets/blog/better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f/preview-image.png
description: "by Akash A. Get practical steps and examples from 'Better RAG with Active Retrieval Augmented Generation FLARE'."
---

Welcome to our deep dive into Forward-Looking Active Retrieval Augmented Generation (FLARE), an innovative approach enhancing the accuracy and reliability of large language models (LLMs). weâ€™ll explore how FLARE tackles the challenge of hallucination in LLMs, especially in complex, long-form content generation tasks.

Hallucination in LLMs refers to generating incorrect or baseless content. This issue becomes more pronounced in tasks involving extensive outputs, such as long-form question answering, open-domain summarization, and Chain of Thought reasoning. FLARE aims to mitigate these inaccuracies by integrating external, verified information during the generation process.

## **What is FLARE:**

FLARE stands for Forward-Looking Active Retrieval Augmented Generation. Itâ€™s a methodology that supplements LLMs by actively **incorporating external information as the model generates content**. This process significantly reduces the risk of hallucination, ensuring the content is continuously checked and supported by external data.

**Traditional Retrieval-Augmented Generation:** In traditional retrieval-augmented generation models, the approach is generally to perform a **single retrieval at the beginning of the generation process**. This method involves taking an initial query, for instance, â€œSummarize the Wikipedia page for Narendra Modi,â€ and retrieving relevant documents based on this query. The model then uses these documents to generate content. This approach, however, has its limitations, especially when dealing with long and complex texts.

## **Limitations of Traditional Methods:**

- **Single Retrieval**: Once the initial documents are retrieved, the model continues to generate content based solely on this initial information.
- **Absence of Flexibility**: As the generation progresses, the model doesnâ€™t update or retrieve new information to adapt to the evolving context of the generated content.
- **Potential for outdated or Incomplete information:** If new information becomes relevant as the text progresses, the model may not capture this due to its reliance on the initially retrieved documents.
- **Multiple Retrievals**: Utilise past context to retrieve additional information at a **fixed interval. means **every 10 words or 1 sentence They are going to retrieve doesn't matter if you want to retrieve it or not.

**FLAREâ€™s Methodology:**

Multiple Retrievals: Instead of a fixed retrieval, FLARE uses multiple retrievals at different intervals &** **knows when to retrieve & what to retrieve.

**when to retrieve**: When LLM lacks the required knowledge & LLM generates low-probability tokens.

**What to retrieve**: it will consider what LLM intends to generate in the future.

Understanding FLAREâ€™s Iterative Generation Process:

FLARE operates by iteratively generating a temporary next sentence, using it as a query to retrieve relevant documents, if they contain low-probability tokens, and regenerates the next sentence until reaches the end of the overall generation

There are two types of FLARE:

1. **FLARE Instruct**: This mode prompts the model to generate specific queries for information retrieval. The model pauses generation, retrieves the necessary data, and then resumes, integrating the new information. let's understand this in the figure

![FLARE Instruct diagram](/assets/blog/better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f/1*8jwkT_22QfxrHOp5XYhmRg.png)

Imagine a scenario where an AI model is tasked with generating a *summary about Joe Biden*, prompted by a userâ€™s input query. Hereâ€™s how the process unfolds

1. User Query: The task begins with the userâ€™s request: â€œ*Generate a summary about Joe Biden.*â€
2. Initial Sentence Generation: The model starts crafting the content, generating an opening line like, â€œ*Joe Biden attended*.â€
3. Focused Search Initiation: At this juncture, the model activates a search query, â€œ*[Search(Joe Biden University)]*.â€
4. Pausing and Searching: The content generation is temporarily paused. The model then deep dives into searching about â€œ*Joe Biden University*.â€
5. Retrieval and Integration: Next, the model communicates with the retriever to fetch relevant data for â€œ*Joe Biden University.*â€ It effectively retrieves and integrates information such as, â€œ*the University of Pennsylvania*, *where he earned*.â€
6. Continued Search and Update: The process doesnâ€™t stop here. The model again launches a search, this time with â€œ*[Search(Joe Biden degree)]*.â€ Following the same protocol, it retrieves and integrates new data, such as information about his law degree.

This iterative process, blending generation and retrieval, ensures that the AI model produces a well-informed and accurate summary, dynamically incorporating relevant and up-to-date information. This is how FLARE instruct is working.

now let's understand FLARE Direct

1. **FLARE Direct**: Here, the model uses the generated content as a direct query for retrieval when it encounters tokens with low confidence. Letâ€™s delve into this with an example:

![FLARE Direct step 1](/assets/blog/better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f/1*6J-dsF2FHLwZyp3JbtQY4w.png)

1. Initiating the Query: We start with a language model input: â€œ*Generate a summary about Joe Biden*.â€

1. The model generates a response.

![FLARE Direct step 2](/assets/blog/better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f/1*IBHJLCeaTprY4hNLODdXiw.png)

3. If the generated sentence is accurate and has high confidence, it is accepted as a correct sentence.

![Accepted sentence](/assets/blog/better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f/1*y7hGZcjkxnGV7p5IEbDg4w.png)

4. letâ€™s say the model produces a sentence but with some low confidence (elements are highlighted) â€œ*the University of Pennsylvania*â€ and â€œ*a law degree*.â€ The model has very low confidence for these lines

![Low confidence tokens](/assets/blog/better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f/1*MxQWmvCEk3SzUDbmF-9UCA.png)

now there are two methods to handle this issue\
![Implicit vs explicit query](/assets/blog/better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f/1*O9iJul2Xjlwvs0u7Q1m8YA.png)

**Addressing Low Confidence Information:** To rectify or verify low-confidence information, FLARE Direct employs two approaches:

- **Implicit Query by Masking** (Highlighted in Orange): This involves identifying keywords or phrases in the sentence, such as â€œJoe Biden attendedâ€ and â€œwhere he earned.â€ The model then searches these key terms in its database (vectorDB) to retrieve relevant and accurate information.
- **Explicit Query by Question Generation **(Highlighted in Green): Here, the model is prompted to formulate specific questions related to the input query. Examples might include: â€œWhat university did Joe Biden attend?â€ and â€œWhat degree did Joe Biden earn?â€ These questions are then used to extract relevant data from the database, ensuring the informationâ€™s accuracy and relevance.

By employing these methods, FLARE Direct effectively refines and verifies the content, enhancing the accuracy and reliability of the generated summary.

## To set up this chain, we will need three things:

1. An LLM to generate the answer
2. An LLM to generate hypothetical questions to use in retrieval
3. A retriever to use to look up answers for

We need the LLM we use to produce answers to return log probabilities so we can detect uncertain tokens. Because of this, we STRONGLY suggest using the OpenAI wrapper (Note: not the ChatOpenAI wrapper, as it does not return log probabilities).

We can use any LLM to generate hypothetical questions for retrieval. In this notebook, we'll opt for ChatOpenAI since it's fast and cost-effective.

## let's do some Hands-On coding

Below is the code for Gradio, you can run it on a local system. We are using `arvixloader `questions, so you can ask quetions to paper directly

Here is an example [https://arxiv.org/pdf/2305.06983.pdf](https://arxiv.org/pdf/2305.06983.pdf).

You need to pass this number to query [2305.06983](https://arxiv.org/pdf/2305.06983.pdf) and then you can ask any questions based on the paper.

## Other important parameters to understand:

1. `max_generation_len`: The maximum number of tokens to generate before stopping to check if any are uncertain
2. `min_prob`: Any tokens generated with probability below this will be considered uncertain

```python
from langchain import PromptTemplate, LLMChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.vectorstores import LanceDB
from langchain.document_loaders import ArxivLoader
from langchain.chains import FlareChain
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import os
import gradio as gr
import lancedb
from io import BytesIO
from langchain.llms import OpenAI
import getpass

# pass your api key
os.environ["OPENAI_API_KEY"] = "sk-yourapikeyforopenai"

llm = OpenAI()

model_name = "BAAI/bge-large-en"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': False}
embeddings = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)
# here is example https://arxiv.org/pdf/2305.06983.pdf
# you need to pass this number to query 2305.06983
# fetch docs from arxiv, in this case it's the FLARE paper
docs = ArxivLoader(query="2305.06983", load_max_docs=2).load()
# instantiate text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
# split the document into chunks
doc_chunks = text_splitter.split_documents(docs)
# lancedb vectordb
db = lancedb.connect('/tmp/lancedb')
table = db.create_table("documentsai", data=[
    {"vector": embeddings.embed_query("Hello World"), "text": "Hello World", "id": "1"}
], mode="overwrite")
vector_store = LanceDB.from_documents(doc_chunks, embeddings, connection=table)
vector_store_retriever = vector_store.as_retriever()
flare = FlareChain.from_llm(
    llm=llm,
    retriever=vector_store_retriever,
    max_generation_len=300,
    min_prob=0.45
)
# Define a function to generate FLARE output based on user input
def generate_flare_output(input_text):
    output = flare.run(input_text)
    return output

input = gr.Text(
    label="Prompt",
    show_label=False,
    max_lines=1,
    placeholder="Enter your prompt",
    container=False,
)
iface = gr.Interface(
    fn=generate_flare_output,
    inputs=input,
    outputs="text",
    title="My AI bot",
    description="FLARE implementation with lancedb & bge embedding.",
    allow_screenshot=False,
    allow_flagging=False,
)
iface.launch(debug=True)
```

![FLARE Gradio demo](/assets/blog/better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f/1*pQnW_zp1wMWgIt5DLtEaLw.png)

## Summary

FLARE, short for Forward-Looking Active Retrieval Augmented Generation, improves Large Language Models (LLMs) by actively incorporating external information to minimize false information in content creation. It outperforms conventional models by dynamically retrieving information from multiple sources and adjusting to changing contexts. FLARE Instruct and FLARE Direct demonstrate their ability to produce more precise and trustworthy content. The blog also discusses key implementation details and real-world uses utilizing LanceDB and vector databases.

Explore the full potential of this cutting-edge technology by visiting the [vector-recipes](https://github.com/lancedb/vectordb-recipes). Itâ€™s filled with real-world examples, use cases, and recipes to inspire your next project. We hope you found this journey both informative and inspiring. Cheers!
```

---

## ðŸ“„ æ–‡ä»¶: building-rag-on-codebases-part-1.md

---

```md
---
title: "Building RAG on codebases: Part 1"
date: 2024-11-06
author: Sankalp Shubham
author_bio: "Applied AI + Backend Engineer. Currently working on agentic codegen. Interested in all things AI and distributed systems."
author_twitter: "dejavucoder"
author_github: "sankalp1999"
author_linkedin: "sankalp-shubham"
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/building-rag-on-codebases-part-1/preview-image.png
meta_image: /assets/blog/building-rag-on-codebases-part-1/preview-image.png
description: "Building a Cursor-like @codebase RAG solution. Part 1 focuses on indexing techniques, chunking strategies, and generating embeddings in LanceDB."
---

## Introduction

If you've used the Cursor code editor's `@codebase` feature, you're probably familiar with how helpful it can be to help your agent understand codebases. Whether you need to find relevant files, trace execution flows, or understand how classes and methods are used, `@codebase` gathers the necessary context to carry out your task. In this post, we'll discuss a new tool, [**CodeQA**](https://github.com/sankalp1999/code_qa), built using LanceDB, that demonstrates how this functionality might work under the hood.

Like you may be used to doing in Cursor, CodeQA can help answer queries that span across your entire codebase, with relevant snippets, file names and references. It supports Java, Python, Rust and Javascript, and can be extended to other languages easily. There are two crucial parts to this -- codebase indexing (Part 1) and the retrieval pipeline, discussed in the next post in this series ([Part 2](/blog/building-rag-on-codebases-part-2)).
This post is all about codebase indexing with the help of tree-sitter and then generating embeddings. 

See a preview video of CodeQA in action [here](https://x.com/dejavucoder/status/1790712123292840326)!

## What to expect

This post focuses on concepts and the approaches one can take rather than specific implementation details (except some code snippets).
We'll highligh **why in-context learning isn't the best approach** for code QA. We'll also illustrate the limitations of naive semantic code search, and elaborate on the types of chunking that can help improve results. Specifically, we'll highlight a syntax-level chunking using the tree-sitter library.

{{< admonition >}}
Some basic prior knowledge is assumed for you to follow along in this post: Python programming, and the notion of vectors/embeddings in retrieval.
{{< /admonition >}}

## Problem statement

The goal is to build an application that will help the user (or agent) understand a codebase and generate accurate code by providing the following capabilities:

- Answer natural language questions about a codebase with accurate, relevant information
- Provide contextual code snippets and analyze code usage patterns, including:
   - How specific classes and methods are used
   - Details about class constructors and implementations
- Track and utilize code references to:
   - Identify where code elements are used across the codebase
   - Provide source references for all answers
- Leverage gathered context to assist with code generation and suggestions

Questions can range from simple (single-hop) to complex (multi-hop). Single-hop questions can be answered using information from a single source, while multi-hop questions require the LLM to gather and synthesize information from multiple sources to provide a complete answer.

Examples:

- "What is this git repository about?"
- "How does a particular method work?" â†’ where you may not mention the exact name of method
- "Which methods do xyz things?" â†’ it is supposed to identify the relevant method/class names from your query
- They can be more complex, e.g. "What is the connection between `EncoderClass` and `DecoderClass`?" â†’ The system is supposed to answer by gathering context from multiple sources (multi-hop question answering)

Now that the problem statement is clear, let's think about how we might implement a solution. LLMs are a great tool to help solve this!

## Why can't GPT-4 answer questions for my codebase?

While GPT-4 was trained on a vast amount of code, it doesn't know about your specific codebase. It doesn't know about the classes you've defined, the methods you're using, or the overall purpose of your project.

It can answer general programming questions like "how does useEffect work", but can't answer questions about your custom code like "how does `generateEmbeddings()` work?" because it has never seen your specific implementation that named it that way.

When asked about your specific code base, GPT-4 will either admit it doesn't know or hallucinate (more likely, the latter) -- making up totally plausible, but incorrect, answers.

## In-context learning approach

![](/assets/blog/building-rag-on-codebases-part-1/code-rag-1-icl.png)

Modern LLMs like Anthropic's Claude and Google's Gemini have very large context windows -- ranging from 200K tokens in Claude Sonnet to 2M tokens in Gemini Pro. These models excel at learning from context (in-context learning) and can effectively learn patterns and perform specific tasks when provided with examples in the prompt, making them powerful few-shot learners ([source](https://arxiv.org/abs/2005.14165)).

This means you can plug all your code from smaller/medium-sized codebases (i.e., all your side-projects) into Claude Sonnet and even large codebases into the latest Gemini Pro. This way you can ground the LLMs with your codebase context and ask questions. I frequently use the above two for codebase understanding, especially Gemini (it works surprisingly well). Here's a trick you could use -- try changing a github repo's URL from `github` -> `uithub` (replace "g" with "u"). This will render the repo's structure in a way you can directly copy-paste it into a prompt for an LLM. A tool I use locally sometimes is [code2prompt](https://github.com/mufeedvh/code2prompt).

{{< admonition >}}
A quick primer on few-shot learning:
You can provide information or examples in the context of the LLM -- think, the system prompt or the larger prompt itself. This info can be used as reference or can be used to _learn_ patterns and perform specific tasks.

[In-context learning](https://www.hopsworks.ai/dictionary/in-context-learning-icl), or ICL, is a method where you provide information to the LLM at inference time, and it's able to answer your questions even though it wasn't trained on that context, because it learns patterns or memorizes new information on the fly.

A recent paper titled "_In-Context Learning with Long-Context Models: An In-Depth Exploration_" shows that LLMs with large context show increased performance when presented thousands of examples. See this tweet [thread](https://x.com/abertsch72/status/1786392584765538350) for more info.
{{< /admonition >}}

## Why ICL is not the best approach

In this section, we'll elaborate on why ICL may not be the best approach for code QA.

#### Performance degradation as context window fills up
Pasting a few files as context in Claude Code or Cursor's chat UI is feasible until things don't get larger than the context window. As the context window fills up, the model's code generation and understanding start to degrade. The sooner you overwhelm the LLM, the less likely it would be able to answer your queries or generate useful code.

#### Cost inefficiency
If you are paying per LLM API call, sending lots of tokens for each session can quickly get expensive.

#### Time inefficiency
Pasting large codebases into Gemini works but it can get time-consuming in two ways - Gemini takes time to process the initial prompt with all the code, and then response times increase as the context window fills up. It's good for self-dev purposes.

#### Poorer UX
From an end-user perspective, it's not a fair ask to expect the user to copy-paste their entire codebase into the LLM each time they want to answer a question or generate some code.

#### Relevance Issues
Dumping too much code into context can actually harm the quality of responses. The model might get distracted by irrelevant code sections instead of focusing on the specific parts needed to answer the query.

## Towards more relevant context

It's clear we need to minimize the irrelevant tokens that make their it into the context for the LLM. Recall that our query can be in natural language and may not contain the correct class or method names, due to which exact matches or fuzzy search will not work. To map our query to relevant code symbols (e.g., class names, method names, code blocks), we can leverage semantic search using vector embeddings.  In the following sections, we'll explore how to effectively extract and index code chunks to generate high-quality embeddings.

## Embeddings 101 and the importance of structure

If you're new to this topic, you can refer to this excellent [article](https://simonwillison.net/2023/Oct/23/embeddings/) by Simon Willison for a primer on embeddings.

### Understanding embeddings and chunking

Let's walk through a simple example to understand embeddings and how chunking works. Say we want to find relevant quotes about "ambition" from Paul Graham's blog posts.

First, let's look at why we need to break up (or "chunk") the text. Embedding models can only handle text up to a certain length -- usually between 1024 to 8192 tokens. When we have a long piece of text, like this blog post, we need to split it into smaller pieces that fit into the LLM's context window. This helps us match relevant content while keeping the meaning intact.

Here's how we typically handle this:
- We break down the text into manageable chunks that:
   - are small enough for the embedding model to process
   - still make sense when read on their own
   - keep enough context to be useful
- Splitting is usually done in the following ways:
   - fixed size chunking (splitting into equal-sized pieces)
   - split based on number of tokens
   - use smart splitters that adapt to the content (like RecursiveCharacterTextSplitter)

To help with this, we can use tools like LangChain or LlamaIndex. These frameworks come with built-in splitters that handle different types of content:
- `RecursiveCharacterTextSplitter` for general text
- Special splitters for markdown
- Splitters that understand the meaning of the text

One important thing to remember is that different kinds of content need different splitting approaches. A blog post, code, and technical documentation each have their own structure - and we need to preserve that structure to get good results from our embedding models.

Here's the whole process from start to finish:
1. Split your content into chunks based on what kind of content it is
2. Turn those chunks into embeddings using a model (like `sentence-transformers/bge-en-v1.5`)
3. Save these embeddings in a database or simple CSV file
4. Also save the original text and any other useful info
5. When someone wants to retrieve via semantic search:
   - convert their search into an embedding
   - find similar embeddings (using math like dot product or cosine similarity)
   - return the matching chunks of text

This understanding of chunking and embeddings is crucial as we move forward to discuss code-specific chunking strategies, where maintaining code structure becomes even more important.

#### References

- [Chunking](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/8a30b5710b3dd99ef2239fb60c7b54bc38d3613d/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)
- [OpenAI Platform](https://platform.openai.com/docs/tutorials/web-qa-embeddings)

## Towards a naive semantic search solution

The process works as follows:

1. Embed the entire codebase
2. User provides a search query 
3. Convert query to embedding and calculate cosine similarity
4. [Semantic code search completes]
5. Get top 5 matching code blocks
6. Feed the actual code (metadata) into the LLM as context
7. LLM generates the answer

> We need to figure out how to embed our codebase for best possible semantic search.

Providing context to LLM via semantic search or other retrieval techniques (could be SQL) to aid it in generation (and avoid hallucination) is called **Retrieval Augmented Generation**. I recommend reading [**Hrishiâ€™s three part series on RAG**](https://olickel.com/retrieval-augmented-research-1-basics) going from basics to somewhat advanced RAG techniques (but first finish reading my post, thanks). My posts are application of part 1 and part 3.

The following images from the paper "[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)".

![](/assets/blog/building-rag-on-codebases-part-1/code-rag-1-1.png)

![](/assets/blog/building-rag-on-codebases-part-1/code-rag-1-2.png)

## Chunking a codebase

Chunking blocks of code like we do for text (fixed-token length, paragraph-based, etc.) will not automatically lead to good results. As developers, we intuitively know that code has a specific syntax with meaningful unit and well-defined structure, such as classes, methods, and functions. To effectively process and embed code, we need to maintain its _semantic integrity_.

![](/assets/blog/building-rag-on-codebases-part-1/code-rag-1-3.png)

The intuition is that the structure that's inherently present in code would be similar to other code that has a similar structure in the latent space of the embedding. Another factor is that embeddings may have been trained on code snippets, so they might be able to better capture the relationships in the code's primitives.

During retrieval, entire blocks of methods or at least, _references_ to the blocks, would help a lot, rather than retrieving fragments of a method. We would also like to provide contextual information like references, either for locating the embeddings, or while feeding context into the LLM.

### Method/class-level chunking

You can choose to do method-level chunking or class-level chunking. For reference, see the [OpenAI cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/Code_search_using_embeddings.ipynb), which extracts all functions from code and embeds them. 

### Syntax-level chunking

Code presents unique chunking challenges:
- How to handle hierarchical structures?
- How to extract language-specific constructs (constructors, class)? 
- How to make it language-agnostic? Different languages have different syntax.

That's where syntax-level chunking comes in. 

- We can parse the code into an abstract syntax tree (AST) representation
- Traverse the AST and extract relevant subtrees or nodes as chunks, such as function declarations, class definitions, entire class code or constructor calls. It allows to extract from varying level of granularity uptill a single variable
- Also possible to get codebase wide references with some implementation
- By leveraging the AST, it's possible to capture the hierarchical structure and relationships within the code


### How to construct the AST?
Python standard library comes with a convenient module, `ast`. While this may be robust, it's limited to Python code only. A more language-agnostic solution was needed for CodeQA.

Upon digging deeper - through technical blogs, exploring GitHub repositories, and discussing with friends who work on developer tools, we found that a pattern began to emerge: the term "tree-sitter" kept coming up in these sources.

What made this interesting wasn't just that we discovered a new term, but realizing _how widely adopted it was_ in the developer tooling ecosystem. Here is a summary:

#### YC-backed companies were using it
Buildt (YC 23) mentioned it in their [technical discussions](https://news.ycombinator.com/item?id=35000562)

![](/assets/blog/building-rag-on-codebases-part-1/code-rag-1-4.png)

#### Most modern code editors were built on it

Cursor.sh uses it for their [codebase indexing](https://x.com/amanrsanger/status/1750023209733464559). Their approach to constructing code graphs relies heavily on tree-sitter's capabilities!

#### Developer tools were standardizing on it
Aider.chat, an AI-powered terminal-based pair programmer, uses tree-sitter for their AST processing.
They have an excellent write-up on [building repository maps with tree-sitter](https://aider.chat/docs/repomap.html)

## What _is_ tree-sitter?

[Tree-sitter](https://tree-sitter.github.io/tree-sitter) is a parser-generator tool and an incremental parsing library. It can build a concrete syntax tree for a source file and efficiently update the syntax tree as the source file is edited. Tree-sitter aims to be the following:

- **General** enough to parse any programming language
- **Fast** enough to parse on every keystroke
- **Robust** enough to provide useful results even with syntax errors
- **Dependency-free** with a pure C runtime library

It's used in code editors like Atom, VSCode for features like syntax highlighting and code-folding. Apparently, the neovim people are also treesitter fanatics. 
The key feature is incremental parsing: it enables efficient updates to be made to the syntax tree as code changes, making it _perfect_ for IDE features like syntax highlighting and auto-indentation.

While exploring code editor internals, we found that they typically use AST libraries + LSP (Language Server Protocol). Though LSIF indexes (LSP's knowledge format) are an alternative for code embedding, these are skipped in CodeQA due to the complexity of multi-language support.

If you're curious to learn more, see the tree-sitter explainer [video](https://youtu.be/09-9LltqWLY?si=gg4ECnCPr_W7duMR).

---

### Diving into the syntax with tree-sitter

The simplest way to get started is with the `tree_sitter_languages` module. It comes with pre-built parsers for all supported programming languages.

```bash
pip install tree-sitter-languages
```

You can try playing on [tree-sitter playground](https://tree-sitter.github.io/tree-sitter/playground) . 

### Extracting methods and classes (or arbitrary code symbols) from AST

You can find the code for this section in [tutorial/sample_one_traversal.py](https://github.com/sankalp1999/code_qa/blob/main/tutorial).

Let's look at the tree-sitter's AST for the following code:

```python
class Rectangle:
    def __init__(self, width, height):
        self.width = width
        self.height = height
    
    def calculate_area(self):
        """Calculate the area of the rectangle."""
        return self.width * self.height
        
my_rectangle = Rectangle(5, 3)
area = my_rectangle.calculate_area()
```

Tree-sitter's AST for the above code (simplified):

```yaml
module [0, 0] - [12, 0]
  class_definition [0, 0] - [7, 39]  # Rectangle
    name: identifier [0, 6] - [0, 15]
    body: block [1, 4] - [7, 39]
      function_definition [1, 4] - [3, 28]  # __init__
        name: identifier [1, 8] - [1, 16]
        parameters: parameters [1, 16] - [1, 37]
          // ... parameter details ...
        body: block [2, 8] - [3, 28]
          // ... constructor implementation ...
      
      function_definition [5, 4] - [7, 39]  # calculate_area
        name: identifier [5, 8] - [5, 22]
        parameters: parameters [5, 22] - [5, 28]
          // ... parameter details ...
        body: block [6, 8] - [7, 39]
          // ... method implementation ...
  
  expression_statement [9, 0] - [9, 30]  # my_rectangle = Rectangle(5, 3)
    assignment
      left: identifier  # my_rectangle
      right: call
        function: identifier  # Rectangle
        arguments: argument_list
          integer  # 5
          integer  # 3
  
  expression_statement [10, 0] - [10, 36]  # area = my_rectangle.calculate_area()
    assignment
      left: identifier  # area
      right: call
        function: attribute
          object: identifier  # my_rectangle
          attribute: identifier  # calculate_area
        arguments: argument_list  # ()
```

### Using recursive tree traversal to extract methods and classes

Reading the code from a file and parsing it into an AST is done as per the following snippet:

```python

from tree_sitter_languages import get_parser

# Initialize parser and read code
parser = get_parser("python")
code = """
class Rectangle:
    def __init__(self, width, height):
        self.width = width
        self.height = height
    
    def calculate_area(self):
        \"\"\"Calculate the area of the rectangle.\"\"\"
        return self.width * self.height
        
my_rectangle = Rectangle(5, 3)
area = my_rectangle.calculate_area()
"""

# Parse into AST
tree = parser.parse(bytes(code, "utf8"))

```


We traverse the AST recursively and look for node types that we want to extract.

- Each node has a type like `class_definition` or `function_definition` (and many more like `expression_statement`, `assignment`, `identifier`, etc.). The node types can vary with language e.g For Java, method is method_declaration, for Rust it's function_item, javascript method_definition etc.
- We can use the `child_by_field_name` method to get the child node with a specific field name.
- We can get the text of the node using the `text` attribute. Text content is stored in bytes so we need to decode it.
- Nodes form a tree like structure and we can access children using `node.children`

```python

# Extract classes and methods from AST
def extract_classes_and_methods(node):
    results = {
        'classes': [],
        'methods': []
    }
    
    def traverse_tree(node):
        # Extract class definitions
        if node.type == "class_definition":
            class_name = node.child_by_field_name("name").text.decode('utf8') 
            class_code = node.text.decode('utf8') # Gets entire source code for the class
            results['classes'].append({
                'name': class_name,
                'code': class_code
            })
            
        # Extract method definitions
        elif node.type == "function_definition":
            method_name = node.child_by_field_name("name").text.decode('utf8')
            method_code = node.text.decode('utf8') # Gets entire source code for the method
            results['methods'].append({
                'name': method_name,
                'code': method_code
            })
            
        # Recursively traverse children
        for child in node.children:
            traverse_tree(child)
    
    traverse_tree(node)
    return results

# Use the extraction function
extracted = extract_classes_and_methods(tree.root_node)

# Print results
for class_info in extracted['classes']:
    print(f"\nFound class {class_info['name']}:")
    print(class_info['code'])

for method_info in extracted['methods']:
    print(f"\nFound method {method_info['name']}:")
    print(method_info['code'])
```

### Using tree-sitter Queries

Below is a snippet showing how to define queries and use them to extract classes and methods. See the full tutorial [here](https://github.com/sankalp1999/code_qa/blob/main/tutorial).

```python

class_query = language.query("""
    (class_definition
        name: (identifier) @class.name
    ) @class.def
""")

# Query for function (method) definitions, capturing the name and definition
method_query = language.query("""
    (function_definition
        name: (identifier) @method.name
    ) @method.def
""")

def extract_classes_and_methods(root_node):
    results = {
        'classes': [],
        'methods': []
    }
    
    # Extract classes
    for match in class_query.matches(root_node):
        captures = {name: node for node, name in match.captures}
        class_name = captures['class.name'].text.decode('utf8')
        class_code = captures['class.def'].text.decode('utf8')
        results['classes'].append({
            'name': class_name,
            'code': class_code
        })
    
    # Extract methods
    for match in method_query.matches(root_node):
        captures = {name: node for node, name in match.captures}
        method_name = captures['method.name'].text.decode('utf8')
        method_code = captures['method.def'].text.decode('utf8')
        results['methods'].append({
            'name': method_name,
            'code': method_code
        })
    
    return results

```

You can read more about tree-sitter [queries](https://tree-sitter.github.io/tree-sitter/using-parsers#pattern-matching-with-queries) and [tagged captures](https://tree-sitter.github.io/tree-sitter/code-navigation-systems#tagging-and-captures) if you'd like to go deeper!

Queries are defined as follows:

```python
class_query = language.query("""
    (class_definition
        name: (identifier) @class.name
        body: (block) @class.body
    ) @class.def
""")
```

#### Queries
In tree-sitter, queries are patterns that match specific syntactic structures within the abstract syntax tree (AST) of your code. They allow you to search for language constructs, such as class definitions or function declarations, by specifying the hierarchical arrangement of nodes that represent these constructs.

#### Tags, or captures
These are labels assigned to particular nodes within your query patterns using the `@` symbol. By tagging nodes, you can extract specific parts of the matched patterns for further analysis or processing, such as names, bodies, or entire definitions.

In the code snippet above, the `class_query` is designed to match `class_definition` nodes in Python code and capture key components:

- `@class.name`: Captures the `identifier` node that represents the class name.
- `@class.body`: Captures the `block` node that contains the body of the class.
- `@class.def`: Captures the entire `class_definition` node.

Using this query, you can extract detailed information about each class in the code, such as the class name and its contents, which is useful for tasks like code analysis, refactoring, or documentation generation.

You can see projects storing the queries as `.scm` files often in projects like [aider](https://github.com/Aider-AI/aider/tree/main/aider/queries) and [locify](https://github.com/ryanhoangt/locify/tree/main/locify/tree_sitter).


The implementation of CodeQA for this blog post is similar, and you can check the file [treesitter.py](https://github.com/sankalp1999/code_qa/blob/main/treesitter.py) for more details. You can see how queries were defined for different languages, too.

---

### Codebase wide references

For codebase wide references, we mainly find the function calls and class instantiations/object creation. The code from `preprocessing.py` looks like this:

```python
def find_references(file_list, class_names, method_names):
    references = {'class': defaultdict(list), 'method': defaultdict(list)}
    files_by_language = defaultdict(list)
    
    # Convert names to sets for O(1) lookup
    class_names = set(class_names)
    method_names = set(method_names)

    for file_path, language in file_list:
        files_by_language[language].append(file_path)

    for language, files in files_by_language.items():
        treesitter_parser = Treesitter.create_treesitter(language)
        for file_path in files:
            with open(file_path, "r", encoding="utf-8") as file:
                code = file.read()
                file_bytes = code.encode()
                tree = treesitter_parser.parser.parse(file_bytes)
                
                # Single pass through the AST
                stack = [(tree.root_node, None)]
                while stack:
                    node, parent = stack.pop()
                    
                    # Check for identifiers
                    if node.type == 'identifier':
                        name = node.text.decode()
                        
                        # Check if it's a class reference
                        if name in class_names and parent and parent.type in ['type', 'class_type', 'object_creation_expression']:
                            references['class'][name].append({
                                "file": file_path,
                                "line": node.start_point[0] + 1,
                                "column": node.start_point[1] + 1,
                                "text": parent.text.decode()
                            })
                        
                        # Check if it's a method reference
                        if name in method_names and parent and parent.type in ['call_expression', 'method_invocation']:
                            references['method'][name].append({
                                "file": file_path,
                                "line": node.start_point[0] + 1,
                                "column": node.start_point[1] + 1,
                                "text": parent.text.decode()
                            })
                    
                    # Add children to stack with their parent
                    stack.extend((child, node) for child in node.children)

    return references
```

It's a stack-based tree traversal to find the references. CodeQA takes a simpler approach here rather than using queries/tags. If the `identifier`'s name matches a known class name and its parent node type indicates class usage (e.g., type annotation, object creation), it's recorded as a class reference. The same is true for methods.

---

## Conclusions

In this post, we discussed how to go from a naive semantic code search solution to a more elaborate,syntax-level chunking strategy with tree-sitter. We're almost done with preprocessing! In [Part 2](/blog/building-rag-on-codebases-part-2), we'll see how to embed, some tricks to improve embedding search and then some post-processing techniques.

## References

References are listed in the order they appeared in the post:

1. [codeQA Github Link](https://github.com/sankalp1999/code_qa)
2. [Language models are few shot learners](https://arxiv.org/abs/2005.14165)
3. [In-Context Learning with Long-Context Models: An In-Depth Exploration Twitter thread](https://x.com/abertsch72/status/1786392584765538350)
4. [An Intuitive Introduction to Text Embeddings (Stack Overflow blog)](https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/)
5. [Chunking tutorial (FullStackRetrieval)](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/8a30b5710b3dd99ef2239fb60c7b54bc38d3613d/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)
6. [OpenAI Web QA Embeddings Tutorial](https://platform.openai.com/docs/tutorials/web-qa-embeddings)
7. [Hrishi's three part series on RAG - Part 1](https://olickel.com/retrieval-augmented-research-1-basics)
8. [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)
9. [OpenAI Cookbook - Code search using embeddings](https://github.com/openai/openai-cookbook/blob/main/examples/Code_search_using_embeddings.ipynb)
10. [Buildt (YC 23) Hacker News Comment](https://news.ycombinator.com/item?id=35000562)
11. [Aman Sanger's Twitter thread on codebase indexing](https://twitter.com/amanrsanger/status/1750023216461234450)
12. [Building a better repository map with tree sitter - Aiderchat](https://aider.chat/docs/repomap.html)
13. [Tree-sitter Github](https://tree-sitter.github.io/tree-sitter/)
14. [Tree-sitter Playground](https://tree-sitter.github.io/tree-sitter/playground)
15. [Tree-sitter explained video](https://youtu.be/09-9LltqWLY?si=gg4ECnCPr_W7duMR)
16. [treesitter-implementations.py Github Link](https://github.com/sankalp1999/code_qa/blob/main/treesitter_implementations.py)
17. [tree-sitter.py Github Link](https://github.com/sankalp1999/code_qa/blob/main/treesitter.py)
18. [preprocessing.py Github Link](https://github.com/sankalp1999/code_qa/blob/main/preprocessing.py)
```

---

## ðŸ“„ æ–‡ä»¶: building-rag-on-codebases-part-2.md

---

```md
---
title: "Building RAG on codebases: Part 2"
date: 2024-11-07
author: Sankalp Shubham
author_bio: "Applied AI + Backend Engineer. Currently working on agentic codegen. Interested in all things AI and distributed systems."
author_twitter: "dejavucoder"
author_github: "sankalp1999"
author_linkedin: "sankalp-shubham"
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/building-rag-on-codebases-part-2/preview-image.png
meta_image: /assets/blog/building-rag-on-codebases-part-2/preview-image.png
description: "Building a Cursor-like @codebase RAG solution. Part 2 focuses on
the generating embeddings and the retrieval strategy using a combination of techniques in LanceDB."
---

## Quick recap

In the [previous post](/blog/building-rag-on-codebases-part-1), we covered the indexing stage of building a question-answering (QA) system for codebases. We discussed why models like GPT-4 can't inherently answer code questions and the limitations of in-context learning. We also explored semantic code search with embeddings and the importance of proper codebase chunking. We then showcased a syntax-level chunking approach with abstract syntax trees (ASTs) using tree-sitter, demonstrating how to extract methods, classes, constructors, and cross-codebase references.

Now that the most of the indexing has been discussed, let's proceed to the next stages!

## What to expect

In this post, we'll cover the following topics:

- Final step for codebase indexing: Adding LLM-generated comments
- Considerations for choosing embeddings and vector databases
- Techniques to improve retrieval: HyDE, BM25, re-ranking
- Choosing re-rankers: An in-depth explanation of re-ranking (bi-encoding vs. cross-encoders)

If you've ever worked with embeddings, you'll know that a single-stage embedding-based (semantic) search is not always useful, and you'd need some additional steps in the retrieval pipeline like HyDE, hybrid vector-search and re-ranking.

In next few sections, we'll discuss methods to improve our overall search and the relevant implementation details. Refer to the diagram in the [previous post](/blog/building-rag-on-codebases-part-1) for more information on the architecture of the CodeQA system.

## Adding LLM comments

This is technically part of codebase indexing too, but it felt more in line to be discussing it in this post.

### Implementing LLM-generated comments for methods (optional)

LLM comments are not included in the current version of CodeQA because it slows down the indexing process. The relevant file `llm_comments.py` is still there in the repo.

Since our queries will be in natural language, we integrated a natural language component by adding 2-3 lines of documentation for each method. This creates an annotated codebase, with each LLM-generated comment providing a concise overview. These annotations enhance keyword and semantic search, allowing for more efficient searches based on both "what the code does" and "what the code _is_".

Here's a cool blogpost which validated these ideas: "[Three LLM tricks that boosted embeddings search accuracy by 37% â€” Cosine](https://www.buildt.ai/blog/3llmtricks)".

### Meta-characteristic search

One of the core things we wanted to have is for people to be able to search for characteristics of their code which weren't directly described in their code, for example a user might want to search for **all generic recursive functions**, which would be very difficult if not impossible to search for through conventional string matching/regex and likely wouldn't perform at all well through a simple embedding implementation. This could also be applied to non-code spaces too; a user may want to ask a question of an embedded corpus of Charles Dickens asking **find all cases where Oliver Twist is self-reflective** which would not really be possible with a basic embedding implementation.

Our solution with Buildt was: For each element/snippet we embed a textual description of the code to get all of the meta characteristics, and we get those characteristics from a fine-tuned LLM. By embedding the textual description of the code alongside the code itself, it allows you to search both against raw code as well as the characteristics of the code, which is why we say you can "search for what your code does, rather than what your code is". This approach works extremely well and without it questions regarding functionality rarely return accurate results. This approach could easily be ported to prose or any other textual form which could be very exciting for large knowledge bases.

There are pitfalls to this approach: it obviously causes a huge amount of extra cost relative to merely embedding the initial corpus, and increases latency when performing the embedding process - so it may not be useful in all cases, but for us it is a worthwhile trade-off as it produces a magical search experience.

## Embedding and choice of vector database

We use OpenAI's `text-embedding-3-large` by default as it's very good -- plus almost everyone has OpenAI API keys, so it's great for demo purposes as in this post. There's also an option for using `jina-embeddings-v3`, which are supposedly better than `text-embedding-3-large` in benchmarks,
so it's worth trying that too.

OpenAI embeddings are cost-effective and have a sequence length of 8,191 tokens, and easily accessible via API. Sequence length is important for an embedding model, because it allows the model to capture long-length dependencies and do more with the context.

## Things to consider for embeddings

Let's look at what you'd normally consider when choosing an embedding model.

### Benchmarks

You can checkout the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for embedding ranking with scores. Look at the scores, sequence length, the languages that are supported. Locally available models need your compute, so look at size for open source models.

### Latency

API-based embeddings can be slower than local ones simply because of network round-trips added latency. So if you want speed and you have the compute, it may be better to use local embeddings.

### Cost

If you're constrained by cost, go open source via `sentence-transformers`, or `bge-en-v1.5` or `nomic-embed-v1`. Otherwise, most closed source embeddings are cheap and fast to use.

### Use-case

The models should be applicable for your use-case. Research accordingly on Hugging Face or the model's documentation and/or research paper. If you are embedding code, the embeddings you use _must_ have had code in pre-training data.

You can also use **fine-tuned embeddings** to significantly improve your downstream performance. LlamaIndex has a high-level API to fine-tune all Hugging Face embeddings. Jina recently [announced](https://x.com/JinaAI_/status/1785337862755356685) a fine-tuning API, which is worth a try.

### Privacy

Privacy for data is a major concern for several people and companies. For many projects, you may prioritize performance and choose OpenAI, not considering privacy concerns. For others, you may prioritize privacy first.

Below is a quote from a SourceGraph blog post:

> While embeddings worked for retrieving context, they had some drawbacks for our purposes. Embeddings require all of your code to be represented in the vector space and to do this, we need to send source code to an OpenAI API for embedding. Then, those vectors need to be stored, maintained, and updated. This isnâ€™t ideal for three reasons:
>
> - Your code has to be sent to a 3rd party (OpenAI) for processing, and not everyone wants their code to be relayed in this way.
> - The process of creating embeddings and keeping them up-to-date introduces complexity that Sourcegraph admins have to manage.
> - As the size of a codebase increases, so does the respective vector database, and searching vector databases for codebases with >100,000 repositories is complex and resource-intensive. This complexity was limiting our ability to build our new multi-repository context feature.

For a deeper understanding of codebase indexing methodologies, it's recommended to review the SourceGraph blog post in much more detail, because their approach is similar to the CodeQA implementation, but operates at a significantly larger scale. They developed an AI coding assistant named [Cody](https://github.com/sourcegraph/cody) using this approach. It's worth also noting that they have since moved away from using embeddings in their architecture.

Another recommended blog post to read is "[How Cody understands your codebase](https://sourcegraph.com/blog/how-cody-understands-your-codebase)".

## Vector database: LanceDB

I use [LanceDB](https://lancedb.com/) because itâ€™s fast, open source and easy to use - you can just `pip install` and import it as a library -- no API key required. They support integrations for almost all embeddings (available on Hugging Face) and most major players like OpenAI, Jina and so on. There's easy support for integration for re-rankers, algorithms, embeddings, and third-party libraries for RAG.

### Things to consider

- Support for all the integrations you need - e.g LLMs, different companies,
- Recall and Latency
- Cost
- Familiarity/Ease of use
- Open source / closed source

### Implementation details

Code for embedding the codebase and making the tables can be found in the [create_tables.py](https://github.com/sankalp1999/code_qa/blob/main/create_tables.py) file in the repo. Two separate LanceDB tables are maintained -- one for methods, and another for classes and miscellaneous items like `README` files. Keeping things separate allows us to query separate metadata for specific vector searches -- get the closest class, then get the closest methods.

In the implementation, you'll notice that we donâ€™t generate embeddings manually. That part is handled by LanceDB itself, implicitly. Just add your chunks, and LanceDB handles all the batch-embedding generation with retry logic in the background, via the embedding registry.

## Retrieval

Once we have the tables ready, we can issue queries, and it outputs the most similar documents using brute-force search (cosine similarity/dot product). These results are going to be relevant but not as accurate as you think. Embedding search feels like magic, _until it doesn't_. Based on experiments from other projects, the results are relevant but often noisy plus the ordering can be wrong often.See this [tweet thread](https://x.com/eugeneyan/status/1767403777139917133) to learn about some shortcomings.

## Improving embedding search

### BM25

The first thing to try is to combine semantic search with a keyword based search like BM25. This is what's used in the [semantweet search](https://github.com/sankalp1999/semantweet-search) project. These are supported out-of-the-box already by many vector databases. semantweet search also used LanceDB, which supports _hybrid search_: semantic search + BM25 out-of-the-box. All it takes is some additional code to create a native full-text based index in LanceDB.

Recall is how many of the relevant documents are we retrieving / number of documents. It's used to measure search performance.

## Filtering using meta-data

Another effective approach to enhance semantic search is to use metadata filtering. If your data contains attributes like dates, keywords, or metrics, you can use SQL to pre-filter the results before performing semantic search. This two-step process can significantly improve search quality and performance. It's important to note that embeddings exist as independent points in latent space, so this pre-filtering step helps narrow down the relevant search space.

## Reranking

Using re-rankers after the vector search is an easy way to improve results significantly. In short, re-rankers do a cross-attention between query tokens and embedding search result tokens.

Letâ€™s try to understand how they work on a high level. In CodeQA v2, we use `answerdotai/answerai-colbert-small-v1` as it's the best performing local re-ranker model based on some available benchmarks (take all benchmarks with a grain of salt), with performance close to Cohere Re-ranker 3 (which was used in CodeQA v1).

### Cross-encoding (re-ranking) vs bi-encoding (embeddings based search)

Embeddings are obtained from models like GPT-3 â†’ decoder-only transformer
or BERT (encoder-only transformer, BERT-base is 12 encoders stacked together).

Both GPT-3 and BERT-classes of models can be made to operate in two styles â†’ cross-encoder and bi-encoder. They have to be trained (or fine-tuned) for the same. we'll not get into the training details here.

```python
from sentence_transformers import SentenceTransformer, CrossEncoder

# Load the embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2") # biencoder

# Load the cross-encoder model
cross_model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-12-v2") # cross-encoder
```

#### Cross-encoder

A cross-encoder concatenates the query with each document and computes relevance scores.
```
<query><doc1>
<query><doc2>
```

Each pair of concatenated \(query + document\) is passed through a pre-trained model (like BERT), going through several layers of attention and feed-forward networks. The self-attention mechanism helps to capture the interaction between the query and the doc (all tokens of query interact with document)

We get the output from a  hidden layer (usually the last one) to get contextualized embeddings. These embeddings are pooled to obtain a fixed size representation. These are then passed through a linear layer and then softmax/sigmoid to obtain logits â†’ relevance scores â†’ `[0.5, 0.6, 0.8, 0.1, â€¦]`.

Letâ€™s say we have \(D\) documents and \(Q\) queries. To calculate relevance score for 1 query, we will have \(D\) (query + doc) passes. through the model. For \(Q\) queries, we will have \(D \times Q\) passes since _each_ concatenation of \(D\) and \(Q\) is unique.

#### Bi-encoder

A bi-encoder approach (or the **embeddings search approach)** encodes documents and queries separately, and calculates the dot product. Letâ€™s say you have \(D\) docs and \(Q\) queries.

Precompute \(D\) embeddings. We can reuse the embedding instead of calculating again. Now for each query, compute dot product of \(D\) and \(Q\). Dot product can be considered an \(O(1)\) operation.

- compute cost of encoding \(D\) docs â†’ \(D\)
- compute cost of encoding \(Q\) queries â†’ \(Q\)
- compute cost then becomes \(D + Q\)

This is _much_ faster than the cross-encoding approach.

Since every token in query interacts with the documents and assigns a relevance score of query vs. each document, the cross-encoder is more accurate than bi-encoders, but they are slower, since an individual processing of each pair is required (each \(Q\) + \(D\) combination is unique so cannot precompute embeddings).

Thus, we can stick to a bi-encoder approach (generating embeddings, encode query, encode documents and store in a vector database, then calculate similarity) for fast retrieval. Then, we can use rerankers (cross-encoders) to get the `top-k` results for improving the results.

## Reranking demonstration

In the below example, note that `How many people live in New Delhi? No idea.` has the most lexical similarity so cos similarity / bi-encoder approach will say itâ€™s most relevant.

```python
from sentence_transformers import SentenceTransformer, CrossEncoder
import numpy as np

# Load the bi-encoder model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Load the cross-encoder model
cross_model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-12-v2")

# Function to calculate cosine similarity
def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

query = "How many people live in New Delhi?"
documents = [
    "New Delhi has a population of 33,807,000 registered inhabitants in an area of 42.7 square kilometers.",
    "In 2020, the population of India's capital city surpassed 33,807,000.",
    "How many people live in New Delhi? No idea.",
    "I visited New Delhi last year; it seemed overcrowded. Lots of people.",
    "New Delhi, the capital of India, is known for its cultural landmarks."
]

# Encode the query and documents using bi-encoder
query_embedding = embedding_model.encode(query)
document_embeddings = embedding_model.encode(documents)

# Calculate cosine similarities
scores = [cosine_similarity(query_embedding, doc_embedding) for doc_embedding in document_embeddings]

# Print initial retrieval scores
print("Initial Retrieval Scores (Bi-Encoder):")
for i, score in enumerate(scores):
    print(f"Doc{i+1}: {score:.2f}")

# Combine the query with each document for the cross-encoder
cross_inputs = [[query, doc] for doc in documents]

# Get relevance scores for each document using cross-encoder
cross_scores = cross_model.predict(cross_inputs)

# Print reranked scores
print("\nReranked Scores (Cross-Encoder):")
for i, score in enumerate(cross_scores):
    print(f"Doc{i+1}: {score:.2f}")
```

Outputs:
```
Initial Retrieval Scores (Bi-Encoder):
Doc1: 0.77
Doc2: 0.58
Doc3: 0.97
Doc4: 0.75
Doc5: 0.54

Reranked Scores (Cross-Encoder):
Doc1: 9.91
Doc2: 3.74
Doc3: 5.64
Doc4: 1.67
Doc5: -2.20
```

The outputs after better formatting demonstrate the effectiveness of the cross-encoder approach.

## HyDE (hypothetical document embeddings)

The userâ€™s query is most likely going to be in english and less likely to be in code. But our embeddings are mostly made up of code. If you think about the latent space, code would be nearer to code than english (natural language). This is the idea of the [HyDE paper](https://arxiv.org/abs/2212.10496).

You ask an LLM to generate a _hypothetical_ answer to your query and then you use this (generated) query for embedding search. The intuition is the that embedding of the hypothetical query is going to be closer in latent/embedding space to your data than your actual natural language query.

### Implementation details

The code used in this section is mainly from `app.py` and `prompts.py`.
It uses `gpt-4o-mini` for both HyDE queries as it's cheap, fast and decent with code-understanding capabilities. Here's how a HyDE query would look:

```python
# app.py
def openai_hyde(query):
    chat_completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": HYDE_SYSTEM_PROMPT
            },
            {
                "role": "user",
                "content": f"Help predict the answer to the query: {query}",
            }
        ]
    )
    return chat_completion.choices[0].message.content

# prompts.py
HYDE_SYSTEM_PROMPT = '''You are an expert software engineer. Your task is to predict code that answers the given query.

Instructions:
1. Analyze the query carefully.
2. Think through the solution step-by-step.
3. Generate concise, idiomatic code that addresses the query.
4. Include specific method names, class names, and key concepts in your response.
5. If applicable, suggest modern libraries or best practices for the given task.
6. You may guess the language based on the context provided.

Output format:
- Provide only the improved query or predicted code snippet.
- Do not include any explanatory text outside the code.
- Ensure the response is directly usable for further processing or execution.'''
```

The hallucinated query is used to perform an initial embedding search, retrieving the top 5 results from our tables. These results serve as context for a second HyDE query. In the first query, the programming language was not known but with the help of the fetched context, the language is most likely known now.

The second HyDE query is more context aware and expands the query with relevant code-specific details.

```python
# app.py
def openai_hyde_v2(query, temp_context, hyde_query):
    chat_completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": HYDE_V2_SYSTEM_PROMPT.format(query=query, temp_context=temp_context)
            },
            {
                "role": "user",
                "content": f"Predict the answer to the query: {hyde_query}",
            }
        ]
    )
    return chat_completion.choices[0].message.content

# prompts.py
HYDE_V2_SYSTEM_PROMPT = '''You are an expert software engineer. Your task is to enhance the original query: {query} using the provided context: {temp_context}.

Instructions:
1. Analyze the query and context thoroughly.
2. Expand the query with relevant code-specific details:
    - For code-related queries: Include precise method names, class names, and key concepts.
    - For general queries: Reference important files like README.md or configuration files.
    - For method-specific queries: Predict potential implementation details and suggest modern, relevant libraries.
3. Incorporate keywords from the context that are most pertinent to answering the query.
4. Add any crucial terminology or best practices that might be relevant.
5. Ensure the enhanced query remains focused and concise while being more descriptive and targeted.
6. You may guess the language based on the context provided.

Output format: Provide only the enhanced query. Do not include any explanatory text or additional commentary.'''
```

By leveraging the LLM's understanding of both code and natural language, it generates an expanded, more contextually-aware query that incorporates relevant code terminology and natural language descriptions. This two-step process helps bridge the semantic gap between the user's natural language query and the codebase's technical content.

A vector search is performed using the second query and top-K results are re-ranked using [answerdotai/answerai-colbert-small-v1](https://huggingface.co/answerdotai/answerai-colbert-small-v1) and then, the relevant metadata is fetched.

Note that references are fetched from meta data and combined with code to be feeded as context for the LLM.

```python
# app.py, from the function def generate_context(query, rerank=False)

hyde_query = openai_hyde(query)

method_docs = method_table.search(hyde_query).limit(5).to_pandas()
class_docs = class_table.search(hyde_query).limit(5).to_pandas()

temp_context = '\n'.join(method_docs['code'] + '\n'.join(class_docs['source_code']) )

hyde_query_v2 = openai_hyde_v2(query, temp_context)

logging.info("-query_v2-")
logging.info(hyde_query_v2)

method_search = method_table.search(hyde_query_v2)
class_search = class_table.search(hyde_query_v2)

if rerank: # if reranking is selected by user from the UI
    method_search = method_search.rerank(reranker)
    class_search = class_search.rerank(reranker)

method_docs = method_search.limit(5).to_list()
class_docs = class_search.limit(5).to_list()

top_3_methods = method_docs[:3]
methods_combined = "\n\n".join(f"File: {doc['file_path']}\nCode:\n{doc['code']}" for doc in top_3_methods)

top_3_classes = class_docs[:3]
classes_combined = "\n\n".join(f"File: {doc['file_path']}\nClass Info:\n{doc['source_code']} References: \n{doc['references']}  \n END OF ROW {i}" for i, doc in enumerate(top_3_classes))
```

All the above context is plugged into an LLM that chats with the user. `gpt-4o` is for this task as we need a strong LLM for reasoning over the retrieved context and chatting in natural language with the user.

We have completed our RAG experiment here. In our analogy with Cursor, when the user mentions `@codebase`, then only embeddings are retrieved otherwise existing context is used. If the LLM
is not confident about the context, it will tell the user to mention `@codebase` (because of the system prompt).

```python
# app.py, from the function def home()
    rerank = True if rerank in [True, 'true', 'True', '1'] else False

            if '@codebase' in query:
                query = query.replace('@codebase', '').strip()
                context = generate_context(query, rerank)
                app.logger.info("Generated context for query with @codebase.")
                app.redis_client.set(f"user:{user_id}:chat_context", context)
            else:
                context = app.redis_client.get(f"user:{user_id}:chat_context")
                if context is None:
                    context = ""
                else:
                    context = context.decode()

            # Now, apply reranking during the chat response if needed
            response = openai_chat(query, context[:12000])  # Adjust as needed
```

This completes the walkthrough!

## Possible improvements

### Latency

There are a lot of improvements possible, especially in the latency department. Currently, queries tagged with `@codebase` and no re-ranking take about 10-20 seconds. With re-ranking enabled, the retrieval is in 20-30 second range (which is definitely slow). However, if you look at the user expectations with tools like Cursor, users seem happy with latencies hovering around the commonly seen 15-20 second mark.

- The easiest way to cut latency is to use an LLM like `Llama3.1 70B` from a high throughput inference provider like Groq or Cerebras. This should cut latency by atleast 10 seconds (speed up the HyDE calls by gpt4o-mini) -- though, this may impact retrieval quality due to the capabilities of the LLM. You can try out using local embeddings to tackle network latency while balancing its effects with cost/speed.

- Our latency bottleneck is also in the HyDE queries. Is it possible to reduce them to one or totally eliminate them altogether? Maybe a combination of BM25 based keyword search on the [repository map](https://aider.chat/docs/repomap.html) can help reduce at least one HyDE query.

### Accuracy

The low hanging fruit to improve accuracy is:

- Try better embeddings or use fine-tuned embeddings
- Implement evals and optimize accuracy with help of a feedback loop

## Conclusion

Throughout this two-part series, we've explored the intricacies of building an effective code question-answering system. In [Part 1](/blog/building-rag-on-codebases-part-1), we laid the foundation by discussing the importance of proper codebase chunking and semantic code search. In this post, we went deeper into advanced techniques to enhance retrieval quality:

- We explored how LLM-generated comments can bridge the gap between code and natural language queries
- We discussed the critical considerations for choosing embeddings and vector databases
- We examined various techniques to improve retrieval accuracy, including:

- Hybrid search combining semantic and keyword-based approaches
- The power of cross-encoders for reranking results
- Using HyDE to bridge the semantic gap between natural language queries and code

Hopefully, this series has provided valuable insights into building practical code QA systems, with LanceDB in the mix! The complete implementation is available on [GitHub](https://github.com/sankalp1999/code_qa), and we encourage you to experiment with these techniques in your own projects.

Thank you for reading!

## Learn more about CodeQA

- Check out the code on[Github](https://github.com/sankalp1999/code_qa).
- See a live demo [here](https://youtu.be/K4IoQf2Wf8I?si=eKYKztWax0sYhE_-).

## References

Links are listed in the order they appear in the post:

1. [Part 1 blog post](/blog/building-rag-on-codebases-part-1)
2. [Three LLM tricks that boosted embeddings search accuracy by 37% â€” Cosine](https://www.buildt.ai/blog/3llmtricks)
3. [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
4. [JinaAI's fine-tuning API announcement](https://x.com/JinaAI_/status/1785337862755356685)
5. [How Cody understands your codebase](https://sourcegraph.com/blog/how-cody-understands-your-codebase)
6. [Cody on GitHub](https://github.com/sourcegraph/cody)
7. [LanceDB](https://lancedb.com/)
8. [FAISS by Meta](https://github.com/facebookresearch/faiss)
9. [create_tables.py on GitHub](https://github.com/sankalp1999/code_qa/blob/main/create_tables.py)
10. [Twitter thread on embedding search shortcomings](https://x.com/eugeneyan/status/1767403777139917133)
11. [Semantweet search on GitHub](https://github.com/sankalp1999/semantweet-search)
12. [BM25 benchmarks on LanceDB site](https://lancedb.github.io/lancedb/hybrid_search/eval/)
13. [Cohere reranker v3](https://cohere.com/blog/rerank-3)
14. [ColBERT and late interaction](https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/)
15. [HyDE paper](https://arxiv.org/abs/2212.10496)
16. [Ragas documentation](https://docs.ragas.io/en/stable/)
```

---

## ðŸ“„ æ–‡ä»¶: case-study-coderabbit.md

---

```md
---
title: "Case Study: How CodeRabbit Leverages LanceDB for AI-Powered Code Reviews"
date: 2025-09-03
draft: false
featured: false
categories: ["Case Study"]
image: /assets/blog/case-study-coderabbit/preview-image.png
meta_image: /assets/blog/case-study-coderabbit/preview-image.png
description: "How CodeRabbit leverages LanceDB-powered context engineering turns every review into a quality breakthrough."
author: "Qian Zhu"
author_avatar: "/assets/authors/qian-zhu.jpg"
author_bio: "Software Engineer at LanceDB specializing in cloud infrastructure, AI-powered development tools, and knowledge management systems."
author_github: "QianZhu"
author_linkedin: "qianzhu56"
author_twitter: "QianZhu"
---

[CodeRabbit](https://www.coderabbit.ai/) is the industryâ€™s most advanced AI code reviewer with thousands of customers and used by over 100,000 open-source projects. Its unique [context engineering](https://www.coderabbit.ai/blog/context-engineering-ai-code-reviews) approach designed specifically for code reviews helps catch difficult to find bugs, refactor code, identify edge cases, and improve overall code quality.

While other code review tools merely skim the surface looking for simple bugs, [CodeRabbit](https://www.coderabbit.ai/) ingests critical context from multiple data sources intelligently: past PRs, Jira/Linear tickets to reveal developer intent, code graph analysis, custom instructions for personalization of reviews, and chat based learnings from users. The result? Reviews with human-like precision that reduce PR merge time and bugs in half. 

At the core of [CodeRabbit's](https://www.coderabbit.ai/) tech stack lies [LanceDB](https://lancedb.com/),  the vector database that transforms chaotic context into real-time architectural insight. While other solutions buckle under scale, LanceDB delivers **millisecond semantic searches** across [CodeRabbit's](https://www.coderabbit.ai/) living knowledge graph: dynamically querying tens of thousands of tables (PRs, issues, code dependencies, tribal learnings) indexing **millions of daily code interactions**. 

> "LanceDB transformed how we handle context at scale. While other vector databases hit cost and performance walls, LanceDB scales effortlessly with our growthâ€”from startup to enterprise. Its multimodal capabilities and deployment flexibility were game-changers, enabling us to deliver the depth of analysis our customers expect while maintaining sub-second response times across millions of code reviews."
>
> â€” Rohit Khanna, VP of Engineering at CodeRabbit

This is the contextual understanding that is critical for modern, complex codebases. Designed for enterprise customers, LanceDB supports cloud and on-prem deployments with secure, low-latency performance. As [CodeRabbit's](https://www.coderabbit.ai/) customer base exploded, LanceDB scaled effortlessly, preserving speed while avoiding spiraling infrastructure costs. 

The result: every pull request improves code quality, while LanceDB handles the complexity behind the scenes.

## Introduction: CodeRabbit - defining the future of AI-powered code reviews

In the AI coding era where the pace of software generation is far more than human review capacity, [CodeRabbit](https://www.coderabbit.ai/) has emerged as the category leader in AI-powered code reviews. Trusted by engineering teams across industries to deliver speed, precision, and scale. Purpose-built for modern development, [CodeRabbit](https://www.coderabbit.ai/) doesn't just assist with reviews, it transforms them into a competitive advantage.

Today, [CodeRabbit](https://www.coderabbit.ai/) performs tens of thousands of code reviews every day, proving its ability to support enterprise-scale workflows without compromising quality. Its seamless integration with IDEs like VS Code, Cursor, and Windsurf with its lightweight extension has fueled widespread adoption, from fast-moving startups to some of the worldâ€™s most demanding companies. By delivering reviews that are fast, context-aware, secure, and remarkably accurate, [CodeRabbit](https://www.coderabbit.ai/) is defining how developers impose guardrails on AI generated code.

[CodeRabbit](https://www.coderabbit.ai/) impact:
- **Massive Throughput**: Processes millions of pull requests monthly, proving enterprise-grade reliability and scale.
- **Productivity Gains**: Customers report 50%+ reduction in manual review effort, with some teams achieving 80%+ faster development cycles.
- **Fewer Issues**: AI-powered context analysis reduces code issues by up to 50% when compared with manual code reviews
- **Flexible Integration**: Instantly usable across leading Git platforms and code editors, delivering real-time feedback without disrupting developer flow.

## Challenge: context-aware code reviews at scale

For [CodeRabbit](https://www.coderabbit.ai/) to deliver AI code reviews at the level of a senior developer, it must construct meaningful context from fragmented signals across massive and evolving codebases. Unlike static linting tools, effective AI review requires connecting the dots across code changes, architectural patterns, historical decisions, and team conventions, all in real time.

This challenge unfolds across three layers:

#### 1. Code Layer â€“ review beyond the diff
Most pull requests appear self-contained, but reviewing them requires understanding changes across files, modules, and dependency chains. Understanding the actual impact means going beyond the diff:

    - The "Grep Trap": regex and rule-based tools fall short when semantics matterâ€”like tracing
    all authentication logic linked to `userAuth()`, even when naming or patterns vary across services.
    - Code relationships: finding relevant code to what was updated in the PR.

#### 2. Project Layer â€“ keep up with changing architecture
Todayâ€™s codebases are constantly evolving with new architectural decisions coming in place. Understanding change impact means capturing cross-repository relationships and historical design intent:

    - Dependency graph limitations: static analysis often misses runtime and dynamic behavior,
    or architectural coupling between microservices.
    - Historical blind spots: critical context, such as why a fix was made, often lives in old PRs, Jira 
    tickets, or Slack threads. These signals are valuable but scattered.

#### 3. Human Layer â€“ intent, convention, and tribal knowledge
Even perfect syntax and architecture can violate design intent. Great reviews demand cultural awareness:

    - Tribal knowledge gap: Teams follow unspoken rulesâ€”like â€œinput validation always lives in
    lib/securityâ€â€”that arenâ€™t enforced by compilers or captured in documentation.
    - Architectural drift: A "simple" refactor might unintentionally break invisible service boundaries 
    or violate implicit contracts.

**And Then, Volatility**

Context is high-dimensional, unstructured, and constantly evolving. Every new commit, comment, and design decision reshapes what matters.

> "Retrieving the right context is like finding needles in a million haystacks... while the haystacks are constantly reshaping themselves."

### Why semantic search wasn't enough
Thereâ€™s no question that building this kind of context demands deep semantic intelligence. [CodeRabbit](https://www.coderabbit.ai/) initially adopted a popular closed-source database to enable vector-based similarity search across its knowledge graph. It was a step in the right direction, but only part of the solution:
- **Cost at scale**: As usage expanded to millions of pull requests and tens of thousands of daily queries, their pricing model became unsustainable. Real-time retrieval over massive repositories incurred runaway costs, especially when processing large amounts of text.
- **Deployment constraints**: The existing product offered no viable path for true on-premises deployment. For [CodeRabbit's](https://www.coderabbit.ai/) enterprise customers, many operating in regulated or air-gapped environments, this was a dealbreaker. They required strict data residency guarantees and full infrastructure control.

### Why LanceDB became the only solution that worked

Scaling semantic context retrieval in real-world environments meant looking for other alternatives. [CodeRabbit](https://www.coderabbit.ai/) needed a vector database designed not for theoretical benchmarks, but for the messy, high-velocity realities of software engineering.

| Task | [CodeRabbit](https://www.coderabbit.ai/) Requirements |
|---------|-------------|
| Semantic search at scale | Retrieve relevant context for 50K+ daily PRs with P99 latency under 1 second |
| Metadata filtering | Combine embeddings with filters (e.g., "Go files on auth modified after Jan 2024") |
| Dynamic indexing | Ingest and optimize new context types on the fly |
| High velocity upserts | Continuously handle new commits, no downtime, no re-indexing |
| Infrastructure agnostic | Seamless cloud + on-prem deployment to meet strict enterprise security and data residency needs |
|Cost-effective | Sustain performance even as traffic scales 100x, without runaway costs |

#### Enter LanceDB: Purpose-Built for Real-World AI Scaling
LanceDB solved [CodeRabbit's](https://www.coderabbit.ai/) challenges decisively, unlocking a new level of performance, security, and efficiency:
- **Cost efficiency**: with its optimized compute and storage design, LanceDB maintains sub-second latency and stable costs, even as usage scales 100x.
- **On-Prem deployments**: a single lightweight binary deploys in secure, air-gapped environments in minutesâ€”no infrastructure rework, no code changes required.

**Results**: [CodeRabbit](https://www.coderabbit.ai/) now delivers real-time, architecture-aware code reviews at enterprise scale, without sacrificing performance, security, or cost. With LanceDB at its core, [CodeRabbit](https://www.coderabbit.ai/) reviews every pull request and becomes a centralized software quality guardrail.

## Solution: LanceDB as the context engine behind CodeRabbit
To solve the complexity of real-time, context-aware code review, [CodeRabbit](https://www.coderabbit.ai/) built a sophisticated context engineering pipeline powered by LanceDB, a vector database purpose-built for semantic retrieval at scale. LanceDB enables [CodeRabbit](https://www.coderabbit.ai/) to ingest, embed, and retrieve high-dimensional knowledge across fragmented sources, serving as the brain behind its AI review engine.

![](/assets/blog/case-study-coderabbit/coderabbit-arch.png)
**Figure 1:** LanceDB as part of CodeRabbit's architecture

#### Engineering with LanceDB
At the heart of [CodeRabbit's](https://www.coderabbit.ai/) architecture is its Context Engineering approach, which continuously synthesizes code, known issues, and review instructions stored and queried through LanceDB. This system empowers [CodeRabbit](https://www.coderabbit.ai/) to surface relevant, project-specific insights in milliseconds for every code review. More details on how [CodeRabbit](https://www.coderabbit.ai/) works behind the scenes can be found in their [engineering blog](https://www.coderabbit.ai/blog/coderabbit-deep-dive).

####  Multi-source data ingestion
LanceDB ingests and indexes data from diverse sources:
    - Code structure: captures both syntactic and semantic structure.
    - Issue trackers: Jira and Linear tickets provide intent and problem context.
    - Historical reviews: Past pull requests and decisions enable pattern recognition

####  Real-time retrieval & dynamic Context Assembly
When a pull request is opened, LanceDB powers a real-time context retrieval pipeline:
    - Embedding generation: newly pushed code is parsed and embedded
    - Semantic lookup: LanceDB retrieves all information relevant to the review


####  LLM-orchestrated review, powered by Vector Search
The retrieved context drives [CodeRabbit's](https://www.coderabbit.ai/) LLM review engine, enabling precise and actionable feedback:
    - LanceDB-sourced results feed into task-specific LLMs.
    - Similar past reviews and fixes improve future LLM comments
    - Details from linked issues ensure that reviews are aligned with goals

####  Always-on learning loop
LanceDB continuously evolves as [CodeRabbit](https://www.coderabbit.ai/) learns from new data:
    - Live feedback integration: developer chats and PR outcomes are re-embedded
    - Auto-updating vectors: updates are triggered automatically without manual reindexing.
    - Pattern drift detection: As team behaviors evolve, the vector index adapts automatically.

By integrating LanceDB as the backbone of its context engine, [CodeRabbit](https://www.coderabbit.ai/) transforms fragmented engineering signals into actionable intelligence, powering real-time, architecture-aware code reviews that scale with confidence.

> â€œLanceDB was the only vectordb that fit our unique usecase - scaled to a large number of customers with massive tables and supported running on various custom environments. This helped us land more customers, and grow faster.â€
> 
> Ganesh Patro, software engineer at [CodeRabbit](https://www.coderabbit.ai/)

## Summary
LanceDB has become the semantic engine behind [CodeRabbit's](https://www.coderabbit.ai/) evolution into a category-defining AI code review platform, unlocking both massive scale and precision that was previously unattainable. LanceDB has empowered [CodeRabbit](https://www.coderabbit.ai/) to deliver:
- **Sub-second latency** for semantic search across millions of code interactions
- **Millions of pull requests processed monthly**: LanceDB's optimized upsert and indexing pipeline ensures real-time ingestion without any performance degradation
- **Effortless on-prem and cloud deployment**: LanceDB enables [CodeRabbit](https://www.coderabbit.ai/) to run securely in both air-gapped enterprise and cloud environments without code changes or operational overhead

Looking ahead, [CodeRabbit](https://www.coderabbit.ai/) is setting its sights on redefining the future of AI-native engineering. The team is expanding its context graph to encompass even richer signals, runtime traces, architectural diagrams, CI/CD events, and more, creating a living blueprint of every system it touches. 

With deeper integrations into planning tools and observability stacks, [CodeRabbit](https://www.coderabbit.ai/) aims to not just review code, but act as a centralized governance layer identifying risks and guiding developers with insights drawn from millions of engineering years worth of built-in intelligence.
```

---

## ðŸ“„ æ–‡ä»¶: case-study-cognee.md

---

```md
---
title: "How Cognee Builds AI Memory Layers with LanceDB"
date: 2025-09-23
draft: false
featured: false
categories: ["Case Study"]
image: /assets/blog/case-study-cognee/preview-image.jpg
meta_image: /assets/blog/case-study-cognee/preview-image.jpg
description: "How Cognee uses LanceDB to deliver durable, isolated, and low-ops AI memory from local development to managed production."
author: "David Myriel"
author_avatar: "/assets/authors/david-myriel.jpg"
author_bio: "Writer."
author_github: "davidmyriel"
author_linkedin: "davidmyriel"
author_twitter: "davidmyriel"
author2: "Vasilije Markovic"
author2_avatar: "/assets/authors/vasilije-markovic.jpg"
author2_bio: "Founder @ Cognee"
author2_github: "Vasilije1990"
author2_linkedin: "vasilije-markovic-13302471"
author2_twitter: "cognee_"
---

## Company Overview

[Cognee](https://cognee.ai) helps agents retrieve, reason, and remember with context that has structure and time. It ingests sources such as files, APIs, and databases, then [chunks and embeds content](https://docs.cognee.ai/core-concepts/overview), enriches entities and relations, and builds a queryable [knowledge graph](https://docs.cognee.ai/core-concepts/main-operations/cognify). 

Applications built on [Cognee](https://cognee.ai) query both the graph and the vectors to answer multi-step questions with clearer provenance. This platform is designed for teams building autonomous agents, copilots, and search in knowledge-heavy domains.

**Figure 1:** Query and transform your LLM context anywhere using cognee's memory engine. 

![cognee](/assets/blog/case-study-cognee/cognee.jpg)

*The Cognee interface enables developers to build and query AI memory systems with both graph and vector search capabilities.*

## The Challenge

Agent teams often struggle with stateless context and homegrown RAG stacks. They juggle a graph here, a vector store there, and ad hoc rules in between. This raises reliability risks and slows iteration. [Cognee](https://cognee.ai) needed a vector store that matched its isolation model, supported per-workspace development, and stayed simple for day-to-day work.

### The Isolation Problem

Cognee's isolation model is the way it separates memory stores so that every developer, user, or test instance gets its own fully independent workspace. Instead of sharing a single vector database or graph across contexts, Cognee spins up a dedicated set of backendsâ€”most notably a [LanceDB store](https://lancedb.com/docs/)â€”for each workspace.

This approach solves a critical problem in AI development: when multiple developers work on the same project, or when running tests in parallel, shared state can lead to unpredictable behavior, data corruption, and debugging nightmares. Traditional vector databases require complex orchestration to achieve this level of isolation.

## LanceDB as a Solution

[Cognee](https://cognee.ai) chose [LanceDB](https://lancedb.com) because it fits the way engineers actually build and test memory systems. Since [LanceDB](https://lancedb.com) is [file based](https://lancedb.com/docs/overview/), [Cognee](https://cognee.ai) can spin up a separate store per test, per user, and per workspace. 

### Why File-Based Storage Matters

Developers run experiments in parallel without shared state, and they avoid the overhead of managing a separate vector database service for every sandbox. This approach shortens pull-request cycles, improves demo reliability, and makes multi-tenant development more predictable.

Unlike traditional vector databases that require running servers, [LanceDB's file-based approach](https://lancedb.com/docs/overview/) means each workspace gets its own directory on disk. This eliminates the complexity of managing database connections, ports, and service dependencies during development and testing.

{{< admonition "note" "Data and embeddings together" >}}
[LanceDB](https://lancedb.com) stores original data and embeddings together through the [Lance columnar format](https://lancedb.com/docs/overview/lance/). Pipelines become simpler because there is no extra glue code to keep payloads and vectors consistent.
{{< /admonition >}}

## How the Pieces Fit

[Cognee](https://cognee.ai) delivers a durable memory layer for AI agents by unifying a knowledge graph with high-performance vector search. It ingests files, APIs, and databases, then applies an [Extractâ€“Cognifyâ€“Load model](https://docs.cognee.ai/) to chunk content, enrich entities and relationships, add temporal context, and write both graph structures and embeddings for retrieval.

[LanceDB](https://lancedb.com) is the default vector database in this stack, which keeps embeddings and payloads close to each other and removes extra orchestration. Because [LanceDB](https://lancedb.com) is [file based](https://lancedb.com/docs/overview/), [Cognee](https://cognee.ai) can provision a clean store per user, per workspace, and per test, so teams iterate quickly without shared state or heavy infrastructure.

### From Development to Production

This architecture scales from a laptop to production without changing the mental model. Developers can start locally with [Cognee](https://cognee.ai)'s UI, build and query memory against [LanceDB](https://lancedb.com), and validate behavior with isolated sandboxes.

![Cognee architecture diagram](/assets/blog/case-study-cognee/cognee_architecture_diagram.jpg)

*The Cognee architecture shows how data flows from sources through the ECL pipeline to both graph and vector storage backends.*

When it is time to go live, the same project can move to Cognee's hosted service - [cogwit](https://platform.cognee.ai/), which manages [Kuzu](https://kuzudb.com) for graph, [LanceDB](https://lancedb.com) for vectors, and [PostgreSQL](https://postgresql.org) for metadata, adding governance and autoscaling as needed. With Cognee UI, teams can switch between local and cloud (cogwit) environments easily.

[Cognee](https://cognee.ai)'s Memify pipeline keeps memory fresh after deployment by cleaning stale nodes, strengthening associations, and reweighting important facts, which improves retrieval quality without full rebuilds.

The result is a simpler and more reliable path to agent memory than piecing together separate document stores, vector databases, and graph engines.

### Local Development Workflow

This diagram shows sources flowing into [Cognee](https://cognee.ai) on a developer's machine, with each workspace writing to its own [LanceDB](https://lancedb.com) store. The benefit is clean isolation: tests and demos never collide, sandboxes are easy to create and discard, and engineers iterate faster without running a separate vector database service.

Each workspace operates independently, with its own:
- Vector embeddings stored in [LanceDB](https://lancedb.com/docs/storage/)
- Knowledge graph structure
- Metadata and temporal context
- Query and retrieval capabilities

**Figure 2:** Local-first memory with per-workspace isolation

```mermaid
flowchart LR
  %% Figure 1: Local-first memory with per-workspace isolation
  subgraph S["Sources"]
    s1[Files]
    s2[APIs]
  end

  subgraph C["Cognee (local)"]
    c1[Ingest]
    c2[Cognify]
    c3[Load]
  end

  subgraph L["LanceDB (per workspace)"]
    l1[/.../workspace-a/vectors/]
    l2[/.../workspace-b/vectors/]
  end

  Agent[Agent or App]

  s1 --> c1
  s2 --> c1
  c1 --> c2
  c2 --> c3

  %% Write vectors per isolated workspace
  c3 --> l1
  c3 --> l2

  %% Query path
  Agent -->|query| c2
  c2 -->|read/write| l1
  l1 -->|results| c2
  c2 -->|answers| Agent
```
### User Interface

The Cognee Local interface provides a workspace for building and exploring AI memory directly on a developer's machine. 

From the sidebar, users can connect to local or cloud instances, manage datasets, and organize work into notebooks. Each notebook offers an interactive environment for running code, querying data, and visualizing results. 

![Cognee Local UI](/assets/blog/case-study-cognee/cognee-ui-screenshot.png)

*The Cognee Local UI provides an intuitive interface for building and querying AI memory systems with both graph and vector search capabilities.*

The UI supports importing data from multiple sources, executing searches with graph or vector retrieval, and inspecting both natural-language answers and reasoning graphs. It is designed to make experimentation straightforwardâ€”users can ingest data, build structured memory, test retrieval strategies, and see how the system interprets and connects entities, all within a single environment. This local-first approach makes it easy to prototype, debug, and validate memory pipelines before moving them to a hosted service.

### Memory Processing Pipeline

[Cognee](https://cognee.ai) converts unstructured and structured inputs into an evolving knowledge graph, then couples it with embeddings for retrieval. The result is a memory that improves over time and supports graph-aware reasoning.

This pipeline processes data through three distinct phases:
1. **Extract**: Pull data from various sources (files, APIs, databases)
2. **Cognify**: Transform raw data into structured knowledge with embeddings
3. **Load**: Store both graph and vector data for efficient retrieval

**Figure 2: Cognee memory pipeline**

```mermaid
flowchart LR
  %% Figure 2: Extract -> Cognify -> Load
  E[Extract]

  subgraph CG["Cognify"]
    cg1[Chunk]
    cg2[Embed]
    cg3[Build knowledge graph]
    cg4[Entity and relation enrichment]
    cg5[Temporal tagging]
  end

  subgraph LD["Load"]
    ld1[Write graph]
    ld2[Write vectors]

    subgraph Backends
      b1[(Kuzu)]
      b2[(LanceDB)]
    end

    svc[Serve graph and vector search]
  end

  E --> cg1 --> cg2 --> cg3 --> cg4 --> cg5
  cg5 --> ld1
  cg5 --> ld2

  ld1 --> b1
  ld2 --> b2

  b1 --> svc
  b2 --> svc
```

Here the Extract, Cognify, and Load stages turn raw inputs into a [knowledge graph and embeddings](https://www.cognee.ai/blog/deep-dives/cognee-graphrag-supercharging-search-with-knowledge-graphs-and-vector-magic), then serve both graph and vector search from [Kuzu](https://kuzudb.com) and [LanceDB](https://lancedb.com). Users gain higher quality retrieval with provenance and time context, simpler pipelines because payloads and vectors stay aligned, and quicker updates as memory evolves.

### Production Deployment Path

Teams begin locally and promote the same model to Cognee's hosted service - [cogwit](https://platform.cognee.ai/) - when production requirements such as scale and governance come into play. This seamless transition is possible because the same data structures and APIs work in both local and hosted environments.

Learn more about [LanceDB](https://lancedb.com) and explore [Cognee](https://cognee.ai).

**Figure 3: Hosted path with Cognee's production service**

```mermaid
flowchart LR
  %% Figure 3: Local -> Cogwit hosted path
  LUI[Local Cognee UI]

  subgraph CW["Cognee Hosted Service"]
    APIs[Production memory APIs]
    note1([Autoscaling â€¢ Governance â€¢ Operations])

    subgraph Backends
      K[(Kuzu)]
      V[(LanceDB)]
      P[(PostgreSQL)]
    end
  end

  Teams[Teams or Agents]

  LUI -->|push project| CW
  APIs --> K
  APIs --> V
  APIs --> P

  Teams <-->|query| APIs
```
This figure shows a smooth promotion from the local UI to [cogwit](https://platform.cognee.ai/) - Cognee's hosted service, which manages [Kuzu](https://kuzudb.com), [LanceDB](https://lancedb.com), and [PostgreSQL](https://postgresql.org) behind production APIs. Teams keep the same model while adding autoscaling, governance, and operational controls, so moving from prototype to production is low risk and does not require a redesign.

## Why LanceDB fit

[LanceDB](https://lancedb.com) aligns with [Cognee](https://cognee.ai)'s isolation model. Every test and every user workspace can have a clean database that is trivial to create and remove. This reduces CI friction, keeps parallel runs safe, and makes onboarding faster. When teams do need more control, [LanceDB](https://lancedb.com) provides modern indexing options such as [IVF-PQ](https://lancedb.com/docs/search/vector-search/) and [HNSW-style graphs](https://lancedb.com/docs/search/vector-search/), which let engineers tune recall, latency, and footprint for different workloads.

### Technical Advantages

Beyond isolation, [LanceDB](https://lancedb.com) offers several technical advantages that make it ideal for Cognee's use case:

- **Unified Storage**: Original data and embeddings stored together eliminate synchronization issues
- **Zero-Copy Evolution**: Schema changes don't require data duplication
- **Hybrid Search**: Native support for both [vector search](https://lancedb.com/docs/search/vector-search/) and full-text search
- **Performance**: Optimized for both interactive queries and batch processing
- **Deployment Flexibility**: Works locally, in containers, or in cloud environments

{{< admonition "note" "Indexing Data" >}}
Treat index selection as a product decision. Start with defaults to validate behavior, then profile and adjust the index and quantization settings to meet your target cost and latency.
{{< /admonition >}}

## Results

These results show that the joint Cogneeâ€“LanceDB approach is not just an internal efficiency gain but a direct advantage for end users. Developers get faster iteration and clearer insights during prototyping, while decision makers gain confidence that the memory layer will scale with governance and operational controls when moved to production. This combination reduces both time-to-market and long-term maintenance costs.

### Development Velocity Improvements

> "LanceDB gives us effortless, truly isolated vector stores per user and per test, which keeps our memory engine simple to operate and fast to iterate." </br> - Vasilije Markovic, Cognee CEO

Using LanceDB has accelerated Cognee's development cycle and improved reliability. File-based isolation allows each test, workspace, or demo to run on its own vector store, enabling faster CI, cleaner multi-tenant setups, and more predictable performance. Onboarding is simpler since developers can start locally without provisioning extra infrastructure.

### Product Quality Gains

At the product level, combining Cognee's knowledge graph with [LanceDB's vector search](https://lancedb.com/docs/search/vector-search/) has improved retrieval accuracy, particularly on multi-hop reasoning tasks like [HotPotQA](https://hotpotqa.github.io). The Memify pipeline further boosts relevance by refreshing memory without full rebuilds. Most importantly, the same local setup can scale seamlessly to [cogwit](https://platform.cognee.ai/) - Cognee's hosted service, giving teams a direct path from prototype to production without redesign.

Key benefits include:
- **Faster development cycles** due to isolated workspaces
- **Reduced environment setup time** with file-based storage
- **Improved multi-hop reasoning accuracy** with graph-aware retrieval
- **Seamless deployments** when scaling to production

## Recent product releases at Cognee

[Cognee](https://cognee.ai) has shipped a focused set of updates that make the memory layer smarter and easier to operate. A new local UI streamlines onboarding. The [Memify](https://www.cognee.ai/blog/cognee-news/product-update-memify) post-processing pipeline keeps the [knowledge graph](https://en.wikipedia.org/wiki/Knowledge_graph) fresh by cleaning stale nodes, adding associations, and reweighting important memories without a full rebuild.

### Memify Pipeline in Action

The Memify pipeline represents a significant advancement in AI memory management, enabling continuous improvement without costly rebuilds. 

Here is a typical example of the [Memify pipeline](https://docs.cognee.ai/core-concepts/main-operations/memify) for post-processing knowledge graphs:

```python
import asyncio
import cognee
from cognee import SearchType

async def main():
    # 1) Add two short chats and build a graph
    await cognee.add([
        "We follow PEP8. Add type hints and docstrings.",
        "Releases should not be on Friday. Susan must review PRs.",
    ], dataset_name="rules_demo")
    await cognee.cognify(datasets=["rules_demo"])  # builds graph

    # 2) Enrich the graph (uses default memify tasks)
    await cognee.memify(dataset="rules_demo")

    # 3) Query the new coding rules
    rules = await cognee.search(
        query_type=SearchType.CODING_RULES,
        query_text="List coding rules",
        node_name=["coding_agent_rules"],
    )
    print("Rules:", rules)

asyncio.run(main())
```

Self-improving memory logic and time awareness help agents ground responses in what has changed and what still matters. A private preview of [graph embeddings](https://en.wikipedia.org/wiki/Graph_embedding) explores tighter coupling between structure and retrieval. [Cognee](https://cognee.ai) also reports strong results on graph-aware evaluation, including high correctness on multi-hop tasks such as [HotPotQA](https://hotpotqa.github.io). Cognee's hosted service - [cogwit](https://platform.cognee.ai/) opens this experience to teams that want managed backends from day one.

### What's Next

Looking ahead, Cognee is focusing on:
- Enhanced graph embedding capabilities for better structure-retrieval coupling
- Improved time-aware reasoning for dynamic knowledge updates
- Expanded integration options for enterprise workflows
- Advanced analytics and monitoring for production deployments

{{< admonition "note" "ECL Model" >}}
[Cognee](https://cognee.ai) follows an ECL model: Extract data, Cognify into graphs and embeddings, and Load into graph and vector backends. The model maps cleanly to [LanceDB](https://lancedb.com)'s unified storage and indexing.
{{< /admonition >}}

## Why this joint solution is better

Many stacks bolt a vector database to an unrelated document store and a separate graph system. That adds orchestration cost and creates brittle context. By storing payloads and embeddings together, [LanceDB](https://lancedb.com) reduces integration points and improves locality. [Cognee](https://cognee.ai) builds on this to deliver graph-aware retrieval that performs well on multi-step tasks, while the Memify pipeline improves memory quality after deployment rather than forcing rebuilds. The combination yields higher answer quality with less operational churn, and it does so with a simple path from a laptop to a governed production environment.

### The Integration Advantage

Traditional approaches require complex orchestration between multiple systems:
- Vector database for similarity search
- Graph database for relationships
- Document store for original content
- Custom glue code to keep everything synchronized

Cognee + LanceDB eliminates this complexity by providing a unified platform that handles all these concerns natively.

{{< admonition >}}
Do not treat memory as a thin [RAG layer](https://en.wikipedia.org/wiki/Retrieval-augmented_generation). [Cognee](https://cognee.ai)'s structured graph, time awareness, and post-processing combine with [LanceDB](https://lancedb.com)'s tuned retrieval to create durable and auditable context for agents.
{{< /admonition >}}

### Getting Started is Simple

Developers can start local, install [Cognee](https://cognee.ai), and build a memory over files and APIs in minutes. [LanceDB](https://lancedb.com) persists embeddings next to the data, so indexing and queries remain fast. When the prototype is ready, they can push to [cogwit](https://platform.cognee.ai/) - Cognee's hosted service and inherit managed [Kuzu](https://kuzudb.com), [LanceDB](https://lancedb.com), and [PostgreSQL](https://postgresql.org) without changing how they think about the system. Technical decision makers get a simpler architecture, a clear upgrade path, and governance features when needed.

### The Business Case

For organizations, this means:
- **Faster time-to-market**: No complex infrastructure setup required
- **Lower operational overhead**: Unified platform reduces maintenance burden
- **Better developer experience**: Local development matches production environment
- **Reduced vendor lock-in**: Open source components provide flexibility
- **Scalable architecture**: Grows from prototype to enterprise without redesign

## Getting started

1. Install [Cognee](https://github.com/topoteretes/cognee), build a small memory with the local [LanceDB](https://lancedb.com) store, and enable [Memify](https://docs.cognee.ai/core-concepts/main-operations/memify) to see how post-processing improves relevance. 
2. When production [SLAs](https://en.wikipedia.org/wiki/Service-level_agreement) and compliance become priorities, promote the same setup to [cogwit](https://platform.cognee.ai/) - Cognee's hosted service and gain managed scale and operations.

### Next Steps

Ready to build AI memory systems that scale? Start with the [Cognee documentation](https://docs.cognee.ai) and explore how [LanceDB](https://lancedb.com) can power your next AI application. The combination of local development simplicity and production scalability makes this stack ideal for teams building the future of AI.

{{< admonition "callout" "Local MCP Pattern" >}}
[Cognee](https://cognee.ai)'s local [MCP pattern](https://modelcontextprotocol.io) keeps data on the machine and uses [LanceDB](https://lancedb.com) and [Kuzu](https://kuzudb.com) under the hood. This approach is a strong fit for teams with strict data boundaries.
{{< /admonition >}}
```

---

## ðŸ“„ æ–‡ä»¶: case-study-dosu.md

---

```md
---
title: "Case Study: Meet Dosu - the Intelligent Knowledge Base for Software Teams and Agents"
date: 2025-07-31
draft: false
featured: false
categories: ["Case Study"]
image: /assets/blog/case-study-dosu/preview-image.png
meta_image: /assets/blog/case-study-dosu/preview-image.png
description: "How Dosu uses LanceDB to transform codebases into living knowledge bases with real-time search and versioning."
author: "Qian Zhu"
author_avatar: "/assets/authors/qian-zhu.jpg"
author_bio: "Software Engineer at LanceDB specializing in cloud infrastructure, AI-powered development tools, and knowledge management systems."
author_github: "QianZhu"
author_linkedin: "qianzhu56"
author_twitter: "QianZhu"
author2: "Michael Ludden"
author2_avatar: "/assets/authors/michael-ludden.jpg"
author2_bio: "VP of Marketing | Advisor | Head of Growth at Dosu"
author2_linkedin: "mludden"
author2_twitter: "michael_ludden"
---

## About Dosu

[Dosu](https://dosu.dev) is giving time back to engineers by turning your codebase into a living knowledge base, so you can build instead of answering questions and maintaining docs all day. It generates docs, helps maintain them, answers questions on issues, and auto-labels pull requests, issues, and discussions on GitHub. With seamless integration into GitHub and Slack, Dosu provides context-aware, conversational assistance that reduces manual toil and accelerates development workflows. 

[LanceDB](https://lancedb.com/) powers Dosu's performance engine for multimodal search at scale. Its unique blend of blazing-fast [vector + full-text search](https://lancedb.com/docs/search), built-in [reranking](https://lancedb.com/docs/reranking), and [immutable versioning](https://lancedb.com/docs) enables Dosu to handle massively changing datasets (like code, docs, and discussions) with millisecond latency and full historical traceability. LanceDB's lightweight, [local-first architecture](https://lancedb.com/docs/quickstart) makes it ideal for secure on-prem deployment and rapid iteration for local development.

By building on LanceDB, Dosu doesn't just respond. It anticipates, accelerates, and elevates how teams build software.

## From Prototype to Production: The Dosu Success Story

In today's fast-moving software landscape, Dosu has emerged as the intelligent knowledge base trusted by modern software teams. With over 50,000 GitHub installations and as an official partner of the Cloud Native Computing Foundation (CNCF), Dosu transforms AI from a passive tool into an active, context-aware collaborator.

Dosu learns your architecture, patterns, and decisions across GitHub and Slack, proactively surfacing relevant Dosu-generated documentation on PRs, answering questions, and resolving issues in context. At the core is a hybrid Retrieval-Augmented Generation (RAG) system powered by high-performance [vector search](https://lancedb.com/docs/search/vector-search/) that weaves code, conversations, and documentation into a unified, searchable knowledge graph.

For organizations, Dosu is more than a productivity boost. It's a strategic layer that helps ensure knowledge stays up-to-date and accurate, so software teams and AI agents can make better decisions without having to spend half their day on manual maintenance. It helps teams onboard faster, reduces maintenance overhead, and preserves institutional knowledge.

For agents specifically, Dosu is a must-have to ensure LLM systems make good decisions based on up-to-date information, rather than stale documentation. This is an increasingly pernicious problem where agentic AI makes more frequent codebase changes, resulting in documentation that's more difficult to keep up-to-date, which could lead other AI systems to have wrong or stale context and make increasingly inaccurate decisions. Dosu addresses the problem by ensuring documentation stays up-to-date when changes in code are detected. 

Dosu isn't just another AI copilot. It's your always-on intelligent knowledge base, keeping on top of new knowledge when it's created, maintaining context, and accelerating how software teams and the agentic systems that support them build the future.

## The Challenge

### pgvector Didn't Quite Work Out

To power core features like auto-labeling, context-aware documentation, and proactive Q&A, Dosu initially turned to pgvector, a straightforward solution that handled basic vector search. But as their AI platform matured, the engineering team hit a major bottleneck.

Dosu's core features include auto-labeling, issue triage and Q&A, and documentation generation.

**Figure 1:** Auto-labeling of GitHub issues and pull requests
![](/assets/blog/case-study-dosu/dosu-2.png)

**Figure 2:** GitHub issue triage and question-answering capabilities
![](/assets/blog/case-study-dosu/dosu-3.png)

As Dosu's AI-powered platform matured, its initial choice of pgvector quickly became a bottleneck. With each model iteration, adjusting embedding dimensions, updating schemas, testing similarity metrics, engineers faced tedious database migrations. These migrations, once occasional, became a daily hurdle, especially during rapid prototyping. Instead of building product features, engineers were writing scripts to keep their database in sync.

Performance limitations compounded the problem: deploying pgvector in production was fragile, and scalability was limited. Dosu needed a fundamentally different approach -- **a vector database that prioritizes developer experience without sacrificing performance at scale, while remaining lightweight enough for seamless deployment**.

### The Ideal Vector Database for Dosu: LanceDB

That's when Dosu found LanceDB, a solution that immediately felt like a perfect fit. From day one, LanceDB delivered on every requirement, transforming their workflow:

| Feature | Description |
|---------|-------------|
| **Superior Developer Experience** | [LanceDB's intuitive APIs and robust tooling](https://lancedb.com/docs) drastically reduced the time from idea to production, from months to days, empowering Dosu's team to prototype, test, and deploy faster than ever before. |
| **Lightweight On-Premises Deployment** | With full support for isolated, [on-prem deployments](https://lancedb.com/docs/cloud), Dosu could ensure customer source code never left internal environments -- an absolute must-have for enterprise adoption. |
| **Real-Time Synchronization** | LanceDB enabled seamless, continuous ingestion from diverse sources like GitHub, issue trackers, and Slack, indexing data in real time so AI assistants always worked with the most up-to-date context. |
| **Advanced Hybrid Search** | Combining [full-text and vector search with integrated rerankers](https://lancedb.com/docs/reranking), LanceDB delivered highly relevant results while preserving the flexibility to support Dosu's custom ranking models that are tailored for code semantics and developer intent. |
| **Time-Travel Versioning** | Built-in support for [versioned queries](https://lancedb.com/docs/tables/versioning/) meant Dosu could track code evolution, query historical states, and enable AI to reason over not just the present, but the past, understanding how solutions evolved and accessing historical context for better decision-making. |

## The Solution

Within just a few weeks, Dosu successfully integrated LanceDB into their production environment and scaled it to support thousands of customers. With infrastructure friction eliminated, the team's development velocity surged. Engineers could finally shift their focus from maintaining database scaffolding to building product features that matter. LanceDB unlocked the freedom to innovate without compromise.

> With pgvector, every change meant a new migration. LanceDB lets us avoid the overhead and just point to a directory for local development. It's helped our developers build, test, and ship faster! </br> Pavitra Bhalla, Engineer @ Dosu

With LanceDB, Dosu's engineers shifted their focus from maintaining database scaffolding to building product features that matter. The result: faster prototyping, easier local development, and rapid scaling to thousands of customers. [See how to get started.](https://lancedb.com/docs/quickstart/)

**Figure 3:** LanceDB as part of Dosu's architecture
![](/assets/blog/case-study-dosu/dosu-1.png)

Dosu ingests and synthesizes multimodal development data, including source code, GitHub activity, and documentation, to build a living knowledge graph that reflects the state and evolution of a software project. It transforms code structures like functions and classes into semantic embeddings while also embedding context from related issues, pull requests, and discussion threads. This enriched data, tagged with structured metadata such as repository identifiers, timestamps, and file paths, is stored and indexed in LanceDB for high-performance retrieval.

LanceDB's flexible architecture allows Dosu to support both real-time ingestion of new activity and batch processing of historical data. Whether it's a freshly opened pull request or a legacy code review thread, everything is searchable within milliseconds thanks to LanceDB's [SSD-optimized hybrid index and auto-tuned performance](https://lancedb.com/docs/indexing/vector-index/).

### Auto-Labeling GitHub Issues

Dosu automatically triages GitHub issues and PRs by embedding their content and running hybrid semantic searches against the [LanceDB index](https://lancedb.com/docs/indexing). It identifies similar past cases, ranks them based on context and scope, and uses this information to apply accurate, context-aware labels -- achieving over 90% accuracy. This automation reduces manual triage by up to 70%, particularly in high-traffic open-source projects, accelerating response time and enabling maintainers to focus on higher-leverage work.

### Documentation Generation

To keep internal documentation fresh and relevant, Dosu continuously pulls relevant signals from [LanceDB's knowledge base](https://lancedb.com/docs), including commits, discussions, and historical annotations. It then synthesizes accurate, up-to-date references like architecture diagrams, API guides, and changelogs, which can be auto-committed to source repositories. The entire process is powered by LanceDB's [built-in versioning](https://lancedb.com/docs/tables/versioning/), enabling teams to audit how documentation was generated and roll back or compare versions as needed.

This transparency and traceability make Dosu suitable for compliance-heavy environments, while also accelerating iteration in fast-moving teams. With LanceDB's intuitive, file-based setup, developers can build and test locally with zero migration overhead and full reproducibility.

## The Result

By simplifying everyday workflows while maintaining high-performance [hybrid search](https://lancedb.com/docs/search/hybrid-search/) and [versioned storage](https://lancedb.com/docs/tables/versioning/), LanceDB aligns perfectly with Dosu's mission: to empower developers with tools that are fast, reliable, and easy to use. Integrating LanceDB-powered AI intelligence has revolutionized how engineering teams manage and scale their codebases with Dosu:

| Benefit | Details |
|---------|---------|
| **Rapid Time-to-Value** | Dosu delivers value quickly. Prototypes are up in hours and production deployments in weeks. Teams can automate 80% of GitHub issue labeling within days using built-in [LanceDB integrations](https://lancedb.com/docs/integrations). |
| **Point-in-Time Intelligence** | LanceDB's [versioning](https://lancedb.com/docs/tables/versioning/) lets Dosu offer historical insight into your codebase. Teams can trace decisions, debug AI behavior, and query knowledge snapshots from any point in time. |
| **Enterprise-Grade Performance** | Dosu supports high-scale performance with tens-of-milliseconds query speeds, even across millions of vectors. It processes thousands of GitHub events daily on moderate hardware, outperforming heavier alternatives by 10x in efficiency. |

> LanceDB powers the heart of our AI at Dosu. Its lightning-fast search, built-in versioning, and effortless scalability let us move faster, ship smarter, and deliver cutting-edge intelligence to millions of developersâ€”without compromise. </br> Devin Stein, Founder and CEO @ Dosu

## The Road Ahead: Smarter, Safer, and More Context-Aware AI Development

Looking ahead, [Dosu](https://dosu.dev) is charging into its next chapter with bold upgrades to further its mission of providing an always up-to-date, intelligent knowledge base that can be a trusted source of truth for agents and people alike to use for making better decisions. Advanced versioning will bring powerful new tools for traceabilityâ€”enabling tagging, annotations, and automated evaluation to track and refine model behavior with surgical precision. 

With deeper integration into tools and wikis, Dosu will unlock richer, domain-specific intelligenceâ€”turning organizational knowledge into up-to-date, accurate, actionable insights. The future of software development that Dosu is building is one where knowledge is up-to-date by default and accessible to all.
```

---

## ðŸ“„ æ–‡ä»¶: case-study-netflix.md

---

```md
---
title: "Netflix's Media Data Lake and the Rise of the Multimodal Lakehouse"
date: 2025-08-22
draft: false
featured: true
categories: ["Case Study"]
image: /assets/blog/case-study-netflix/preview-image.png
meta_image: /assets/blog/case-study-netflix/preview-image.png
description: "How Netflix built a Media Data Lake powered by LanceDB and the Multimodal Lakehouse to unify petabytes of media assets for machine learning pipelines."
author: David Myriel
author_avatar: "/assets/authors/david-myriel.jpg"
author_bio: "Writer."
author_twitter: "davidmyriel"
author_github: "davidmyriel"
author_linkedin: "davidmyriel"
---

Hollywood may run on storytelling, but behind every frame of a Netflix production lies another narrative written in data. 

Every scene, audio track, subtitle, and script fragment is not just creative output. It is raw material for the next generation of machine learning. Petabytes of multimodal assets are spread across formats and workflows. Each one must be discoverable, queryable, and ready for research and training.

{{< admonition "note" "The Media Data Lake" >}}
To meet this challenge, Netflix has introduced [Media ML Data Engineering](https://netflixtechblog.com/from-facts-metrics-to-media-machine-learning-evolving-the-data-engineering-function-at-netflix-6dcc91058d8d) and built the **Media Data Lake**. This is a new specialization at Netflix that bridges the gap between traditional data engineering and the unique demands of media-centric machine learning.
{{< /admonition >}}

The system unifies media assets, enriches them with metadata and embeddings, and serves them seamlessly into machine learning pipelines. 

At the core of the platform sits [LanceDB and the Multimodal Lakehouse](/blog/multimodal-lakehouse/).

![multimodal-lakehouse](/assets/blog/case-study-netflix/lakehouse.png)

## From Metrics Tables to Media Tables

Traditional pipelines were good at producing structured tables for metrics. Media data demands more. It is multimodal, unstructured, and deeply entangled with creative workflows. A single title may yield millions of frames, hours of multichannel audio, captions, embeddings, and annotations.

> The nature of media data is fundamentally different. It is multi-modal, it contains derived fields from media, it is unstructured and massive in scale, and it is deeply intertwined with creative workflows and business asset lineage. </br> â€” Dao Mi and the Media ML Data Engineering team, Netflix Tech Blog

The Media Data Lake organizes this into **Media Tables**. These are structured but flexible datasets that contain both canonical metadata and the outputs of advanced machine learning models. [With Lance as the underlying format](/docs/overview/lance/), Netflix can [evolve data and schema](/docs/tables/schema/) without costly rewrites. It can index embeddings alongside raw blobs. And it can query petabytes of content with low-latency, high-concurrency performance proven in production.

**Figure 1:** A sample media table used by the [Netflix Engineering Team](https://netflixtechblog.com/from-facts-metrics-to-media-machine-learning-evolving-the-data-engineering-function-at-netflix-6dcc91058d8d)

![multimodal-lakehouse](/assets/blog/case-study-netflix/media-table.png)

## Architecture in Motion

The architecture combines real-time and offline processing. Lightweight queries expose raw assets and derived features for exploration. A **distributed inference layer** executes [GPU-powered batch jobs over entire libraries](/docs/geneva/jobs/). A **pythonic API** provides programmatic access to the Media Tables. Visual interfaces allow interactive browsing of assets. This dual design enables both fast experimentation and reliable productionization. Traditional data lakes and warehouses were never built to support this balance.

**Figure 2:** Conceptual architecture of a Media Data Lake

```mermaid
graph LR
    classDef source fill:#e1f5fe,stroke:#333,stroke-width:1px,rx:6,ry:6
    classDef lake fill:#c8e6c9,stroke:#333,stroke-width:1px,rx:6,ry:6
    classDef table fill:#fff3e0,stroke:#333,stroke-width:1px,rx:6,ry:6
    classDef access fill:#f3e5f5,stroke:#333,stroke-width:1px,rx:6,ry:6
    classDef apps fill:#ffebee,stroke:#333,stroke-width:1px,rx:6,ry:6

    A[Data Sources]:::source -->|ingest| B[Media Data Lake<br/>LanceDB]:::lake
    B -->|organize| C[Media Tables<br/>Metadata + Embeddings]:::table
    C -->|serve| D[API/UI Access]:::access
    D -->|train + search| E[ML Training +<br/>Search Applications]:::apps
```
Figure 2 shows how Media Tables unify raw media, derived features, and machine learning outputs under one schema. This integration eliminates silos between creative assets and ML workflows, allowing researchers to query, explore, and train models on the same foundation. Built on LanceDB, it ensures that both interactive exploration and large-scale training run from a single source of truth.

## Media Data Lake vs. Traditional Data Lakes

The table contrasts traditional data lakes with the Media Data Lake. Traditional systems handle structured data well but struggle with multimodal assets, schema and data evolution, and hybrid querying.

| Feature | Traditional Data Lake | Media Data Lake (Netflix \+ LanceDB) |
| ----- | ----- | ----- |
| Data Types | Structured tables (text, numeric) | Multimodal (video, audio, images, text, embeddings) |
| Schema | Rigid, costly to evolve | Zero-copy data evolution |
| Query | SQL only | SQL, vector search, full-text search |
| Workload | Batch analytics | Real-time queries and GPU batch processing|
| Users | Analysts, BI teams | Machine learning engineers, researchers, creative ops |

## Where the Multimodal Lakehouse Shines

[The Multimodal Lakehouse](/blog/multimodal-lakehouse/) is built on LanceDB and the Lance format. It treats [unstructured blobs](/docs/storage/) as first-class citizens with uncompromised [random access I/O](/docs/overview/lance/). This is critical for shuffling and filtering media efficiently during training. [Zero-copy data evolution](/docs/tables/schema/) means new features and annotations can be added without duplicating petabytes of data.

Hybrid querying combines [vector search](/docs/search/vector-search/), [full-text search](/docs/search/full-text-search/), and [SQL](/docs/search/sql-queries/) over the same table. This enables workflows like retrieving shots by visual similarity, filtering by dialogue, and joining against structured metadata. All of this happens in one unified system.

> With LanceDB, blobs and embeddings are not afterthoughts. They are core building blocks in the data model.

On the compute side, [declarative Python UDFs](/docs/geneva/udfs/) allow engineers to define feature extraction jobs that scale across hundreds of thousands of CPU cores and GPU slots. 

This UDF will create an embeddings column directly from a text column. It uses a preloaded model to batch-encode sentences into fixed-length 384-dimensional vectors. 

```python
from geneva import udf
@udf(schema=pa.list_(pa.float32(), list_size=384))
def embed_text(text: pa.Array) -> pa.Array: # batched udf
    MODEL = load_model() 
    sentences = text.to_pylist()
    embeddings = MODEL.encode(sentences, show_progress_bar=False)

    return pa.array(embeddings.tolist(), type=pa.list_(pa.float32(), list_size=384))
```

Features can be computed incrementally with checkpointing and preemption. Expensive GPU inference runs can pause and resume without wasted cycles. [The system supports billions of vectors indexed in hours and tens of petabytes](/docs/enterprise/) managed under a single abstraction. 

{{< admonition "Note" "Production Tested" >}}
This performance has been proven in production by AI leaders such as ByteDance, MidJourney, Runway and Character AI.
{{< /admonition >}}

## Machine Learning Workflow Reimagined

Training pipelines also benefit directly. The Multimodal Lakehouse integrates with [PyTorch and JAX loaders](https://lancedb.github.io/lance/integrations/pytorch/). Models can train on named SQL views over raw assets and features, with direct access to blob data through efficient APIs. [A distributed cache fleet delivers more than five million IOPS from NVMe SSDs.](/docs/enterprise/architecture/) It reduces cloud storage costs while sustaining more than twenty thousand QPS of vector search traffic. This allows Netflix to experiment with large multimodal models while maintaining production-grade cost efficiency.

> One of the most exciting developments is the rise of media tables, structured datasets that not only capture traditional metadata, but also include the outputs of advanced ML models. â€” Netflix Tech Blog

This revolution is about transforming media into structured, queryable, reusable fuel for the next generation of machine learning. That fuel can power HDR remastering, localization quality metrics, narrative understanding, multimodal search, and future applications that have not yet been imagined.

## A New Level of Collaboration

By anchoring its Media Data Lake on LanceDB, Netflix has created one of the first large-scale demonstrations of [the Multimodal Lakehouse vision](/blog/multimodal-lakehouse/). 

With LanceDB, developers, data scientists, ML researchers, and product engineers can work together seamlessly on building intelligent applications. By abstracting away the complexity of vector storage, indexing, and retrieval, LanceDB ensures data solutions are not only fit for purpose but also easy to extend.

**Figure 3:** Teams can collaborate around the Multimodal Lakehouse as a source of truth

```mermaid
flowchart LR
  %% Nodes
  DP1[Data Producer 1]
  DP2[Data Producer 2]
  DP3[Data Producer 3]

  MDE{{Media ML Data Lake}}

  AE[Analytics Engineer]
  DS[Data Scientist]
  RS[Research Scientist]

  %% Flows
  DP1 --> MDE
  DP2 --> MDE
  DP3 --> MDE

  MDE --> AE
  MDE --> DS
  MDE --> RS

  %% Styling
  classDef center fill:#f8c8c8,stroke:#e74c3c,stroke-width:2px,color:#000,rx:6,ry:6
  classDef producers fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000,rx:6,ry:6
  classDef analytics fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000,rx:6,ry:6
  classDef dataScience fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000,rx:6,ry:6
  classDef research fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000,rx:6,ry:6

  class MDE center
  class DP1,DP2,DP3 producers
  class AE analytics
  class DS dataScience
  class RS research
```

For Netflix, it means faster iteration cycles and richer insights into creative workflows. For the wider ecosystem, it signals the arrival of a new standard. One platform that unifies multimodal data, computation, and machine learning into a single, production-ready foundation.

## Learn More About LanceDB

The foundation that powers Netflixâ€™s Media Data Lake is open source and available to everyone. [LanceDB](/) and the [Lance format](https://lancedb.github.io/lance/) bring the same capabilities to any team working with multimodal data. With fast random access, zero-copy data evolution, hybrid search, and scalable feature engineering, LanceDB is redefining what a data lake can do in the age of AI.

{{< admonition "Note" "Technical White Paper" >}}
To learn more about the Multimodal Lakehouse, read our study [Why Multimodal Data Needs a Better Lakehouse?](/download/)
{{< /admonition >}}
```

---

## ðŸ“„ æ–‡ä»¶: chat-with-csv-excel-using-lancedb.md

---

```md
---
title: "Chat with Your Stats Using Langchain Dataframe Agent & LanceDB Hybrid Search"
date: 2024-06-30
author: LanceDB
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/chat-with-csv-excel-using-lancedb/preview-image.png
meta_image: /assets/blog/chat-with-csv-excel-using-lancedb/preview-image.png
description: "In this blog, weâ€™ll explore how to build a chat application that interacts with CSV and Excel files using LanceDBâ€™s hybrid search capabilities."
---

![Hybrid search overview](/assets/blog/chat-with-csv-excel-using-lancedb/image-4-2.png)

In this blog, weâ€™ll explore how to build a chat application that interacts with CSV and Excel files using LanceDBâ€™s hybrid search capabilities. With LanceDB, performing direct operations on large-scale columnar data efficiently.

A common issue with directly loading data using pandas or similar methods is that they work well only on a small scale. But what if you have billions of rows? How would you go about handling such large datasets?

This is where a comes into play. You can efficiently retrieve relevant information from larger datasets by utilizing LanceDBâ€™s hybrid search and Full-Text Search (FTS) methods. Once the data is retrieved, it can be passed to a pandas agent for further processing.

Letâ€™s discuss these concepts hands-on using sample export-import data for demonstration purposes. We will take sample export-import data for the demo.

Below is sample data related to export import which has HS codes
![Sample export-import data](/assets/blog/chat-with-csv-excel-using-lancedb/1*BqcZKYB3XLQLJSWAheGUrQ.png)
Our objective is to extract the HS code for each commodity while also considering the share and percentage. You can choose any columns that suit your use case. Now, let's proceed to use LanceDB for storing and indexing this data.

## Full-Text Search (FTS) for Commodity

I used Full-Text Search (FTS) for the â€˜Commodityâ€™ field. This FTS will check the query and if anything is related to it, it will try to fetch or retrieve the relevant data. You can always set the FTS based on your use case to ensure that the most relevant information is retrieved efficiently.

## Reranker model and hybrid search

![Hybrid reranker diagram](/assets/blog/chat-with-csv-excel-using-lancedb/1_Zh4Jju6uiCYFO9HHvO5sIA.webp)

We use a reranker model, specifically the `LinearCombinationReranker`, to enhance the search results. This reranker allows you to adjust the weight between text search (BM-25) and semantic (vector) search. A weight of 0 means pure text search, while a weight of 1 means pure semantic search. By using a hybrid search, we combine the strengths of both search methods to provide more accurate and relevant results.

For more details on the reranker model and hybrid search, you can check out the [LanceDB documentation on hybrid search](https://lancedb.github.io/lancedb/hybrid_search/hybrid_search/).

## Code Breakdown

1. Creating an Embedding Function:

```python
from lancedb.embeddings import EmbeddingFunctionRegistry

registry = EmbeddingFunctionRegistry.get_instance()
func = registry.get("sentence-transformers").create(device="cpu")
```

2. Defining the Data Model:

```python
from lancedb.pydantic import LanceModel, Vector

class Words(LanceModel):
    HSCode: int = func.SourceField()
    Year_2022_2023: str = func.SourceField()
    Year_2023_2024: str = func.SourceField()
    Commodity: str = func.SourceField()
    vector: Vector(func.ndims()) = func.VectorField()
```

3. Creating a Table and Adding Data:

```python
import lancedb

db = lancedb.connect("./.lancedb")
table = db.create_table("exim", schema=Words, mode="overwrite")
table.add(data=df)
```

4. Creating an FTS Index:

- `table.create_fts_index("Commodity")`

I want to do FTS on `Commodity` column so I selected it

5. Using a Reranker for Hybrid Search:

```python
from lancedb.rerankers import LinearCombinationReranker

reranker = LinearCombinationReranker(weight=0.3)
query = 'what is HS code of sugar'
lance_reranker_op = (
    table.search(query, query_type="hybrid")
         .rerank(reranker=reranker)
         .limit(5)
         .to_pandas()
)

lance_reranker_op
```

This code demonstrates how to set up and use LanceDB for hybrid search, leveraging both FTS and semantic search with a reranker to get accurate results from your dataset.

## Using the Results with a Pandas DataFrame Agent

Now we can pass our results DataFrame to a pandas DataFrame agent for further processing.

First, letâ€™s clean up the DataFrame by removing columns that are not important for the next steps:

```python
df = lance_reranker_op.drop(columns=['_relevance_score', 'vector'])

# The cleaned DataFrame
result_from_lancedb = df
lance_reranker_op = lance_reranker_op.copy()
```

Next, we will set up the pandas DataFrame agent using LangChain:

```python
from langchain.agents.agent_types import AgentType
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain_openai import ChatOpenAI
import pandas as pd

agent = create_pandas_dataframe_agent(
    ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613"),
    result_from_lancedb,
    verbose=True,
    agent_type=AgentType.OPENAI_FUNCTIONS,
    handle_parsing_errors=True,
    allow_dangerous_code=True,
)
```

output:

```python
> Entering new AgentExecutor chain...

Invoking: `python_repl_ast` with `{'query': "df[df['Commodity'] == 'OTHER COCONUTS']['HSCode'].values[0]"}`

80119The HS code of "OTHER COCONUTS" is 80119.

> Finished chain.
{'input': 'what is Hs code of OTHER COCONUTS ',
 'output': 'The HS code of "OTHER COCONUTS" is 80119.'}
```

Thatâ€™s how you can build a system to interact with your data using LanceDBâ€™s hybrid search capabilities. If your data is in CSV format, this approach is particularly helpful. Additionally, you can experiment with different hybrid methods and ranked models to compare your results and find the most effective solution for your use case.

For more detailed guides, visit our blog where we cover various methods and tools for data interaction and improvement. You can also explore our [vectorDB recipes](https://github.com/lancedb/vectordb-recipes) on GitHub for more generative AI and vector database-related projects.

For the full executable script refer to our Colab notebook:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1iqXfTkKqbNOiit_jTUFWRvTmhkk8kovX?usp=sharing)
Stay tuned for more updates and tutorials on leveraging vector databases for efficient data retrieval and interaction!
```

---

## ðŸ“„ æ–‡ä»¶: chunking-analysis-which-is-the-right-chunking-approach-for-your-language.md

---

```md
---
title: "Chunking Analysis: Which is the right chunking approach for your language?"
date: 2025-01-27
draft: false
featured: false
categories: ["Community"]
image: /assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/chunking-analysis-which-is-the-right-chunking-approach-for-your-language.png
description: "Explore chunking analysis: which is the right chunking approach for your language? with practical insights and expert guidance from the LanceDB team."
author: Shresth Shukla
author_avatar: "/assets/authors/community.jpg"
author_bio: "AI/ML engineer passionate about multilingual NLP and retrieval systems. Contributes to the LanceDB community with practical insights on chunking strategies and RAG applications."
author_twitter: ""
author_github: ""
author_linkedin: ""
---

Before we get into specific chunking approaches, we should first ask **whether chunking depends on the language at all** and **how much chunking affects retrieval across languages** when building RAG applications.

Many teams use the same chunking approach for every language, and often it worksâ€”but is it actually right? Do we have evidence to back that choice? In this post, we'll explore that by answering two questions:

1. Does chunking depend on the language?
2. Which is the right chunking approach for your language?

We'll analyze and compare major chunking approaches like fixed character splitting, semantic, and clustering methods using text in English, Hindi, French, and Spanish.

We'll briefly review the basics and then focus on how chunking impacts retrieval in RAGs. First, we'll examine how chunk sizes affect the number of chunks across languages. Next, we'll analyze how chunking the same text in different languages produces different results and similarity scores. Finally, we'll cover advanced chunking methods like semantic and clustering approaches.

{{< admonition info "Key Finding" >}}
This analysis highlights that chunking significantly impacts retrieval, especially in a multilingual context.
{{< /admonition >}}

I've attached the link to the Colab notebook at the end of the blog so you can run the experiments yourself and adapt them to your data.

### Quick Intro

We have come a long way from when chunking and RAG were just buzzwords. Today, they are widely used in production, with more companies building multilingual RAG systems, with new RAG-related papers appearing almost every other day. Chunking, in the context of retrieval systems, refers to **breaking down longer texts into smaller pieces** (chunks) and saving them as dense vector representations using techniques like transformer-based models or word embeddings.

![OpenAI Chunking Example](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-3.png)
*Image from OpenAI Blog*

We'll be using *sentence-transformers'* multilingual embeddings for this analysis, but you can definitely experiment with other multilingual embeddings available in the market for this. It maps sentences and paragraphs to a 384-dimensional dense vector space and can be used for tasks like clustering or semantic search. There's no specific reason behind choosing this model, but I wanted an embedding model that I could use for these four languages (Hindi, English, Spanish, and French), and it turned out it could serve the purpose.

I looked on the internet to see if there was any analysis I could read to understand how different languages behave when it comes to chunking, but sadly I wasn't able to find one. To understand why we even need such analysis, we need to understand how languages behave, and that's exactly what we'll do today. We'll see how languages behave differently, which confirms that we might need to process texts in different languages separately. Then we'll move on to trying out chunking approaches on these texts.

![Language Behavior Analysis](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-4.png)

The most common approach that people use these days is... I think you can make a guess! It's *RecursiveCharacterTextSplitter()*. This analysis will help you understand how different languages behave when we use *RecursiveCharacterTextSplitter()* in our code and what could be the best chunk size for it. Refer to the code to do it on your text and experiment further.

As we start our analysis, it's important to know what approaches exist and when to use or avoid them. Overall, the major chunking methods are the following, and we'll try some of them today:

1. CharacterTextSplitter()
2. TokenTextSplitter()
3. RecursiveCharacterTextSplitter()
4. Semantic Chunking
5. Clustering Based Approaches (CRAG)
6. LLM-based chunking (Agent assisted)

### How do you find the best chunk size for RecursiveCharacterTextSplitter()?

Before we understand how languages behave, let's first see in detail what happens when we use different languages with *RecursiveCharacterTextSplitter()*, and then we'll understand why it might be happening in the second section of this blog.

Having seen this myself, I believe that people often use this approach directly while building their projects and then experiment with chunk sizes. I've seen people using this in production, so it seems to be the best fit for analyzing fixed character splitting. I analyzed all four languages, and things turned out interesting.

![Sample English Text](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-5.png)
*Sample English Text Used*

I used different chunk sizes for processing and stored the chunks in LanceDB as vectors, with separate tables per language and chunk size. I used a 50-character overlap for all chunk sizes during this analysis; you can experiment with other overlaps in the notebook.

We can divide the text into chunks and save them into LanceDB using the following codeâ€“

```python
import os
from typing import List, Dict, Any, Tuple
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
import lancedb
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import nltk

nltk.download('punkt_tab')

def process_text(text: str,
                chunk_sizes: List[int],
                model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                db_path: str = "/content/lancedb_data",
                language: str = "text") -> Dict:
    
    # Initialize
    embeddings = HuggingFaceEmbeddings(model_name=model_name)
    db = lancedb.connect(db_path)
    processed_data = {}

    # Preprocess text
    text = text.replace("\n", " ")
    for chunk_size in chunk_sizes:
        print(f"\nCreating chunks of size {chunk_size}")

        # Create chunks
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=50
        )
        chunks = splitter.create_documents([text])
        chunk_texts = [chunk.page_content for chunk in chunks]
        embeddings_list = embeddings.embed_documents(chunk_texts)

        # Store in LanceDB. For each language and for that specific chunk we'll create a single table and store all chunks there.
        table_name = f"{language}_chunks_{chunk_size}"
        df = pd.DataFrame({
            "text": chunk_texts,
            "vector": embeddings_list,
            "chunk_index": range(len(chunks)),
            "char_length": [len(chunk) for chunk in chunk_texts]
        })

        db.create_table(table_name, data=df, mode="overwrite") 
        #table with same name will be overwritten when we rerun the query over another text.

        # Store metadata
        processed_data[chunk_size] = {
            "num_chunks": len(chunks),
            "table_name": table_name,
            "avg_length": df['char_length'].mean()
        }

    return processed_data
```

The summary of processing English chunks across these four sizes looks like this:

![English Chunk Summary](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-7.png)

When we used a chunk size of 100, the English text was divided into 227 chunks. As we increase the chunk size, the number of chunks starts reducing, which is expected since more text fits into a single chunk.

But what happens when we process the same text in multiple languagesâ€”English, Hindi, French, and Spanish? We processed the same text across all these languages, and the results turned out to be quite interesting.

![Language Chunk Distribution](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-9.png)
*Distribution of no. of chunks vs chunk sizes across languages*

{{< admonition important "Language Behavior Insight" >}}
There's a noticeable difference with smaller chunk sizes. With a chunk size of 100, all four languages were split into multiple chunks (i.e., 227, 236, 253, and 238). The same piece of information was stored in fewer or more chunks depending on the language.
{{< /admonition >}}

This pattern changes as we increase the chunk size. When the chunk size increases from 100 to 400, we observe that the difference in the number of chunks becomes less significant with the Recursive Character Text Splitter. English and Hindi texts were split into almost the same number of chunks after a size of 100, while French and Spanish behaved somewhat similarly after 100.

An initial observation is that English and Hindi are quite similar in how they distribute information across chunks. While this isn't a generalization, similar analyses could be conducted for other languages.

We still can't choose a chunk size until we test retrieval and see how performance changes with chunk size. Our embedding data is stored in LanceDBm, with corresponding `*.lance` files for each language and chunk.

Here's the code to query from tables stored in LanceDB:

```python
def search_chunks(query: str,
                 chunk_size: int,
                 language: str = "text",
                 model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                 db_path: str = "/content/lancedb_data",
                 top_k: int = 3) -> Dict:

    # Initialize
    embeddings = HuggingFaceEmbeddings(model_name=model_name)
    db = lancedb.connect(db_path)

    # Get query embedding
    print(f"Processing query: '{query}'")
    query_embedding = embeddings.embed_query(query)

    # Search in LanceDB
    table_name = f"{language}_chunks_{chunk_size}"
    table = db.open_table(table_name)
    results = table.search(query_embedding).metric('cosine').limit(top_k).to_pandas()

    # Calculate similarity scores
    similarities = 1 - results['_distance'].values

    # Prepare results
    search_results = {
        'texts': results['text'].tolist(),
        'similarity_scores': similarities.tolist(),
        'char_lengths': results['char_length'].tolist(),
        'query': query,
        'chunk_size': chunk_size,
        'avg_similarity': np.mean(similarities)
    }
    
    return search_results
```

Refer notebook for implementation.

Before we move on to comparisons, here's what the result for one chunk size looks like. I searched for the question "**à¤à¤œà¥€à¤†à¤ˆ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?**". For a chunk size of 400 in the Hindi language, we have the following results:

![Hindi Query Results](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-11.png)

If we examine the combined results for a chunk size of 400 for the same question â€” "What is AGI?" across all the other languages, they look like this. Note that this plot is based solely on the results for a chunk size of 400. Do you notice anything unusual? Any strange behavior?

![Cross-Language Results](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-13.png)

Let's take a pause and look at the results we got through the retrieval. 

![English Results](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-14.png)
*Results for English*

![Hindi Results](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-15.png)
*Results for Hindi*

![French Results](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-16.png)
*Results for French*

![Spanish Results](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-17.png)
*Results for Spanish*

If you look at the retrieved results for chunk size 400, English and Hindi clearly performed better than French and Spanish. Another quick observation from here is that the first retrieved result in the case of correct answers differs greatly from the other two chunks, which do not contain relevant text and are unrelated to the question.

Quick note â€” We are using the cosine distance metric provided by **LanceDB** to compare vectors. It also offers other distance metrics that you can use per your use case or experimentation.

![LanceDB Distance Metrics](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-18-1.png)
*Other metrics provided by LanceDB*

Starting with the **Hindi** language, for the query â€” "**à¤à¤œà¥€à¤†à¤ˆ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?**", the similarity scores for multiple chunk sizes look like this:

![Hindi Similarity Scores](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-19.png)

{{< admonition tip "Chunk Size Finding" >}}
The top (first) retrieved chunk consistently has the highest similarity score. However, **the average score reveals that smaller chunk sizes generally yield better results with higher similarity scores compared to larger chunk sizes.** This suggests that smaller chunk sizes may improve overall retrieval precision, though they risk losing context.
{{< /admonition >}}

On the other hand, larger chunks contain more context and information but might not achieve the best average similarity search score.  

What about English? â€” look at this chart below.

![English Similarity Scores](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-20.png)

Now letâ€™s look at the combined plot for all four languages.

![Combined Language Analysis](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-21.png)

The initial interpretation of this plot shows that we get different results with different chunk sizes for each language. Even though we used the same text and queried on the same chunk sizes, in each language, **one chunk size performed better than the others, and there is no direct solution to determine the best size universally.**

One clear insight from the above analysis is that, across the four languages processed, Spanish and French behaved differently compared to English and Hindi, both in terms of chunking and retrieval.

Now we can ask why this might be happening and how languages actually differ from each other.

### How Languages Behave?

You should understand that language structure, particularly morphology (the study of word formation), plays a crucial role in chunking. **Morphology determines how words are formed and how they change to express different meanings, tenses, or cases.** This can affect the length and number of chunks, as words in some languages might be more compact or more descriptive than in others.

![Language Structure Impact](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-22.png)

In our case, if you notice, English and Hindi share certain grammatical structures and sentence patterns that make their **chunking behavior more similar.** English typically uses a Subject-Verb-Object (SVO) structure, while Hindi often uses a Subject-Object-Verb (SOV) structure. While there are inflections in Hindi, they do not lead to drastically different chunking behavior compared to English.

At the same time, both **French** and **Spanish** have more complex morphology compared to **English** or **Hindi**, meaning they have a larger number of word forms depending on factors such as gender, number, verb conjugation, and case. These features make chunking and text segmentation different in these languages.

Let's take an example to understand this better. For the sentence ***"I eat apples"*** in English, the subject ("I") comes first, followed by the verb ("eat"), and then the object ("apples"). This structure is simple and consistent. In Hindi, it would be â€” **"à¤®à¥ˆà¤‚ à¤¸à¥‡à¤¬ à¤–à¤¾à¤¤à¤¾ à¤¹à¥‚à¤à¥¤"**. Here, the subject ("à¤®à¥ˆà¤‚") comes first, followed by the object ("à¤¸à¥‡à¤¬"), and finally the verb ("à¤–à¤¾à¤¤à¤¾ à¤¹à¥‚à¤"). The verb is always at the end, which makes it different from English. In French, it'd be â€” **"Je mange des pommes".** French follows the SVO order, just like English. However, when adjectives are involved, they often come **after** the noun. In Spanish, it'd be â€” **"Yo como manzanas".** Spanish also follows SVO but with some flexibility. Spanish allows dropping the subject ("Yo"), while English, Hindi, and French typically do not.

Note that these (French and Spanish) also follow SVO, but with more frequent adjective-noun inversion and **different punctuation rules**, which can make chunking different because the chunks may need to account for these structural nuances.

That's why we need something that not only splits our text using some character limit but also understands the context and what is being talked about. That's where Semantic Chunking would come into play.

![Advanced Chunking Approaches](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-25.png)
*Things are gonna be interesting now.*

## Advanced Chunking: Semantic and Clustering based approach

Limiting chunking to a fixed number of characters often causes loss of information and context between chunks. It rarely makes sense to split text arbitrarily. We need an approach that considers both meaning and context; one such way is semantic chunking.

First, we split the text into individual sentences using (not mandatory) NLTK's sentence tokenizer. This preserves natural language boundaries rather than breaking text arbitrarily using the number of characters. Here, we can experiment with different models and especially language-specific models available for tokenization. 

For example, NLTK provides the option to pass a language as a parameter while splitting into sentences. We'll use the "*sent_tokenize()*" function here. It relies on pre-trained language models to determine sentence boundaries. It not only searches for punctuation marks like '?', ',', '.', and '!', but also accounts for abbreviations like "Mr." and "U.S.A" to make sure these are not interpreted as boundaries, which might happen if you use simple regex code.

There's another alternative to NLTK, i.e., to use spaCy models. spaCy also provides language-specific tokenizers, and you can definitely experiment with that if you want to.

I tried it on all four languages by passing the language as a parameter and got almost similar results, so I combined approaches for those languages. In Hindi, punctuation is different; instead of full stops (.), it uses "|" (danda). I used `sent_tokenize()` for all the other three languages and a simple regex splitter for Hindi.

Once we split the larger text into sentences, the next step is to combine these sentences based on semantic similarity to form coherent groups. Two simple strategies are:

1. **Sequential Semantic Chunking**: Group sentences in order based on similarity. If similarity between the current and next chunk is above a threshold, group them; otherwise, start a new group.
2. **Semantic + Clustering**: Create embeddings for all sentences, cluster them, and then form chunks by grouping sentences within each cluster. The number of clusters can be chosen programmatically using minimum and maximum chunk size preferences.

### Sequential Semantic Chunking

We'll try both approaches one by one. Starting with the first approach, it's recommended when the order of content is more important because in this approach, we combine the chunks in sequential order. Ideal use cases could be, for example, if you are processing chat messages, you might want to retain the order of your text.

The high-level flow of semantic chunking would look like this â€“ 

![Sequential Semantic Chunking](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/semantic_chunking.png)
*Sequential Semantic Chunking*

Clearly, we'll take the text and divide it into sentences. Since there are chances that many sentences will be created during the splitting of the text, we'll group those sentences in sequence if they are discussing the same topic. Remember those large number of chunks being created when we used fixed character text splitters? Around 200-something, right? What do you think will happen this time? Initially, when we split the text into sentences, we got 90 chunks for English. (Already reduced xd)

Here's the code snippet that you can refer to for this approach. (use Colab for full code and testing it)â€“

```python
import re
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def store_embeddings(chunks: List[str], table_name: str, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", overwrite=True):
    # Create embeddings
    model = SentenceTransformer(model_name)
    embeddings = model.encode(chunks)

    # Connect to LanceDB
    db = lancedb.connect("./lancedb")

    # If table exists and overwrite is True, recreate it
    
    if table_name in db.table_names() and overwrite:
        db.drop_table(table_name)
    # Create new table
    table = db.create_table(
        table_name,
        data=[{
            "text": chunk,
            "vector": embedding
        } for chunk, embedding in zip(chunks, embeddings)]
    )
    return table

#check colab for full code.

def combine_chunks(chunks: list) -> list:
    for i in range(len(chunks)):
        combined_chunk = ""
        if i > 0:
            combined_chunk += chunks[i - 1]["chunk"]
        combined_chunk += chunks[i]["chunk"]
        if i < len(chunks) - 1:
            combined_chunk += chunks[i + 1]["chunk"]
        chunks[i]["combined_chunk"] = combined_chunk

    return chunks

def add_embeddings_to_chunks(model, chunks: list) -> list:
    combined_chunks_text = [chunk["combined_chunk"] for chunk in chunks]
    chunk_embeddings = model.encode(combined_chunks_text, show_progress_bar=True)
    for i, chunk in enumerate(chunks):
        chunk["embedding"] = chunk_embeddings[i]
    return chunks

def calculate_cosine_distances(chunks: list) -> list:
    distances = []
    for i in range(len(chunks) - 1):
        current_embedding = chunks[i]["embedding"]
        next_embedding = chunks[i + 1]["embedding"]

        similarity = cosine_similarity([current_embedding], [next_embedding])[0][0]
        distance = 1 - similarity

        distances.append(distance)
        chunks[i]["distance_to_next"] = distance

    return distances
```

To explain a bit about the process going on here â€“ For each pair of consecutive embeddings, the cosine similarity is calculated. Once we get the similarity, we can calculate the distance between each sentence. Then we calculate the threshold value. The threshold is calculated as the percentile value (e.g., the 90th percentile) of the cosine distances. It is essentially a filter to identify "significant transitions" in the text, where the semantic similarity between consecutive sections drops sharply. Then we iterate over the distances and compare each one to the threshold value. If a distance exceeds the threshold, it indicates a significant semantic transition, and its position is added to crossing points.

![Semantic Chunking Analysis](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-26.png)
*Check Colab for full code*

Look at this chart above. Did you notice something? Once we do semantic chunking, we get 11 crossing points, and here we combine similar sentences together. That means these were the points where there was a slight change in context, and other things would have been discussed in the text. For each crossing point, we'll create a new chunk from there to the next crossing point. This helps in two waysâ€“ 

1. First, we have reduced the number of chunks drastically.
2. More information related to that specific context is now available in those chunks instead of simply splitting based on character count.

For English, I got 90 sentences initially and 11 chunks of different sizes. A few are very large in size, and a few are small in terms of content, which might be because we started discussing new content in the later part of our text. Still, there are multiple ways to improve the current approach, and it's important to do so because we might get chunks that are often too large in size, which may create issues later.

![Hindi Language Analysis](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-27.png)
*Plot for Hindi Language - only 4 chunks*

So while this approach made more sense than RecursiveCharacterTextSplitter(), it has its own issues, like very high variance in chunk sizes due to solely matching on the semantics of sentences. That means a few chunks are over 4000 characters long, while some are only 400. The good part of doing this analysis over all the languages is that there's a similar pattern being observed while processing all of these languages.

![French Analysis](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-29.png)
*For French*

![English Analysis](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-30.png)
*For English*

![Spanish Analysis](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-31.png)
*For Spanish*

![Hindi Analysis](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-34.png)
*For Hindi*

They all behaved similarly and had 10-11 chunks at the end, which were far better than many chunks with irrelevant information. The only issue is that we haven't defined any chunk size limit here. I'll leave this experimentation to you to create chunks based on both semantics and chunk sizes and then examine the results.

But this approach might not work always. The reason being, what if I started talking about one topic, mixed it with some other topic, and then came back to discussing the same topic again? That means the content should be chunk in such a way that if I ask a certain question about that context, I should get all the details about that particular topic in a chunk, which we might miss due to semantic chunking. What is the solution then? A smart solution would be Clustering-based chunking.

### **Semantic and Clustering Combined**

I read about the clustering-based approach in this paper titled - [CRAG](https://arxiv.org/pdf/2406.00029v1).

![CRAG Paper](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-28.png)
*It's a good read if you are looking to try it out this approach.*

This paper was introduced in May 2024 and provides a detailed insight into how using a clustering-based approach could help you ***reduce your LLM call costs*** by reducing the number of tokens being sent.

Logically, there are various ways of doing clustering, and it depends on your use case or type of content. Ideally, if you think about it, there could be infinite ways of doing this, depending on what works and what doesn't. For exampleâ€“ 

1. You can create embeddings of all the text data initially and then create clusters and divide them into chunks. (Does this remind you of any recent paper? Late chunking?)
2. Or you can split your text into sentences and then create clusters on top of their embeddings.
3. Or maybe you can think of other approaches, xd.

I'll try the second one today. We have the text, we'll divide it into sentences, and then we'll use different clustering approaches, with the simplest and most effective one being K-means, and group sentences belonging to similar clusters. That makes sense, right?

A major issue is, that we might need to define the number of chunks as a parameter while using K-means, which we can choose to find programmatically by taking minimum and maximum chunk size preferences as parameters. Another alternative could be trying different clustering approaches like DBSCAN or any other.

Flow in this approach would be like this â€“ 

![Clustering Based Approach](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/clustering_based_appraoch.png)
*High level flow of this chunking approach*

Let's do it. We'll first take the text and divide it into sentences. I used the following code depending on the language.

```python
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import numpy as np
from typing import List
import nltk
import re
from nltk.tokenize import sent_tokenize

#This is just a part of code. Check notebook for full code for testing.

class SemanticTransformerChunker:
    def __init__(
        self,
        model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        min_chunk_size: int = 100,
        max_chunk_size: int = 2000,
        threshold_percentile: int = 90
    ):
        
    def determine_optimal_clusters(
        self,
        text_length: int,
        num_sentences: int
    ) -> int:

        """
        Determine optimal number of clusters based on text length
        and desired chunk sizes
        """

        avg_chars_per_sentence = text_length / num_sentences
        min_sentences = max(2, self.min_chunk_size / avg_chars_per_sentence)
        max_sentences = self.max_chunk_size / avg_chars_per_sentence

        return max(2, min(
            num_sentences // 2,  # Don't create too many clusters
            int(num_sentences / min_sentences)  # Ensure minimum chunk size
        ))

    def chunk_text(
        self,
        text: str,
        num_chunks: int = None
    ) -> List[str]:

        # Split into sentences (for those 3 languages)
        sentences = self.split_into_sentences(text)

        if len(sentences) < 3:
            sentences = self.split_hindi_text(text)
            print("splitting hindi text into sentences")

        # Get sentence embeddings
        embeddings = self.get_embeddings(sentences)

        # Determine number of clusters if not provided. it's better to determine this programmatically but we'll see how this might be concerning as well.
        if num_chunks is None:
            num_chunks = self.determine_optimal_clusters(
                len(text),
                len(sentences)
            )

        # Cluster sentences -- #if num_chunks given is more than no. of sentences, we'll take count of sentences as clusters.
        kmeans = KMeans(
            n_clusters=min(num_chunks, len(sentences)),

            random_state=42
        )

        clusters = kmeans.fit_predict(embeddings)
        # Sort sentences by cluster and position
        sentence_clusters = [(sent, cluster, i) for i, (sent, cluster)
                           in enumerate(zip(sentences, clusters))]
        sentence_clusters.sort(key=lambda x: (x[1], x[2]))

        # Combine sentences into chunks
        chunks = []
        current_chunk = []
        current_cluster = sentence_clusters[0][1]

        for sentence, cluster, _ in sentence_clusters:
            if cluster != current_cluster:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_cluster = cluster
            current_chunk.append(sentence)
            
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        return chunks
```

Initially, we got 87 sentences from our text. After clustering these sentence embeddings, we ended up with 43 clusters for English, which is a reasonable balance. The same analysis on other languages behaved oddly for Hindi but worked decently for French and Spanish.

![Hindi Clustering Results](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-38.png)
*For Hindi, it only created 7 chunks*

![French Clustering Results](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-39.png)
*For French we got 42 chunks*

![Spanish Clustering Results](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-40.png)
*For Spanish, we got 42 chunks*

Now that we have cluster-based chunks, we can check retrieval quality. I saved all the embedding vectors into **LanceDB** with separate tables per language. I asked three questions to evaluate whether the retrieved chunks made sense. The first five chunks for the question "**What is AGI**" are:

![AGI Query Results 1](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-41.png)

![AGI Query Results 2](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-42.png)

I believe the first 3-4 chunks contain enough information to answer the question "What is AGI?" On asking a different question, say, "**Does AGI control itself?**" we gotâ€”

![AGI Control Query](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-43.png)

This perfectly makes sense since the context is talking about the query being asked.

![Query Results Analysis](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-44.png)

For each query, the top three chunks contained enough information to answer it. Now letâ€™s look at the final approach before returning to our original questions.

### LLM based Chunking

People often call this approach "Agentic Chunking." As the name suggests, in this approach, we utilize LLMs during the chunking process to create quality chunks, so they can provide better retrieval.

Usually, there are two common approaches:

1. Give the entire text to an LLM and ask it to split into N chunks.
2. Use an LLM to create propositions that add meaning to individual sentences, then chunk those propositions.

The second approach is quite popular these days. You use **LLM to create propositions** for your sentences. When we do this, the sentences can stand on their own meaning, meaning you don't necessarily need much context to understand what is being talked about or who the sentence is referring to.

For example, if our text looks like this â€“ "Aman is a good guy. He recently met a boy named Akhil. They started playing chess. Aman liked chess. He plays chess at grandmaster level."

When we pass this text to the LLM to create propositions, it would come out like this â€“

![LLM Propositions Example](/assets/blog/chunking-analysis-which-is-the-right-chunking-approach-for-your-language/image-46.png)
*Can you notice the difference?*

['Aman is a good guy.', 'Aman met a boy named Akhil.', 'They started playing chess.', 'Aman liked chess.', 'Aman plays chess at grandmaster level.']

Did you notice the second sentence in the lineup? In our text, it was "*He recently met a boy named Akhil,*" while in the propositions, it turned out to be "*Aman met a boy named Akhil,*" which does not need surrounding context to understand who is being referred to. I hope you got the point.

Code to create propositionsâ€“ 

```python
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import CommaSeparatedListOutputParser
from typing import List
import google.generativeai as genai
from tqdm import tqdm
from langchain import hub
from langchain.pydantic_v1 import BaseModel

#check colab for full code
def create_propositions(text: str, llm: ChatGoogleGenerativeAI) -> List[str]:
    """Extract propositions from text using Gemini"""
    output_parser = CommaSeparatedListOutputParser()

    try:
        # Get propositions from Gemini
        prompt = get_proposition_prompt(text)
        # prompt = hub.pull("wfh/proposal-indexing")

        response = llm.invoke(prompt)

        response = response.content #make sure you add .content since the response it AI message
        # Parse and clean propositions
        propositions = output_parser.parse(response)
        propositions = [p.strip() for p in propositions if p.strip()]
        return propositions

    except Exception as e:
        print(f"Error processing text: {e}")
        return []
```

Check Colab for full code

Once we have these propositions, we can follow the same steps as in the semantic or clustering approaches to group them and create the final chunks. However, I wouldn't recommend this approach for large corpora, as it becomes expensive. Similarly, passing whole documents to LLMs and asking them to return stopping points or chunks directly is rarely cost-efficient.

{{< admonition warning "Cost Consideration" >}}
If you believe token costs will eventually drop to zero, you'd probably love this approach. Until then, it's usually better to use a more cost-effective method and spend your budget on querying rather than chunking.
{{< /admonition >}}

Still, I've included the code to create propositions if you want to try it yourself.

### What did we learn?

From our analysis, different languages behave differently when it comes to chunking and retrieval. For some languages, like Hindi, smaller chunk sizes give better precision (which is nearly true for all the languages), while for languages like English, larger chunks (around 400 or more) often work better because they capture more context, improving average retrieval accuracy with *RecursiveCharacterTextSplitter().*

We also saw how semantic or clustering-based approaches produce better chunks than fixed character splitters. While the high-level approach is similar across languages, sub-steps like sentence splitting need to be language-aware. My recommendation is to copy the notebook, plug in your own text, and see how your language and use case behave.

Clearly, there's no "one-size-fits-all" formula. If you are using Recursive, you should experiment with chunk sizes. If you are using semantic or clustering-based approaches, you should experiment with parameters like thresholds, number of clusters, and sentence tokenizers.

The choice of embedding model also plays a big role. Multilingual embeddings may treat different languages in different ways, leading to variations in chunking and retrieval. I tried using another embedding model from Sentence Transformers and, while the overall results were similar, the similarity scores differed slightly. There are also strong multilingual models from providers like Cohere that are worth exploring.

There is a trade-off between chunk size and context. Smaller chunks give higher precision but may lose some context, while larger chunks contain more context but may not match as well for retrieval and often contain irrelevant text, which might distract LLMs. You can also store multiple segment lengths within a single database, as we effectively did with semantic chunking by not passing fixed chunk sizes.

Spanish, French, English, and Hindi behaved differently at times and similarly at others. This shows that, beyond context, some languages need special treatment to get the best results. We can look into the specific features of each language, like sentence structure and word formation, and design better preprocessing strategiesâ€”especially for languages like Spanish and French. Using language-specific models can help here.

Finally, as much as answer quality depends on chunks, it also depends on the **retrieval process**. How we store these chunks in vector databases matters a lot. Better indexing and retrieval strategies can improve performance before we even touch the LLM. Thatâ€™s material for another blog.

Now, coming back to the two questions we asked at the start:

{{< admonition conclusion "Research Findings" >}}
**Does chunking depend on language?** â€“ I'd say yes. Not completely, but some part of it definitely depends on language, and it matters a lot how you split your text into sentences in a way that makes sense. Choosing different tokenizers will help improve the quality of the overall chunk. So, the answer would be â€“ YES!

And the second question, **which is the right chunking approach for your language?** â€“ I'd say it depends on your use case and type of content. While chunking depends on language, it also depends on what your content is, whether it is structured or unstructured, whether your document contains text in multiple languages, or if it contains graphics that need to be handled separately, or if it is code written in Python or any other language.
{{< /admonition >}}

We have different splitters these days, and they are used for different purposes. If you are working on text-only use cases and want a practical way to choose a chunking approach, use the Colab notebook to upload your text files and test it yourself.

Here's the link to the [Colab Notebook](https://colab.research.google.com/drive/1DRT5e4_up84afkCMJtP_uTUjvNvPuVWW?usp=sharing).

## Key Takeaways

But still, if you are someone looking for a direct answer, I'll give you a starting point. If you work on â€“

1. **English** - If you use *RecursiveCharacterTextSplitter()* with your application. If you don't want to use Recursive, use the clustering-based approach directly. It gave the best results and is the most cost-effective approach available.
2. **Hindi** - I'd not recommend *Recursive* here unless it's the worst case and you have no time. For this, try out different text splitters using NLTK or spaCy models and then start with the Semantic Chunk first. If it doesn't perform well, you can definitely consider clustering-based approaches.
3. **French** and **Spanish** - Notice that these languages are kind of similar when compared with English. If you are someone working on them for the first time, go with clustering-based approaches for both of them. They performed very decently.

Overall, I hope this blog has given you a sense of direction to analyze and find the correct chunk size and chunk approach for your language using **LanceDB**. You can go ahead and experiment further with more questions on the same text or with your own text. Feel free to share your results and findings if you perform further analysis on other languages or different questions.

#### References 

1. [Research Papers](https://arxiv.org/pdf/2406.00029v1), 
2. [Blogs](https://towardsdatascience.com/improving-rag-chunking-with-clustering-03c1cf41f1cd), and 
3. an awesome [notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) by Full stack retrieval.
```

---

## ðŸ“„ æ–‡ä»¶: chunking-techniques-with-langchain-and-llamaindex.md

---

```md
---
title: "Chunking Techniques with Langchain and LlamaIndex"
date: 2024-04-20
author: Prashant Kumar
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/chunking-techniques-with-langchain-and-llamaindex/preview-image.png
meta_image: /assets/blog/chunking-techniques-with-langchain-and-llamaindex/preview-image.png
description: "In our last blog, we talked about chunking and why it is necessary for processing data through LLMs.  We covered some simple techniques to perform text chunking."
---

In our last blog, we talked about chunking and why it is necessary for processing data through LLMs. We covered some simple techniques to perform text chunking.

In this blog, we will comprehensively cover all the chunking techniques available in Langchain and LlamaIndex.
![Chunking overview](/assets/blog/chunking-techniques-with-langchain-and-llamaindex/0*FJOKBN99gTfv_9ED.jpg)
> The aim is to get the data in a format where it can be used for anticipated tasks, and retrieved for value later. Rather than asking â€œHow should I chunk my data?â€, the actual question should be â€œWhat is the optimal way for me to pass data to my language model that it needs for its task?â€

Let's begin with Langchain first!

## Chunking Techniques in Langchain

![LangChain text splitters](/assets/blog/chunking-techniques-with-langchain-and-llamaindex/1*LlJfztZOhVrryfG5uRouiQ.png)

Before jumping into chunking, make sure to first install **Langchain-text-splitters**

```bash
pip install langchain-text-splitters
```

These snippets only cover the relevant sections of code. To follow along with the working code, use this Colab:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/tutorials/Langchain-LlamaIndex-Chunking/Langchain_Llamaindex_chunking.ipynb)
### Text Character Splitting

This is the simplest method. This splits based on characters and measures chunk length by number of characters.

```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator="\n\n",
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False,
)

texts = text_splitter.create_documents([state_of_the_union])
print(texts[0].page_content)
```

This outputs chunks with 1000 characters:

```python
Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:

Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty. They have done so during periods of prosperity and tranquility. And they have done so in the midst of war and depression; at moments of great strife and great struggle.
```

### Recursive Character Splitting

This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split them in order until the chunks are small enough. The default list is `["\n\n", "\n", " ", ""]` . It includes overlapping text which helps build context between text splits.

```python
# Recursive Split Character

# This is a long document we can split up.
with open("state_of_the_union.txt") as f:
    state_of_the_union = f.read()

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size=1000,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False,
)

texts = text_splitter.create_documents([state_of_the_union])
print("Chunk 2: ", texts[1].page_content)
print("Chunk 3: ", texts[2].page_content)
```

Here are Chunk 2 and Chunk 3 in output showing an overlap of 100 character:

```python
Chunk 2:  It's tempting to look back on these moments and assume that our progress was inevitable, that America was always destined to succeed. But when the Union was turned back at Bull Run and the Allies first landed at Omaha Beach, victory was very much in doubt. When the market crashed on Black Tuesday and civil rights marchers were beaten on Bloody Sunday, the future was anything but certain. These were times that tested the courage of our convictions and the strength of our union. And despite all our divisions and disagreements, our hesitations and our fears, America prevailed because we chose to move forward as one nation and one people.

Again, we are tested. And again, we must answer history's call.

Chunk 3:  Again, we are tested. And again, we must answer history's call.
One year ago, I took office amid two wars, an economy rocked by severe recession, a financial system on the verge of collapse and a government deeply in debt. Experts from across the political spectrum warned that if we did not act, we might face a second depression. So we acted immediately and aggressively. And one year later, the worst of the storm has passed.

But the devastation remains. One in 10 Americans still cannot find work. Many businesses have shuttered. Home values have declined. Small towns and rural communities have been hit especially hard. For those who had already known poverty, life has become that much harder.
This recession has also compounded the burdens that America's families have been dealing with for decades -- the burden of working harder and longer for less, of being unable to save enough to retire or help kids with college.
```

### HTML Section Splitter

HTML Section Splitter is a â€œstructure-awareâ€ chunker that splits text at the element level and adds metadata for each header â€œrelevantâ€ to any given chunk.

```python
url = "https://www.utoronto.ca/"

# Send a GET request to the URL
response = requests.get(url)
if response.status_code == 200:
    html_doc = response.text

headers_to_split_on = [
    ("h1", "Header 1"),
    ("p", "paragraph")
]

html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
html_header_splits = html_splitter.split_text(html_doc)
html_header_splits[0].page_content
```

This outputs the HTML Header element:

```python
Welcome to University of Toronto
```

### Code Splitter

CodeTextSplitter allows you to split your code with multiple languages supported. Supported Languages are Python, JS, TS, C, Markdown, Latex, HTML, and Solidity.

```python
    from langchain_text_splitters import Language, RecursiveCharacterTextSplitter

    with open("code.py") as f:
        code = f.read()

    python_splitter = RecursiveCharacterTextSplitter.from_language(
        language=Language.PYTHON, chunk_size=100, chunk_overlap=0
    )
    python_docs = python_splitter.create_documents([code])
    python_docs[0].page_content
```

This splits code according to the Python language in chunks of 100 characters:

```python
    from youtube_podcast_download import podcast_audio_retreival
```

### Recursive Json Splitting

This JSON splitter goes through JSON data from the deepest levels first and creates smaller JSON pieces. It tries to keep nested JSON objects intact but may divide them if necessary to ensure that the chunks fall within a specified range from the minimum to the maximum chunk size.

```python
    from langchain_text_splitters import RecursiveJsonSplitter
    import json
    import requests

    json_data = requests.get("https://api.smith.langchain.com/openapi.json").json()

    splitter = RecursiveJsonSplitter(max_chunk_size=300)
    json_chunks = splitter.split_json(json_data=json_data)
    json_chunks[0]
```

Result:

```
    {'openapi': '3.1.0',
     'info': {'title': 'LangSmith', 'version': '0.1.0'},
     'servers': [{'url': 'https://api.smith.langchain.com',
       'description': 'LangSmith API endpoint.'}]}
```

### Semantic Splitting

This method splits the text based on semantic similarity. Here weâ€™ll use OpenAI Embeddings for semantic similarity.

```python
    #!pip install --quiet langchain_experimental langchain_openai

    import os
    from langchain_experimental.text_splitter import SemanticChunker
    from langchain_openai.embeddings import OpenAIEmbeddings

    # Add OpenAI API key as an environment variable
    os.environ["OPENAI_API_KEY"] = "sk-****"

    with open("state_of_the_union.txt") as f:
        state_of_the_union = f.read()

    text_splitter = SemanticChunker(OpenAIEmbeddings())

    docs = text_splitter.create_documents([state_of_the_union])
    print(docs[0].page_content)
```

Semantic splitting result:

```
    Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:

    Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty.
```

### Split by Tokens

Language models come with a token limit, which you can not surpass.
To prevent issues, make sure to track the token count when dividing your text into chunks. Be sure to use the same tokenizer as the language model to count the tokens in your text.

For now, weâ€™ll use Tiktoken as a tokenizer
```python
    # ! pip install --upgrade --quiet tiktoken
    from langchain_text_splitters import CharacterTextSplitter

    with open("state_of_the_union.txt") as f:
        state_of_the_union = f.read()

    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=0)
    texts = text_splitter.split_text(state_of_the_union)

    print(texts[0])
```
Token Splitting using Tiktoken results:

```
    Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:

    Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty. They have done so during periods of prosperity and tranquility. And they have done so in the midst of war and depression; at moments of great strife and great struggle.
```

These are the most important text-splitting/ chunking techniques using Langchain. Now letâ€™s see similar techniques with their implemented in LlamaIndex.

## LlamaIndex Chunking Techniques with Implementation

![](https://cdn-images-1.medium.com/max/800/1*Xl9yLVVAarsdC-sbXJSr3g.png)

Make sure to install `llama_index` package.

```python
    ! pip install llama_index tree_sitter tree_sitter_languages -q
```

In LlamaIndex, Node parsers terminology is used which breaks down a list of documents into Node objects where each node represents a distinct chunk of the parent document, inheriting all attributes from the Parent document to the Children nodes.

### Node Parserâ€Šâ€”â€ŠSimple File

To make it easier to read nodes, there are different file-based parsers you can use. Theyâ€™re designed for different kinds of content, like JSON or Markdown. One simple way is to use the FlatFileReader with the SimpleFileNodeParser. This setup automatically picks the right parser for the content type youâ€™re dealing with.

```python
    from llama_index.core.node_parser import SimpleFileNodeParser
    from llama_index.readers.file import FlatReader
    from pathlib import Path

    # Download for running any text file
    !wget https://raw.githubusercontent.com/lancedb/vectordb-recipes/main/README.md

    md_docs = FlatReader().load_data(Path("README.md"))

    parser = SimpleFileNodeParser()

    # Additionally, you can augment this with a text-based parser to accurately
    # handle text length

    md_nodes = parser.get_nodes_from_documents(md_docs)
    md_nodes[0].text
```

This outputs:
```
    VectorDB-recipes
    <br />
    Dive into building GenAI applications!
    This repository contains examples, applications, starter code, & tutorials to help you kickstart your GenAI projects.

    - These are built using LanceDB, a free, open-source, serverless vectorDB that **requires no setup**.
    - It **integrates into python data ecosystem** so you can simply start using these in your existing data pipelines in pandas, arrow, pydantic etc.
    - LanceDB has **native Typescript SDK** using which you can **run vector search** in serverless functions!

    <img src="https://github.com/lancedb/vectordb-recipes/assets/5846846/d284accb-24b9-4404-8605-56483160e579" height="85%" width="85%" />

    <br />
    Join our community for support - <a href="https://discord.gg/zMM32dvNtd">Discord</a> â€¢
    <a href="https://twitter.com/lancedb">Twitter</a>

    ---

    This repository is divided into 3 sections:
    - [Examples](#examples) - Get right into the code with minimal introduction, aimed at getting you from an idea to PoC within minutes!
    - [Applications](#projects--applications) - Ready to use Python and web apps using applied LLMs, VectorDB and GenAI tools
    - [Tutorials](#tutorials) - A curated list of tutorials, blogs, Colabs and courses to get you started with GenAI in greater depth.
```

### Node Parserâ€Šâ€”â€ŠHTML

This node parser uses Beautiful Soup to understand raw HTML content. Itâ€™s set up to read certain HTML tags automatically, like â€œpâ€, and â€œh1â€ through â€œh6â€, â€œliâ€, â€œbâ€, â€œiâ€, â€œuâ€, and â€œsectionâ€. You can also choose which tags it pays attention to if you want to customize it.

```  import requests
    from llama_index.core import Document
    from llama_index.core.node_parser import HTMLNodeParser

    # URL of the website to fetch HTML from
    url = "https://www.utoronto.ca/"

    # Send a GET request to the URL
    response = requests.get(url)
    print(response)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Extract the HTML content from the response
        html_doc = response.text
        document = Document(id_=url, text=html_doc)

        parser = HTMLNodeParser(tags=["p", "h1"])
        nodes = parser.get_nodes_from_documents([document])
        print(nodes)
    else:
        # Print an error message if the request was unsuccessful
        print("Failed to fetch HTML content:", response.status_code)
```

This returns the output with an HTML tag in the metadata:

```
    [TextNode(id_='bf308ea9-b937-4746-8645-c8023e2087d7', embedding=None, metadata={'tag': 'h1'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='https://www.utoronto.ca/', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='247fb639a05bc6898fd1750072eceb47511d3b8dae80999f9438e50a1faeb4b2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7c280bdf-7373-4be8-8e70-6360848581e9', node_type=<ObjectType.TEXT: '1'>, metadata={'tag': 'p'}, hash='3e989bb32b04814d486ed9edeefb1b0ce580ba7fc8c375f64473ddd95ca3e824')}, text='Welcome to University of Toronto', start_char_idx=2784, end_char_idx=2816, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), TextNode(id_='7c280bdf-7373-4be8-8e70-6360848581e9', embedding=None, metadata={'tag': 'p'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='https://www.utoronto.ca/', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='247fb639a05bc6898fd1750072eceb47511d3b8dae80999f9438e50a1faeb4b2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='bf308ea9-b937-4746-8645-c8023e2087d7', node_type=<ObjectType.TEXT: '1'>, metadata={'tag': 'h1'}, hash='e1e6af749b6a40a4055c80ca6b821ed841f1d20972e878ca1881e508e4446c26')}, text='In photos: Under cloudy skies, U of T community gathers to experience near-total solar eclipse\nYour guide to the U of T community\nThe University of Toronto is home to some of the worldâ€™s top faculty, students, alumni and staff. U of T Celebrates recognizes their award-winning accomplishments.\nDavid Dyzenhaus recognized with Gold Medal from Social Sciences and Humanities Research Council\nOur latest issue is all about feeling good: the only diet you really need to know about, the science behind cold plunges, a uniquely modern way to quit smoking, the â€œsex, drugs and rock â€˜nâ€™ rollâ€ of university classes, how to become a better workplace leader, and more.\nFaculty and Staff\nHis course about the body is a workout for the mind\nProfessor Doug Richards teaches his students the secret to living a longer â€“ and healthier â€“ life\n\nStatement of Land Acknowledgement\nWe wish to acknowledge this land on which the University of Toronto operates. For thousands of years it has been the traditional land of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and we are grateful to have the opportunity to work on this land.\nRead about U of Tâ€™s Statement of Land Acknowledgement.\nUNIVERSITY OF TORONTO - SINCE 1827', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n')]
```

### Node Parserâ€Šâ€”â€ŠJSON

To handle JSON documents, weâ€™ll use a JSON parser.

```python
    from llama_index.core.node_parser import JSONNodeParser

    url = "https://housesigma.com/bkv2/api/search/address_v2/suggest"

    payload = {"lang": "en_US", "province": "ON", "search_term": "Mississauga, ontario"}

    headers = {
        'Authorization': 'Bearer 20240127frk5hls1ba07nsb8idfdg577qa'
    }

    response = requests.post(url, headers=headers, data=payload)

    if response.status_code == 200:
        document = Document(id_=url, text=response.text)
        parser = JSONNodeParser()

        nodes = parser.get_nodes_from_documents([document])
        print(nodes[0])
    else:
        print("Failed to fetch JSON content:", response.status_code)
```

Above code outputs:

```
    status True data house_list id_listing owJKR7PNnP9YXeLP data
    house_list house_type_in_map D data house_list price_abbr 0.75M data
    house_list price 749,000 data house_list price_sold 690,000 data
    house_list tags Sold data house_list list_status public 1 data
    house_list list_status live 0 data house_list list_status s_r Sale
```

### Node Parserâ€Šâ€”â€ŠMarkdown

To handle Markdown files, weâ€™ll use a Markdown parser.

```
    # Markdown
    from llama_index.core.node_parser import MarkdownNodeParser

    md_docs = FlatReader().load_data(Path("README.md"))
    parser = MarkdownNodeParser()

    nodes = parser.get_nodes_from_documents(md_docs)
    nodes[0].text
```

This output same as the Simple File parser showed:

Now we have seen the node parser, let's see how to do chunking by utilizing these node parsers.

```
    VectorDB-recipes
    <br />
    Dive into building GenAI applications!
    This repository contains examples, applications, starter code, & tutorials to help you kickstart your GenAI projects.

    - These are built using LanceDB, a free, open-source, serverless vectorDB that **requires no setup**.
    - It **integrates into python data ecosystem** so you can simply start using these in your existing data pipelines in pandas, arrow, pydantic etc.
    - LanceDB has **native Typescript SDK** using which you can **run vector search** in serverless functions!

    <img src="https://github.com/lancedb/vectordb-recipes/assets/5846846/d284accb-24b9-4404-8605-56483160e579" height="85%" width="85%" />

    <br />
    Join our community for support - <a href="https://discord.gg/zMM32dvNtd">Discord</a> â€¢
    <a href="https://twitter.com/lancedb">Twitter</a>

    ---

    This repository is divided into 3 sections:
    - [Examples](#examples) - Get right into the code with minimal introduction, aimed at getting you from an idea to PoC within minutes!
    - [Applications](#projects--applications) - Ready to use Python and web apps using applied LLMs, VectorDB and GenAI tools
    - [Tutorials](#tutorials) - A curated list of tutorials, blogs, Colabs and courses to get you started with GenAI in greater depth.
```

### Code Splitter

Code Splitter allows you to split your code with multiple languages supported. You can just mention the name of the language and do splitting.

```python
    # Code Splitting

    from llama_index.core.node_parser import CodeSplitter
    documents = FlatReader().load_data(Path("app.py"))
    splitter = CodeSplitter(
        language="python",
        chunk_lines=40,  # lines per chunk
        chunk_lines_overlap=15,  # lines overlap between chunks
        max_chars=1500,  # max chars per chunk
    )
    nodes = splitter.get_nodes_from_documents(documents)
    nodes[0].text
```

This creates a chunk of 40 lines of code as a result.

### Sentence Splitting

The `SentenceSplitter` attempts to split text while respecting the boundaries of sentences.

```python
from llama_index.core.node_parser import SentenceSplitter

splitter = SentenceSplitter(
    chunk_size=1024,
    chunk_overlap=20,
)
nodes = splitter.get_nodes_from_documents(documents)
nodes[0].text
```

This results in a chunk of 1024-size:

```
    Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:
    Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty. They have done so during periods of prosperity and tranquility. And they have done so in the midst of war and depression; at moments of great strife and great struggle.
    It's tempting to look back on these moments and assume that our progress was inevitable, that America was always destined to succeed. But when the Union was turned back at Bull Run and the Allies first landed at Omaha Beach, victory was very much in doubt. When the market crashed on Black Tuesday and civil rights marchers were beaten on Bloody Sunday, the future was anything but certain. These were times that tested the courage of our convictions and the strength of our union. And despite all our divisions and disagreements, our hesitations and our fears, America prevailed because we chose to move forward as one nation and one people.
    Again, we are tested. And again, we must answer history's call.
```

### Sentence Window Node Parser

The `SentenceWindowNodeParser` functions similarly to other node parsers, but with the distinction of splitting all documents into individual sentences. Each resulting node also includes the neighboring â€œwindowâ€ of sentences surrounding it in the metadata. Itâ€™s important to note that this metadata wonâ€™t be accessible to the LLM or embedding model.

```python
    import nltk
    from llama_index.core.node_parser import SentenceWindowNodeParser

    node_parser = SentenceWindowNodeParser.from_defaults(
        window_size=3,
        window_metadata_key="window",
        original_text_metadata_key="original_sentence",
    )
    nodes = node_parser.get_nodes_from_documents(documents)
    nodes[0].text
```

It results:

```
    Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:
    Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union.
```

### Semantic Splitting

Semantic chunking offers a new method in which, instead of breaking text into chunks of a fixed size, a semantic splitter dynamically chooses where to split between sentences, based on embedding similarity.

```python
    from llama_index.core.node_parser import SemanticSplitterNodeParser
    from llama_index.embeddings.openai import OpenAIEmbedding
    import os

    # Add OpenAI API key as an environment variable
    os.environ["OPENAI_API_KEY"] = "sk-****"

    embed_model = OpenAIEmbedding()
    splitter = SemanticSplitterNodeParser(
        buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model
    )

    nodes = splitter.get_nodes_from_documents(documents)
    nodes[0].text
```

Semantic Splitting results:

```
    Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:

    Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty.
```

### Token Text Splitting

```python
The `TokenTextSplitter` attempts to split to a consistent chunk size according to raw token counts.

    from llama_index.core.node_parser import TokenTextSplitter

    splitter = TokenTextSplitter(
        chunk_size=254,
        chunk_overlap=20,
        separator=" ",
    )
    nodes = splitter.get_nodes_from_documents(documents)
    nodes[0].text
```

Token Splitting results:

```
    Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:
    Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty. They have done so during periods of prosperity and tranquility. And they have done so in the midst of war and depression; at moments of great strife and great struggle.
    It's tempting to look back on these moments and assume that our progress was inevitable, that America was always destined to succeed. But when the Union was turned back at Bull Run and the Allies first landed at Omaha Beach, victory was very much in doubt. When the market crashed on Black Tuesday and civil rights marchers were beaten on Bloody Sunday, the future was anything but certain. These were times that tested the courage of our convictions and the strength of our union. And despite all our divisions and disagreements, our hesitations and our fears, America prevailed because we chose to move forward as one nation and one people.
    Again, we are tested. And again, we must answer history's call.
    One year ago, I took office amid two wars, an economy
```

### Hierarchical Node Parser

This node parser divides nodes into hierarchical structures, resulting in multiple hierarchies of various chunk sizes from a single input. Each node includes a reference to its parent node.

```python
    from llama_index.core.node_parser import HierarchicalNodeParser

    node_parser = HierarchicalNodeParser.from_defaults(
        chunk_sizes=[512, 254, 128]
    )

    nodes = node_parser.get_nodes_from_documents(documents)
    nodes[0].text
```

The results of the Hierarchical parser look like:

```
    Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:
    Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty. They have done so during periods of prosperity and tranquility. And they have done so in the midst of war and depression; at moments of great strife and great struggle.
    It's tempting to look back on these moments and assume that our progress was inevitable, that America was always destined to succeed. But when the Union was turned back at Bull Run and the Allies first landed at Omaha Beach, victory was very much in doubt. When the market crashed on Black Tuesday and civil rights marchers were beaten on Bloody Sunday, the future was anything but certain. These were times that tested the courage of our convictions and the strength of our union. And despite all our divisions and disagreements, our hesitations and our fears, America prevailed because we chose to move forward as one nation and one people.
    Again, we are tested. And again, we must answer history's call.
    One year ago, I took office amid two wars, an economy rocked by severe recession, a financial system on the verge of collapse and a government deeply in debt. Experts from across the political spectrum warned that if we did not act, we might face a second depression. So we acted immediately and aggressively. And one year later, the worst of the storm has passed.
    But the devastation remains. One in 10 Americans still cannot find work. Many businesses have shuttered. Home values have declined. Small towns and rural communities have been hit especially hard. For those who had already known poverty, life has become that much harder.
    This recession has also compounded the burdens that America's families have been dealing with for decades -- the burden of working harder and longer for less, of being unable to save enough to retire or help kids with college.
    So I know the anxieties that are out there right now. They're not new. These struggles are the reason I ran for president. These struggles are what I've witnessed for years in places like Elkhart, Ind., and Galesburg, Ill. I hear about them in the letters that I read each night.
```

**Colab Walkthrough**

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/tutorials/Langchain-LlamaIndex-Chunking/Langchain_Llamaindex_chunking.ipynb)

### Conclusion

Langchain and Llama Index are popular tools, and one of the key things they do is "chunking." This means breaking down data into smaller pieces, which is important for making the tools work well. These platforms provide a variety of ways to do chunking, creating a unified solution for processing data efficiently. This article will guide you through all the chunking techniques you can find in Langchain and Llama Index.

**References**

- The aim is to get the data in a format where it can be used for anticipated tasks, and retrieved for value later. Rather than asking â€œHow should I chunk my data?â€, the actual question should be â€œWhat is the optimal way for me to pass data to my language model that it needs for its task?â€ - [link](https://medium.com/@bavalpreetsinghh/llamaindex-chunking-strategies-for-large-language-models-part-1-ded1218cfd30)
```

---

## ðŸ“„ æ–‡ä»¶: columnar-file-readers-in-depth-apis-and-fusion.md

---

```md
---
title: "Columnar File Readers in Depth: APIs and Fusion"
date: 2024-06-18
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/columnar-file-readers-in-depth-apis-and-fusion/columnar-file-readers-in-depth-apis-and-fusion.png
description: "The API used to read files has evolved over time, from simple full table reads to batch reads and eventually to iterative record batch readers. Lance takes this a step further to return a stream of read tasks."
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "westonpace"
---

The API used to read files has evolved over time, from simple "full table" reads to batch reads and eventually to iterative "record batch readers". These more sophisticated APIs add additional power and flexibility. Lance takes this a step further to return a "stream of read tasks". This originates from our decision to separate scheduling from decode and enables operator fusion. The decode work can be fused into later steps in a query pipeline. In this article, I briefly recap the different APIs, talk about Lance's approach, and then show some results explaining the benefits.

{{< admonition info "ðŸ“š Series Navigation" >}}
This is part of a series of posts on the details we've encountered building a columnar file reader:
1. [Parallelism without Row Groups](/blog/file-readers-in-depth-parallelism-without-row-groups/)
2. **APIs and Fusion** (this article)
3. [Backpressure](/blog/columnar-file-readers-in-depth-backpressure/)
4. [Compression Transparency](/blog/columnar-file-readers-in-depth-compression-transparency/)
5. [Column Shredding](/blog/columnar-file-readers-in-depth-column-shredding/)
6. [Repetition & Definition Levels](/blog/columnar-file-readers-in-depth-repetition-definition-levels/)
{{< /admonition >}}

## Full Table Reads

To kick off our history of read APIs we can start with the most basic API, a full table read. Its main power is simplicity. Given a file, create a table. This can be a large contiguous table like what you would get from pandas `read_parquet` or an internally chunked table like what you get from pyarrow's `parquet.read_table`. The pros and cons are pretty straightforward.

![Whole File Read](/assets/blog/columnar-file-readers-in-depth-apis-and-fusion/Whole-File-Read.png)

*Columns we want are decoded (decompressed) and moved entirely into memory*

**Pros:** 
- âœ… Simplicity

**Cons:** 
- âŒ Only works if the entire file fits in memory 
- âŒ No fusion of decode and query

The downside here is very limiting. We generally expect that files will be larger than memory. Even if the file isn't (maybe we partitioned a huge dataset into many small files) we still probably want to stream through our data so our reader doesn't require an embarrassing amount of RAM. If we try and make the files *even smaller* then the per-file overhead is too great. This approach is not suitable for large scale search tasks. We also don't get any fusion, we'll talk about that later.

{{< admonition tip "ðŸ’¡ Still Useful" >}}
Full table reads are still ideal for unit tests, demos, and small tasks and the API isn't going anywhere anytime soon.
{{< /admonition >}}

## Read Batch

The next API is also pretty simple. We take in a file and a "batch number" and return a batch of data.

![Batch Read](/assets/blog/columnar-file-readers-in-depth-apis-and-fusion/Batch-Read.png)

*Reading an individual batch at a time allows us to read less data at once*

**Pros:** 
- âœ… Simplicity 
- ðŸŸ¡ Better control over memory

**Cons:** 
- ðŸŸ¡ Requires agreed upon definition of a batch 
- ðŸŸ¡ Often requires the user get involved in parallelism 
- âŒ No fusion of decode and query

This API is pretty popular in Parquet which has a row group concept and thus a natural concept of "batches". A good example is pyarrow's `ParquetFile.read_row_group`. Unfortunately, it requires the writer to decide what resolution the reader will use for reading the file. Also, batches encourage per-batch parallelism which is something we wanted to explicitly avoid in LanceDB (explained in [an earlier post](/blog/file-readers-in-depth-parallelism-without-row-groups/)). For example, we get things like this advice from DuckDb:

> DuckDB works best on Parquet files with row groups of 100K-1M rows each. The reason for this is that DuckDB can only parallelize over row groups â€“ so if a Parquet file has a single giant row group it can only be processed by a single thread.

In addition, we force the user to decide how many groups to read at once. This often forces the user to make inconvenient decisions based on I/O parallelism. For example, if I'm reading a single column I probably want to read many groups at once. If I'm reading many columns then I don't. This is a complexity that users would rather not deal with.

## Record Batch Reader

Finally we reach the modern "record batch reader". This API returns an iterator of batches (synchronous or asynchronous).

![Iterative Read](/assets/blog/columnar-file-readers-in-depth-apis-and-fusion/Iterative-Read.png)

**Pros:** 
- âœ… Moves I/O parallelism into the reader 
- âœ… Flexible batch sizes 
- âœ… Low memory requirements

**Cons:** 
- ðŸŸ¡ Slightly more complex 
- âŒ No fusion of decode and query

This API is powerful and it should be no surprise it is popular in modern open source file readers. We can see an example in pyarrow dataset's `to_batches` or parquet-rs' `ParquetRecordBatchReader`. The batch size is now very flexible. If a user wants small batches then a reader can deliver them. This is even true in formats like Parquet that use row groups because the reader can iterate over data pages without requiring a full read of the row group (provided the row group is large enough. Very small row groups can still sabotage this kind of approach). Note that only parquet-rs' implementation actually does this correctly. Pyarrow's implementation still largely relies on appropriately sized row groups (it iterates in smaller batches but still loads entire row groups into memory).

## Perilous parallelism

Both of the examples I've provided have slightly different approaches to parallelism than Lance. This is not technically a fault of the API but it does motivate our decision for a different API and so I'll explain briefly.

Pyarrow's dataset reader achieves parallelism through "batch readahead".

In parquet-rs parallelism is provided by creating multiple record batch readers by splitting up the row groups. Both approaches have drawbacks.

![Perilous Parallelism](/assets/blog/columnar-file-readers-in-depth-apis-and-fusion/Perilous-Parallelism.png)

*Challenges faced when getting parallelism correct*

The multiple readers approach forces the user to make I/O parallelism decisions. The user must pick how many readers to create and this depends on the filesystem, the number of columns being read, the size of those columns, and is just plain tricky to calculate.

The pyarrow readahead approach avoids the multiple readers problem but is flawed because of the previously mentioned limitation that pyarrow still requires full record batch reads. Achieving sufficient parallelism is almost guaranteed to require too much memory or too small row groups (and expensive file rewrites are required to tweak this parameter).

Note again that these challenges are not a result of the API (record batch reader) but rather a limitation with other aspects of the implementations.

## Can we do better?

If we implement an ideal record batch reader API, is this the best we can do? It turns out the answer is "no". To demonstrate this I hooked up the Lance file reader to the Apache Datafusion query engine (we use Datafusion extensively in Lance and are big fans). I then ran TPC-H query 6 which is simple but has a filter step that should at least introduce some compute work. I tested with TPC-H scale factor 10 and I tested the case where the data was hot in the kernel page cache to avoid being I/O bound (which is not always realistic but important for today and I'll leave the rest of that discussion for a future blog post). I tested with both a record batch reader and with what Lance calls a "read task reader". Note, I am testing the Lance format in both cases.

![RBR vs RTR](/assets/blog/columnar-file-readers-in-depth-apis-and-fusion/RBR-vs-RTR.png)

*The "read task" reader is very consistently about 30% faster*

What's going on?

To help illustrate the difference I used the Intel vTune profiler to profile the slower approach:

![Screenshot from vTune](/assets/blog/columnar-file-readers-in-depth-apis-and-fusion/Screenshot-from-2024-06-09-23-12-14.png)

100% "memory bound" might not be something that developers are used to seeing ðŸ˜„. It's certainly not something I ever encountered before working on query engines. However, memory latency / bandwidth, and it's companion "cache utilization", are real issues in query engine design. To understand what is going on here we have to really break down the scanning process.

![Memory Bottleneck](/assets/blog/columnar-file-readers-in-depth-apis-and-fusion/Memory-bottleneck.png)

*Multiple cores accessing the main memory can lead to a memory bottleneck*

Both the decode thread and the filter thread are accessing main memory. The decode thread pulls data out of main memory, places it in the CPU cache / registers, decodes the data, and then puts it back into main memory. The filter thread then reads this decoded data from main memory. Ideally, we want to find some way to avoid that trip to main memory.

Here's a simple analogy. Let's imagine we have two chefs making sandwiches. We could divide the work amongst chefs (we're just going to imagine they store their bread in the refrigerator for some reason):

- **Chef A**
  - Go into the fridge and get two slices of bread and a jar of mayo
  - Apply mayonnaise on the two slices of bread
  - Put the bread slices back in the fridge
- **Chef B**
  - Go into the fridge and get one slice of turkey and two slices of bread with mayo (from chef A)
  - Put the turkey on the bread
  - Put the finished sandwich in the fridge

Alternatively, we could have both chefs do the same thing:

- **Chef A & B:**
  - Go into the fridge, get two slices of bread, a jar of mayo, and one slice of turkey
  - Make a sandwich
  - Put the finished sandwich back in the fridge

These both seem like reasonable approaches to making sandwiches. However, let's now move the fridge 400 meters away from the workstation. Suddenly the first approach is much less appealing because it requires two trips to the refrigerator per sandwich.

## Fusion to the rescue

The solution to this problem is a concept sometimes described as "operator fusion" (the concept is also sometimes referred to as "data parallelism").

> Operator fusion is a way to improve performance by merging one operator [..] into a different operator so that they are executed together without requiring a roundtrip to memory

In order to do any kind of work on data we have to move it from main memory into the CPU cache (and into the registers). If we can perform two operations back to back then we can avoid a round trip to main memory. Here is a simple example that doesn't even use threads:

```python
# Two passes through the data (no fusion)

def decode(data_stream):
  decoded = []
  for cache_sized_batch in data_stream:
    decoded.append(decode(cache_sized_batch))
  return decoded

def filter(data_stream):
  filtered = []
  for cache_sized_batch in data_stream:
    filtered.append(filter(cache_sized_batch))
  return filtered

decoded_and_filtered = filter(decode(data_stream))

# One pass through the data (fused)

def decode(data_stream):
  for cache_sized_batch in data_stream:
    yield decode(cache_sized_batch)

def filter(data_stream):
  for cache_sized_batch in data_stream:
    yield filter(cache_sized_batch)

decoded_and_filtered = list(filter(decode(data_stream)))
```

This example points out a neat point. "Fusing" two operators does not mean they need to be mashed into a single method. It only means that we need to avoid running the operations on two different and distinct passes through the data (or two different CPU cores).

## "Read Task" Reader

This leads us to the newest file reader API, utilized by Lance, which is an iterator of "read tasks". A read task performs the actual decode from the on-disk format into the desired in-memory layout of the data.

**Pros:** 
- âœ… Moves I/O parallelism into the reader 
- âœ… Flexible batch sizes 
- âœ… Low memory requirements 
- âœ… Fusion of decode and query

**Cons:** 
- ðŸŸ¡ Slightly more complex

The read task reader is the API used by Lance. Actually, Lance offers all of the above APIs as convenience APIs, but they delegate to the read task reader.

The read task reader supports all of the features we want. The user can choose whatever batch size they would like. Read tasks are delivered quickly and (mostly) synchronously. Parallelism in Lance is done by read ahead. The user picks how many batches they want to compute in parallel and grabs that many read tasks (rust streams make this easy with the buffered API).

Here are a few charts demonstrating these benefits as well as some of the performance characteristics of the Lance file reader.

![Latency vs Batch Size](/assets/blog/columnar-file-readers-in-depth-apis-and-fusion/Latency-v-Batch-Size.png)

*Full control over batch sizes lets us pick batches ideal for CPU cache locality*

![Latency vs Threads](/assets/blog/columnar-file-readers-in-depth-apis-and-fusion/Latency-v-Threads.png)

*We can spin the number of CPU threads up and down without affecting our I/O parallelism*

The I/O parallelism is completely independent from the CPU parallelism. The user can grab 20 read tasks or a single read task at a time and it will have no affect on how much parallel I/O the reader performs (assuming the user is keeping up and back pressure is not applied). An added bonus is that we will never read the file out of order (there is simply no reason to do so).

Finally, because the decode task is executed by the user (the query engine), we now have the ability for operators to be fused:

![Memory Fusion](/assets/blog/columnar-file-readers-in-depth-apis-and-fusion/Memory-fusion.png)

*Fusing the operators means we can use the cache and avoid a round trip to main memory*

## Does it matter?

Operator fusion is a "query engine best practice" and we wanted to make sure we handle it. However, it is one of those things that is incredibly difficult to prove useful in actual practice. In many situations it doesn't matter.

- If you're I/O bound then it doesn't matter
- If you're not highly parallel then it doesn't matter (prefetch hides RAM latency)
- If you're not doing any CPU work then it doesn't matter
- If you need to cross a language boundary then you cannot (today) fuse the work (there is no "read task reader" in the C data interface)
- RAM bandwidth is skyrocketing in modern systems

Still, in the situations where it does matter, it's a great tool to have available. Also, just to test that last bullet point I tried running the experiment on my macbook (the M1 has a staggering amount of RAM bandwidth) and still saw considerable speedup with the fused approach (and an incredible 220ms runtime, the M1 is an awesome chip). My best guess is the prefetch isn't entirely able to overcome the latency but these effects are hard to measure.

## What about Parquet?

Nothing I've talked about in this blog post is particular to the Lance file format. A Parquet file reader can (and maybe should) be written that separates scheduling from decode and allows for decode / query fusion.

There is one caveat: it may not be possible to create such a reader when generalized block compression is used (e.g. snappy, gzip). This is because compression blocks don't normally line up across columns / batch sizes. This means you either have to run the decompression multiple times or you have to run the decompression on a separate thread from the query. On the other hand, it may be possible to size your compression blocks so that they all have the same # of rows (or divide into some common number) and then use this as your batch size. However, this only applies to full range reads, which is something we don't want to rely on in Lance.

Lastly, I should mention that there has been a lot of recent activity and discussion on the Parquet mailing list about potentially creating a new Parquet version. This is pretty cool and we are following along. Maybe someday the Lance writer can create files that are parquet-reader-compatible but still performant for our random access use cases.

## P.S. (fun trivia)

The fact that I am using DataFusion to demonstrate operator fusion did not escape my notice. I wondered if it was a coincidence or if the DataFusion name was inspired by operator fusion in some way and so I asked on the DataFusion discord where Andy Grove revealed that yes, it was indeed a coincidence.

> my algorithm for coming up with product names is to write one list of words that are closely related to the project (so "data" came from that list) and then a bunch of random words that I think sound cool so that included things like "fire", "storm", "fusion" and then pair words together. I also felt that I could come up with some meaning later about how fusion could be justified, but I don't recall if I was aware of the concept of operator fusing back then. Also, I was driving a Ford Fusion at the time, which was a possible influence ðŸ˜ƒ -Andy Grove

## What's Next

LanceDB is upgrading the modern data lake (postmodern data lake?). Support for multimodal data, combining search and analytics, embracing embedded in-process workflows and more. If this sort of stuff is interesting then check us out and come join the conversation.

If you're interested in learning more about our file format innovations or contributing to Lance, hop on over to our [Discord](https://discord.gg/G5DcmnZWKB) or [Github](https://github.com/lancedb/lance) and we'd be happy to talk to you!
```

---

## ðŸ“„ æ–‡ä»¶: columnar-file-readers-in-depth-backpressure.md

---

```md
---
title: "Columnar File Readers in Depth: Backpressure"
date: 2024-03-25
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/columnar-file-readers-in-depth-backpressure/columnar-file-readers-in-depth-backpressure.png
description: "Streaming data applications can be tricky. When you can read data faster than you can process the data then bad things tend to happen. The various solutions to this problem are largely classified as backpressure."
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "westonpace"
---

Streaming data applications can be tricky. When you can read data faster than you can process the data then bad things tend to happen. The most common scenario is you run out of memory and your process crashes. When the process doesn't crash, it often breaks performance (e.g. swapping or pausing inefficiently). The various solutions to this problem are largely classified as "backpressure". It's been a while since I last posted in this series and one of the things I've been spending a lot of time on is getting backpressure right. It's proven to be quite tricky ðŸ™ƒ In this blog post I want to explain some classic approaches for backpressure and then describe the approach we've been taking in the Lance file reader.

{{< admonition info "ðŸ“š Series Navigation" >}}
This is part of a series of posts on the details we've encountered building a columnar file reader:
1. [Parallelism without Row Groups](/blog/file-readers-in-depth-parallelism-without-row-groups/)
2. [APIs and Fusion](/blog/columnar-file-readers-in-depth-apis-and-fusion/)
3. **Backpressure** (this article)
4. [Compression Transparency](/blog/columnar-file-readers-in-depth-compression-transparency/)
5. [Column Shredding](/blog/columnar-file-readers-in-depth-column-shredding/)
6. [Repetition & Definition Levels](/blog/columnar-file-readers-in-depth-repetition-definition-levels/)
{{< /admonition >}}

## Push vs. Pull?

Whenever we talk about backpressure we also often talk about push vs. pull. In a push-based system a producer creates and launches a task as soon as data is available. This makes parallelism automatic but backpressure is tricky. In a pull-based system the consumer creates a task to pull a batch of data and process it. This makes backpressure automatic but parallelism is tricky.

The reality is that every multi-threaded scanner / query engine is doing both push and pull somewhere if you look hard enough. Both approaches have to solve this problem and systems tend to use the same classic solution to do so.

## Classic Solution: Row Group Queue

The classic solution is to limit the number of row groups that can be processed with a queue somewhere between the producer and the consumer. If the queue fills up the then file reader will stop reading. There are a variety of ways this queue can be implemented (e.g. a blocking queue in a push-based workflow or a readahead queue in a pull-based workflow).

![Classic Row Group Queue](/assets/blog/columnar-file-readers-in-depth-backpressure/Classic-Backpressure.png)

*A classic reader adds batches to a queue which a classic engine pulls from*

As an example we can consider the pyarrow scanner which has a `from_dataset` method with the following parameter:

```
batch_readahead - int, default 16

    The number of batches to read ahead in a file. This might not work for all file formats. Increasing this number will increase RAM usage but could also improve IO utilization.
```

I don't want this post to be too long so I'm going to summarize the issues I've historically encountered with the row group queue approach:

- It's often unclear whether this limit applies to file-based row groups (which tend to be large) or compute-based batches (which tend to be small).
- The correct limit will depend on the query (how many columns are being loaded)
- Not all batches are the same size and so the correct limit can even fluctuate within a single query.
- Queuing batches means you are inserting a thread transfer between decode and compute. This is something we wanted to avoid in Lance.

![Variable Batch Sizes](/assets/blog/columnar-file-readers-in-depth-backpressure/Batch-Backpressure-Problems.png)

*All three queues hold the same number of batches but have drastically different RAM usage*

Our motivation for Lance is to find an approach that is simple to configure, can be consistently applied to all datasets, and can work without a concept of row groups.

## Lance Solution: I/O Buffer

To understand the Lance solution we need to revisit the concept of separating I/O from decoding that we described in [an earlier post](/blog/columnar-file-readers-in-depth-apis-and-fusion/). Every Lance reader has a "scheduling thread" and a "decode thread". The scheduling thread determines what I/O we need to issue and the decode thread takes the data, as it arrives, and decodes it.

In between these two threads is a special component called the I/O scheduler. It receives I/O requests from the scheduling thread, sorts them according to their priority order, throttles how many concurrent requests we can make, and then delivers the results to the decoders. This gives us the perfect place to apply backpressure.

![Lance I/O Buffer](/assets/blog/columnar-file-readers-in-depth-backpressure/v2-Backpressure.png)

When the I/O scheduler finishes an I/O request it places the data in a queue for the decoders, the I/O buffer. If this queue fills up then we know the decoders are falling behind. The I/O scheduler will then stop issuing new I/O requests. When the decoders catch up then the buffer will start to empty and we can start issuing I/O requests again.

The best part about the I/O buffer is that the I/O buffer size is in **bytes** and not **rows**. What's more, we can easily calculate the correct value. We want to make sure that we have enough room in the buffer to saturate our I/O.

{{< admonition tip "ðŸ’¡ Cloud Storage Considerations" >}}
If we are working with cloud storage, we need a lot of space! This is true even if your compute layer is operating on small 1MiB batches and this is often surprising to people. We typically want to make many concurrent requests to cloud storage (e.g. at least 64-128) and those requests should be large (at least 8-16MiB). Our current default is 2GiB which gives us enough space for 128x16-MiB requests.
{{< /admonition >}}

## One Small Problemâ€¦

Unfortunately, we have one small wrinkle that's caused us a lot of trouble. When we are reading variable-size data types (e.g. strings, lists, etc.) we don't know up-front how many items we need to read from disk. For example, if the user asks for 10,000 strings then will a column need 1KiB? 1MiB? 50MiB? We simply don't know.

We never want to pause I/O. This means we don't want to stop and wait until the string sizes come back before we continue reading other columns. As a result, we end up scheduling some "lower priority" requests while we are waiting for the string sizes to come back. Once we know the string sizes we can make a high priority request to get the data.

Unfortunately, if our I/O buffer fills up with low priority items then we might not have space for a high priority request once our string sizes get back to us. The decoder works in priority order and, since our high priority request is stuck, the decoder gets stuck and the whole system grinds to a deadlock.

![Deadlock Scenario](/assets/blog/columnar-file-readers-in-depth-backpressure/v2-Backpressure-Priority.png)

*Example deadlock scenario*

Our solution today is to allow requests to bypass the backpressure limit if they are higher priority than anything in the backpressure queue. This works, but it's a bit messy, and it got us thinking, "wouldn't it be great if we knew the priority ahead of time?" It turns out that we can record that information when we write the file and this is exactly what we plan to do in our 2.1 format.

{{< admonition info "ðŸ’¡ Lance 2.1 Preview" >}}
I don't think I've mentioned 2.1 yet but we learned a few things working on 2.0 (like this) and I'll be doing a retrospective at some point to share them and talk about our upcoming plans.
{{< /admonition >}}

It's unlikely users will ever notice (the current system works very well anyways) but it will let us rip out several complicated branches from our scheduling and decoding logic once we have finalized 2.1 and so I'm excited to move to the new system.

## Conclusion

Configuring backpressure in Lance is very easy. In fact, it is so easy you should never really have to mess with it in normal scenarios (the default of 2GiB works well for most situations). We can read massive data files, turn around and process them slowly, and never have to worry about the reader using way too much memory. This is absolutely essential when working with multi-modal data because the large data types make it all too easy to waste RAM on the machine whenever you have an expensive processing step.

## What's Next

LanceDB is upgrading the modern data lake (postmodern data lake?). Support for multimodal data, combining search and analytics, embracing embedded in-process workflows and more. If this sort of stuff is interesting then check us out and come join the conversation.

If you're interested in learning more about our file format innovations or contributing to Lance, hop on over to our [Discord](https://discord.gg/G5DcmnZWKB) or [Github](https://github.com/lancedb/lance) and we'd be happy to talk to you!
```

---

## ðŸ“„ æ–‡ä»¶: columnar-file-readers-in-depth-column-shredding.md

---

```md
---
title: "Columnar File Readers in Depth: Column Shredding"
date: 2025-05-15
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/columnar-file-readers-in-depth-column-shredding/columnar-file-readers-in-depth-column-shredding.png
description: "Explore columnar file readers in depth: column shredding with practical insights and expert guidance from the LanceDB team."
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "westonpace"
---

Record shredding is a classic method used to transpose rows of potentially nested data into a flattened tree of buffers that can be written to the file. A similar technique, cascaded encoding, has recently emerged, that converts those arrays into a flattened tree of compressed buffers. In this article we explain and connect these ideas, but also explore the impacts of all this shredding on our goal of random access I/O, which leads to new approaches such as reverse shredding.

{{< admonition info "ðŸ“š Series Navigation" >}}
This is part of a series of posts on the details we've encountered building a columnar file reader:
1. [Parallelism without Row Groups](/blog/file-readers-in-depth-parallelism-without-row-groups/)
2. [APIs and Fusion](/blog/columnar-file-readers-in-depth-apis-and-fusion/)
3. [Backpressure](/blog/columnar-file-readers-in-depth-backpressure/)
4. [Compression Transparency](/blog/columnar-file-readers-in-depth-compression-transparency/)
5. **Column Shredding** (this article)
6. [Repetition & Definition Levels](/blog/columnar-file-readers-in-depth-repetition-definition-levels/)
{{< /admonition >}}

### Record Shredding Overview

At the start of our journey we typically have some kind of row-based table, or perhaps it is a stream of events, or maybe a vector of objects. All of these are "row based" structures and they are extremely intuitive to work with and a natural representation of data. This is typically the format that data arrives at our database and the format that data emerges from our database.

Unfortunately, this format is not great for bulk processing or storage. There are a lot of specific reasons for this but the general idea is that we don't typically operate on all of our columns at once. Working with data we don't need has costs (I/O, cache, etc.) As a result, we need to transpose our data from a row-based structure to a column-based structure.

![Row to Column Transformation](/assets/blog/columnar-file-readers-in-depth-column-shredding/Rows-to-Columns-1-.png)
*Record shredding is the engine of the rowâ†’column transformation*

## Level 1: Table â†’ Columns

The first level of transpose is the simplest one. We split the **table** into **columns**. If we're lucky to have simplistic data, like a 2D matrix, or a spreadsheet, then this basically completes our journey. However, realistic data is rarely so simple. In fact, you might not even think you have a table. Tables are 2D structures after all and our objects don't feel quite so flat and two dimensional.

My first deep dive into data analysis (an indeterminate number of years ago) dealt with results from a manufacturing test system. Each time a test ran it would generate a test result. The test result had some basic properties (date of test, id & version of device under test, id & version of test equipment, etc.) but also some nested properties. For example, there were various tests that would run and each would run 20 iterations at different test frequencies and we would measure a score for each input frequency.

{{< admonition tip "ðŸ’¡ What is a Row?" >}}
What is a row in this case? Is it a single test frequency? Or is it an entire test result? What if we run a series of tests as part of a test suite?

The answer is, as usual, "it depends". One of the best studies of this problem is the R philosophy of [tidy data](https://r4ds.hadley.nz/data-tidy.html). If you are struggling with these kinds of questions then I can't recommend that link enough.
{{< /admonition >}}

For now, let's pretend our test results are in YAML and we receive one row per YAML test result and that looks something like this:

```yaml
timestamp: 514883700000
dut_sn: "D0N70P3ND34D1N51D3"
dut_version: "123.45"
bench_sn: "5H4K35H4K35H4K35H4K3"
tests:
  rise_time:
    - frequency: 10
      value: 0.3
    - frequency: 20
      value: 0.5
    - frequency: 30
      value: 0.2
    - frequency: 40
      value: 0.45
  settling_time:
    - frequency: 10
      value: 0.4
    - frequency: 20
      value: 0.6
    - frequency: 30
      value: 0.3
    - frequency: 40
      value: 0.55
```
*This YAML document shows a single row of our data in a row-based representation*

If we only apply a single level of transpose we get five columns, `"timestamp", "dut_sn", "dut_version", "bench_sn", and "tests"`. This last column (`tests`) is tricky. There are multiple columns inside of it and even a nested list of values. One classic strategy to deal with this problem (or, more accurately, not deal with it) is to just convert the "tests" column into a JSON object.

![Level 1 Shredding](/assets/blog/columnar-file-readers-in-depth-column-shredding/Shredding-Level-1-1-.png)
*Example level 1 encoding of our data*

There are several issues with this strategy, but today we're focused on I/O. So let's ignore the JSON encode / decode cost. The main problem is that it still mixes unrelated data into our data of interest. For example, if we want to analyze our `rise_time` experiments we still are going to suffer because those values are bunched together with `settling_time`. Fortunately, we can do better.

## Level 2: Column â†’ Array

Our first pass gave a rather simple transposition of our data. We are now working with columns and not rows. However, some of these columns are nested columns, and those are basically little rows. We can do better. We can shred.

{{< admonition fun-fact "ðŸ¤” Etymology Trivia" >}}
I lost myself in a little bit of etymology trivia here. Most discussions of record shredding cite the "[Dremel paper](https://research.google/pubs/dremel-interactive-analysis-of-web-scale-datasets-2/)" as the source of the technique (it is, and this paper also introduces repetition and definition levels which is a future topic). However, the paper never uses the word "shredding". If you know where that specific word originated from then let me know and I'll update this box!
{{< /admonition >}}

At this next level we are going to convert a sequence of columns into a flattened sequence of arrays. To do this we need to look in more detail at the struct array.

### Structs

For clarity, by "struct" I am talking about a high level concept, and not any specific language feature. In JSON & YAML we'd call these "objects". In XML we'd call them elements. In Arrow we call them structs. Looking at our above test result example we have `tests` which is a struct. It has two child fields `rise_time` and `settling_time`. To shred this, all we need to do is split out each child array.

![Level 2 Shredding](/assets/blog/columnar-file-readers-in-depth-column-shredding/Shredding-Level-2.png)
*Level 1 gives us a set of columns while level 2 gives us a set of arrays*

However...there is one small sticking point (isn't there always), which is nullability. Consider the following YAML representation of three points `(7, 3), NULL, (NULL, 5)`:

```yaml
- point:
    x: 7
    y: 3
- point: null
- point:
    x: null
    y: 5
```
*The middle point is null*

If we *just* split this into two different arrays we would get:

```yaml
point.x:
 - 7
 - null
 - null
point.y:
 - 3
 - null
 - 5
```
*The null middle point flows into the children in this version of shredding*

But then, when we reassemble this, we could also get:

```yaml
- point:
    x: 7
    y: 3
- point:
    x: null
    y: null
- point:
    x: null
    y: 5
```
*When we return, the middle point is no longer null because we lost that information*

Note that this isn't *quite* the same as what we started with. Interestingly, the solution to this problem is right about where Arrow, Parquet, and ORC start to diverge. That being said, none of these solutions operate at Level 2, and so there is no *real* reference solution at this layer. In the interest of fun, let's invent one real quick, sticking with our YAML.

```yaml
point.x:
 - value: 7
 - nonvalue: 1 
 - nonvalue: 0
point.y:
 - value: 3
 - nonvalue: 1
 - value: 5
```

This value / non-value scheme may look familiar to those with advanced Parquet knowledge. We'll be returning to it with a vengeance in the next blog of this series ;)

This is a level 2 YAML encoding of our points. We encode every item as an object with one of two properties. If the object has the `value` property it points to the value. If the object has the `nonvalue` property it points to a special integer. If this special integer is 0 then the item itself is null. If it is non-zero then you have to walk that far up the structure tree to find the actually null (e.g. 1 means "parent is null", 2 means "grandparent is null", etc.)

**What about lists?** At level 2, we are just moving from columns to arrays. A list is already a single array. There isn't anything we have to do here. All we are doing at level 2 is flattening out structs. We will see lists soon in level 3.

## Level 3: Array â†’ Buffer

By this point, things are feeling pretty column-oriented. To get further, we have to really start thinking about "what is an array really?" As an example, let's look at our original manufacturing test example. Stopping after level 2 we have:

```yaml
timestamp: 514883700000
dut_sn: "D0N70P3ND34D1N51D3"
dut_version: "123.45"
bench_sn: "5H4K35H4K35H4K35H4K3"
tests.rise_time.frequency:
  - - 10
    - 20
    - 30
    - 40
tests.rise_time.value:
  - - 0.3
    - 0.5
    - 0.2
    - 0.45
tests.settling_time.frequency:
  - - 10
    - 20
    - 30
    - 40
tests.settling_time.value:
  - - 0.4
    - 0.6
    - 0.3
    - 0.55
```
*Columnar representation with flattened arrays, still a single row of data*

Let's pretend we took a few more measurements and let's zoom in on just the `tests.rise_time.frequency` column.

```yaml
tests.rise_time.value:
  - - 0.3
    - 0.5
    - 0.2
    - 0.45
  - - 0.33
    - 0.43
    - 0.25
    - 0.4
  - - 0.5
    - 0.32
    - 0.6
    - 0.1
```
*Now we have three rows but are just looking at one specific column (the data type for this column is a list of floats)*

Again, it sure looks pretty columnar. But...*"what is a list really?"* Or, more concretely, what does this look like in RAM (and/or storage)? Let's take a look at the memory representation we get from the naive python representation of `[[0.3, 0.5, 0.2, 0.45], [0.33, 0.43, 0.25, 0.4], [0.5, 0.32, 0.6, 0.1]]`

![What is a List](/assets/blog/columnar-file-readers-in-depth-column-shredding/What-is-a-List.png)
*Our list of objects array (in python) occupies at least 7 distinct locations in RAM*

A "list of lists" in this case is a contiguous array of objects, each pointing to a list object (which contains data like the list size, etc.). Each list object then points to a contiguous array of values. Our data is scattered all over the place. This is "not great"â„¢ï¸ for columnar analytics. Now we can understand our goal for level 3. We want to go from a flattened sequence of arrays to a flattened sequence of buffers.

{{< admonition warning "â— Buffer Definition" >}}
When I talk about "buffers" here I am talking about contiguous arrays of fixed size values. I.e. a "buffer of integers" or a "buffer of bits". The important thing is that we have "one range in RAM/disk" and not the details of how we are converting objects to bytes.
{{< /admonition >}}

### Decomposing lists

There is at least two ways we can turn a list array into a sequence of flattened buffers. In Arrow and ORC we use "offsets + values". In Parquet we use repetition levels. Repetition levels are more complicated, and I'm going to talk about them in a future post, so let's look at the "offsets + values" approach instead. Once again, we can visualize this using YAML.

```yaml
tests.rise_time.value.offsets:
  - 0
  - 4
  - 8
  - 12
tests.rise_time.value.values:
  - 0.3
  - 0.5
  - 0.2
  - 0.45
  - 0.33
  - 0.43
  - 0.25
  - 0.4
  - 0.5
  - 0.32
  - 0.6
  - 0.1
```

The `values` buffer is a single contiguous buffer of flattened values. The `offsets` buffer is a single contiguous buffer of offsets. If we want to find the list at index `i` we just need to grab `offsets[i]` and `offsets[i+1]`. For more complete examples, including lists with varying numbers of items, you can look up Arrow tutorials (or even the [reference itself](https://arrow.apache.org/docs/format/Columnar.html#variable-size-list-layout)).

{{< admonition tip "ðŸ’¡ Why Buffers?" >}}
We want "buffers" and not "arrays" because buffers give us large contiguous access patterns. This is obviously important for storage but, with the rise of vectorization, it's equally important for compute.
{{< /admonition >}}

### Revisiting Structs

Now that we have this third level of shredding we can look at the Arrow approach to solving the struct nullability problem. In Arrow (and ORC), nullability (or more intuitively, "validity") is a separate bit buffer that stores a 1 for each non-null entry and a 0 for each null entry. Let's put all this together and look at our original manufacturing test data, but now split into buffers using Arrow's encoding scheme.

![Level 3 Arrow Shredding](/assets/blog/columnar-file-readers-in-depth-column-shredding/Shredding-Level-3-Arrow.png)
*Level 3 is where we start to go beyond "columnar" and into something else (bufferlar is not a great word I think)*

I keep mentioning that Parquet's encoding scheme is different, but it is still accomplishing the same goal. Without going into great detail let's look at the same data with Parquet's encoding scheme. This difference in schemes is rather interesting. Note that one of them feels more "transparent" ðŸ˜‰

![Arrow vs Parquet Style](/assets/blog/columnar-file-readers-in-depth-column-shredding/Shredding-Level-3-A-vs-P.png)
*Two different approaches for converting list arrays into buffers*

## Level 4+: Buffer â†’ Compressed Buffer

At this point our data has been shredded into nice contiguous buffers. Most tutorials on shredding will stop here. We won't. *We...can...shred...more*!

We've seen that we can look at our shredding so far as a tree of operations that brought us from our root node (a row) to a series of leaf nodes (the buffers). A relatively recent paper ([BtrBlocks](https://www.google.com/search?client=firefox-b-1-d&q=btrblocks)) pointed out that compression can also be viewed in this way. In fact, compression can, in some ways, be thought of as just *more layers of shredding*. We can split a single buffer into a flattened sequence of buffers, where the sum of our leaf buffers is smaller than the original buffer.

Let's look at an example. Again, let's pretend we have three rows but now we can look at the `frequency` value (a list of integers):

```yaml
tests.rise_time.frequency.offsets:
 - 0
 - 4
 - 8
 - 12
tests.rise_time.frequency.values:
 - 10
 - 20
 - 30
 - 40
 - 10
 - 20
 - 30
 - 40
 - 10
 - 20
 - 30
 - 40
```

Both of these buffers look pretty compressible to me. There are a lot of patterns in the data (compression is the art of exploiting patterns). Let's cheat and use some a priori knowledge that we're repeating in groups of 4. We can apply a "frame-of-reference + delta" encoding in chunks of 4.

```yaml
tests.rise_time.frequency.offsets:
 - 0
 - 4
 - 8
 - 12
tests.rise_time.values.chunk_width: 4
tests.rise_time.values.frame_of_reference:
 - 0
 - 0
 - 0
tests.rise_time.values.delta:
 - 10
 - 10
 - 10
 - 10
 - 10
 - 10
 - 10
 - 10
 - 10
 - 10
 - 10
 - 10
```

Now we've got even better patterns. Those are constant arrays. We can decompose those even further with "run length encoding".

```yaml
tests.rise_time.frequency.offsets:
 - 0
 - 4
 - 8
 - 12
tests.rise_time.values.chunk_width: 4
tests.rise_time.values.frame_of_reference.values:
 - 0
tests.rise_time.values.frame_of_reference.run_lengths:
 - 3
tests.rise_time.values.delta.values:
 - 10
tests.rise_time.values.delta.run_lengths:
 - 12
```

So now we've shredded a single buffer (`tests.rise_time.values`) with 12 elements into four different buffers, each with a single element. In fact, we added *two* different levels to our tree. Each compression technique adding a new level.

![Level 4+ Shredding](/assets/blog/columnar-file-readers-in-depth-column-shredding/Shredding-Level-4.png)
*This is getting pretty complex. Surely we're done now right? ðŸ˜…*

## Too Much Shredding?

Is there such a thing as too much shredding? No, of course not, this is the wonder of vectorized compute. We get great compression and great scan performance, what's not to like.

Yes, possibly, once we start to consider random access patterns. To understand why let's imagine we want to read a single value. In other words, one column value on one particular row (if you're thinking in spreadsheet terms this would be one cell). For example, maybe we want to grab the "vector column" in the 400th row because our secondary index tells us it's a potential match. Let's look what happens to that value as we shred our data.

![Too Much Shredding](/assets/blog/columnar-file-readers-in-depth-column-shredding/Too-Much-Shredding.png)
*Our target value as we descend the shredding tree. We go from one read to access our value to four leaf buffer reads by the time it is fully shredded.*

Initially, everything is fine. As we move from row to column we can ignore the columns we don't need. Same thing as we go from column to array. However, once we start shredding our array into multiple buffers (and beyond) we start to end up with multiple pieces of information that represent our single value. For example, if we split a struct array into a validity buffer and a values buffer then we need to read both buffers to read a single value.

All of this splitting spreads our single value out among several different buffers. This is not great for random access. It means we now need to perform multiple different IOPS to retrieve a single value. In the diagram above we'd need to perform four IOPS to access any one value.

### Reverse Shredding

It turns out, we can sort of solve this problem, by reversing the shredding process after compression. Let's look at a concrete example. Let's imagine we want a single values list from our test file above. That list might be split into four different buffers, not even considering compression. We have the struct validity buffer, the list validity buffer, the list offsets buffer, and the list values buffer. This means we need to perform four different IOPS to grab this single list.

![Data Spreading Problem](/assets/blog/columnar-file-readers-in-depth-column-shredding/Too-much-shredding-2-1-.png)
*Our array has been split into four buffers and so accessing a single item from `tests.values` requires accessing four separate locations.*

But what if we could stitch some of these buffers back together? For example, we could write a list offset, and then immediately write the validity. Or, if we switch to writing lengths we could write the list length, then the list validity, and then the list values. Can we even stitch the struct validity buffer in there somehow? What if we applied compression, could we stitch together different compression buffers?

![Zipped Shredding Solution](/assets/blog/columnar-file-readers-in-depth-column-shredding/Zipped-shredding.png)
*If we have a single "zipped" buffer then our value is still split into four pieces of information, but they are stored contiguously.*

It turns out the answer to all of these questions is "yes, sometimes". We call this stitching process "**zipping**" because it behaves very similarly to python's zip function. Here are the rules for zipping:

- If the compression is transparent we can zip compressed buffers back together.
- List offsets, list validity, and struct validity, can all be zipped together if we use something called repetition and definition levels (the next blog post in this series).
- If we zip together variable length items then we need something new called the "repetition index" (will be covered in a future blog post).

![Unshredding Process](/assets/blog/columnar-file-readers-in-depth-column-shredding/Unshredding.png)
*It's best to think of zipping as an additional layer, and not moving backwards in the tree. We are zipping compressed buffers which is different than zipping and then compressing.*

This is great news for random access. Sadly, it isn't free. All of this zipped data has to be unzipped when we read the data. The unzipping process can be costly. If the unzipping is too costly it can hurt our scan performance. All of this begs the question "when can we zip our data and when is the zipping too expensive?" This might sound a lot like the question "when can we use transparent encoding and when is it not worth it?" Once again, you'll need to stay tuned for the answer. We are getting closer, just a few more pieces to examine.

## What's Next

LanceDB is upgrading the modern data lake (postmodern data lake?). Support for multimodal data, combining search and analytics, embracing embedded in-process workflows and more. If this sort of stuff is interesting then check us out and come join the conversation.

If you're interested in learning more about our file format innovations or contributing to Lance, hop on over to our [Discord](https://discord.gg/G5DcmnZWKB) or [Github](https://github.com/lancedb/lance) and we'd be happy to talk to you!
```

---

## ðŸ“„ æ–‡ä»¶: columnar-file-readers-in-depth-compression-transparency.md

---

```md
---
title: "Columnar File Readers in Depth: Compression Transparency"
date: 2025-04-29
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/columnar-file-readers-in-depth-compression-transparency/columnar-file-readers-in-depth-compression-transparency.png
description: "Explore columnar file readers in depth: compression transparency with practical insights and expert guidance from the LanceDB team."
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "westonpace"
---

Conventional wisdom states that compression and random access do not go well together.  However, there are many ways you can compress data, and some of them support random access better than others.  Figuring out which compression we can use, and when, and why, has been an interesting challenge.  As we've been working on 2.1 we've developed a few terms to categorize different compression approaches.

{{< admonition info "ðŸ“š Series Navigation" >}}
This is part of a series of posts on the details we've encountered building a columnar file reader:
1. [Parallelism without Row Groups](/blog/file-readers-in-depth-parallelism-without-row-groups/)
2. [APIs and Fusion](/blog/columnar-file-readers-in-depth-apis-and-fusion/)
3. [Backpressure](/blog/columnar-file-readers-in-depth-backpressure/)
4. **Compression Transparency** (this article)
5. [Column Shredding](/blog/columnar-file-readers-in-depth-column-shredding/)
6. [Repetition & Definition Levels](/blog/columnar-file-readers-in-depth-repetition-definition-levels/)
{{< /admonition >}}

### Setting the stage

Compression in Lance happens on large chunks of data for a single array.  As batches of data come in, we split those batches into individual columns, and each column queues up data independently.  Once a column's queue is large enough, we trigger a flush for that column, and that initiates the compression process.

We don't have row groups, so we can accumulate columns individually, and flushing one column does not mean that we will be flushing other columns.  This means, by the time we hit the compression stage, we typically have a large (8MB+ by default) block of column data that we need to compress.

![Compression process flow](/assets/blog/columnar-file-readers-in-depth-compression-transparency/Compression-Process.png)

Each column accumulates arrays until it has enough data (8MB+) to justify creating a block.

## Transparent vs. Opaque

The most impactful (for random access) distinction is determining whether an encoding is opaque or not.  **Opaque** compression creates a block of data that must be fully decoded in order to access a single value.  In contrast, **transparent** compression allows us to decompress a single value individually, without interacting with the other values in the block *(to be clear, these are not industry standard terms, it's just what we use within Lance)*.

As a simple example, we can consider delta encoding.  In delta encoding we only encode the difference between one value and the next.  The deltas can often (but not always) be in a smaller distribution (and thus easier to compress) than the original values.  Delta encoding is *opaque*.  If I want the 6th value in a delta encoded array then I need to decode all of the values leading up to it (I suppose you could call this semi-opaque since we don't need to decode the values after it but that distinction is not useful for us).

Other opaque encodings include the "back referencing" family of encodings such as GZIP, LZ4, SNAPPY, and ZLIB.  These encodings encode a value by encoding a "backwards reference" to a previous occurrence of that sequence.  If you don't have the previous values you can't interpret this backward reference.

![Opaque encoding example](/assets/blog/columnar-file-readers-in-depth-compression-transparency/Opaque-Encoding-2-.png)

Delta encoding + bit packing gives great compression here. We go from 16 bytes to 3 bytes. However, our value gets "smeared" across a large portion of the data. We normally end up loading the entire block to decompress a single value.

To see an example of a transparent encoding we can look at bit packing.  In bit packing we compress integer values that don't use up their whole range by throwing away unused bits.  For example, if we have an INT32 array with the values `[0, 10, 5, 17, 12, 3]` then we don't need more than 5 bits per value.  We can throw away the other 27 bits and save space.

This encoding is transparent (as long as we know the compressed width, more on that later) because, to get the 6th value, we know we simply need to read the bits `bits[bit_width * 5..bit_width * 6]`.  I hope it is clear why this property might be useful for random access reads but we will be expanding on this more in a future post.

![Transparent encoding example](/assets/blog/columnar-file-readers-in-depth-compression-transparency/Transparent-Encoding-1-.png)

Bit packing alone isn't as effective in this case, I end up with 4 bytes instead of 3, but it **is** transparent*. *If I want to load a single value I only need to read that one byte.

### Can Variable Length Compression be Transparent?

Variable length layouts, for example, lists or strings, are not so straightforward.  We typically need to do a bit of indirection to get the value, even when there is no compression.  First, we look up the size of the value, then we look up the value itself.  For example, if we have the string array `["compression", "is", "fun"]` then we really (in Arrow world) have two buffers.  The offsets array and the values array.  Accessing a single value requires two read operations.

Variable length layouts require two buffer reads to access a single value

In our definition of transparent, *we still consider this to be random access*.  Intuitively we can see that we still don't need the entire block to access a single value.  As a result, we can have transparent variable length encodings.  A very useful compression here is FSST.  The details are complicated but FSST is basically partial dictionary encoding where the dictionary is a 256 byte symbol table.  As long as you have that symbol table (this is metadata, discussed later) then you can decode any individual substring. This means that FSST encoding is, in fact, transparent.

![FSST style encoding example](/assets/blog/columnar-file-readers-in-depth-compression-transparency/FSST-Style-Encoding.png)

Using a symbol table we transparently create a compressed representation of a string array.

A more complete definition, which is what we've ended up with in our compression [traits](https://github.com/lancedb/lance/blob/e6010729fad8a8581cc0b25411c22599bdbf3450/rust/lance-encoding/src/encoder.rs#L240-L253), is something like:

{{< admonition tip "ðŸ’¡ Transparency Definition" >}}
For an encoding to be transparent then the result of the encoding must either be an empty layout, fixed width layout, or a variable width layout. The layout must have the same number of elements as the array. There may also optionally be one or more buffers of metadata. When decoding, if we have the metadata, we must be able to decode any element in the compressed layout individually.
{{< /admonition >}}

We limit the layouts to empty, fixed width, and variable width because we know how to "schedule" those.  We can convert an offset into one of those layouts into a range of bytes representing the data.  We could potentially support additional layouts in the future.

{{< admonition note "ðŸ”œ Future Question" >}}
Just because we are given an 8MB chunk of data doesn't mean we have to compress the entire thing as one piece. We can break it up into smaller opaque chunks. Do we still get good compression with small chunks? Does this mean the opaque encoding can still be used with random access? How do we know which chunk to access? These are all valid questions and we will discuss these questions in a future post.
{{< /admonition >}}

### Mysterious "metadata"

Our definition mentions "metadata", which isn't something we've talked about yet.  However, it is critical when we look at random access.  To understand, let's take another look at bit packing.  I mentioned that we could decode the data transparently, *as long as we knew the compressed bit width*.  How do we know this?  We put it in the metadata.  That means our naive bit packing algorithm has 1 byte of metadata.

Now lets look at a different way we could approach bit packing.  To see why we might do this consider that we are encoding at least 8MB of integer data.  What if we have just one large value in our array.  That single outlier will blow our entire operation out of the water.  There's lots of ways around this (e.g. replacing outliers with an unused value, moving outliers to metadata, etc.) but a simple one is to compress the data in chunks.

For simplicity let's pretend we compress it in eight 1MB chunks (chunks of 1024 values is far more common but this is a blog post and we don't want to think too hard).  The chunk that has our outlier won't compress well but the remaining chunks will.  However, each compressed chunk could have a different bit width.  That means we now have 8 bytes of metadata instead 1 byte of metadata.  Is this bad?  The answer is "it depends" and we will address it in a future blog post.

![Bit packing with outlier handling](/assets/blog/columnar-file-readers-in-depth-compression-transparency/Bit-Packing-Outlier.png)

By chunking we can reduce the impact of an outlier. This is a general technique that applies to most compression algorithms. But now our bit width metadata is more extensive.

We can even eliminate metadata completely by making the compression opaque.  All we need to do is inline the compressed bit width of a chunk into the compressed buffer of data itself.  This is, in fact, the most common technique in use today, because random access has not typically been a concern.  *Putting all of this together we can see that a single technique (bit packing) can be implemented in three different ways, with varying properties (transparent vs. opaque, size of metadata).*

![Three approaches to bit packing](/assets/blog/columnar-file-readers-in-depth-compression-transparency/Bit-Packing-3-Ways-1.png)

Three different ways to compress with bitpacking showing tradeoffs between transparent/opaque and more/less metadata

### Null Stripped vs. Null Filled

The next category considers how we handle nulls.  We can either **fill nulls** or **strip nulls**.  This is more or less independent of the compression we are using.  When we compress something in a *null stripped*manner we remove the nulls before we compress the values.  When we compress something in a *null filled *manner we insert garbage values in spots where nulls used to be.  Those values can be anything and, in some cases, a compression algorithm might have a particular preference (e.g. when bit packing we probably don't want to put huge outlier values in null spots).

![Null handling approaches](/assets/blog/columnar-file-readers-in-depth-compression-transparency/Null-strip-fill-1.png)

Two equally valid representations of the array [0, NULL, NULL, 3, NULL, NULL, NULL, NULL]

We can also see that stripping out nulls makes our compression opaque.  In the above example we still have a fixed-width layout but we no longer have the right number of values.  We have to walk through our validity bitmap in order to find the offset into our values buffer.  This means we need the entire validity bitmap before we can decode any single value.

{{< admonition question "â“ Challenge Exercise" >}}
What if we had a mostly-null array and we decided to speed things up by declaring the validity buffer to be metadata? After all, this could just a compressed bitmap, it's probably not *too* large. Would this be transparent? Does it meet our definition above? Is this good or bad? How large is too large?
{{< /admonition >}}

{{< admonition note "ðŸ‘¾ Implementation Note" >}}
This garbage data is rarely very large. Fixed size types tend to be quite small. Variable-width types don't need "garbage bytes". They only need a garbage offset (and actually, it's not really garbage, it must equal the previous offset). 

One curious exception to this rule is vector embeddings as they are both fixed-size and quite large (typically 3-6KB) 

Another (perhaps obvious) exception is mostly-null arrays.
{{< /admonition >}}

### Putting it in Action: Parquet & Arrow

In Parquet our block equivalent is the column chunk.  Parquet divides the column chunk into pages (similar to the chunked bit packing example above) and it assumes all compression is **opaque**.  As a result, Parquet is able to **strip nulls** before compression.  However, when we perform random access into a Parquet column we must always read an entire page.

In Arrow's memory format, there isn't really much compression (the IPC format supports some opaque buffer compression).  However, it is important in Arrow that random access be reasonably fast.  All of the layouts are transparent.  Arrow does not remove nulls and it must **fill nulls** with garbage values.  However, with most arrays, we can access a single value in O(1) read operations.  **Bonus challenge:***At the moment there is exactly one exception to these rules.  Do you know what it is?*

### Conclusion:  Which is best?

I work on Lance, a file format that is aiming to achieve fast full scan **and** random access performance.  You probably think my answer will be that we need to switch everything to use transparent compression and then are expecting some statement about minimizing metadata.  Unfortunately, it's not quite that simple.  Sometimes we find ourselves in situations where we can use opaque metadata.  Sometimes more metadata is a good thing. We're going to need to explore a few more concepts before we can really evaluate things.  It's probably bad form to end a blog without actually answering anything but ðŸ¤·

See you next time.

{{< admonition info "ðŸš€ Join the Community" >}}
LanceDB is upgrading the modern data lake (postmodern data lake?). Support for multimodal data, combining search and analytics, embracing embedded in-process workflows and more. If this sort of stuff is interesting then check us out and come join the conversation.

[Join us on Discord](https://discord.gg/G5DcmnZWKB)
{{< /admonition >}}
```

---

## ðŸ“„ æ–‡ä»¶: columnar-file-readers-in-depth-repetition-definition-levels.md

---

```md
---
title: "Columnar File Readers in Depth: Repetition & Definition Levels"
date: 2025-06-02
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/columnar-file-readers-in-depth-repetition-definition-levels.png
description: "Explore columnar file readers in depth: repetition & definition levels with practical insights and expert guidance from the LanceDB team."
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "westonpace"
---

Repetition and definition levels are a method of converting structural arrays into a set of buffers. The approach was made popular in Parquet and is one of the key ways Parquet, ORC, and Arrow differ. In this blog I will explain how they work by contrasting them with validity & offsets buffers, and discuss how this encoding impacts I/O patterns.

{{< admonition info "ðŸ“š Series Navigation" >}}
This is part of a series of posts on the details we've encountered building a columnar file reader:
1. [Parallelism without Row Groups](/blog/file-readers-in-depth-parallelism-without-row-groups/)
2. [APIs and Fusion](/blog/columnar-file-readers-in-depth-apis-and-fusion/)
3. [Backpressure](/blog/columnar-file-readers-in-depth-backpressure/)
4. [Compression Transparency](/blog/columnar-file-readers-in-depth-compression-transparency/)
5. [Column Shredding](/blog/columnar-file-readers-in-depth-column-shredding/)
6. **Repetition & Definition Levels** (this article)
{{< /admonition >}}

## What Problem are we Solving?

Repetition levels are used when we need to convert a **column** into **buffers**. We discussed this briefly in the post on column shredding. In particular, we can look at three categories of columns: structs, lists, and primitives. Let's consider the following schema:

![Schema Diagram](/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/RepDef-Blog-Schema.png)
*A pretty simplistic schema for some kind of "person" record in a database somewhere*

Let's gather some sample data:

```yaml
- name: Reacher
  address: null
- name: Herman Munster
  address:
    street:
      - 1313 Mockingbird Ln
    city: Mockingbird Heights
- name: Spiderman
  address:
    street: null
    city: New York City
- name: Fry
  address:
    street:
      - null
      - Apartment 00100100
    city: New New York
- name: Black Bolt
  address:
    street: []
    city: ""
```

We have two columns (name and address). Using the approach we described in the blog post on column shredding we need to turn this into arrays and buffers. We know, from that post, that we are going to transpose our data into leaf arrays (name, address.street, and address.city). However, there are a few details we have to consider:

- How do we express null values?
- How do we distinguish null values from null lists or null structs?
- How do we express lists as buffers?

### How does Arrow Solve this Problem?

Arrow solves this problem with validity bitmaps and offsets arrays. Let's take a look at one possible representation of our sample data in Arrow. In order to make things interesting for those that know Arrow already, we'll exploit some idiosyncrasies.

![Arrow Encoding](/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/RepDef-Sample-Data-Arrow.png)
*The Arrow encoding of our sample data above. There are ten buffers. Each buffer is given a label and the values are shown.*

{{< admonition tip "ðŸ§  Arrow Skills Challenge" >}}
If you want to advance your Arrow skills here are some homework questions:

1. There is a bit in the address.street.validity bitmap that is green. Why do you think that is? What's interesting about this bit?
2. The address.street.items.offsets array starts with 9. Is this legal? If so, why would this be allowed?
3. There is an easter egg in address.city.chars. Why doesn't this show up in our data? How could this happen in normal processing?
4. Where is address.city.validity?
{{< /admonition >}}

### Validity Buffers

Validity buffers tell us whether an item is null or not. Nearly all arrays in Arrow have a validity buffer. The validity buffer is optional. If an item is nested inside structs or lists you might need to look at ancestor validity buffers to know if the item is null or not.

![Validity Buffers](/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/Validity-Buffers.png)
*Validity Buffer Example: The top part shows nested data structure with nulls, middle shows outer validity buffer, bottom shows item validity buffer*

{{< admonition warning "âš ï¸ Naming Convention" >}}
Validity buffers are often called "null buffers" (or null bitmaps). This is fine but I personally avoid this because I find it unintuitive since a 1 means "valid" and a 0 means "null".
{{< /admonition >}}

### Offset Buffers

Offset buffers tell us how many items we have in each list when we have a list array. Empty lists are always a list of size 0. Nulls are usually a list of size zero. Arrow uses offsets, and not lengths, because this enables random access. To grab any single list we just need to look at the two offsets to determine the position in the items array.

![Offset Buffers](/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/Offset-Buffers.png)
*Offset Buffer Example: Shows how lists are encoded using offset arrays for random access*

{{< admonition fun-fact "ðŸ§ Fun Fact" >}}
You may have noticed that string arrays and list arrays are *very* similar. In an early version of the 2.0 format we encoded strings as `list<u8>`. This turned out to be really confusing for several reasons and we abandoned the idea before we stabilized 2.0.
{{< /admonition >}}

### Sharing is Caring but it Spreads Data

In Arrow, validity and offset buffers are shared by nested children. This means there might be many leaf arrays that all share values from any single offset or validity buffer.

![Buffer Sharing](/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/Buffer-Sharing.png)
*The address validity array is shared by the street part and the city part of our address. Can you think of a way an offsets buffer would be shared?*

Sharing buffers like this is nice and compact and we avoid redundancy which is great if we want to change a value (e.g. mark an item null). However, it means we now have to look in several different locations to determine if an item is null. If our data is in memory that's usually fine. However, if our data is on disk, then this can become an impediment to random access.

![Data Spreading](/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/Data-Spreading.png)
*A single value is spread out on the disk across several buffers.*

Fortunately, we just saw a way to solve this in our previous post. We can zip the buffers together. Unfortunately, we can only do this if all the buffers have the same length. If we have lists, this probably isn't the case. So, we need to find some way to tweak our validity and offset buffers so they become zippable, and then zip them up. It turns out, if we do this, we can also take advantage of some redundancy to compress the buffers. This process gives us repetition and definition levels!

{{< admonition warning "ðŸ™ˆ Fair Warning" >}}
Starting with Arrow may be the most confusing way to explain repetition and definition levels. My feelings won't be hurt if you jump straight to the "why do we care" section.
{{< /admonition >}}

## Repetition & Definition Levels

Repetition and definition levels, which I may just call **repdef levels**, are an alternate technique that can be used instead of validity and offset buffers. They solve the same problem, but in a different way. Why would we want to do this? Because, as we just described above, repdef levels are zipped together which can help random access.

### Definition Levels Replace Validity Buffers

We will start with definition levels. These replace validity buffers. Let's start with a simple example where we have three levels of nesting giving us three validity bitmaps:

```
metadata.validity:         [ 0, 1, 1, 1, 1 ]
metadata.user.validity:    [ 1, 1, 0, 1, 0 ]
metadata.user.ip.validity: [ 0, 1, 0, 0, 1 ]
```

To start with, we just zip our items up.

```
validity: [ 010, 111, 100, 110, 101 ]
```

Or, visually:

![Road to Definition Levels](/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/Road-to-Def-Levels-1-1-.png)
*Zipping our buffers co-locates the data for each item*

At this point, we're doing pretty good. We've got a single buffer. The validity bits for any particular row are next to each other. We will need 1 bit per level of nesting. That's not bad. However, we can do better. There's redundancy here and we can exploit it. Look in more detail at the third and fifth item of our example.

![Redundancy in Definition Levels](/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/Road-to-Def-Levels-2-1-.png)
*Both blocks represent the same logical state despite different bit patterns*

In the third item, both the user and ip validity bits are 0. We have a null user. In the fifth item the user validity bit is 0 but the ip validity bit is 1. However, *we still have a null user*. A null item can't have valid children. **Once we encounter a null bit then the remaining bits are meaningless.** In other words, after we zip three validity buffers, instead of the 8 possible values that we'd expect from a 3-bit sequence, we only have 4:

- Item is valid: 111
- ip is null: 110
- user is null: 10?
- metadata is null: 0??

Before, when we zipped these three levels, we needed three bits per value. However, if there are only 4 possibilities, we should only need 2 bits per value. We can assign any numbering scheme we want to these possibilities. In classic fashion, I picked the exact opposite scheme that Parquet picked ðŸ¤· when I wrote this in Lance (I didn't read those repetition and definition tutorials in enough detail).

| Meaning | Definition Level (Parquet) | Definition Level (Lance) |
|---------|---------------------------|-------------------------|
| Item is valid | max_def | 0 |
| Item is null | max_def - 1 | 1 |
| Parent is null | max_def - 2 | 2 |
| ......... | ......... | ......... |
| Column root is null | 0 | max_def |

**Putting it all together:** We can convert N validity bitmaps into a single buffer of definition levels with logâ‚‚(N + 1) bits per value. Zipping and compressing at the same time.

### Repetition Levels Replace Offsets

So now we have a way of handling validity buffers. Let's take a look at our offsets buffers. Remember, offset buffers tell us how many items we have in each list.

```
Lists: [ [0, 1, 2], [3, 4, 5] ], [ [6, 7] ]
Outer Offsets: 0, 2, 3
Inner Offsets: 0, 3, 5, 6, 8
```

Our goal is to get something that we can zip together with our items so we need it to have the same length.

```
Items            : 0 1 2 3 4 5 6 7
Outer Repetition : * * * * * * * *
Inner Repetition : * * * * * * * *
```

{{< admonition tip "ðŸ’¡ Think About It" >}}
Take a moment to try and solve the problem yourself. What would you put in those * slots to mark your lists?
{{< /admonition >}}

Were you able to figure it out? Perhaps there is more than one way?

Repetition bitmaps work by inserting a 0 whenever we start a new list and inserting a 1 whenever we are continuing the previous list.

```
Items            : 0 1 2 3 4 5 6 7
Outer Repetition : 0 1 1 1 1 1 0 1
Inner Repetition : 0 1 1 0 1 1 0 1
```

Again, more visually:

![Road to Repetition Levels](/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/Road-to-Rep-Levels.png)
*A 0 in our repetition bitmap points to the start of a new list*

This is starting to look a lot like our definition bitmaps. We have a new bitmap for each level of repetition. Once again, there is the exact same redundancy that we can exploit. Whenever we start a new outer list we *must* start a new inner list at the same time, and so the lower level bits don't matter when the high level bit is zero. I'll skip a few steps because the conclusion is the same. We combine all of our bitmaps into a single buffer of levels. Once again, Lance and Parquet completely disagree on what order we should use ðŸ˜‡

| Meaning | Repetition Level (Parquet) | Repetition Level (Lance) |
|---------|----------------------------|--------------------------|
| No new list | max_rep | 0 |
| New innermost list | max_rep - 1 | 1 |
| ......... | ......... | ......... |
| New outermost list | 0 | max_rep |

### Putting it All Together

Let's assume we have a column of `tests` which is a struct that has an item `cases` which is a list of structs and those structs have a field `points` which is a list of structs and those structs have fields `x` and `y` and we want to decode the leaf field `x`. Here is one example record:

```yaml
tests:
  cases:
    - points:
       - x: 7
         y: 3
       - x: 2
         y: 4
    - points:
       - x: 0
         y: 0
```

Ok, now let's work backwards and test our skills. Let's look at just the column `x` and look at some sample items and repetition and definition levels:

```
items      : 0 1 2 3 4 5
repetition : 0 2 0 0 1 2
definition : 4 4 0 1 2 3
```

Can figure out the original document? First, in order to interpret our levels, we actually need to know the schema and nullability of the document. This is because we don't want to waste a precious repetition or definition slot when we know a particular level is non-nullable.

For our example we will assume that both of our structs can be null, as well as our value. However, we have no null lists. We also need one level for "valid item". Next, we know there are two lists and we need one level for "does not start new list". Putting this together we get:

| Level | Definition Meaning | Repetition Meaning |
|-------|-------------------|-------------------|
| 0 | Null test | New list of cases |
| 1 | Null case | New list of points |
| 2 | Null point | No new list |
| 3 | Null x | - |
| 4 | Valid item | - |

At this point it should be a straightforward exercise to construct the original representation. I encourage you to give it a try.

{{< admonition info "Click for the answer" >}}
```yaml
- tests:
    cases:
      - points:
          - x: 0
          - x: 1
- null
- tests:
    cases:
      - null
      - points:
          - null
          - x: null
```
{{< /admonition >}}

### Null Stripping?

In the previous example I included garbage values for nulls. Note that the items array has 2, 3, 4, and 5 but those numbers don't appear anywhere in the answer. If we want our repetition and definition levels to line up with our values so we can zip them then we do need some kind of dummy value. However, this isn't a requirement for repdef levels. You could completely reconstruct the above array without having those dummy values. In fact, Parquet always strips out null values.

### Empty / Null Lists?

I've intentionally avoided empty & null lists in my examples. Let's look at them now. Null lists are pretty straightforward. They are a "new list" of sorts so we will use our list repetition level. They are null so we will reserve a definition level.

Now let's look at an empty list. They are also a "new list" of sorts so we will use our list repetition level. However, what should we use for the definition level. If we use the same we used for nulls then they show up as a null list. If we use the definition level for "valid item" then we will try and add an item to the list and that isn't right either.

![Repetition Levels with Empty and Null Lists](/assets/blog/columnar-file-readers-in-depth-repetition-definition-levels/Rep-Levels-Empty-Null-Lists.png)
*Distinguishing between null lists and empty lists requires different definition levels*

It turns out that empty lists *also* need to reserve a definition level. This means a list array that has both empty lists and null lists will take up two definition level spots.

## Why Do We Care?

We now have two completely different ways to convert struct and list arrays to a set of buffers. The Arrow (validity & offsets) approach and the RepDef (repetition & definition) approach. How do these two approaches compare?

**Both can be compressed.** Repetition levels and definition levels are naturally compressed as they are created. They can be further compressed with run length encoding (this is done in Parquet) which is often useful as it is common to have only a few nulls in an array.

However, validity buffers can be compressed as compressed bitmaps (e.g. roaring bitmaps is one approach) and offsets buffers can be bitpacked and possibly even delta encoded for compression. In fact, I would wager it's pretty difficult to pick one approach over the other purely based on compression (though it would be an interesting study if anyone knows of one).

**Offsets support random access.** If you have an offsets array then you can access the Nth list in O(1) time. If you have repetition levels you have to search the repetition levels until you find the Nth instance of your desired level. This requires O(N) time.

**Validity buffers are a single source of "physical" truth.** If you have validity buffers then you have one source of truth for "physical validity" (the validity of a single level). If you need to mark a struct as NULL you can do that by changing one bit in one buffer. With definition levels, to mark a struct as NULL, you need to change the level buffer of every child.

**Definition levels are a single source of "logical" truth.** If you have validity buffers then you need to read multiple buffers to figure out if a value is "logically NULL" (e.g. is this NULL or any of its parents?) With definition levels you only need to read a single spot.

**Repdef Levels are not "zero copy".** Like many techniques used in file storage (compression, zipping buffers, etc.) there is a cost to pay. Repdef levels are no exception. In order to convert the data back into Arrow format we must pay a conversion cost to "unravel" the repdef levels back into validity & offset buffers.

## Why does Lance care?

The reason I got interested in repetition and definition levels, while working on Lance, is because **repetition levels and definition levels are single buffers that can be zipped together.** With repetition and definition levels the number of buffers any single leaf array has remains relatively small. This is useful for random access as it keeps our data closer together instead of spread out all over the place. However, the lack of an offsets buffer does present a challenge for random access, something we will tackle in the next post.

In fact, at this point, we've now covered several techniques that come to similar conclusions. Transparent compression, buffer zipping, and repdef levels are all tradeoffs between CPU costs and random access performance. In the next post, which will cap this recent sequence of posts, we will look at the concept of structural encoding. This ties all these posts together and shows how Lance decides between these various techniques.

## What's Next

LanceDB is upgrading the modern data lake (postmodern data lake?). Support for multimodal data, combining search and analytics, embracing embedded in-process workflows and more. If this sort of stuff is interesting then check us out and come join the conversation.

If you're interested in learning more about our file format innovations or contributing to Lance, hop on over to our [Discord](https://discord.gg/G5DcmnZWKB) or [Github](https://github.com/lancedb/lance) and we'd be happy to talk to you!
```

---

## ðŸ“„ æ–‡ä»¶: columnar-file-readers-in-depth-structural-encoding.md

---

```md
---
title: "Columnar File Readers in Depth: Structural Encoding"
date: 2025-08-07
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/columnar-file-readers-in-depth-structural-encoding/preview-image.png
meta_image: /assets/blog/columnar-file-readers-in-depth-structural-encoding/preview-image.png
description: "Deep dive into LanceDB's dual structural encoding approach - mini-block for small data types and full-zip for large multimodal data. Learn how this optimizes compression and random access performance compared to Parquet."
keywords: ["structural encoding", "columnar file format", "LanceDB", "mini-block encoding", "full-zip encoding", "Parquet comparison", "data compression", "random access performance", "vector database", "multimodal data", "columnar storage", "Arrow format", "data engineering"]
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "weston-pace-cool-dude"
---

Structural encoding describes how the "structure" of arrays is encoded into buffers and placed into a file. The choice of structural encoding places rules on how data is compressed, how I/O is scheduled, and what kind of data we need to cache in RAM. Parquet, ORC, and the Arrow IPC format all define different styles of structural encoding. 

[Lance is unique](/docs/overview/lance/) in that we have two different kinds of structural encoding that we choose between based on the data we need to write. In this blog post we will describe the structural encodings currently in use by Lance, why we need two, and how they compare to other approaches.

{{< admonition info "ðŸ“š Series Navigation" >}}
This is part of a series of posts on the details we've encountered building a columnar file reader:

1. [Parallelism without Row Groups](/blog/file-readers-in-depth-parallelism-without-row-groups/)
2. [APIs and Fusion](/blog/columnar-file-readers-in-depth-apis-and-fusion/)
3. [Backpressure](/blog/columnar-file-readers-in-depth-backpressure/)
4. [Compression Transparency](/blog/columnar-file-readers-in-depth-compression-transparency/)
5. [Column Shredding](/blog/columnar-file-readers-in-depth-column-shredding/)
6. [Repetition & Definition Levels](/blog/columnar-file-readers-in-depth-repetition-definition-levels/)
7. **Structural Encoding** (this article)
{{< /admonition >}}

## What is Structural Encoding?

Structural encoding is the first step we encounter when we want to convert an Arrow array into an on-disk buffer. We start with a large chunk (~8MB by default) of array data. In the previous articles we described how we can take this array and convert it into a collection of compressed buffers. Now we need to take that collection of buffers and write it into a "disk page" (which is basically one big buffer in our file).

{{< image-grid >}}
![Structural Encoding Overview](/assets/blog/columnar-file-readers-in-depth-structural-encoding/overview.png)
_Structural encoding defines how we take compressed buffers and write them into a disk page_
{{< /image-grid >}}

### Structural Encoding Defines I/O Patterns

The structural encoding we choose is important because it defines the I/O patterns that we can support. For example, let's look at a very simple structural encoding. We will write 2 bytes to tell us how many buffers we have, 4 bytes per buffer to describe the buffer length, and then lay out the buffers one after the other.

```ascii
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚# of buffers (2 bytes)     â”‚       â–²
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚bytes in buffer 0 (4 bytes)â”‚       â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚bytes in buffer 1 (4 bytes)â”‚       â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚...                        â”‚       â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚bytes in buffer N (4 bytes)â”‚   ~ 8 MB Data
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚buffer 0                   â”‚       â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚buffer 1                   â”‚       â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚...                        â”‚       â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â”‚
â”‚buffer N                   â”‚       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â–¼
```

This is super simple and very extensible. It is, in fact, very similar to the approach used in Arrow IPC and ORC. It's got a number of things going for it. However, there is one rather big problem. _How can we read a single value?_

Unfortunately, we know nothing about the makeup of the buffers. Are these buffers transparent or opaque? We don't know. Is there some fixed number of bytes per value in each buffer? We don't know. The only thing we can do is read the entire 8MB of data, decode and decompress the entire 8MB of data, and then select the value we want. As a result, this approach generally gives bad random access performance because there is too much **read amplification**.

{{< admonition tip "Read Amplification" >}}
_Read amplification_ is a bit of jargon that describes reading more than just the data we want. For example, if we need to read 8MB of data to access a single 8-byte nullable integer, then we have **a lot** of read amplification; the 8-byte (plus one bit) read is _amplified_ into 8MB.
{{< /admonition >}}

Note: both Arrow IPC and ORC try to solve this problem a little. Arrow IPC is able to perform random access into the buffers if the data is uncompressed, but that requirement is not acceptable for us. ORC is able to chunk the values in a buffer so that only target chunks need to be read, but there is still considerable amplification.

There is one further problem with the above approach. If a value is split into N different streams, then we need to perform at least N different IOPS to read that value. To use terms from a previous blog post, we are over-shredded and there is no zipping of values.

{{< image-grid >}}
![Streams IOPS](/assets/blog/columnar-file-readers-in-depth-structural-encoding/streams.png)
_A stream-based structural encoding requires N different IOPS to read a single value_
{{< /image-grid >}}

### Structural Encoding Constrains Compression

Since the structural encoding is where we figure out which bytes we need to read from disk, it is highly related to several of the encoding concepts we described in earlier posts. Let's review:

- Transparent compression (e.g., bit packing) ensures a value is not "spread out" when compressed. Opaque encoding (e.g., delta encoding) allows the value to be spread out but could potentially be more efficient.
- Reverse shredding (or zipping) buffers allows us to recombine multiple transparently compressed buffers into a single buffer. This avoids a value getting spread out but it requires extra work per-value to zip and unzip.
- Repetition and definition levels are a way to encode validity and offset information in a form that is already zipped. However, this form is not zero-copy from the Arrow representation and requires some conversion cost.

Each of these options has some potential trade-offs. In [Lance](/docs/overview/lance/), when we pick structural encoding, we end up solidifying these choices. As a result, in Lance, we don't expect there to be a single structural encoding. It is a pluggable layer and there are already two implementations (technically three since we use a unique structural encoding for all-null disk pages).

Each is described further, but in summary, mini-block allows maximum compression but introduces amplification while full zip avoids amplification but limits compression.

| **Structural Encoding** | **Transparency**    | **Zipping**       | **RepDef** |
| ----------------------- | ------------------- | ----------------- | ---------- |
| **Mini Block**          | Can be opaque       | No zipping needed | Optional   |
| **Full Zip**            | Must be transparent | Zipping required  | Mandatory  |

## Mini Block: Tiny Types Tolerate Amplification

Most columns in most datasets are small. Think 8-byte doubles and integers, small strings or enums, boolean flags, etc. When dealing with small data types, we want to minimize the amount of per-value work. In addition, when working with small values, a small amount of read amplification goes a long way. To handle these types, the mini-block structural encoding is used.

- Data is grouped into "mini blocks" and we aim for 1-2 disk sectors (4KiB-8KiB) per block.
- Blocks are indivisible. We cannot read a single value and must read and decode an entire block.
- We can use opaque encoding and avoid expensive per-value zipping.

![Mini Block Encoding](/assets/blog/columnar-file-readers-in-depth-structural-encoding/mini-block.png)
_With mini-block we allow a small amount of read amplification to maximize compression_

### Repetition Index

In order to support random access, we need to translate row offsets quickly into block offsets and also be able to find the file offsets of the block. To support this, we store a repetition index for the mini-block encoding. This lightweight structure consists of 2 bytes per mini-block.

- The first 4 bits store the log base 2 of the number of values. This allows us to handle anywhere from 2-64Ki values (we cap it at 4Ki values for other reasons). Note: this means we need each mini-block (except the last) to have a power-of-two number of values. Since values are small, this isn't much of a limitation and it has other benefits when it comes to aligning mini-blocks across multiple columns into batches.
- The remaining 12 bits return the number of 8-byte words in the mini-block. If the data size is not exactly divisible by 8, then we pad. This ensures mini-blocks are 8-byte aligned and allows mini-blocks to be up to 32KiB.

{{< image-grid >}}
![Mini Block Repetition Index](/assets/blog/columnar-file-readers-in-depth-structural-encoding/mini-block-rep.png)
_The repetition index for mini-block is a tiny structure (we typically cache it in RAM) that allows us random access into mini-blocks_
{{< /image-grid >}}

{{< admonition tip "Random Access Only" >}}
The repetition index does not need to be read when performing a full scan. However, it tends to be quite small, and avoiding it may not be worthwhile.
{{< /admonition >}}

### Mini Block vs. Parquet

Those of you who are familiar with Parquet will probably find mini-block encoding familiar. The concepts are basically the same. The repetition index is equivalent to Parquet's page offset index (though more compact). The mini-block is equivalent to Parquet's page (though our default size is much smaller). We can support any of the same encoding schemes that Parquet uses and, like Parquet, we require a small amount of read amplification.

## Full Zip: Large Types Allow More Work Per Value

Lance needs to deal with [multimodal data](/docs/overview/). Types like vector embeddings (3-6KiB/value), documents (10s of KBs/value), images (100s of KiB/value), audio (KBs-MBs/value), or video (MBs/value) do not fit into our normal database concept of "small values". These types would end up giving us a single mini-block per value and, not surprisingly, this does not perform well!

To handle this, we use a different structural encoding for large data types, called full-zip encoding. During compression, we compress an entire 8MiB disk page, but we require transparent compression. This means we can take the resulting buffers (repetition, definition, and any value buffers) and zip them together. This allows us to read a single value without any read amplification.

![Full Zip Encoding](/assets/blog/columnar-file-readers-in-depth-structural-encoding/full-zip.png)
_Full-zip encoding utilizes zipping and transparent compression instead of mini-blocks or pages_

- The entire 8MiB page is compressed at once (mini-block compresses individual blocks at a time) but we limit compression to transparent compression techniques (e.g., no delta or byte stream split)
- Because compression is transparent, we know the start and stop of each compressed value and we can zip all value buffers together.
- Repetition and definition information are combined into a "control word" placed at the start of each value. If the value is a variable-length value (pretty typical), then this is also where we put the length of the value.

### Repetition Index

Just like mini-block encoding, we need to have a repetition index to support random access. Unlike the mini-block encoding, this repetition index is quite a bit larger. The repetition index points to the start of each value (it's very much like the Arrow offsets array in a binary or string data type). The resulting array of offsets can, of course, be compressed ðŸ˜

![Full Zip Repetition Index](/assets/blog/columnar-file-readers-in-depth-structural-encoding/full-zip-rep.png)
_The repetition index for full-zip data works just like an Arrow offsets array_

{{< admonition tip "Repetition Index" >}}
This repetition index is also only required when performing random access. It tends to be quite large and so skipping it during full scans can be beneficial.
{{< /admonition >}}

### Full Zip vs. Parquet

Parquet has no concept of full-zip encoding. When encoding large data types, you either have very large pages (e.g., 1MB) and lots of read amplification or you have a page-per-value. Page-per-value is quite similar to full-zip encoding. Value and structure buffers are co-located, the page offset index acts like a repetition index, etc. However, the overhead can be significant, both in terms of storage overhead (page descriptor overhead, the "number of values" part of the page offset index, etc.) and processing overhead (searching through the page offset index, page decoding overhead, etc.).

## Does it Matter?

This is the part of the blog post where I share amazing benchmark results that show this new two-encoding approach blows the old-fashioned Parquet out of the water. I spent over a month doing detailed benchmarking, optimizing, experiment design, etc. It took a long time, and I had to overcome all sorts of noise and artifacts to eventually converge on something I feel represents reality. We wrote a whole [paper](https://arxiv.org/pdf/2504.15247) on it, but I'll summarize the findings here. When I was finally finished, I came to the somewhat disappointing answer that _I still don't know if all this is necessary._

### The Roofline Model

I started with a basic premise. Can I fully exploit an NVMe disk for both random access and full scans? To start with, I benchmarked an NVMe disk to figure out how many IOPS/s it can support. The answer was ~750K and we see an immediate drop-off as soon as we start requesting more than 4KiB per IOP. Note, this is much higher than cloud storage (cloud storage caps out around 10-20K IOPS/s).

Next, for full scans, I benchmarked the throughput of my disk. I was able to achieve a little over 3GBps.

![Roofline Model](/assets/blog/columnar-file-readers-in-depth-structural-encoding/storage-roofline.png)
_NVMe has much better random access than cloud storage but cloud storage should have higher bandwidth if IOPS are large_

This makes the measure of success quite simple. Achieve 750K rows/second random access against an NVMe and 12.5GBps full scan performance against cloud storage.

{{< admonition tip "NVMe Array" >}}
For a long time my goal was to saturate an NVMe. Then I learned people build storage devices with an array of NVMe disks that can achieve millions of IOPS/s!
{{< /admonition >}}

### Making Parquet Sing

I was somewhat surprised by Parquet's ability to handle random access. However, it did require some pretty complex configuration:

- I had to disable page-level statistics (once you start reaching page-per-value, these statistics will explode a file)
- I had to keep the number of row groups small (this is no surprise as row groups are inherently bad)
- I had to disable dictionary encoding (parquet-rs cannot cache dictionaries like Lance can)
- I had to make pages small (this was rather hard and I had to set a number of different settings like write batch size and page size and so on)
- I had to use the page offset index (this took up a lot of memory in page-per-value cases)
- I had to make sure to use the row selection API (e.g., Parquet's equivalent of the take operation)
- I had to make sure to cache metadata (Lance needs to do this too, but it's more automated)
- I had to use synchronous I/O (more on this later)

### Making Lance Sing

I just used Lance's default configuration (ok, I did have to resort to synchronous I/O and bump up the default page size ðŸ˜‡, more on this later).

## Results

Both Parquet and Lance are able to come quite close to maximizing the IOPS of a single NVMe (can hit ~400K values/second random access), but we hit non-format limitations (e.g., syscall overhead) before we get there and we need a better I/O implementation.

![Random Access Results](/assets/blog/columnar-file-readers-in-depth-structural-encoding/random-access.png)
_Syscall overhead limits the random access performance of both formats_

{{< admonition question "Parquet can do Random Access?!" >}}
It often surprises people that Parquet can perform efficient random access, but the principles are solid. With the page offset index, we perform one IOP per value. [LanceDB's](/) primary gripe with Parquet remains "row groups" and (for [vector search](/docs/search/)) the RAM required for the page offset index.
{{< /admonition >}}

Lance is able to keep up with (or beat) Parquet at full scan speeds in all the scenarios I tested. This includes scenarios where Parquet is configured with large page sizes. So our design is indeed able to maximize both full scan performance and random access performance.

![Full Scan Results](/assets/blog/columnar-file-readers-in-depth-structural-encoding/full-scan.png)
_Pulling down 12.5GBps from cloud storage while decompressing it...is quite hard!_

However, both Parquet and Lance fell short of the 3GB/s disk utilization roofline and well short of the 12.5GB/s cloud storage roofline. There is a fair amount of headroom available for improving full scan speeds.

### One (not so) Minute Detail

There is one significant difference between Parquet and Lance that isn't really related to performance but is somewhat critical if you happen to be implementing vector search ðŸ˜‰

If you have large fixed-width values that can't be compressed (_cough_ vector embeddings _cough_), then a full-zip structural encoding can be used without any repetition index. The equivalent Parquet would be page-per-value with a large page offset index which would incur significant RAM overhead. In other words, in this situation, a Parquet reader would need ~20GB of RAM per billion vectors, and Lance does not.

## Conclusion

This post has covered a lot. We described the two kinds of structural encoding that Lance employs and discussed some of our benchmarking. Our design is flexible and is likely able to fully utilize an NVMe's random access capabilities while matching Parquet's full scan capabilities.

We are still well below the roofline performance that is possible. This is true for both random access and full scans. There is yet more work to do to improve the I/O scheduling, compression techniques, and overhead. I'm confident our flexible design will allow us to get there.

### Want to Learn More?

[LanceDB](/docs/overview/) is building the data lake for AI & ML. Support for [multimodal data](/docs/overview/), combining [search](/docs/search/) and analytics, embracing embedded in-process workflows and more. If this sort of stuff is interesting, then check us out and come join the conversation on [Discord](https://discord.gg/G5DcmnZWKB) or [Github](https://github.com/lancedb/lance) and we'd be happy to talk to you!

## Appendix: Lessons Learned

You can probably skip the following sections unless you are the kind that really wants to know the details ðŸ˜„

### Random Access

**Lesson 1: simple async I/O is too slow.** With asynchronous I/O, both Parquet and Lance capped out around 50K reads/second. For Lance, at least, the problem is the time it takes to switch things between threads.

![I/O Gaps](/assets/blog/columnar-file-readers-in-depth-structural-encoding/io-gaps.png)
_Waiting for threads to wake up and switch introduces gaps in our I/O_

There are gaps while we wait for a thread to be notified, wake up, and grab the next item from the queue. These gaps only lasted a microsecond or less. When it comes to full scans, this is negligible. When it comes to random access, however, this is too expensive (a 1-sector read is only 1-2 microseconds). For the remainder of my experiments, I switched to synchronous I/O, though that has its own limitations.

**Lesson 2: pread64 doesn't cut it.** Once on synchronous I/O, both Parquet and Lance were able to reach around 400K reads/second. This is surprisingly close to our roofline but doesn't quite reach it. It turns out the bottleneck at this point is syscall overhead. For these experiments, we were using pread64 and the overhead of each call was simply too large.

**Next Steps: io_uring.** Both of the above problems should be solvable with io_uring. We would have asynchronous I/O without gaps (technically it's a sort of semi-synchronous I/O) while also avoiding syscall overhead. In fact, Lance's scheduler is pretty well suited for io_uring, so I think it should be a straightforward lift. The main obstacle has been that 400K IOPS is actually pretty good and we haven't needed to get much faster yet, so it's hard to prioritize this work.

### Full Scans

For the next task, I wanted to make sure that Lance's full scan performance could match Parquet and, even better, saturate S3's bandwidth. Once again, I was somewhat disappointed.

**Lesson 3: Need rolling compression.** In order to saturate S3 bandwidth, we need to make many (64-128) large (8-16 MB) parallel I/O requests. This is the rationale behind Lance's default page size of 8MB. Unfortunately, the way a Lance column writer is currently implemented is as follows:

- Accumulate 8MB of uncompressed data
- Pick the best compression strategy to use
- Compress the data and write a page

The end result, when the data is highly compressible, is that pages might be much smaller than 8MB. In the [future](https://github.com/lancedb/lance/issues/4371), I'd like to adopt some kind of "rolling" compression where we pick a compression strategy and create a compressor when we have 8MB of uncompressed data, and then keep feeding the compressor data until the compressed page is greater than 8MB. For my experiments, I was able to bump up the default page size to work around this limitation.

![Rolling Compression](/assets/blog/columnar-file-readers-in-depth-structural-encoding/rolling-compression.png)
_Rolling compression keeps adding to an existing compressor until we've built up a big enough page_

**Lesson 4: Need better compression for larger values.** Compression worked really well for small values (integers, names, labels, etc.). However, when we tried our compressors on large string values (code, websites) where each value was tens of kilobytes or more, we didn't love the results. Both code and websites should be highly compressible, but our current FSST implementation was not very effective. I suspect it is an [artifact of the way we are using it](https://github.com/lancedb/lance/issues/4405) (or a bug) and not a limitation in FSST.

For simplicity, I switched to LZ4. LZ4 did give me good compression, but then the decompression speed was surprisingly not quick enough to keep up with the disk's bandwidth. Our decompression speeds fell below what LZ4 benchmarks typically advertise, so I think there is some work to do there. One problem is that, in order to make LZ4 transparent (remember, these are large values and so we need transparent encodings), I needed to resort to LZ4-per-value, and I don't know if the LZ4 implementation has a lot of per-call overhead that I need to find a way to avoid.

### In-Memory Results

All of the above experiments were carefully designed to make sure that the data we were loading was not cached in memory already. This ensures we are truly testing I/O and that we have good I/O patterns. However, we did perform some results against in-memory data and there is some good evidence that the different structural encodings have significantly different performance patterns. Continuing these experiments to get a fuller picture of how the various formats do against in-memory data would be some interesting future work.
```

---

## ðŸ“„ æ–‡ä»¶: convert-any-image-dataset-to-lance.md

---

```md
---
title: "Convert Any Image Dataset to Lance"
date: 2024-04-10
author: LanceDB
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/convert-any-image-dataset-to-lance/preview-image.png
meta_image: /assets/blog/convert-any-image-dataset-to-lance/preview-image.png
description: "In our article, we explored the remarkable capabilities of the Lance format, a modern, columnar data storage solution designed to revolutionize the way we work with large image datasets in machine learning."
---

In our [previous](https://vipul-maheshwari.github.io/2024/03/29/effortlessly-loading-and-processing-images-with-lance-a-code-walkthrough) article, we explored the remarkable capabilities of the Lance format, a modern, columnar data storage solution designed to revolutionize the way we work with large image datasets in machine learning. For the same purpose, I have converted the *cinic* and *mini-imagenet* datasets to their lance versions. For this write-up, I will use the example of *cinic* dataset to explain how to convert any image dataset into the Lance format with a single script and unlocking the full potential of this powerful technology.

Just in case, here are the [cinic](https://www.kaggle.com/datasets/vipulmaheshwarii/cinic-10-lance-dataset) and [mini-imagenet](https://www.kaggle.com/datasets/vipulmaheshwarii/mini-imagenet-lance-dataset) datasets in lance.

## **Processing Images**

The `process_images` function is the heart of our data conversion process. It is responsible for iterating over the image files in the specified dataset, reading the data of each image, and converting it into a `PyArrow RecordBatch` object on the binary scale. This function also extracts additional metadata, such as the filename, category, and data type (e.g., train, test, or validation), and stores it alongside the image data.

```python
def process_images(data_type):
    # Get the current directory path
    images_folder = os.path.join("mini_imagenet", data_type)

    # Define schema for RecordBatch
    schema = pa.schema([
        ('image', pa.binary()),
        ('filename', pa.string()),
        ('category', pa.string()),
        ('data_type', pa.string())
    ])

    # Iterate over the categories within each data type
    for category in os.listdir(images_folder):
        category_folder = os.path.join(images_folder, category)

        # Iterate over the images within each category
        for filename in tqdm(os.listdir(category_folder), desc=f"Processing {data_type} - {category}"):
            # Construct the full path to the image
            image_path = os.path.join(category_folder, filename)

            # Read and convert the image to a binary format
            with open(image_path, 'rb') as f:
                binary_data = f.read()

            image_array = pa.array([binary_data], type=pa.binary())
            filename_array = pa.array([filename], type=pa.string())
            category_array = pa.array([category], type=pa.string())
            data_type_array = pa.array([data_type], type=pa.string())

            # Yield RecordBatch for each image
            yield pa.RecordBatch.from_arrays(
                [image_array, filename_array, category_array, data_type_array],
                schema=schema
            )
```

By leveraging the `PyArrow` library, the `process_images` function ensures that the image data is represented in a format that is compatible with the Lance format. The use of `RecordBatch` objects allows for efficient data structuring and enables seamless integration with the subsequent steps of the conversion process.

One of the key features of this function is its ability to handle datasets with a hierarchical structure. It iterates over the categories within each data type, ensuring that the metadata associated with each image is accurately captured and preserved. This attention to detail is crucial, as it allows us to maintain the rich contextual information of our image dataset, which can be invaluable for tasks like classification, object detection, or semantic segmentation.

## **Writing to Lance**

The `write_to_lance` function takes the data generated by the `process_images` function and writes it to Lance datasets, one for each data type (e.g., train, test, validation). This step is where the true power of the Lance format is unleashed.

The function first creates a `PyArrow` schema that defines the structure of the data to be stored in the Lance format. This schema includes the image data, as well as the associated metadata (filename, category, and data type). By specifying the schema upfront, the script ensures that the data is stored in a consistent and organised manner, making it easier to retrieve and work with in the future.

```python
def write_to_lance():
    # Create an empty RecordBatchIterator
    schema = pa.schema([
        pa.field("image", pa.binary()),
        pa.field("filename", pa.string()),
        pa.field("category", pa.string()),
        pa.field("data_type", pa.string())
    ])

    # Specify the path where you want to save the Lance files
    images_folder = "mini_imagenet"

    for data_type in ['train', 'test', 'val']:
        lance_file_path = os.path.join(images_folder, f"mini_imagenet_{data_type}.lance")

        reader = pa.RecordBatchReader.from_batches(schema, process_images(data_type))
        lance.write_dataset(
            reader,
            lance_file_path,
            schema,
        )
```

Next, the function iterates through the different data types, creating a Lance dataset file for each one. The `lance.write_dataset` function is then used to write the `RecordBatchReader`, generated from the `process_images` function, to the respective Lance dataset files.

The benefits of this approach are numerous. By storing the data in the Lance format, you can take advantage of its columnar storage and compression techniques, resulting in significantly reduced storage requirements. Additionally, the optimized data layout and indexing capabilities of Lance enable lightning-fast data loading times, improving the overall performance and responsiveness of your machine learning pipelines.

### **Loading into Pandas**

The final step in the process is to load the data from the Lance datasets into Pandas DataFrames, making the image data easily accessible for further processing and analysis in your machine learning workflows.

The `loading_into_pandas` function demonstrates this process. It first locates the Lance dataset files, created in the previous step, and creates a Lance dataset object for each data type. The function then iterates over the batches of data, converting them into Pandas DataFrames and concatenating them into a single DataFrame for each data type.

```python
def loading_into_pandas():
    # Load Lance files from the same folder
    current_dir = os.getcwd()
    images_folder = os.path.join(current_dir, "cinic")

    data_frames = {}  # Dictionary to store DataFrames for each data type

    for data_type in ['test', 'train', 'val']:
        uri = os.path.join(images_folder, f"cinic_{data_type}.lance")

        ds = lance.dataset(uri)

        # Accumulate data from batches into a list
        data = []
        for batch in tqdm(
            ds.to_batches(columns=["image", "filename", "category", "data_type"], batch_size=10),
            desc=f"Loading {data_type} batches",
        ):
            tbl = batch.to_pandas()
            data.append(tbl)

        # Concatenate all DataFrames into a single DataFrame
        df = pd.concat(data, ignore_index=True)

        # Store the DataFrame in the dictionary
        data_frames[data_type] = df

        print(f"Pandas DataFrame for {data_type} is ready")
        print("Total Rows: ", df.shape[0])

    return data_frames
```

This approach offers several advantages. By loading the data in batches, the function can efficiently handle large-scale image datasets without running into memory constraints. Additionally, the use of Pandas DataFrames provides a familiar and intuitive interface for working with the data, allowing you to leverage the rich ecosystem of Pandas-compatible libraries and tools for data manipulation, visualization, and analysis.

Moreover, the function stores the DataFrames in a list, indexed by the data type. This structure enables us to easily access the specific subsets of your dataset (e.g., train, test, validation) as needed, further streamlining your machine learning workflows. I mean it's too smooth guys.

## **Putting It All Together**

By running the provided script, you can convert your image datasets, whether they are industry-standard benchmarks or your own custom collections, into the powerful Lance format. This transformation unlocks a new level of efficiency and performance, empowering you to supercharge your machine learning projects.  I have used the same script for the `mini-imagenet` too, make sure your data directory looks like this

![Directory structure](/assets/blog/convert-any-image-dataset-to-lance/unnamed.png)

here is the complete script for your reference..

```python
import os
import pandas as pd
import pyarrow as pa
import lance
import time
from tqdm import tqdm

def process_images(data_type):
    # Get the current directory path
    images_folder = os.path.join("cinic", data_type)

    # Define schema for RecordBatch
    schema = pa.schema([
        ('image', pa.binary()),
        ('filename', pa.string()),
        ('category', pa.string()),
        ('data_type', pa.string()),
    ])

    # Iterate over the categories within each data type
    for category in os.listdir(images_folder):
        category_folder = os.path.join(images_folder, category)

        # Iterate over the images within each category
        for filename in tqdm(os.listdir(category_folder), desc=f"Processing {data_type} - {category}"):
            # Construct the full path to the image
            image_path = os.path.join(category_folder, filename)

            # Read and convert the image to a binary format
            with open(image_path, 'rb') as f:
                binary_data = f.read()

            image_array = pa.array([binary_data], type=pa.binary())
            filename_array = pa.array([filename], type=pa.string())
            category_array = pa.array([category], type=pa.string())
            data_type_array = pa.array([data_type], type=pa.string())

            # Yield RecordBatch for each image
            yield pa.RecordBatch.from_arrays(
                [image_array, filename_array, category_array, data_type_array],
                schema=schema,
            )

# Function to write PyArrow Table to Lance dataset
def write_to_lance():
    # Create an empty RecordBatchIterator
    schema = pa.schema([
        pa.field("image", pa.binary()),
        pa.field("filename", pa.string()),
        pa.field("category", pa.string()),
        pa.field("data_type", pa.string()),
    ])

    # Specify the path where you want to save the Lance files
    images_folder = "cinic"

    for data_type in ['train', 'test', 'val']:
        lance_file_path = os.path.join(images_folder, f"cinic_{data_type}.lance")

        reader = pa.RecordBatchReader.from_batches(schema, process_images(data_type))
        lance.write_dataset(
            reader,
            lance_file_path,
            schema,
        )

def loading_into_pandas():
    # Load Lance files from the same folder
    current_dir = os.getcwd()
    print(current_dir)
    images_folder = os.path.join(current_dir, "cinic")

    data_frames = {}  # Dictionary to store DataFrames for each data type

    for data_type in ['test', 'train', 'val']:
        uri = os.path.join(images_folder, f"cinic_{data_type}.lance")

        ds = lance.dataset(uri)

        # Accumulate data from batches into a list
        data = []
        for batch in tqdm(
            ds.to_batches(columns=["image", "filename", "category", "data_type"], batch_size=10),
            desc=f"Loading {data_type} batches",
        ):
            tbl = batch.to_pandas()
            data.append(tbl)

        # Concatenate all DataFrames into a single DataFrame
        df = pd.concat(data, ignore_index=True)

        # Store the DataFrame in the dictionary
        data_frames[data_type] = df

        print(f"Pandas DataFrame for {data_type} is ready")
        print("Total Rows: ", df.shape[0])

    return data_frames

if __name__ == "__main__":
    start = time.time()
    write_to_lance()
    data_frames = loading_into_pandas()
    end = time.time()
    print(f"Time(sec): {end - start}")
```

Take the different splits of the train, test and validation through different Dataframes and utilize the information for your next image classification task

```python
train = data_frames['train']
test = data_frames['test']
val = data_frames['val']
```

and this is how the training dataframe looks like

```python
train.head()
```

![Training DataFrame head](/assets/blog/convert-any-image-dataset-to-lance/unnamed-1.png)

The benefits of this approach are numerous:

1. Storage Efficiency: The columnar storage and compression techniques employed by Lance result in significantly reduced storage requirements, making it an ideal choice for handling large-scale image datasets.

2. Fast Data Loading: The optimized data layout and indexing capabilities of Lance enable lightning-fast data loading times, improving the overall performance and responsiveness of your machine learning pipelines.

3. Random Access: The ability to selectively load specific data subsets from the Lance dataset allows for efficient data augmentation techniques and custom data loading strategies tailored to your unique requirements.

4. Unified Data Format: Lance can store diverse data types, such as images, text, and numerical data, in a single, streamlined format. This flexibility is invaluable in machine learning, where different modalities of data often need to be processed together.

By adopting the Lance format, we can literally elevate our machine learning workflow to new heights, unlocking unprecedented levels of efficiency, performance, and flexibility. Take the first step by running the provided script and converting your image datasets to the Lance format â€“ the future of machine learning data management is awaiting for you, who knows if you find your second love with lance format.
```

---

## ðŸ“„ æ–‡ä»¶: creating-a-fintech-agent.md

---

```md
---
title: "Creating a FinTech AI Agent From Scratch"
date: 2025-02-27
draft: false
featured: false
categories: ["Engineering", "FinTech"]
image: /assets/blog/creating-a-fintech-agent/creating-a-fintech-agent.png
description: "Explore fintech ai agent from scratch with practical insights and expert guidance from the LanceDB team."
author: Vipul Maheshwari
author_avatar: "/assets/authors/community.jpg"
author_bio: "ML Engineer and FinTech enthusiast specializing in AI agents, predictive modeling, and financial technology applications."
---

AI agents are popping up everywhere, and they're only going to become more common in the next few years. Think of them as a bunch of little digital assistantsâ€”agentsâ€”that keep an eye on things, make decisions, and get stuff done. It makes me wonder: how are we going to manage all these agents down the road? How do we trust them to handle decisions for us and let them run on their own?

These days, many frameworks are out there to help you with AI agents quickly. Honestly, it feels like a new "low-code" tool shows up every other day, promising to make building agents a breeze. They're all over the placeâ€”simple drag-and-drop setups or text boxes where you plug in a few prompts and some data, and just like that, your agent's ready to go.


## The Architecture

Every AI agent has three main pieces at its core: the Model, the Tools, and the Reasoning Loop.

1. **The Model**: This is the Large Language Model (LLM) that drives the agent. It's the brain behind it all, trained on piles of text to understand language and tackle tricky instructions. This is what lets the agent think and talk almost like a person.

2. **The Tools**: The agent uses specific skills to interact with the world and get things done. Whether it's grabbing data, hitting up APIs, or working with other programs, the tools are what make the agent useful.

3. **The Reasoning Loop**: This is how the agent figures things out. It's the process it uses to solve problems, pick the best path forward, and wrap up tasks.

In this post, I'm going to mix things up a little. Instead of leaning on a framework, I'll walk you through building a fintech agent from scratch using Python. This one's going to help with decisions about loans and insurance.

## What's the Deal Here?

So how does this fintech agent work? Well, picture yourself running a FinTech company. Customers come at you with stuff like, "Can I borrow $10K for a new kitchen? I'm 26 years old," or "I crashed my carâ€”will you cover it?" Haha, I'd probably shut that insurance claim down real quick.

The point is, that we need a system that's fast, sharp, and reliable when it comes to deciding on loan requests and insurance claims. So, what I did is this: instead of one lone agent handling everything, I split it into threeâ€”Loan, Insurance, and a Kernel agent.

![Creating a FinTech AI Agent From Scratch](/assets/blog/creating-a-fintech-agent/creating-a-fintech-agent.png)

1. **Loan Agent**: Takes a query, figures out what they want (like "home improvement"), and uses an ML model to predict if they're good for the loan based on age, income, and the amount.

2. **Insurance Agent**: Reads a claim, checks it against past cases, and decides if it's worth paying outâ€”kind of like a semantic search over a claims database.

3. **Kernel Agent**: The brains that tie it all together, sending queries to the right place.

Now the question always comes of why raw Python? Well, the most viable reason is flexibility. I'm just putting together a proof of concept here, and companies can easily tweak it to fit their own data and needsâ€”no black-box framework nonsense. The Credit Risk Dataset and synthetic claims are just placeholders; real firms would swap in their own loan histories and claim records.

## Big Picture!

Ok so what we've got is a Kernel Agent directing traffic, a Loan Agent predicting eligibility with machine learning, and an Insurance Agent checking claims against a vector database.

I'm using the [Credit Risk Dataset](https://www.kaggle.com/datasets/laotse/credit-risk-dataset) as a demo for loans. In reality, companies could use something like a CIBIL score to see if someone's eligible. But for this POC, I grabbed that dataset, trained a random forest model, and used it for predictive analysis. For the insurance agent, I built a synthetic pipeline to whip up a dataset of fake insurance queries. The target's simple: if the claim gets approved based on the conversation, it's a one. These are just placeholders to show how it works; companies can plug in their own data instead. Saying that, let's get started.

## Step 1: Setting Up the Stage - Libraries and Setup

We start by grabbing our toolsâ€”libraries that handle data, models, and vectors

```bash
pip install pandas joblib pyarrow sentence-transformers lancedb mistralai
```

```python
import pandas as pd
import joblib
import pyarrow as pa
from sentence_transformers import SentenceTransformer
import lancedb
from mistralai import Mistral
import os
from abc import ABC, abstractmethod
from typing import Any

# Abstract Tool Class
class Tool(ABC):
    @abstractmethod
    def name(self) -> str:
        pass

    @abstractmethod
    def description(self) -> str:
        pass

    @abstractmethod
    def use(self, *args, **kwargs) -> Any:
        pass

# Initialize Mistral and Embedder
api_key = os.environ.get("MISTRAL_API_KEY", "xxxxxxxxxxxxx")  
if not api_key:
    raise ValueError("Please set the MISTRAL_API_KEY environment variable.")

model = "mistral-large-latest"
client = Mistral(api_key=api_key)
hf_token = "hf_xxxxxxx"
embedder = SentenceTransformer('all-MiniLM-L6-v2', token=hf_token)

# Connect to LanceDB
db = lancedb.connect('./lancedb_data')
```

For the quick demonstration, I am going to use `mistral-large-latest` a model from the Mistral for LLM tasks and the intent classification in the Loan queries. The `Tool` class is our way to keep the agents modular - every tool gets a clear job.

## Step 2: Loan Agent - Predictive Analysis with ML

As soon as a query comes in, the Loan Agent uses its ML tool to spin up the predictions. I am using the [Credit Risk Dataset](https://www.kaggle.com/datasets/laotse/credit-risk-dataset) here. 

### Loading the Demo Data

```python
loan_data = pd.read_csv('./credit_risk_dataset.csv')
loan_data.head()
```

```
   person_age  person_income  loan_amnt  loan_intent  loan_status
0          22          59000      35000     PERSONAL            1
1          21           9600       1000    EDUCATION            0
```

{{< admonition note "Why Credit Risk Dataset?" >}}
It's public, well-structured, and mimics real loan data with features like loan_status (0 = approved, 1 = rejected). For demo purposes, it's spot-on. Companies would swap this for their own credit recordsâ€”same columns, different rows, or maybe other relevant benchmarks that can be utilized for setting up the target in terms of the loan status.
{{< /admonition >}}

### Building the ML Model

For the sake of the demonstration, I have used a Random Forest Classifier here. You can literally play around with different sets of these models, ensemble them, and get better in terms of accuracy and data processing.

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
import joblib

features = ['person_age', 'person_income', 'loan_amnt', 'loan_intent']
target = 'loan_status'

loan_data = loan_data.dropna(subset=features + [target])

preprocessor = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), ['loan_intent'])],
    remainder='passthrough'
)

X = loan_data[features]
y = loan_data[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)
print(f"Model accuracy: {accuracy:.2f}")  # 0.84
joblib.dump(model, 'loan_approval_model.pkl')
```

- **Why Random Forest**? Well as we know, Random forest comes under the ensemble category, and it's robust in nature, handles mixed data (numbers and categories) well, and gives us probabilitiesâ€”not just yes/no. For loans, that's gold; banks love knowing the odds though.
- **How It Works**: We preprocess `loan_intent` into numbers (e.g., "MEDICAL" becomes a categorical encoded number), split the data, train, and test. Got an 84% accuracy which is decent for a demo. Real data could boost or tweak that.
- **Swap It Out**: As I said earlier, companies can use their own loan data hereâ€”same features, their numbers. Train it fresh, and it's tailored to them.

### Completing the Integration for the Loan Agent

Now comes the agent itself:

```python
class PredictiveMLTool:
    def __init__(self):
        self.model = joblib.load('loan_approval_model.pkl')
    
    def use(self, intent, age, income, loan_amount):
        input_data = pd.DataFrame(
            [[age, income, loan_amount, intent]],
            columns=['person_age', 'person_income', 'loan_amnt', 'loan_intent']
        )
        prob_approved = self.model.predict_proba(input_data)[0][0]
        print(f"Probability for the loan approval is: {prob_approved}")
        return "Eligible" if prob_approved > 0.5 else "Not Eligible"

class LoanAgent:
    def __init__(self):
        self.tools = [PredictiveMLTool()]
        self.llm = llm
    
    def process(self, query, params):
        intent = self.extract_intent(query)
        age = params.get('age')
        income = params.get('income')
        loan_amount = params.get('loan_amount')
        return self.tools[0].use(intent, age, income, loan_amount)
    
    def extract_intent(self, query):
        valid_intents = ['DEBTCONSOLIDATION', 'EDUCATION', 'HOMEIMPROVEMENT', 'MEDICAL', 'PERSONAL', 'VENTURE']
        prompt = f"Given the query: '{query}', classify the intent into one of: {', '.join(valid_intents)}. Respond with only the intent in uppercase (e.g., 'HOMEIMPROVEMENT'). If unsure, respond with 'PERSONAL'."
        response = self.llm(prompt)[0]['generated_text'].strip().upper()
        return response if response in valid_intents else 'PERSONAL'
```

- **Why ML Here**? It's predictive, which means it takes raw data and spits out a decision. LLM handles intent because it's quick and decent for text, though companies might use something beefier or their own intent rules. I let the LLM decide what's the intent here.
- **How It Fits**: Query â†’ Intent (e.g., "school fees" â†’ EDUCATION) â†’ ML prediction. It's as simple as the tool's the muscle, and the agent's the coordinator.

If the companies need a different logic to get the target status in terms if the loan can be provided to an individual or not, as I said earlier, they can easily replace the predictive ml modeling with another set of benchmarks or maybe thresholding some different set of metrics to figure out if that person is made for the loan or not.

To make things more smoother, let's introduce our Kernel Agent which will be used for redirecting the queries to the Loan and the Insurance agent based on the query type.

```python
class KernelAgent:
    def __init__(self, loan_agent):
        self.loan_agent = loan_agent
    
    def process_query(self, query, params):
        return self.loan_agent.process(query, params)
```

Well, this is good, but let's not forget we now need to work on our Insurance agent.

## Step 3: Insurance Agent - Vector Search with LanceDB

Before diving in, I want to walk you through my thought process for creating the Insurance Agent. Picture this: you walk into an insurance company, sit down with someone, and start explaining why you need to file a claimâ€”why it's legit and why you deserve it. That conversation is key because it's the starting point for deciding whether the claim gets processed or not.

I tried hunting down a dataset with those kinds of conversations and clear `yes` or `no` labels for claims, but I came up empty. So, to keep things moving, I built a simple synthetic dataset pipeline. It generates a fake dataset with text queries on one side and a target on the otherâ€”showing if the person filing the claim is eligible or not.

Now, let's talk about where the VectorDB comes in. Here's the plan: we'll embed those insurance claim queries first. Then, when a new claim pops up, we'll pull the k-most similar queries from the database, look at their average eligibility scores, and use that to decide if the new query deserves a legitimate claim or not. With that in mind, let's start by generating some synthetic claims for the automotive industry. This is just a quick demoâ€”companies can swap in real conversations instead, and you can tweak the pipeline a bit to handle actual data.

```python
import random
import csv
from mistralai import Mistral

api_key = "xxxxxxxxxxxxxxx"
model = "mistral-large-latest"
client = Mistral(api_key=api_key)

base_prompt = "Generate a detailed query for an auto insurance claim..."
def generate_query(denied=False):
    prompt = base_prompt
    if denied:
        prompt += " Include a reason the claim might be denied (e.g., drunk driving, uninsured vehicle)."
    response = client.chat.complete(model=model, messages=[{"role": "user", "content": prompt}], max_tokens=200)
    return response.choices[0].message.content.strip()

def assign_target(query):
    rejection_keywords = ["drunk", "influence", "racing", "uninsured"]
    return 0 if any(kw in query.lower() for kw in rejection_keywords) else 1

dataset = []
for i in range(100):
    denied = random.random() < 0.4
    query = generate_query(denied)
    target = assign_target(query) if not denied else 0
    dataset.append({"query": query, "target": target})

with open("auto_insurance_claims.csv", "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=["query", "target"])
    writer.writeheader()
    writer.writerows(dataset)
```

To make it a bit more realistic, I have included some of the rejection keywords like `drunk`, `influence`, `racing` and the `uninsured` which is readily used to make the dataset better in terms of the realistic cases where the claims are not processed. Running this will generate a CSV with the relevant data.

Now comes the LanceDB setup...

```python
import pyarrow as pa

df = pd.read_csv('auto_insurance_claims.csv')
embeddings = embedder.encode(df['query'].tolist())
embeddings_list = [embedding.tolist() for embedding in embeddings]

schema = pa.schema([
    pa.field("embedding", pa.list_(pa.float32(), list_size=384)),
    pa.field("target", pa.int32()),
    pa.field("query", pa.string())
])

table = db.create_table("insurance_queries", schema=schema)
df_lance = pd.DataFrame({"embedding": embeddings_list, "target": df['target'], "query": df['query']})
table.add(df_lance)
```

As I said earlier, companies could load their real claim logs here instead. Now comes the main part, we are going to integrate the semantic tooling capability into our Insurance agent.

```python
# Insurance Agent Tools
class SemanticSearchTool(Tool):
    def __init__(self, table):
        self.table = table
        self.embedder = embedder

    def name(self):
        return "Semantic Search Tool"

    def description(self):
        return "Performs semantic search in LanceDB to assess claim approval based on similar past claims."

    def use(self, query, k=5):
        new_embedding = self.embedder.encode([query])[0].tolist()
        results = self.table.search(new_embedding).limit(k).to_pandas()
        approval_rate = results['target'].mean()
        similar_queries = results['query'].tolist()
        decision = "Approved" if approval_rate > 0.5 else "Not Approved"
        print(f"Approval rate among similar claims: {approval_rate*100:.1f}%")
        return {"decision": decision, "similar_queries": similar_queries}

# Insurance Agent
class InsuranceAgent:
    def __init__(self, table):
        self.tools = [SemanticSearchTool(table)]

    def process(self, query):
        return self.tools[0].use(query)
```

This is it, let's integrate our Insurance agent within our Kernel agent along with the Loan Agent.

## Step 4: Completion of Kernel Agent

```python
# Kernel Agent
class KernelAgent:
    def __init__(self, loan_agent, insurance_agent):
        self.loan_agent = loan_agent
        self.insurance_agent = insurance_agent
        self.client = client
        self.model = model

    def classify_query(self, query):
        prompt = f"""
        Given the query: '{query}', classify it as 'loan' or 'insurance'.
        Respond with only 'loan' or 'insurance'.
        If unsure, respond with 'unknown'.
        """
        response = self.client.chat.complete(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=10
        )
        print(f"Query type : {response.choices[0].message.content.strip().lower()}")
        return response.choices[0].message.content.strip().lower()

    def process_query(self, query, params=None):
        query_type = self.classify_query(query)
        if query_type == 'loan':
            if not params:
                return "Error: Loan query requires parameters (age, income, loan_amount)."
            return self.loan_agent.process(query, params)
        elif query_type == 'insurance':
            return self.insurance_agent.process(query)
        else:
            return "Error: Unable to classify query as 'loan' or 'insurance'."
```

Now all you need to do is toss in a queryâ€”could be anything about loans or insurance. It'll hit the Kernel Agent first, and based on what the query's about, it'll get handed off to either the Loan Agent or the Insurance Agent. From there, they'll handle the rest of the decisions.

```python
# Test Cases
test_cases = [
    {
        "query": "I had a bad accident yesterday, but i think i was a bit drunk, need some insurance",
        "params": None
    },
]

# Run Tests
print("Testing Integrated System with Mistral:")
print("-" * 50)
for case in test_cases:
    query = case["query"]
    params = case.get("params")
    result = kernel_agent.process_query(query, params)
    print(f"Query: '{query}'")
    if params:
        print(f"Params: {params}")
    if isinstance(result, dict):
        print(f"Decision: {result['decision']}")
        print("Similar Queries:")
        for q in result['similar_queries']:
            print(f"- {q}")
    else:
        print(f"Result: {result}")
    print("-" * 50)
```

Think about the potential here: fintech companies can plug in their own tools and data to process loans and insurance claims. Picture how much manual work insurance folks put in, checking every little thing to figure out if a claim's legit or notâ€”or the same deal with loans, deciding if someone qualifies. This could cut out so much of that hassle.

{{< admonition tip "Complete Implementation" >}}
Refer to this [Google Colab notebook](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/fintech-ai-agent/fintech-ai-agent.ipynb) for complete code implementation.

Star ðŸŒŸ [LanceDB recipes](https://github.com/lancedb/vectordb-recipes/) to keep yourself updated with high quality resources & applications for LLMs, multi-modal models and VectorDBs.
{{< /admonition >}}
```

---

## ðŸ“„ æ–‡ä»¶: custom-dataset-for-llm-training-using-lance.md

---

```md
---
title: "Custom Datasets for Efficient LLM Training Using Lance"
date: 2024-03-08
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/custom-dataset-for-llm-training-using-lance/preview-image.png
meta_image: /assets/blog/custom-dataset-for-llm-training-using-lance/preview-image.png
description: "See about custom datasets for efficient llm training using lance. Get practical steps, examples, and best practices you can use now."
---

## Introduction

Large Language Models have become the talk of the town in the past year and a half and the ML world has seen all sorts of funny little Open (and closed) Source LLMs released. Training an LLM however, is not as easy and straightforward as using it for inference or even fine-tuning it.

One of the early key hurdles one faces when training an LLM on their own (in a hobbyist-esque fashion) is loading the data into their LLM during training. You could always train a smaller, more efficient LLM on a smaller GPU but you will be severely limited by how your data is loaded into the model.

Of course, there are many approaches to this but if you want to train your own little LLM on a subset of a larger dataset (for instance a subset of [`codeparrot/github-code`](https://huggingface.co/datasets/codeparrot/github-code) dataset which has a total of 1TB of code data from GitHub), you would need to first download that entire dataset, split it and then use the much smaller subset for training/fine-tuning.

Unless of course, you are disk space poor or someone with a low attention span and doesnâ€™t want to download the whole dataset of a terabyte only to really ever use 50GBs of it. In this case, you are at the right place!

## An outline

Before we dive into this, letâ€™s have a rough outline of the format we want the data in.

First, we would like the text/code data to be pre-processed, tokenized and saved in one large array(-like) structure with tokens in it. This will make it extremely easy to train the LLM since now you can just load `k` tokens for your training tokens (`x`) (where `k` is the context length, 1024, 2048, etc) and `idx+k+1` tokens for your target tokens (`y`) (where idx is the current index of the tokens and y will be 1 token into the future but same length as `x`).

The above arrangement will be pretty easy to pass into an LLM for training if, like me, you write your training scripts yourself.

Second, we should be able to access any chunk of tokens from the dataset without having to load that entire dataset (50 or maybe 100GB) into the memory. Ideally, we would like to make this random access based on indices instead of using offset-magic (like we do when using `numpy.memmap`).

## Lance comes to the rescue

This is where Lance comes to the rescue. Lance is a modern columnar data format that is optimized for ML workflows and datasets. It is written in Rust ensuring great I/O and processing speeds with the ease of using a simpler Python API. Lance uses the Arrow data format in the back end. You can read more about the Lance file format [here](https://lancedb.github.io/lance/format.html).

One of the very nice things that lance offers is that you can access the data from a lance dataset just by specifying the indices and it will only load the data at said indices instead of the entire dataset which is exactly what our second requirement was!

## Coding it out

Enough talking, letâ€™s now see how to do this step by step!

### Creating and saving the dataset

First, weâ€™ll import all the necessary frameworks and define the tokenizer and the dataset we will be using.

```python
import lance
import pyarrow as pa

from tqdm.auto import tqdm

from datasets import load_dataset
from transformers import AutoTokenizer

# Change based on your need
tokenizer = AutoTokenizer.from_pretrained(
    "EleutherAI/gpt-neox-20b"
)

# Only load the Python code files from codeparrot dataset
dataset = load_dataset(
    "codeparrot/github-code",
    streaming=True,
    split="train",
    languages=["Python"]
)
dataset = dataset.shuffle(seed=42)
```

**Note**: In the above code snippet, make sure that `streaming` is set to `True` in `load_dataset` function otherwise it will start downloading the entire codeparrot dataset! Learn more about the streaming mode [here](https://huggingface.co/docs/datasets/en/stream).

Now, letâ€™s define a function that tokenizes the dataset. Remember, we havenâ€™t downloaded the whole dataset so instead of using that function with `.map()` on the dataset, weâ€™ll just return the `input_ids` that the tokenizer returns.

```python
def tokenize(sample):
    return tokenizer(sample['code'])['input_ids']
```

The actual code of each sample is in the `code` attribute.

Now that we have a dataset and tokenizer function ready, letâ€™s write a function that does all this process for as many samples as we need. Iâ€™ll do all this processing in one single function because there just arenâ€™t too many steps, but if you need to do more pre-processing, feel free to divide this into multiple functions!

Weâ€™ll also specify how many total samples we need in our subset. I am going ahead with 5M samples for now.

```python
total_samples = 5_000_000  # 5 Million samples

def process_samples():
    current_sample = 0
    for sample in tqdm(dataset, total=total_samples):
        # If we have added all 5M samples, stop
        if current_sample == total_samples:
            break
        # Tokenize the current sample
        tokenized_sample = tokenize(sample)
        # Increment the counter
        current_sample += 1
        # Yield a PyArrow RecordBatch
        yield pa.RecordBatch.from_arrays(
            [tokenized_sample],
            names=["value"]
        )

# Define the dataset schema
schema = pa.schema([
    pa.field("value", pa.int64())
])
```

A few things to note from above:

- The `process_samples` function doesnâ€™t directly receive any arguments because it will be converted to a Pyarrow `RecordBatchReader` which is a fancy way of saying an â€˜iterator that follows a schemaâ€™.

- The `names` argument just describes the name of the fields in your Batch. In this case, our batch only consists of `input_ids` but I have named it `value` to avoid any confusion.

- Schema describes what type of data (with what field name and data type) will be present in our Pyarrow table.

Finally, letâ€™s convert our `process_samples()` function to `RecordBatchReader` which can iterate over the dataset and then write that dataset to disk.

```python
# The reader takes in a schema and the function
reader = pa.RecordBatchReader.from_batches(
    schema,
    process_samples()
)

# Write the dataset to disk
lance.write_dataset(
    reader,
    "code_parrot_5M_subset.lance",
    schema
)
```

Once we run the above snippet, it will start reading the samples one by one, tokenize them and then save them to a Pyarrow table that will be saved as the Lance dataset.

### Loading the dataset

Loading the dataset will require a bit of trickery, see the function below.

```python
# First make a dataset descriptor and see row count
dataset = lance.dataset("code_parrot_5M_subset.lance")
print(dataset.count_rows())  # Should be 5M total samples

def load_data(dataset, indices):
    # Load the data at these indices
    data = dataset.take(indices).to_pylist()
    # A little short-cut to get the tokens in one list
    data = list(map(lambda x: x['value'], data))
    return data
```

In the above function, we will pass in the dataset descriptor defined before it and the indices we need to fetch. These indices can be a normal list or a numpy array.

## Conclusion

And there you have it! Processing, saving and loading any subset of a very very large ðŸ¤— dataset in under 70 lines of code without using any more than 3GB of RAM!

You can find the complete script [here](https://gist.github.com/tanaymeh/5285a073f4ad7d7e8aa7e952fe220aa4).
```

---

## ðŸ“„ æ–‡ä»¶: designing-a-table-format-for-ml-workloads.md

---

```md
---
title: "Designing a Table Format for ML Workloads"
date: 2024-03-25
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/designing-a-table-format-for-ml-workloads/designing-a-table-format-for-ml-workloads.png
description: "Explore designing a table format for ML workloads with practical insights and expert guidance from the LanceDB team."
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "westonpace"
---

In recent years the concept of a **table format** has really taken off, with explosive growth in technologies like [Iceberg](https://en.wikipedia.org/wiki/Apache_Iceberg), Delta, and Hudi. With so many great options, one question I hear a lot is variations of "why can't Lance use an existing format like ...?"

In this blog post I will describe the Lance table format and hopefully answer that question. The very short TL;DR: **existing table formats don't handle our customer's workflows. Basic operations require too much data copy, are too slow, or cannot be parallelized.**

## What is a Table Format?

A table format is probably more accurately thought of as a protocol. It describes how the basic table operations (adding rows, deleting rows, etc.) happen. In other words, it tells us how data files change, what metadata we need to record, what extra structures (e.g. deletion files) are needed, and so on. In the interest of brevity I'm not going to fully describe every operation in the Lance table format in this article. A more complete description can be found in [our docs](https://lancedb.github.io/lance/format.html). If you are familiar with Iceberg or Delta, then Lance is not very different. In the following sections I'll focus on what is different.

{{< admonition fun-fact >}}
"Table formats stuff the CRUD into file formats" is a questionable tagline
{{< /admonition >}}

## Modern Workloads

When I talk about "modern workloads" I'm generally talking about large ML workloads that are developing, training, or using various models. These workloads are not limited to LLMs (there are many types of models) and they can be very diverse. However, we have noticed a few commonalities.

### The Curse of Wide Data

As data science gets more sophisticated, scientists are bringing their work to bear on larger and larger data types. Text is moving from "simple labels" to things like complex prose, websites, and source code. Multi-modal data such as audio, images, and video are being introduced. Even numerical data can get large when we consider items like tensors, vector embeddings, etc.

As I've worked with wide data I've come to an interesting observation that I am going to start referring to as *the curse of wide data* (because that's a fun sounding name). **If some of your data is wide then most of your data is wide.** One of our team cats has agreed to assist with a demonstration:

{{< image-grid >}}
![Wide Data Demonstration](/assets/blog/designing-a-table-format-for-ml-workloads/chunky_lance-1.png)
*Visualizing Data Width: A playful demonstration of how a single wide column can dominate data storage requirements, featuring our team mascot.*
{{< /image-grid >}}

If cats are not convincing then we can try a simple example. If you take the TPC-H line items table (the kind of thing all your favorite databases are optimized against) and add a single 3KiB vector embedding column then that table will go from 16 columns to 17 columns...and will go from 0% wide data to 99% wide data.

![TPC-H with Wide Data](/assets/blog/designing-a-table-format-for-ml-workloads/TPCH-with-wide.png)
*Data Distribution Analysis: Adding a 3KB vector embedding to the TPC-H line items table shifts the data distribution dramatically, with the embedding consuming 99% of the total storage.*

### Tables Grow Horizontally *and* Vertically

Your internal model of a database table is probably something that grows mostly *longer* with time. For example, we have a sales table that gains new rows each time a sale is made at some company. Or maybe we are recording user clicks on some website. Or perhaps our table gains a new row every time a new post about table formats is made.

Once you put data scientists into the mix, something strange starts to happen. The table starts to grow horizontally, often growing outward from a single column. For example, let's consider a research project that starts by scraping Wikipedia and creating one fat column "article text" which contains the Wikipedia markup. Then the data scientists get to work. They add a new column, "sentiment analysis", that measures if the article has a positive or negative tone. Then they add "political bias". Then they go and scrape some more data and add an "edit count" column. This process repeats and repeats until, in some frightening cases, we can have hundreds or thousands of columns!

![Growing in 2D](/assets/blog/designing-a-table-format-for-ml-workloads/growing-in-2d.png)
*ML data starts with a core collection of observances. Over time both new observances AND new features are added to the data.*

This process of adding columns repeats again and again as scientists discover new interesting things about the underlying data and make more and more sophisticated observations. Some researchers are now even starting to worry about datasets with [tens of thousands of columns](https://arxiv.org/abs/2404.08901) (although we find most users are more in the hundreds-to-thousands scenario).

![Feature Engineering](/assets/blog/designing-a-table-format-for-ml-workloads/so-many-features.png)
*Feature Engineering Complexity: The exponential growth of feature columns in ML datasets, illustrating how sophisticated analysis can lead to an ever-expanding set of data attributes.*

### Data is Messy

This is not a novel observation but it bears repeating. This has many different faces and implications. Cleanup operations (editing, deleting, etc.) are important. Data needs to be searchable so users can understand what they have. Data is often scraped or taken directly from other sources and foreign keys are super common (I can't remember the last customer that *didn't* have some kind of UUID column). Existing data often needs deduplicated. New data needs to be deduplicated on entry (I sometimes wonder if users are even aware it's possible to add data without using merge). Users need to do a lot of exploration of the data.

## Modern Problems (and some modern solutions)

I'll now explain the problems we encountered with existing table formats. For each problem I'll explain how we've solved it in Lance and I'll also make an attempt at explaining some of the alternate solutions we've seen out there. Let's start with perhaps the most common reason users have given for switching to Lance.

### Data Evolution > Schema Evolution 

Table formats offer "zero copy schema evolution". This means you can add columns to your table after you've already added data to your table. This is great but there is one problem and it's in the fine print.

![Schema Evolution Fine Print](/assets/blog/designing-a-table-format-for-ml-workloads/schema-evolution-not-free.png)
*FREE Schema Evolution (existing rows not included)*

That's right, new columns can only be populated going forwards. All existing rows will either be NULL or will be given a default value. This makes perfect sense in the classic "data grows vertically" scenario. If our sales team started rewarding "loyalty bucks" with each purchase then the new "loyalty bucks" column doesn't make sense for all past transactions and we can set it to zero.

![Vertical Table Schema Evolution](/assets/blog/designing-a-table-format-for-ml-workloads/vertical-table-schema-evolution.png)
*Datasets that grow vertically have no meaningful value for new columns and a default value (or NULL) is all you need to use.*

However, this is NOT what we want when our table is growing horizontally. The entire reason we added a new column is because we've calculated some new feature value for all of our rows! So, how do we add this new column in a classic table format? It's simple, we copy all of our data.

{{< image-grid >}}
![Data Copy Meme](/assets/blog/designing-a-table-format-for-ml-workloads/one-banana-meme.png)
*I mean, it's one billion rows Michael, what could it require, 50 gigabytes?*
{{< /image-grid >}}

Well, that's ok...a data copy isn't that expensive. Unless...

![One Billion Rows Comparison](/assets/blog/designing-a-table-format-for-ml-workloads/one-billion-rows.png)
*Scale comparison: 1B rows TPC-H ~45 GB vs 1B rows FineWeb Prompt Data ~2,100 GB vs 1B rows small images w/ Captions ~250,100 GB*

Well, that's ok...how many new columns are we really going to be adding...

{{< image-grid >}}
![More Features](/assets/blog/designing-a-table-format-for-ml-workloads/moar-features.png)
*"Don't forget the has_weird_pink_tree feature, we use that one to detect springtime."*
{{< /image-grid >}}

Ok, maybe this is a problem we actually need to solve...

#### Lance Table Feature One: Two-dimensional Storage

So how does Lance solve this? With more complexity magic. Lance has a *two-dimensional storage layout*. Rows are divided (vertically) into fragments. Fragments are divided (horizontally) into data files. Each data file in a fragment has the same number of rows and provides one or more columns of data. This is different from traditional table formats which only have one dimension.

![2D Storage Layout](/assets/blog/designing-a-table-format-for-ml-workloads/2d-storage.png)
*Each fragment can have any number of data files. Each data file in a fragment must have the same number of rows.*

Initially, as we write new rows, we create one data file per fragment. When we add a new column, instead of rewriting the fragment, we add a new data file to the fragment. In fact, we can use this trick to do a lot of cool things, like splitting a fragment when we update it, but we'll save the advanced tricks for a future blog post. For now, let's focus on our horizontally growing table.

![2D Schema Evolution](/assets/blog/designing-a-table-format-for-ml-workloads/2d-schema-evolution.png)
*Two-Dimensional Storage Evolution: Visualization of Lance's storage strategy, where new columns (green) are added as separate files while existing data (red) remains untouched, enabling efficient schema evolution.*

Every time users add a new column, we write a new data file for each fragment. We don't need to rewrite any data (keep in mind that the "fragments" are not files, just lists in the manifest, so we can modify those). At some point, as we start to get hundreds or thousands of files per fragment, we may want to merge some of these together, which *will* require a rewrite (tbh, I haven't experienced this need yet but I'm playing devil's advocate). However, that rewrite can be done strategically. The large columns (remember: 90%+ of our data) can be left alone and we only need to combine and rewrite the smaller columns.

#### Rebuttal: Why not Two Tables?

There is another way this problem can be solved, which is perhaps more classic, but also more limiting. The two-table approach, perhaps also called the "url-only-in-db" approach, splits the large data and the small data into two different tables, joined with a foreign key of some kind. A specialized storage engine (like Lance) can be used for the large data, while traditional table formats can be used for the small data.

There is nothing particularly wrong here but we find that it ends up being more work than the two-dimensional storage approach described above. You need to come up with some kind of mechanism for keeping the two tables in sync through all the various table format operations. In fact, what you end up doing, is creating a new table format.

It also quickly becomes difficult to know when a column is "for the big table" and when a column is "for the little table". For example, you might want to put your vector embeddings in the large table with your images so you can avoid rewriting those when you add new features. However, vector embeddings are actually something that are regularly replaced (when a new model comes long) or added and removed (to support different search models). You probably want to make sure you're not rewriting your images every time you change your embedding model. This means you either need a third table, your "big data table" needs to utilize two-dimensional storage, or you give up and put the embeddings back in the small table.

{{< admonition >}}
*Quad-table storage format* sounds cool but I hope it never exists.
{{< /admonition >}}

### "Search" is Everywhere

We got our start building vector search and so it's no surprise we handle that case quite well. What *did* surprise us was that search started to pop up *everywhere*. You just need to know the trick: no one ever ever calls it search. Let's explore some sample things people did ask for,

- These feature columns are based on an external dataset that changes all the time so every day I need to pull down the changes. But that's ok, the batch of updates has a foreign key column and so I can use that to update all those rows with new values.
- My data has a tags column where we've classified the data into a few different thousand tags. Each row has one or more tags. When we test our model we often test just one or two tags. It's just a small chunk of the data so it shouldn't take long to load.
- We want to test our model on pictures of cats. The data isn't labeled in this way but there's a "caption" column. Just give us all the images that have cat or kitten or feline or whatever in the caption column.

#### Lance Table Feature Two: Indices & Random Access

The humble index has been synonymous with databases since...forever. However, as OLAP processing moved into columnar storage (and into the cloud) a strange thing happened. It turned out that sequential access of columnar data was so fast, and random access to column data was so slow, that indices were no longer required. Even if you could identify exactly what bits of data you wanted, there was little benefit from reducing the total amount of I/O.

![S3 Read Amplification](/assets/blog/designing-a-table-format-for-ml-workloads/S3-Read-Amplification.png)
*There might be some extra room in the truck.*

LanceDB (the company, not the table format) has changed this equation in a number of different ways. The Lance file format minimizes the number of IOPs and amount of data that needs read. We've also embraced the fact that many of our users are either running locally or have some kind of filesystem caching layer. In fact, a page cache is a big part of our enterprise architecture. As a result, the access patterns have swapped, and the forgotten index has once become essential.

![S3 No Amplification with Cache](/assets/blog/designing-a-table-format-for-ml-workloads/S3-No-Amplification-Cause-Cache-4-.png)
*No refunds for drones throwing results in your face*

Fortunately, while Lance obviously has vector indices, we also have a variety of indices for non-vector data. We use these indices internally, when available, to speed up a number of table format tasks. Let's look at the examples above.

Indices on **foreign key columns** make it super fast to find matching rows and apply updates. Classically, this kind of task would be done with a hash join on the foreign key column. If we have a btree index on the foreign key column we can skip this step entirely. In fact, we can do key-deduplicating writes without any I/O into the old data. This makes things faster even if you don't have any kind of caching layer.

{{< admonition >}}
A hash join on the foreign key column is pretty much the same thing as building a btree index on the fly. In other words, the old approach was to rebuild a btree index on every single operation!
{{< /admonition >}}

In the **tags filtering** example we run into a general expectation our users have. "It's just a small chunk of the data so it shouldn't take long to load". Unfortunately, string filtering, and string loading, can be surprisingly expensive. Let's say we have one billion rows, a "tags" column might easily be 50-80GB, and performing billions of string comparison operations can be pretty time consuming. However, if there's an index (in this case a label_list index), then we can quickly start returning results and the entire query will likely be much faster, especially if the data is in-cache.

In the last, example, involving **captions**, we encounter a surprising relationship. Nearest neighbor search, in threshold mode, can turn "search indices" (like vector indices or full text indices) into a tool that can be used for filtering. We wanted to find all relevant images (cat or kitten or feline). This is exactly the kind of problem that full text search is good at solving. You can either discover a threshold that gives you the correct results or simply pull back a large number of results in FTS order and find the point the results are no longer relevant.

![FTS Thresholds](/assets/blog/designing-a-table-format-for-ml-workloads/FTS-Thresholds-1-.png)
*Search Threshold Analysis: Comparison of semantic search versus full-text search approaches, demonstrating how different search strategies handle variations in query terms and context.*

Existing table formats will often tackle these problems with clustering (a.k.a *primary indices*). They've even come up with some pretty cool innovations here like liquid clustering and z-order clustering which make it easier to handle multiple columns. However, these approaches are often limited in the number of scenarios they can address, there are only so many columns that you can use as primary indices. They also would rely on rewrites for new columns. Even if you were to add two-dimensional storage, you would need a rewrite if you wanted your new column to participate in a primary index.

I think there is a lot of good in primary indices. They have better I/O patterns (don't require random access) since the data is ordered and they are much smaller than secondary indices. We need to get better primary index support into Lance at some point. Still, the data rewriting problem is significant, and it has prevented us from being to take advantage of primary indices in many situations.

### The Well Rounded Implementation

The final issue we encountered is that most table libraries we tried had focused most their time and effort on the query problem. This makes a lot of sense. OLAP is big and complicated. Distributed query engines are cool and fun. Unfortunately, we end up with a bit of an unbalanced implementation.

![Unbalanced Query Engine](/assets/blog/designing-a-table-format-for-ml-workloads/Unbalanced-Query-Engine-1-.png)
*Yes, those are consummate V's (IYKYK).*

At LanceDB, we've discovered that working with big data means that *everything is hard*. Importing initial data needs to be done in parallel and distributed because even small numbers of rows can mean tons of data. Adding a new column might need to be a task that runs in parallel because we're using a complex model to calculate our features and it can be expensive to calculate even a single row's value. Calculating an index needs to be something we can partition across multiple workers. The list goes on.

Basically, the way I like to think of it, is that we need the same API as a regular database (insert, create index, alter column, etc.) but every single thing needs to be capable of running in parallel, and ready to handle big data (don't get us started on batch sizes and RAM consumption ðŸ˜…).

To be fair, I don't think we've got the perfect user friendly API for many of these things. Also, these are primarily library concerns, as most formats can support these operations in parallel. However, these are challenges we are taking on head first, and we've heard multiple times from users that we seem to be getting it right so far.

## What's Next

First, we still have work to do to make sure that we're doing all the things I've described above in the best way possible. We wanted to start writing about this to share the challenges and solutions we've encountered as we hope it will help the design and expansion of existing table formats.

We also want to increase our integration support. Pushing customers to use a single table format is perhaps idealistic. We recently noticed some interest in potentially [adding the Lance file format to Iceberg](https://lists.apache.org/thread/ovyh52m2b6c1hrg4fhw3rx92bzr793n2) and this kind of integration is very exciting. We're also excited by the many unified front-ends to table formats that have arisen. The not-exactly-official "[pyarrow datasets protocol](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.Dataset.html)" has allowed us to integrate with tools like DuckDB and Polars. [Datafusion](https://datafusion.apache.org/) gives us the "table provider" trait and we're seeing more and more things that can consume that. [Flight SQL](https://arrow.apache.org/docs/format/FlightSql.html) gives as a unified "SQL frontend". Tools like [XTable](https://xtable.apache.org/) could even provide metadata-level compatibility.

Work on catalogs is starting to ramp up and provide unified APIs for database management and we're following these moves closely. We'd also like to continue our work developing new kinds of indices. All of our indices are just plain Arrow data (in Lance files) and could be useful elsewhere too. Through the arrow-verse, and the idea of composable data systems, we are finding that users are able to use the right tool for the right job without hard-locking into dependencies.

If you're interested in adding an integration to Lance or learning more about our table format, hop on over to our [Discord](https://discord.gg/G5DcmnZWKB) or [Github](https://github.com/lancedb/lance) and we'd be happy to talk to you!

## Special Thanks

Special thanks to the pets from LanceDB who would like to mention that these photos were perhaps not taken from the most flattering angles.

{{< image-grid >}}
![Pet Bibliography](/assets/blog/designing-a-table-format-for-ml-workloads/Pets-Biliography.png)
*Meet the Team: Uni (orange and white) and Lance (white) - our feline quality assurance specialists. And yes, Lance's name truly is a delightful coincidence!*
{{< /image-grid >}}
```

---

## ðŸ“„ æ–‡ä»¶: effortlessly-loading-and-processing-images-with-lance-a-code-walkthrough.md

---

```md
---
title: "Effortlessly Loading and Processing Images with Lance: a Code Walkthrough"
date: 2024-03-29
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/effortlessly-loading-and-processing-images-with-lance-a-code-walkthrough/preview-image.png
meta_image: /assets/blog/effortlessly-loading-and-processing-images-with-lance-a-code-walkthrough/preview-image.png
description: "Working with large image datasets in machine learning can be challenging, often requiring significant computational resources and efficient data-handling techniques."
---

Working with large image datasets in machine learning can be challenging, often requiring significant computational resources and efficient data-handling techniques. While widely used for image storage, traditional file formats like JPEG or PNG are not optimized for efficient data loading and processing in Machine learning workflows. This is where the Lance format shines, offering a modern, columnar data storage solution designed specifically for machine learning applications.

The Lance format stores data in a compressed columnar format, enabling efficient storage, fast data loading, and fast random access to data subsets. Additionally, the Lance format is maintained on disk, which provides a couple of advantages: It will persist through a system failure and doesnâ€™t rely on keeping everything in memory, which can run out. This also lends itself to enhanced data privacy and security, as the data doesnâ€™t need to be transferred over a network.

One of the other key advantages of the Lance format is its ability to store diverse data types, such as images, text, and numerical data, in a unified format. Imagine having a data lake where each kind of data can be stored seamlessly without separating underlying data types. This flexibility is precious in machine learning pipelines, where different data types often need to be processed together. This unparalleled flexibility is a game-changer in machine learning pipelines, where different modalities of data often need to be processed together for tasks like multimodal learning, audio-visual analysis, or natural language processing with visual inputs.

With Lance, you can effortlessly consider all kinds of data, from images to videos and audio files to text data and numerical values, all within the same columnar storage format. This means you can have a single, streamlined data pipeline that can handle any combination of data types without the need for complex data transformations or conversions. Lance easily handles it without worrying about compatibility issues or dealing with separate storage formats for different data types. And the best part? You can store and retrieve all these diverse data types within the same column.

In contrast, while efficient for tabular data, traditional formats like Parquet may need to handle diverse data types better. By converting all data into a single, unified format using Lance, you can retrieve and process any type of data without dealing with multiple formats or complex data structures.

In this article, I'll walk through a Python code example that demonstrates how to convert a dataset of GTA5 images into the Lance format and subsequently load them into a Pandas DataFrame for further processing.

## Procedure

### 1. Import dependencies

```python
import os
import pandas as pd
import pyarrow as pa
import lance
import time
from tqdm import tqdm
```

We start by importing the necessary libraries, including *os* for directory handling, *pandas* for data manipulation, *pyarrow* for working with Arrow data formats, *lance* for interacting with the Lance format, and *tqdm* for displaying progress bars.

### 2. Process images

```python
def process_images():
    # Get the current directory path
    current_dir = os.getcwd()
    images_folder = os.path.join(current_dir, "./image")

    # Define schema for RecordBatch
    schema = pa.schema([('image', pa.binary())])

    # Get the list of image files
    image_files = [
        filename for filename in os.listdir(images_folder)
        if filename.endswith((".png", ".jpg", ".jpeg"))
    ]

    # Iterate over all images in the folder with tqdm
    for filename in tqdm(image_files, desc="Processing Images"):
        # Construct the full path to the image
        image_path = os.path.join(images_folder, filename)

        # Read and convert the image to a binary format
        with open(image_path, 'rb') as f:
            binary_data = f.read()

        image_array = pa.array([binary_data], type=pa.binary())

        # Yield RecordBatch for each image
        yield pa.RecordBatch.from_arrays([image_array], schema=schema)
```

The *process_images* function is responsible for iterating over all image files in a specified directory and converting them into PyArrow *RecordBatch *objects. It first defines the schema for the *RecordBatch*, specifying that each batch will contain a single binary column named '*image*'.

It then iterates over all image files in the directory, reads each image's binary data, and yields a *RecordBatch *containing that image's binary data.

### 3. Write to Lance

```python
def write_to_lance():
    # Create an empty RecordBatchIterator
    schema = pa.schema([
        pa.field("image", pa.binary())
    ])

    reader = pa.RecordBatchReader.from_batches(schema, process_images())
    lance.write_dataset(
        reader,
        "image_dataset.lance",
        schema,
    )
```

The *write_to_lance* function creates a *RecordBatchReader *from the *process_images* generator and writes the resulting data to a Lance dataset named "*image_dataset.lance*". This step converts the image data into the efficient, columnar Lance format, optimizing it for fast data loading and random access.

### 4. Load into Pandas

```python
def loading_into_pandas():
    uri = "image_dataset.lance"
    ds = lance.dataset(uri)

    # Accumulate data from batches into a list
    data = []
    for batch in ds.to_batches(columns=["image"], batch_size=10):
        tbl = batch.to_pandas()
        data.append(tbl)

    # Concatenate all DataFrames into a single DataFrame
    df = pd.concat(data, ignore_index=True)
    print("Pandas DataFrame is ready")
    print("Total Rows: ", df.shape[0])
```

The* loading_into_pandas* function demonstrates how to load the image data from the Lance dataset into a Pandas DataFrame. It first creates a Lance dataset object from the "*image_dataset.lance*" file. Then, it iterates over batches of data, converting each batch into a Pandas DataFrame and appending it to a list. Finally, it concatenates all the DataFrames in the list into a single DataFrame, making the image data accessible for further processing or analysis.

### 5. Run and measure time

```python
if __name__ == "__main__":
    start = time.time()
    write_to_lance()
    loading_into_pandas()
    end = time.time()
    print(f"Time(sec): {end - start}")
```

The central part of the script calls the *write_to_lance* and *loading_into_pandas* functions, measuring the total execution time for the entire process.

By leveraging the Lance format, this code demonstrates how to efficiently store and load large image datasets for machine learning applications. The columnar storage and compression techniques Lance uses result in reduced storage requirements and faster data loading times, making it an ideal choice for working with large-scale image data.

Moreover, Lance's random access capabilities allow for the selective loading of specific data subsets, enabling efficient data augmentation techniques and custom data loading strategies tailored to your machine learning workflow.

## TL;DR

Lance format provides a powerful and efficient solution for handling multimodal data in machine learning pipelines, streamlining data storage, loading, and processing tasks. By adopting Lance, we can improve our machine learning projects' overall performance and resource efficiency while also benefiting from the ability to store diverse data types in a unified format and maintain data locality and privacy. Here is the whole script for your reference.

```python
    import os
    import pandas as pd
    import pyarrow as pa
    import lance
    import time
    from tqdm import tqdm

    def process_images():
        # Get the current directory path
        current_dir = os.getcwd()
        images_folder = os.path.join(current_dir, "./image")

        # Define schema for RecordBatch
        schema = pa.schema([('image', pa.binary())])

        # Get the list of image files
        image_files = [filename for filename in os.listdir(images_folder)
              		 if filename.endswith((".png", ".jpg", ".jpeg"))]

        # Iterate over all images in the folder with tqdm
        for filename in tqdm(image_files, desc="Processing Images"):
            	# Construct the full path to the image
            	image_path = os.path.join(images_folder, filename)

            	# Read and convert the image to a binary format
            	with open(image_path, 'rb') as f:
                	binary_data = f.read()

            	image_array = pa.array([binary_data], type=pa.binary())

            	# Yield RecordBatch for each image
            	yield pa.RecordBatch.from_arrays([image_array], schema=schema)

    # Function to write PyArrow Table to Lance dataset
    def write_to_lance():
    	# Create an empty RecordBatchIterator
    	schema = pa.schema([
        	pa.field("image", pa.binary())
    	])

    	reader = pa.RecordBatchReader.from_batches(schema, process_images())
    	lance.write_dataset(
        	reader,
        	"image_dataset.lance",
        	schema,
    	)

    def loading_into_pandas():

    	uri = "image_dataset.lance"
    	ds = lance.dataset(uri)

    	# Accumulate data from batches into a list
    	data = []
    	for batch in ds.to_batches(columns=["image"], batch_size=10):
        	tbl = batch.to_pandas()
        	data.append(tbl)

    	# Concatenate all DataFrames into a single DataFrame
    	df = pd.concat(data, ignore_index=True)
    	print("Pandas DataFrame is ready")
    	print("Total Rows: ", df.shape[0])

    if __name__ == "__main__":
    	start = time.time()
    	write_to_lance()
    	loading_into_pandas()
    	end = time.time()
    	print(f"Time(sec): {end - start}")
```

Imagine using Lance-formatted image data to accelerate machine learning and deep learning projects. Something big is coming up. Stay tuned.
```

---

## ðŸ“„ æ–‡ä»¶: enhance-rag-integrate-contextual-compression-and-filtering-for-precision-a29d4a810301.md

---

```md
---
title: "Efficient RAG with Compression and Filtering"
date: 2024-01-09
author: Kaushal Choudhary
author_avatar: /assets/authors/kaushal-choudhary.jpg
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/enhance-rag-integrate-contextual-compression-and-filtering-for-precision-a29d4a810301/preview-image.png
meta_image: /assets/blog/enhance-rag-integrate-contextual-compression-and-filtering-for-precision-a29d4a810301/preview-image.png
description: "Discover about efficient rag with compression and filtering. Get practical steps, examples, and best practices you can use now."
---

## Why Contextual Compressors and Filters?

RAG (Retrieval Augmented Generation) is a technique that helps add additional data sources to our existing LLM Models; however, in most cases, it also brings unnecessary data that is not relevant to the use case.

Compression and filtering are the techniques that can help us extract only the relevant data from the corpus, increasing the efficiency and speed of our application.

Contextual compression is self-explanatory, which means that it compresses the relevant (context) data from the large, raw dataset.

Filtering is a process that further cleans the data after compression from the large dataset. Both in sync can help us drastically reduce storage, retrieval, and cost of operation.

## Working of Contextual Compression

![Contextual compression diagram](/assets/blog/enhance-rag-integrate-contextual-compression-and-filtering-for-precision-a29d4a810301/1*WZToanVYGWBLlaI00seH4A.png)

RAG (Retrieval Augmented Generation) lacks transparency in revealing what it retrieves, making it uncertain which questions the system will encounter. This results in valuable information getting lost amid a mass of irrelevant text, which is not ideal for a production-grade application.

`Contextual compression `addresses this issue with the following advantages:

1. A fundamental retriever collects various pieces of information.
2. This information is then passed through a document compressor.
3. The compressor filters and processes the data, extracting meaningful information that is pertinent to the query.

You can observe a step-by-step demonstration of this process in an interactive Colab notebook.

## Notebook Walk-through

> Follow along with this Colab notebook:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Contextual-Compression-with-RAG/main.ipynb)

For brevity, only important code blocks are presented here, please follow the above link for full python implementation.

We will be using HuggingFace models. Get your API [**token**](https://huggingface.co/settings/tokens).

## Load the Data (PDF)

```python
loader = PyPDFLoader("Science_Glossary.pdf")
documents = loader.load()
print(len(documents))
print(documents[0].page_content)
```

Using [**BLING (Best Little Instruct-following No-GPU)**](https://huggingface.co/llmware) model series on HuggingFace for Embeddings.

```python
embeddings = SentenceTransformerEmbeddings(model_name="llmware/industry-bert-insurance-v0.1")
```

## Instantiate the LLM

***`bling-sheared-llama-1.3b-0.1`*** is part of the BLING (Best Little Instruction-following No-GPU-required) model series, instruct trained on top of a Sheared-LLaMA-1.3B base model.

```python
repo_id = "llmware/bling-sheared-llama-1.3b-0.1"
llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={"temperature": 0.3, "max_length": 500})
```

## Setting up LanceDB

```python
context_data = lancedb.connect("./.lancedb")
table = context_data.create_table("context", data=[
    {"vector": embeddings.embed_query("Hello World"), "text": "Hello World", "id": "1"}
], mode="overwrite")
```

## Retriever

```python
retriever_d = database.as_retriever(search_kwargs={"k": 5, "include_metadata": True})
```

Now that we have set everything, letâ€™s retrieve the relevant contexts that match the query.

```python
docs = retriever.get_relevant_documents(query="What is Wetlands?")
pretty_print_docs(docs)
```

![Initial retrieval results](/assets/blog/enhance-rag-integrate-contextual-compression-and-filtering-for-precision-a29d4a810301/1*mw-3Vvk7wplgsFVp7b68rQ.png)

We can see that the first two results are the most relevant to the query.

Letâ€™s add Contextual Compression with **LLMChainExtractor**

- `LLMChainExtractor` will iterate over the initially returned documents.
- Extract only the relevant context to query from the documents.

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# creating the compressor
compressor = LLMChainExtractor.from_llm(llm=llm)

# compressor retriever = base retriever + compressor
compression_retriever = ContextualCompressionRetriever(
    base_retriever=retriever_d,
    base_compressor=compressor,
)
```

Now, will add ***Filters ***to Contextual Compressions

```python
from getpass import getpass
import os
from langchain.embeddings import OpenAIEmbeddings
from langchain.retrievers.document_compressors import EmbeddingsFilter

os.environ["OPENAI_API_KEY"] = getpass()
embeddings_filter = EmbeddingsFilter(embeddings=embeddings)
compression_retriever_filter = ContextualCompressionRetriever(
    base_retriever=retriever_d, base_compressor=embeddings_filter
)

compressed_docs = compression_retriever_filter.get_relevant_documents(query="What is the Environment?")
pretty_print_docs(compressed_docs)
```

To get the result, we will utilize `RetrievalQA Chain` from Langchain.

```python
from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=compression_retriever_filter,
    verbose=True,
)

# Ask Question
qa("What is Environment?")
```

![RetrievalQA output](/assets/blog/enhance-rag-integrate-contextual-compression-and-filtering-for-precision-a29d4a810301/1*D7rXwGlKsdelh5NlYrlt6w.png)

## Pipelines

We can also join compressors and document transformers for better output and efficiency.

Here, we will create a pipeline comprising of redundant filter + relevant filter.

- `Redundant Filter `:- Identify similar documents and filter out redundancies.
- `Relevant Filter` :- Cleverly chooses the documents which are sufficiently relevant to a query.

```python
from langchain.document_transformers import EmbeddingsRedundantFilter
from langchain.retrievers.document_compressors import DocumentCompressorPipeline

redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)
relevant_filter = EmbeddingsFilter(embeddings=embeddings, k=5)

# creating the pipeline
compressed_pipeline = DocumentCompressorPipeline(transformers=[redundant_filter, relevant_filter])

# compressor retriever
comp_pipe_retrieve = ContextualCompressionRetriever(
    base_retriever=retriever_d, base_compressor=compressed_pipeline
)

# print the prompt
print(comp_pipe_retrieve)

# Get relevant documents
compressed_docs = comp_pipe_retrieve.get_relevant_documents(query="What is Environment?")
pretty_print_docs(compressed_docs)
```

![Compression pipeline results](/assets/blog/enhance-rag-integrate-contextual-compression-and-filtering-for-precision-a29d4a810301/1*ilN4dlfQPy-RJ-Oj2cS3Gw.png)

We can see that it tries to gather the most relevant documents to query.

Visit LanceDB GitHub if you wish to learn more about LanceDB Python and Typescript library.
For more such applied GenAI and VectorDB applications, examples, and tutorials, visit VectorDB-Recipes.
```

---

## ðŸ“„ æ–‡ä»¶: feature-full-text-search.md

---

```md
---
title: "LanceDB WikiSearch: Native Full-Text Search on 41M Wikipedia Docs"
date: 2025-08-11
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/feature-full-text-search/preview-image.png
meta_image: /assets/blog/feature-full-text-search/preview-image.png
description: "No more Tantivy! We stress-tested native full-text search in our latest massive-scale search demo. Let's break down how it works and what we did to scale it."
author: David Myriel
author_avatar: "/assets/authors/david-myriel.jpg"
author_bio: "Writer, Software Engineer @ LanceDB"
author_twitter: "davidmyriel"
author_github: "davidmyriel"
author_linkedin: "davidmyriel"
---

> It's not about the vectors. It's about getting the right result.

Many of our users are building RAG and search apps, and they want three things above all: precision, scale, and simplicity. In this article, we introduce [WikiSearch](https://saas-examples-large-scale.vercel.app), our flagship demo that delivers all [with minimal code](https://github.com/lancedb/saas-examples-large-scale/tree/main/wikipedia-ingest). 

[WikiSearch](https://saas-examples-large-scale.vercel.app) is a very simple [search engine](/docs/overview/) that stores and searches through real Wikipedia entries.
You don't see it, but there is a lot of content sitting in [LanceDB Cloud](https://accounts.lancedb.com/sign-up) - and we use Full Text Search to go through it. Vector search is still there for semantic relevance, and we merge both into a [powerful Hybrid Search solution](/docs/search/hybrid-search/).

Scaling to 41 million documents in production presented significant engineering challenges. Here are the key performance breakthroughs we achieved:

|Metric|Performance|
|-|-|
|**Ingestion**|We processed 60,000+ documents per second with distributed GPU processing|
|**Indexing**|We built vector indexes on 41M documents in just 30 minutes|
|**Write Bandwidth**|We sustained 4 GB/s peak write rates for real-time applications| 

{{< admonition >}}
We previously used [Tantivy](https://github.com/quickwit-oss/tantivy) for our search implementation. Now, we have a native FTS solution built directly into LanceDB that provides better integration and offers superior performance for our specific use cases. This native approach eliminates the need for external dependencies and ensures optimal performance.
{{< /admonition >}}

## Why Full-Text Search Helped

[Full-Text Search (FTS)](/docs/search/full-text-search/) lets you find the exact words, phrases, and spellings people care about. It complements [vector search](/docs/search/vector-search/) by catching precise constraints, rare terms, and operators (phrases, boolean logic, field boosts) that embeddings alone often miss.

It works by tokenization: splitting text into small, searchable pieces called tokens. It lowercases words, removes punctuation, and can reduce words to a base form (e.g., â€œrunningâ€ â†’ â€œrunâ€). 

Here is how basic stemming is enabled for an English-language text:
```python
table.create_fts_index("text", language="English", replace=True)
```

This request creates and stores tokens into an inverted (FTS) index. The tokenizer you choose can be [standard, language-aware, or n-gram & more](/docs/search/full-text-search/). Configuration directly shapes recall and precision, so you have a lot of freedom to play around with the parameters and match them to your use case.

FTS handles multilingual text, too. For French, enable `ascii_folding` during index creation to strip accents (e.g., â€œÃ©â€ â†’ â€œeâ€), so queries match words regardless of diacritics. 

```python
table.create_fts_index(
        "text",
        language="French",
        stem=True,
        ascii_folding=True,
        replace=True,
    )
```
{{< admonition >}}
Now, when you search for the name RenÃ©, you can afford to make a mistake!
{{< /admonition >}}

[FTS is especially important for an encyclopedia or a Wiki](https://saas-examples-large-scale.vercel.app), where articles are long and packed with names and multi-word terms. Tokenization makes variants like â€œNew York City,â€ â€œNew-York,â€ and â€œNYCâ€ findable, and enables phrase/prefix matches. The result is fast, precise lookup across millions of entries.

### FTS and Hybrid Search

FTS is a great way to control search outcomes and [makes vector search better and faster](/docs/search/optimize-queries/). Here's how: 

1. During hybrid search, FTS and vector search run in parallel, each finding their own candidate pools. 
2. FTS finds documents with exact term matches, while vector search finds semantically similar content. 
3. These results are then combined and reranked using techniques like Reciprocal Rank Fusion (RRF) or weighted scoring, giving you the best of both approaches - precise keyword matching and semantic understanding. 

You can often find what embeddings miss, such as rare terms, names, numbers, and words with â€œmust include/excludeâ€ rules. Most of all, you can [combine keyword scores with vector scores to rank by both meaning and exact wording](/docs/reranking/), and show highlights to explain why a result matched. 

In [LanceDB's Hybrid Search](/docs/overview/hybrid-search), native FTS blends text and vector signals with weights or via Reciprocalâ€‘Rank Fusion (RRF) for a [completely reranked search solution](/docs/reranking/).

{{< admonition >}}
To learn more about Hybrid Search, [give this example a try](/docs/search/hybrid-search/).
{{< /admonition >}}

## The 41M WikiSearch Demo

[![Wikipedia Search Demo](/assets/docs/demos/wiki-search.png)](https://saas-examples-large-scale.vercel.app)

{{< admonition "Try it out!" >}}
Want to know who wrote Romeo and Juliet? [Give it a spin!](https://saas-examples-large-scale.vercel.app)
{{< /admonition >}}

The demo lets you switch between semantic (vector), full-text (keyword), and hybrid search modes. [Semantic or Vector Search](/docs/search/vector-search/) finds conceptually related content, even when the exact words differ. [Full-text Search](/docs/search/full-text-search/) excels at finding precise terms and phrases. [Hybrid Search](/docs/search/hybrid-search/) combines both approaches - getting the best of semantic understanding while still catching exact matches. Try comparing the different modes to see how they handle various queries.


## Behind the Scenes

### Step 1: Ingestion

We start with raw articles from Wikipedia and normalize content into pages and sections. Long articles are chunked on headings so each result points to a focused span of text rather than an entire page. 

During ingestion we create a schema and columns, such as `content`, `url`, and `title`. Writes are batched (â‰ˆ200k rows per commit) to maximize throughput.

**Figure 1:** Data is ingested, embedded, and stored in LanceDB. The user runs queries and retrieves WikiSearch results via our Python SDK.
![Wikipedia Search Demo](/assets/blog/feature-full-text-search/process.png)

### Step 2: Embedding 

A parallel embedding pipeline (configurable model) writes vectors into the `vector` column. The demo scripts let you swap the embedding models easily. Here, we are using a basic `sentence-transformers` model.

To learn more about vectorization, [read our Embedding API docs](/docs/embedding/).

### Step 3: Indexing 

We build two indexes per table: a vector index (`IVF_HNSW_PQ` or `IVF_PQ`, depending on your latency/recall/memory goals) over the embedded content, and a native `FTS` index over title and body. 

[This is where you define tokenization and matching options.](/docs/search/full-text-search/) As you configure the [FTS index](/docs/indexing/fts-index/), you can instruct the Wiki to be broken down in different ways.

**Figure 2:** Sample LanceDB Cloud table with schema and defined indexes for each column.
![Indexed Table](/assets/docs/demos/indexed-table.png)

### Step 4: Service 

A thin API fronts LanceDB Cloud. The web UI issues text, vector, or hybrid queries, shows results, and exposes `explain_plan` for each request. Deploying the app is a connection string plus credentials.....that's it! [Check out the entire implementation in GitHub.](https://github.com/lancedb/saas-examples-large-scale/tree/main/wikipedia-ingest)

## How the Search Works

1. [A text query](/docs/search/full-text-search/) first hits the FTS index and returns a pool of candidate document IDs with scores derived from term statistics. 

2. [A semantic query](/docs/search/vector-search/) embeds the input and asks the vector index for nearest neighbors, producing a separate candidate pool with distances. 

3. [In hybrid mode](/docs/search/hybrid-search/) we normalize these signals and combine them into a reranked search result.

Trying out the search function will reveal a lot about the nature of each search. Semantic Search will count in meaning and context, with less direct precision, while Full-Text Search will look for the precise keyword you're looking for. 

**Figure 3:** Semantic search is able to detect that a cosmonaut is also an astronaut.
![Wikipedia Search Demo](/assets/blog/feature-full-text-search/semantic.png)

### Search Parameters

The search interface gives you full visibility into how your queries are processed. You can see the exact search terms being used, which fields are being searched (title, content, or both), and the scoring weights applied to different components. 

This transparency helps you understand why certain results ranked higher and allows you to fine-tune your search strategy.

**Figure 3:** Behind the scenes, you can see all the Search Parameters for your query.
![Wikipedia Search Demo](/assets/blog/feature-full-text-search/parameters.png)

### The Query Plan

Now we're getting serious. `explain_plan` is a very valuable feature that we created to help debug search issues and [optimize performance](/docs/search/optimize-queries/). Toggle it to get a structured trace of how LanceDB executed your query. 

**Figure 5:** The Query Plan can be shown for Semantic & Full Text Search. Hybrid Search will be added soon, with detailed outline of the reranker and its effect.
![Wikipedia Search Demo](/assets/blog/feature-full-text-search/query-plan-1.png)

[The Query Plan](/docs/search/optimize-queries/) shows:

- Which indexes were used (FTS and/or vector) and with what parameters
- Candidate counts from each stage (text and vector), plus the final returned set
- Filters that applied early vs. at reâ€‘rank
- Timings per stage so you know where to optimize

## Performance and Scaling

At ~41 million documents, we needed to add data in batches. We ingested data efficiently using `table.add()`, with batches of 200K rows at once:

```python
BATCH_SIZE_LANCEDB = 200_000 

for i in range(0, len(all_processed_chunks), BATCH_SIZE):
    batch_to_add = all_processed_chunks[i : i + BATCH_SIZE]
    try:
        table.add(batch_to_add)
    except Exception as e:
        print(f"Error adding batch to LanceDB: {e}")
```
{{< admonition >}}
Batching `table.add(list_of_dicts)` is much faster than adding records individually. Adjust `BATCH_SIZE_LANCEDB` based on memory and performance.
{{< /admonition >}}

### Performance at Scale

The core pattern is: parallelize data loading, chunking, and embedding generation, then use `table.add(batch)` within each parallel worker to write to LanceDB. LanceDBâ€™s design efficiently handles these concurrent additions. This example uses modal for performing distributed embedding generation and ingestion. 

Here are some performance metrics from our side. These numbers represent enterprise-grade performance at massive scale:

|Process|Performance| 
|-|-| 
| Ingestion:|Using a distributed setup with 50 GPUs (via Modal), we ingested ~41M rows in roughly 11 minutes endâ€‘toâ€‘end. This translates to processing over 60,000 documents per second.| 
| Indexing:|Vector index build completed in about 30 minutes for the same dataset. Building vector indexes on 41M documents typically takes hours with other solutions.| 
| Write bandwidth:|LanceDBâ€™s ingestion layer can sustain multiâ€‘GB/s write rates (4 GB/s peak observed in our tests) when batching and parallelism are configured properly. This enables real-time data ingestion for live applications.| 

### Which Index to Use?

Use `IVF_HNSW_PQ` for high recall and predictable latency; use `IVFâ€‘PQ` when memory footprint is the constraint and you want excellent throughput at scale. Native `FTS` indexes (title, body) handle tokenization and matching; choose options per your corpus.

{{< admonition >}}
[Read more about Indexing.](/docs/indexing/) 
{{< /admonition >}}

### Trying Things Out

Your numbers will vary based on encoder speed, instance types, and network, but the pattern holds: parallelize embedding, batch writes (e.g., ~200k rows per batch), and build indexes once per checkpointed snapshot. Cloud keeps the table readable while background jobs run, so you can stage changes and cut over via alias swap without downtime.

{{< admonition "Note" "Full Code and Tutorial">}}
Check out the [GitHub Repository.](https://github.com/lancedb/saas-examples-large-scale/tree/main/wikipedia-ingest) 
{{< /admonition >}}

## The Search is Never Complete

Beyond the endless exploration of a large dataset, this demo showcased what's possible when you combine [LanceDB's native Full-Text Search](/docs/search/full-text-search/) with vector embeddings. 

You get the precision of keyword matching, the semantic understanding of embeddings, and the scalability to handle massive datasets - all in one unified platform.

We built this entire app on [LanceDB Cloud](https://accounts.lancedb.com/sign-up), which is free to try and comes with comprehensive tutorials, sample code, and documentation to help you build RAG applications, AI agents, and semantic search engines.
```

---

## ðŸ“„ æ–‡ä»¶: feature-rabitq-quantization.md

---

```md
---
title: "LanceDB's RaBitQ Quantization for Blazing Fast Vector Search"
date: 2025-09-17
draft: false
featured: true
categories: ["Engineering"]
image: "/assets/blog/feature-rabitq-quantization/preview-image.png"
meta_image: "/assets/blog/feature-rabitq-quantization.md/preview-image.png"
description: "Introducing RaBitQ quantization in LanceDB for higher compression, faster indexing, and better recall on highâ€‘dimensional embeddings."
author: David Myriel, Yang Cen
author_avatar: "/assets/authors/david-myriel.jpg"
author_bio: "Writer, Software Engineer @ LanceDB"
author_twitter: "davidmyriel"
author_github: "davidmyriel"
author_linkedin: "davidmyriel"
---

Most [embedding models](/docs/embedding/) give you float32 vectors. That is precise, but wasteful. You pay in storage and query time. Scalar quantization can shrink the size, but not enough. Binary quantization can shrink it further, but at the cost of quality.

Until now LanceDB has relied on **IVF_PQ** (Inverted File with Product Quantization) as its default compression and search strategy. IVF_PQ works well in many cases, but comes with trade-offs: building PQ codebooks is expensive, re-training is required when data distribution shifts, and distance estimation can be slow. Learn more about index types in [Indexing](/docs/indexing/).

Today we are adding **RaBitQ quantization** to LanceDB. You now have a choice. `IVF_PQ` remains the default, but RaBitQ is available when you need higher compression, faster indexing, and better recall in high dimensions. See how this fits into [vector search](/docs/search/vector-search/) and [hybrid search](/docs/overview/hybrid-search/).

> A 1024-dimensional float32 vector needs 4 KB. With RaBitQ and corrective factors the same vector takes about 136 bytes. `IVF_PQ` typically reduces this by 8â€“16x.

## Why this matters for you

High-dimensional vectors behave in ways that seem strange if you are used to 2D or 3D. In high dimensions almost every coordinate of a unit vector is close to zero. This is the **concentration of measure** phenomenon often discussed in [retrieval systems](/docs/search/optimize-queries/).

RaBitQ takes advantage of this. It stores only the binary "sign pattern" of each normalized vector and relies on the fact that the error introduced is bounded. The RaBitQ paper proves the estimation error is **O(1/âˆšD)** (decreases proportionally to the inverse square root of dimensions), which is asymptotically optimal. In practice, the more dimensions your embeddings have, the more RaBitQ gives you for free.

{{< admonition >}}
RaBitQ is most effective with embeddings of 512, 768, or 1024 dimensions. IVF_PQ often struggles more as dimensionality grows, making RaBitQ a strong complement.
{{< /admonition >}}

## What happens under the hood

When you index embeddings in LanceDB with RaBitQ your vectors are shifted around a centroid and normalized. They are snapped to the nearest vertex of a randomly rotated hypercube on the unit sphere. Each quantized vector is stored as **1 bit per dimension**. Index creation is managed the same way as other [vector indexes](/docs/indexing/vector-index/).

Two small corrective factors are also stored:

* the distance from the vector to the centroid  
* the dot product between the vector and its quantized form

At query time your inputâ€”whether text, image, audio, or videoâ€”is transformed in the same way. Comparisons become fast binary dot products, and the corrective factors restore fidelity.

Formally, RaBitQ maps a vector \(v\) to a binary vector \(v_qb\) and a hypercube vertex \(v_h\) on the unit sphere:

\[
v_h[i] = +1/\sqrt{D} \hspace{2em} \text{if} \hspace{0.5em} v_qb[i] = 1 \\  
v_h[i] = -1/\sqrt{D} \hspace{2em}  \text{if} \hspace{0.5em} v_qb[i] = 0
\]  

where \(D\) is the number of dimensions. Each component is either a positive or negative value scaled by the square root of dimensions.

To remove bias, RaBitQ samples a random orthogonal matrix \(P\) and sets the final quantized vector to:

\[
v_q = P Ã— v_h
\]

During indexing, for each data vector \(v\), we compute the residual around a centroid **c**, normalize it, map it to bits via the inverse rotation \(P^{-1}\), and store two corrective scalars alongside the bits:
- **Centroid distance**: \(d_c = ||v - c||\), which is the Euclidean distance from vector to centroid
- **Inner product**: \(d_s = âŸ¨v_q, v_nâŸ©\), which is the dot product between quantized and normalized vectors

At search time, the query is processed with the same rotation to enable extremely fast binary dot products plus lightweight corrections for accurate distance estimates.

**Figure 1a:** 2D illustration of mapping a vector to the nearest unitâ€‘sphere hypercube vertex (RabitQ overview)
![RabitQ 2D mapping](/assets/blog/feature-rabitq-quantization/plot_rq.svg)

**Figure 1b:** Geometry of query and data vectors in RaBitQ
![figure1b](/assets/blog/feature-rabitq-quantization/figure1.png)

This figure shows the geometric relationship between a data vector \(o\), its quantized form \(\bar{o}\), and a query vector \(q\). The auxiliary vector \(e_1\) lies in the same plane. RaBitQ exploits the fact that in high-dimensional spaces, the projection of \(\bar{o}\) onto \(e_1\) is highly concentrated around zero (red region on the right). This allows us to treat it as negligible, enabling accurate distance estimation with minimal computation.

**Difference with IVF_PQ**: PQ requires training a codebook and splitting vectors into subvectors. RaBitQ avoids that entirely. Indexing is faster, maintenance is simpler, and results are more stable when you update your dataset.

## Search in practice

With IVF_PQ you first partition vectors into inverted lists, then run PQ-based comparisons inside each list. With RaBitQ the comparison step is even cheaper: a binary dot product plus small corrections. Both approaches benefit from [reranking](/docs/reranking/) with full-precision vectors.

Both approaches use a candidate set and re-ranking to ensure quality. But because RaBitQâ€™s approximations are tight, you often need fewer candidates to hit the same recall.

For you this means recall above 95 percent with lower latency, even on large [multimodal datasets](/blog/multimodal-lakehouse/) on our test hardware.

{{< admonition >}}
Always re-rank candidates with full precision vectors. IVF_PQ and RaBitQ both rely on approximation for candidate generation.
{{< /admonition >}}

## Research context: extended-RaBitQ

The [RaBitQ paper](https://arxiv.org/abs/2409.09913) also introduces **extended-RaBitQ**, a generalization to multi-bit quantization. Instead of limiting each coordinate to 1 bit, extended-RaBitQ allows for 2â€“4 bits per dimension. This reduces error further and can outperform product quantization under the same compression budget. For background on token-based search that pairs well with these methods, see [Fullâ€‘Text Search](/docs/search/full-text-search/).

âš ï¸ **extended-RaBitQ is not yet available in LanceDB.** We mention it here only as research context. It shows the trajectory of this work, and the same principles that make RaBitQ efficient at 1 bit can be extended to higher precision when needed.

## Benchmarks

![benchmarks](/assets/blog/feature-rabitq-quantization/benchmarks.png)

We tested IVF_PQ and RaBitQ side by side on two public datasets:

* [**DBpedia 1M**](https://huggingface.co/datasets/KShivendu/dbpedia-entities-openai-1M) (OpenAI embeddings)  
* [**GIST1M**](http://corpus-texmex.irisa.fr/) (dense 960-dimensional vectors)

The tests were run on a consumer-grade PC:

* CPU: Intel 12400F (6 cores, 12 threads)  
* Disk: 1TB SSD  
* No GPU

### DBpedia (text embeddings, 768d)

| Metric           | IVF_PQ   | RaBitQ      |
| ---------------- | -------- | ----------- |
| recall@10        | ~92%     | **96%+**    |
| throughput       | ~350 QPS | **495 QPS** |
| index build time | ~85s     | **~75s**    |

**Figure 2:** DBpedia benchmark results (recall@10, throughput, build time)
![DBpedia Benchmark](/assets/blog/feature-rabitq-quantization/dbpedia-bench.png)

### GIST1M (image embeddings, 960d)

| Metric           | IVF_PQ   | RaBitQ          |
| ---------------- | -------- | --------------- |
| recall@10        | ~90%     | **94%**         |
| throughput       | ~420 QPS | **540â€“765 QPS** |
| index build time | ~130s    | **~21s**        |

**Figure 3:** GIST1M benchmark results (recall@10, throughput, build time)
![GIST1M Benchmark](/assets/blog/feature-rabitq-quantization/gist-bench.png)

{{< admonition >}}
On CPUs, RaBitQ already outperforms IVF_PQ on our test hardware. On GPUs the gap should widen further because RaBitQ operations are easier to parallelize.
{{< /admonition >}}

## IVF_PQ vs RaBitQ

When you use LanceDB today, the default quantization method is IVF_PQ. It works well for many workloads and gives you solid compression and recall. With RaBitQ now available, you have another option. The two approaches make very different trade-offs, especially when your vectors are high-dimensional or multimodal.

| Feature                       | IVF_PQ (default)                            | RaBitQ (new)                                   |
| ----------------------------- | ------------------------------------------- | ---------------------------------------------- |
| **Compression ratio**         | 8â€“16x typical                               | Up to 32x                                      |
| **Index build time**          | Slower, requires codebook training          | Faster, no training needed                     |
| **Recall in high dimensions** | Good but drops with >512d                   | Stays high, error shrinks with more dimensions |
| **Query speed**               | Good, but PQ distance estimation is slower  | Faster, binary dot products dominate           |
| **Update handling**           | Requires re-training if distribution shifts | No retraining needed, robust to updates        |
| **Best for**                  | General workloads, balanced compression     | Large multimodal datasets, high-dim embeddings |

What this means in practice is that you donâ€™t have to pick one forever. IVF_PQ remains a great default for balanced workloads. But if you are working with very large datasets, embeddings above 512 dimensions, or multimodal collections where precision matters, RaBitQ can give you higher recall, faster queries, and far smaller storage footprints. Because RaBitQ does not depend on heavy codebook training, you can also update your data more easily without re-training the index.

## What you can do today in LanceDB

With IVF_PQ you already had scalable compression and fast ANN search. Now with RaBitQ you get an additional tool:

* Use **IVF_PQ** for general-purpose workloads where codebooks make sense  
* Use **RaBitQ** when you want maximum compression, faster indexing, or higher recall in high dimensions  
* Mix RaBitQ-S with other modalities for adaptive precision control  
* Keep more embeddings online for [RAG](/docs/overview/), multimodal search, and analytics

## Proven and ready

RaBitQ is not theoretical. Elastic integrated it into their BBQ quantization feature. Tensorchord built a cost-efficient vector search service with it. Both show strong improvements in recall and speed compared to classical product quantization.

Now RaBitQ is available in LanceDB alongside IVF_PQ. You can choose the method that best fits your workload, and even switch as your dataset grows. If youâ€™re exploring LanceDB for the first time, check out our [quickstart](/docs/), the [WikiSearch demo](/blog/feature-full-text-search/), and how this ties into the [Multimodal Lakehouse](/blog/multimodal-lakehouse/).

![outro](/assets/blog/feature-rabitq-quantization/outro.jpeg)
```

---

## ðŸ“„ æ–‡ä»¶: file-readers-in-depth-parallelism-without-row-groups.md

---

```md
---
title: "Columnar File Readers in Depth: Parallelism without Row Groups"
date: 2024-05-14
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/file-readers-in-depth-parallelism-without-row-groups/file-readers-in-depth-parallelism-without-row-groups.png
description: "Explore columnar file readers in depth: column shredding with practical insights and expert guidance from the LanceDB team."
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "westonpace"
---

Recently, I shared our plans for a new file format, Lance v2. As I'm creating a file reader for this new format I plan to create a series of posts talking about the design, challenges and limitations in many existing file readers, and how we plan to overcome these. Much of this discussion is not specific to Lance and I hope this series will be useful to anyone that wants to know more about storage and I/O.

{{< admonition info "ðŸ“š Series Navigation" >}}
This is part of a series of posts on the details we've encountered building a columnar file reader:
1. **Parallelism without Row Groups** (this article)
2. [APIs and Fusion](/blog/columnar-file-readers-in-depth-apis-and-fusion/)
3. [Backpressure](/blog/columnar-file-readers-in-depth-backpressure/)
4. [Compression Transparency](/blog/columnar-file-readers-in-depth-compression-transparency/)
5. [Column Shredding](/blog/columnar-file-readers-in-depth-column-shredding/)
6. [Repetition & Definition Levels](/blog/columnar-file-readers-in-depth-repetition-definition-levels/)
{{< /admonition >}}

In the first post of this series I want to talk about reading files in parallel. Traditionally, this has been done with row groups or stripes. In Lance v2 we got rid of row groups so we'll need to solve this in another way. Fortunately, it turns out that another way is not only possible, but actually delivers better performance, and is probably even a better way to read files in existing formats (e.g. parquet / orc).

## Our First Parallel File Reader

```
parallel for row_group in file:
  arrays = []
  
  parallel for column in row_group:
    array = ArrayBuilder()
    
    for page in column:
      page = read_page()
      array.extend(page.decode())
      
    arrays.append(array)
    
  batch = RecordBatch(arrays)
  yield batch
```

This is just pseudo code of course. The real algorithm is often quite different. The decode might not happen in parallel, the page readers might have read ahead, batches might be yielded as as soon as one page is loaded for each column (instead of buffering an entire row group). These are all important details (and we may talk about them later) but not relevant to the discussion at hand. The main point is that parallelism primarily comes from accessing the row groups in parallel. This is a very intuitive way to handle parallel reading of a file. However, we got rid of row groups in Lance, and so this approach won't work for us.

## Decode Based Parallelism

The solution is quite simple, I decouple my CPU batch size from my I/O read size. A single page of data actually contains quite a few values. For example, a single 1MiB page of integer data will contain at least 250,000 values. We typically want our CPU batch size to be tied to our CPU cache size. This is often 1MiB or less. If I am reading in 10 columns of data, and I have 1MiB pages then, even without any compression, I have 10 batches of data in every page.

{{< admonition info >}}
I've seen these approaches often described as "pipeline parallelism" and "data parallelism" but those terms don't seem to be used with a consistent definition. I'd say this approach is a mixture of both. We are splitting work into two pipeline stages [load] + [decode -> compute] but not for the purposes of parallelism. Our primary parallelism is still "data parallelism" because we are chopping pages into multiple mini-batches (I'll cover the CPU benefits of mini-batches in a future post).
{{< /admonition >}}

If I'm reading a single column of data then I actually end up with the same thing I get with row groups which is one record batch / CPU task for every single data page. This ability to generate batches on a per-page basis instead of a per-group basis is useful (keeps RAM usage down) and is something that sophisticated parquet readers tend to do as well (though I'm not aware of any that apply parallelism here).

![Decode Based Parallelism](/assets/blog/file-readers-in-depth-parallelism-without-row-groups/Decode-Based-Parallelism-1--1.png)

*Our I/O parallelism and CPU parallelism are now fully decoupled*

{{< admonition info >}}
All of the pages in the above diagram have the same size on disk. However, since different columns have different widths (e.g. u16 vs. u64 vs. FSL<FP32, 4096>) they have different numbers of rows. Our ideal read order is maintained (lowest row number comes first).
{{< /admonition >}}

## Which Approach is Better?

I believe the approach we are taking in Lance is not only compatible, but can actually result in better performance. The main difference is that the classic algorithm leads to the following I/O order:

![Classic I/O Order](/assets/blog/file-readers-in-depth-parallelism-without-row-groups/Classic-I-O-Order-1-.png)
*Reading 3 columns with 3 row groups at a time leads to I/O parallelism of 9*

The problem with the classic reading order described above is that if you want parallelism X and you are reading Y columns then you need to perform X * Y reads in parallel. Does this matter? I/O is slow and asynchronous, we can launch as many I/O tasks in parallel as we want right?


{{< admonition info >}}
Note that the X in our equation above is typically based on the # of CPUs (we want enough parallelism to utilize all cores). The above strategy ties together CPU parallelism and I/O parallelism ðŸ˜¦
{{< /admonition >}}

## The Myth of Infinite Parallelism

Modern disks have a lot of parallelism but they do not have infinite parallelism. The OS does an amazing job of load balancing many concurrent requests and so we typically don't care or notice this fact. In fact, over scheduling the disk slightly can be beneficial since you avoid stuttering. However, over scheduling the disk too much turns out to be a problem. To see why, let's look at some actual trace data.

![Thread Performance Comparison](/assets/blog/file-readers-in-depth-parallelism-without-row-groups/image-1.png)

*Reading the same file, in 8MiB chunks, with different # of threads*

Here we read the same file, with the same chunk size, but using different amounts of parallelism. I tested with 128 8-MiB chunks, across both 64 and 16 threads with a Samsung SSD 970 EVO Plus 2TB. Benchmarks report that this disk caps out at around a queue depth of 16 and so it should come as no surprise that both operations took the same amount of time since both allow for at least 16 parallel reads.

## Why is this a problem?

In both cases we got the exact same bandwidth, right about 3GiBps, so this shouldn't be a problem right? However, the average request latency in the first example (64 threads) is much higher than the average latency in the second example (16 threads). Look at our parquet example again and let's consider when we can start outputting record batches. In order to return a record batch we need at least one page from each column. This means, on average, we won't be able to start emitting data until about 9 IOPS have completed.

![Classic I/O Order with Focus](/assets/blog/file-readers-in-depth-parallelism-without-row-groups/Classic-I-O-Order-with-focus-1-.png)

*Since all I/O are in parallel we expect them all to finish at roughly the same time*

What does that mean for our file reader? It means there is a longer delay between reading and actually decoding data. This kind of delay is always necessary to some degree, but we should try to minimize it as much as possible.

![I/O and Decode Parallelism](/assets/blog/file-readers-in-depth-parallelism-without-row-groups/Staggered-Decode.png)

*We want decode to run in parallel with I/O as much as possible*

## What about Cloud Storage?

Cloud storage often likes to boast about infinite I/O that can scale up to whatever bandwidth you need. However, the scaling your cloud provider is talking about it typically the number of servers and not the number of threads. This is especially true when we're talking about concurrent accesses to the same file. For example, most EC2 instances come with 1.25GiBps of network bandwidth. Here is a quote from S3's performance guide (emphasis mine):

> Make one concurrent request for each 85â€“90 MB/s of desired network throughput. To saturate a 10 Gb/s network interface card (NIC), you might use about 15 concurrent requests over separate connections.

Even if your reads are small (this is often the case with point lookups) we have found, through practice, that the same patterns tend to hold. Too many concurrent requests and the average latency grows. We can try buying our way out of this problem by using a network optimized instance. For example, a c7gn.8xlarge will give us 10x more network bandwidth. However, we now have 32 cores, so we probably need to read even more row groups in parallel, which means that we need even more concurrent I/O. It turns out that coupling between I/O and compute parallelism is coming back to bite us.

## Ideal Read Order

Now, let's begin talking about our solution. First, let's look at the ideal order to read our file:

![Better I/O Order](/assets/blog/file-readers-in-depth-parallelism-without-row-groups/Better-I-O-Order.png)

*Pages with lower row numbers are always read first*

It turns out to be very simple. We read pages starting with the first row and moving up to the last row. If our ideal I/O parallelism was 6 in the above example we would read one row group at a time. If it was 12 we would read 2 row groups at a time. If it was 3 we can even read 3 pages at a time and everything will work.

{{< admonition info >}}
I've de-emphasized the row groups in the above picture because we are no longer using them.
{{< /admonition >}}

## Observations in Practice

So far, this discussion has been very theoretical (though it is based on issues we saw with Lance v1). Does it actually work? I think we've done enough internal benchmarking that I am convinced it does. When doing full scans of the NYC taxi dataset or the common crawl dataset the new Lance v2 format is considerably faster than Lance v1. When running locally against my NVME at 3GBps we are still I/O bound and the full read + decode bandwidth is usually within 95% of my disk's bandwidth.

![Lance v2 Performance](/assets/blog/file-readers-in-depth-parallelism-without-row-groups/image-2.png)

*Parallel compute tasks (in brown) run concurrently with the I/O tasks (in pink) while reading a Lance v2 file.*

This is exciting for me as saturating my local NVME disk's bandwidth has been one of my goals with the reader.

{{< admonition info >}}
Final note: there is nothing in this post that is unique to Lance v2. A parquet reader could be built that operates in the same way. I'd be curious to know if anyone has ever tried that, or if anyone knows any existing parquet readers that work in this way, and what kinds of experiences they saw.
{{< /admonition >}}

## Keep Reading

The next post in our series on file readers is now available. We talk about scheduling vs. decoding and why we separate the two processes in Lance.
```

---

## ðŸ“„ æ–‡ä»¶: fluss-integration.md

---

```md
---
title: "Setup Real-Time Multimodal AI Analytics with Apache Fluss (incubating) and Lance"
date: 2025-09-08
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/fluss-integration/preview-image.png
meta_image: /assets/blog/fluss-integration/preview-image.png
description: "Learn how to build real-time multimodal AI analytics by integrating Apache Fluss streaming storage with Lance's AI-optimized lakehouse. This guide demonstrates streaming multimodal data processing for RAG systems and ML workflows."
author: Wayne Wang
author_avatar: "/assets/authors/avatar.jpg"
author_bio: "Senior Software Engineer @ Tencent"
---

With the advent of the generative AI era, 
nearly all digital applications are integrating multimodal AI innovations to enhance their capabilities. 
The successful deployment of multimodal AI applications relies heavily on data, 
particularly for enterprise decision-making systems where high-quality, fresh data is critical. 
For instance, in customer service and risk control systems, 
multimodal AI models that lack access to real-time customer inputs will struggle to make accurate decisions.

[Apache Fluss (incubating)](https://fluss.apache.org) addresses this need as a purpose-built streaming storage for real-time analytics. 

Serving as the real-time data layer for [lakehouse architectures](/blog/multimodal-lakehouse/), 
Fluss features an intelligent tiering service that ensures seamless integration between stream and lake storage. 
This service automatically and continuously converts Fluss data into the lakehouse format, 
enabling the "shared data" feature:

- Fluss as the lakehouse's real-time layer: Fluss provides high-throughput, low-latency ingestion and processing of streaming data, becoming the real-time front-end for the lakehouse.
- The lakehouse as Fluss's historical layer: The lakehouse provides optimized long-term storage with minute-level freshness, acting as the historical batch data foundation for the real-time layer.

Complementing the Fluss ecosystem, [Lance](https://lancedb.github.io/lance/) emerges as the next-generation AI lakehouse platform 
specialized for machine learning and multimodal AI applications. It efficiently handles multimodal data (text, images, vectors) and 
delivers high-performance queries such as lightning-fast vector search (see [LanceDB](https://lancedb.com)).

![Fluss with Lance Lakehouse](/assets/blog/fluss-integration/fluss1.png)

Combining Lance and Fluss unlocks true real-time multimodal AI analytics. 
Consider [Retrieval-Augmented Generation (RAG)](/blog/graphrag-hierarchical-approach-to-retrieval-augmented-generation/), 
a popular generative AI framework that enhances the capabilities of large language models (LLMs) 
by incorporating up-to-date information during the generation process. 
RAG uses vector search to understand a user's query context and retrieve relevant information to feed into LLMs. 
With Fluss and Lance, RAG systems can now:

1. Perform efficient vector search on Lance's historical data, leveraging its powerful indexes.
2. Compute live insights on Fluss's real-time, unindexed streaming data.
3. Combine results from both sources to enrich LLMs with the most timely and comprehensive information.

In this blog, we'll demonstrate how to stream images into Fluss, 
then load the data from the Lance dataset into [Pandas](https://pandas.pydata.org/) DataFrames 
to make the image data easily accessible for further processing and analysis in your machine learning workflows.

## MinIO Setup

### Step 1: Install MinIO Locally

Check out the [official documentation](https://min.io/docs/minio/macos/index.html) for detailed instructions.

### Step 2: Start the MinIO Server

Run this command, specifying a local path to store your MinIO data:

```bash
export MINIO_REGION_NAME=us-west-1
export MINIO_ROOT_USER=minio 
export MINIO_ROOT_PASSWORD=minioadmin 
minio server /tmp/minio-tmp
```

### Step 3: Verify MinIO Running with Web UI

When your MinIO server is up and running, you'll see endpoint information and login credentials:

```
API: http://192.168.3.40:9000  http://127.0.0.1:9000 
   RootUser: minio 
   RootPass: minioadmin 

WebUI: http://192.168.3.40:64217 http://127.0.0.1:64217            
   RootUser: minio 
   RootPass: minioadmin 
```

### Step 4: Create Bucket

Open the Web UI link and log in with these credentials.
You can create a Lance bucket through the Web UI.

## Fluss and Lance Setup

### Step 1: Install Fluss Locally

Check out the [official documentation](https://fluss.apache.org/docs/install-deploy/deploying-local-cluster/) for detailed instructions.

### Step 2: Configure Lance Data Lake

Edit your `<FLUSS_HOME>/conf/server.yaml` file and add these settings:

```yaml
datalake.format: lance

datalake.lance.warehouse: s3://lance
datalake.lance.endpoint: http://localhost:9000
datalake.lance.allow_http: true
datalake.lance.access_key_id: minio
datalake.lance.secret_access_key: minioadmin
datalake.lance.region: eu-west-1
```

This configures Lance as the data lake format, with MinIO as the warehouse.

### Step 3: Start Fluss

```bash
<FLUSS_HOME>/bin/local-cluster.sh start
```

### Step 4: Install Flink Fluss Connector

Download the [Apache Flink](https://flink.apache.org) 1.20 binary package from the [official website](https://flink.apache.org/downloads/).
Download [Fluss Connector Jar](https://fluss.apache.org/downloads) and copy it to Flink classpath `<FLINK_HOME>/lib`.

### Step 5: Configure Flink Fluss Connector

The Flink Fluss connector utilizes the [Apache Arrow](https://arrow.apache.org) Java API, 
which relies on direct memory. 
Ensure sufficient off-heap memory for the Flink Task Manager by 
configuring the `taskmanager.memory.task.off-heap.size` parameter
in `<FLINK_HOME>/conf/config.yaml`:

```yaml
# Increase available task slots.
numberOfTaskSlots: 5
taskmanager.memory.task.off-heap.size: 128m
```

### Step 6: Start Flink

Start Flink locally with:

```bash
<FLINK_HOME>/bin/start-cluster.sh
```

### Step 7: Verify Flink Running

Open your browser to `http://localhost:8081` and make sure the cluster is running.


## Launching the Fluss Tiering Service

### Step 1: Get the Tiering Job Jar

Download the [Fluss Tiering Job Jar](https://fluss.apache.org/downloads).

### Step 2: Submit the Job

```bash
./bin/flink run <path_to_jar>/fluss-flink-tiering-0.8.jar \
    --fluss.bootstrap.servers localhost:9123 \
    --datalake.format lance \
    --datalake.lance.warehouse s3://lance \
    --datalake.lance.endpoint http://localhost:9000 \
    --datalake.lance.allow_http true \
    --datalake.lance.secret_access_key minioadmin \
    --datalake.lance.access_key_id minio \
    --datalake.lance.region eu-west-1 \
    --table.datalake.freshness 30s
```

### Step 3: Confirm Deployment

Check the Flink UI for the Fluss Lake Tiering Service job. 
Once it's running, your local tiering pipeline is good to go.

![Confirm Deployment UI](/assets/blog/fluss-integration/fluss2.png)

## Multimodal Data Processing

We'll walk through a Python code example that demonstrates how to stream a dataset of images into Fluss, 
and subsequently load the tiered Lance dataset into a Pandas `DataFrame` for further processing.

### Step 1: Install Dependencies

Install the required dependencies:

```bash
pip install python-for-fluss pyarrow pylance pandas
```

### Step 2: Create Connection

Create the connection and table with the Fluss Python client:

```python
import fluss
import pyarrow as pa

async def create_connection(config_spec):
    # Create connection
    config = fluss.Config(config_spec)
    conn = await fluss.FlussConnection.connect(config)
    return conn

async def create_table(conn, table_path, pa_schema):
    # Create a Fluss Schema
    fluss_schema = fluss.Schema(pa_schema)
    # Create a Fluss TableDescriptor
    table_descriptor = fluss.TableDescriptor(
        fluss_schema,
        properties={
            "table.datalake.enabled": "true",
            "table.datalake.freshness": "30s",
        },
    )

    # Get the admin for Fluss
    admin = await conn.get_admin()

    # Create a Fluss table
    try:
        await admin.create_table(table_path, table_descriptor, True)
        print(f"Created table: {table_path}")
    except Exception as e:
        print(f"Table creation failed: {e}")
```

### Step 3: Process Images

```python
import os

def process_images(schema: pa.Schema):
	# Get the current directory path
	current_dir = os.getcwd()
	images_folder = os.path.join(current_dir, "image")

	# Get the list of image files
	image_files = [filename for filename in os.listdir(images_folder)
         if filename.endswith((".png", ".jpg", ".jpeg"))]

	# Iterate over all images in the folder with tqdm
	for filename in tqdm(image_files, desc="Processing Images"):
		# Construct the full path to the image
		image_path = os.path.join(images_folder, filename)

		# Read and convert the image to a binary format
		with open(image_path, 'rb') as f:
			binary_data = f.read()

		image_array = pa.array([binary_data], type=pa.binary())

		# Yield RecordBatch for each image
		yield pa.RecordBatch.from_arrays([image_array], schema=schema)
```

This `process_images` function is the heart of our data conversion process. 
It is responsible for iterating over the image files in the specified directory, 
reading the data of each image, and converting it into a [PyArrow](https://arrow.apache.org/docs/python/) `RecordBatch` object as binary.

### Step 4: Writing to Fluss

The `write_to_fluss` function creates a `RecordBatchReader` from the `process_images` generator 
and writes the resulting data to the Fluss `fluss.images_minio` table. 

```python
async def write_to_fluss(conn, table_path, pa_schema):
    # Get table and create writerâ€˜
    table = await conn.get_table(table_path)
    append_writer = await table.new_append_writer()
    try:
        print("\n--- Writing image data ---")
        for record_batch in process_images(pa_schema):
            append_writer.write_arrow_batch(record_batch)
    except Exception as e:
        print(f"Failed to write image data: {e}")
    finally:
        append_writer.close()
```

### Step 5: Check Job Completion

Wait a little while for the tiering job to finish. 
Then, go to the [MinIO](https://min.io/) UI, and you'll see the Lance dataset in your `lance` bucket. You can load it with [Pandas](https://pandas.pydata.org/) using Lanceâ€™s `to_pandas` API.

![Check Job MinIO UI](/assets/blog/fluss-integration/fluss3.png)

### Step 6: Loading into Pandas

While Fluss is writing data into Lance in real time, you can load from the Lance datasets in MinIO.
At this point, you can leverage anything that integrates with Lance to consume the data in any ML/AI application,
including [PyTorch](https://pytorch.org/) for training (see [Lance + PyTorch integration](https://lancedb.github.io/lance/integrations/pytorch/)), 
[LangChain for RAG integration](/docs/integrations/frameworks/langchain/), 
and [LanceDB](https://lancedb.com) for hybrid search.
You can also use Lance's [Blob API](https://lancedb.github.io/lance/guide/blob/) to have efficient, low-level access to the multimodal image data.

Here, we present a simple `loading_into_pandas` example to describe how to load the image data from the Lance dataset 
into a Pandas `DataFrame` using Lance's built-in `to_pandas` API, making the image data accessible for further downstream AI analytics with Pandas.

```python
import lance
import pandas as pd

def loading_into_pandas(table_name):
	dataset = lance.dataset(
        "s3://lance/fluss/" + table_name + ".lance", 
        storage_options={
            "access_key_id": "minio", 
            "secret_access_key": "minioadmin", 
            "endpoint": "http://localhost:9000", 
            "allow_http": "true",
        },
    )
    
    df = dataset.to_table().to_pandas()
	print("Pandas DataFrame is ready")
	print("Total Rows: ", df.shape[0])
```

### Putting It All Together

If you would like to run the whole example end-to-end, use this main function in your Python script:

```python
import asyncio

async def main():
    config_spec = {
        "bootstrap.servers": "127.0.0.1:9123",
    }
    conn = await create_connection(config_spec)
    table_path = fluss.TablePath("fluss", table_name)
    pa_schema = pa.schema([("image", pa.binary())])
    await create_table(conn, table_path, pa_schema)
    await write_to_fluss(conn, table_path, pa_schema)
    # sleep a little while to wait for Fluss tiering
    sleep(60)
    df = loading_into_pandas()
    print(df.head())
    conn.close()

if __name__ == "__main__":
    asyncio.run(main())
```

## Conclusion

The integration of [Apache Fluss (inclubating)](https://fluss.apache.org) and [Lance](https://lancedb.github.io/lance/) creates a powerful foundation for real-time multimodal AI analytics. 
By combining Fluss's streaming storage capabilities with Lance's AI-optimized [lakehouse](https://lancedb.github.io/lance/) features, 
organizations can build multimodal AI applications that leverage both real-time and historical data seamlessly.

Key takeaways from this integration:

- **Real-time Processing**: Stream and process multimodal data in real-time with sub-second latency, enabling immediate insights and responses for time-sensitive AI applications.
- **Unified Architecture**: Fluss serves as the real-time layer while Lance provides efficient historical storage, creating a complete [data lifecycle](https://lancedb.github.io/lance/) management solution.
- **Multimodal Support**: The ability to handle diverse data types like images, text, and vectors makes this stack ideal for modern [multimodal AI applications](/blog/multimodal-lakehouse/).
- **RAG Enhancement**: Real-time data ingestion ensures that [RAG](/blog/graphrag-hierarchical-approach-to-retrieval-augmented-generation/) systems always have access to the most current information, improving the accuracy and relevance of multimodal AI-generated responses.
- **Simple Integration**: As demonstrated in our example, setting up the pipeline requires minimal configuration and can be easily integrated into existing ML workflows.

As multimodal AI applications continue to demand fresher data and faster insights, 
the combination of Fluss and Lance provides a robust, 
scalable solution that bridges the gap between real-time [streaming](https://fluss.apache.org/docs/) and [AI-ready data storage](https://lancedb.github.io/lance/). 
Whether you're building recommendation systems, fraud detection, or customer service chatbots, 
this integration ensures your multimodal AI models have access to both the timeliness of streaming data and the depth of historical context.
```

---

## ðŸ“„ æ–‡ä»¶: from-bi-to-ai-lance-and-iceberg.md

---

```md
---
title: "From BI to AI: A Modern Lakehouse Stack with Lance and Iceberg"
date: 2025-11-24
draft: false
featured: true
categories: ["Engineering"]
image: /assets/blog/from-bi-to-ai-lance-and-iceberg/preview-image.png
meta_image: /assets/blog/from-bi-to-ai-lance-and-iceberg/preview-image.png
description: "A comparison of where Iceberg and Lance sit in the modern lakehouse stack. We highlight emerging architectures that are bridging the worlds of analytics and AI/ML workloads using these two formats, while being built on the same data foundation."
author: Jack Ye
author_avatar: "/assets/authors/jack-ye.jpg"
author_bio: "Software Engineer @ LanceDB"
author_github: "jackye1995"
author_linkedin: "yezhaoqin"
author2: "Prashanth Rao"
author2_avatar: "/assets/authors/prashanth.jpg"
author2_bio: "AI Engineer @ LanceDB"
author2_github: "prrao87"
author2_linkedin: "prrao87"
author2_twitter: "tech_optimist"
---

The modern, composable data stack has evolved around the idea of the *lakehouse* â€” a unified system that blends the flexibility of data lakes (i.e., object stores designed to hold data in open file formats) with the analytical performance and reliability of data warehouses. Projects like [Apache Iceberg](https://iceberg.apache.org/) have been pivotal in making this vision a reality, offering transactional guarantees and schema evolution at scale.

But as AI and machine learning workloads bring with them ever larger amounts of data from multiple modalities (e.g., text, images, audio, video, sensor data), newer formats like [Lance](https://lance.org) are emerging to take the next leap forward. Lance is a high-performance columnar format thatâ€™s purpose-built for AI/ML workloads (training, feature engineering) and multimodal data at petabyte scale.

The goal of this post is to explain where Iceberg and Lance fit in the modern lakehouse stack, while discussing some of their key differences. Weâ€™ll highlight emerging data architectures that are bridging the worlds of analytics and AI/ML workloads using these two formats, all built on the same data foundation.

## Understanding the modern lakehouse stack

The modern lakehouse architecture consists of six distinct technological layers, each serving a specific purpose. Letâ€™s dissect these layers (from the bottom up) to understand where Lance and Iceberg fit in, and how they can work together.

![](/assets/blog/from-bi-to-ai-lance-and-iceberg/lakehouse_stack.png)

### Object store

At the foundation of the lakehouse lies the **object store** â€” these are storage systems characterized by their simple, object-based hierarchy, typically providing high durability guarantees with HTTP-based communication for data transfer.

### File format

The **file format** describes how a single file should be stored on disk. This is where formats like Lance, Parquet, ORC, and Avro are present. The file format defines the internal structure, encoding, and compression of individual data files.

### Table format

The **table format** describes how multiple files work together to form a logical table. Table formats must include features like transactional commits and read isolation, so that multiple writers and readers can safely operate against the same table.

### Catalog spec

The **catalog spec** defines how any system can discover and manage a collection of tables within storage. It acts as the bridge between the storage layer and the compute layer of the stack (starting with the catalog *service*, more on this below).

### Catalog service

A **catalog service** offers easy connectivity to the compute engines on top, and implements one or more catalog specs to provide both table metadata and, optionally, continuous background maintenance (compaction, optimization, index updates) that table formats require to stay performant.

### Compute engine

The **compute engine** is the workhorse built on top of catalog services that leverage their awareness of catalog specs, table formats and file formats to perform complex data workflows. Compute engines are carefully designed to handle a variety of workloads, including SQL queries, analytics processing, vector search, full-text search, machine learning training.

## Differences between Lance and Iceberg

The key insight from the lakehouse architecture described above is that the file format, table format, and catalog spec layers are just **storage** specifications. **Compute power** resides only in the object store, catalog services, and compute engine layers. This clear separation of concerns is what allows lakehouse storage to be flexible, portable, and independently scalable, while opening up the same underlying data for discovery by any catalog service, and for processing by any compatible compute engine.

Iceberg operates at **two of the layers** in the stack: the table format and the catalog spec. It typically uses Parquet as the underlying file format.

Lance spans **three layers of the stack**, because it's simultaneously a file format, table format *and* a catalog spec.

![](/assets/blog/from-bi-to-ai-lance-and-iceberg/lance_and_iceberg.png)

In the sections below, we'll compare and contrast Lance and Iceberg at each of these layers.

### Table format

Iceberg employs a **three-level** metadata hierarchy in its table format: a table metadata file â†’ manifest list â†’ manifest files. The table metadata (a JSON file) rolls up a comprehensive history of past commits and schemas, and stores the partition specs, snapshot references and table properties. Each snapshot points to a manifest list (Avro) that contains metadata about manifest files and partition statistics (also Avro), and the manifests contain lists of data files that sit in the object store. Note that the Iceberg table format itself does not define how to atomically commit data â€” instead, it just describes the latest table metadata location, and itâ€™s left to the catalog service to determine how to actually do the commit.

Lance employs a **single-level** metadata hierarchy, with one manifest file per table version. Lance tables use the notion of *fragments*, rather than partitions. Each commit to a Lance table produces a new manifest file that contains fragments (each with their own data and deletion files) and pointers to the index files (for FTS, vector and other scalar indexes).

![](/assets/blog/from-bi-to-ai-lance-and-iceberg/table_format.png)

### File format

Iceberg supports multiple file formats under the hood. Parquet is the most prevalent and widely used, but Avro and ORC formats are also supported.

From a file format perspective, Lance does away with row groups (unlike Parquet, which heavily relies on them), achieving a high degree of parallelism, achieving 100x the random access performance of Parquet without sacrificing scan performance. There are several other differences between Lance and Parquet that wonâ€™t be discussed here, but you can read more about it in this VLDB 2025 paper: [Efficient Random Access in Columnar Storage through Adaptive Structural Encodings](https://arxiv.org/html/2504.15247v1).

### Catalog spec

Because of the way Iceberg delegates the actual atomic write guarantees to arbitrary catalog services, over the years there have been many protocols developed by the vendors building these catalog services. Icebergâ€™s â€œREST Catalog specâ€ was developed as a wrapper to standardize these different protocols, and any catalog service adopting the spec is required to guarantee the atomicity of the API operation.

Lance uses â€œnamespacesâ€, rather than explicitly defining a catalog spec. In fact, Lance intentionally names it "Lance Namespace" rather than "Lance Catalog", because itâ€™s a thin wrapper to allow storing and managing a Lance table via any catalog service, and is not aimed to be a complete catalog spec. In the future, to provide a full catalog spec experience, Lance aims to use Arrow Flight gRPC as its main standard, to be compatible with Lanceâ€™s vision of being an â€œArrow-native lakehouse formatâ€.

## When Lance is beneficial

In this section, weâ€™ll list the key benefits of using Lance over Iceberg, especially for AI/ML workloads.

### Fast random access for search, ML, and AI

Earlier generations of open table formats (Iceberg, Delta Lake and Hudi) were primarily designed as replacements for Hive. They focus mainly on data warehouse (OLAP) workloads, with tables that are typically â€œlong but narrowâ€.

Lance, on the other hand, is designed from the ground up to support machine learning and AI workloads, with fundamentally different access patterns and support for tables that are â€œ[long and wide](https://lancedb.com/blog/lance-v2/#very-wide-schemas)â€ (e.g., embeddings, blobs and deeply nested data in columns). Lance can index [billions of vectors in hours](https://lancedb.com/blog/case-study-netflix/), storing tens of petabytes of data. For vector search, it can support more than 10,000 QPS with [\<50 ms latency](https://lancedb.com/docs/enterprise/benchmark/) over object storage. For ML training, Lance integrates with PyTorch and JAX data loaders, achieving (through a distributed cache fleet) more than 5 million IOPS from NVMe SSDs.

Combining fast random access with native indexes within the same format is what gives Lance a significant advantage in ML and AI use cases, compared to scan-based approaches that are common in traditional lakehouses relying on Iceberg.

### Multimodal data done right

Multimodal data (images, videos, audio, deeply nested point clouds and their associated embeddings) is becoming more and more common, especially in the age of AI, where itâ€™s never been easier to generate and consume huge amounts of data.

In many Iceberg deployments today, multimodal data is modeled as columns in tables (like any other tabular data), with pointers to the actual data located in object storage. This isnâ€™t ideal from a data governance perspective, because organizations would need separate access control layers and extra operational plumbing across various systems. Itâ€™s also not ideal from a performance perspective, because there is additional I/O and network overhead while fetching individual data items.

Lanceâ€™s file format makes it more convenient to maintain multimodal data natively as blobs inside the columns, with no external lookups (the multimodal data is co-located with metadata and embeddings), thus simplifying governance and management of data thatâ€™s multimodal in nature. Itâ€™s also significantly more performant, because at the table level, Lance can pack multiple smaller rows together while storing very large rows (e.g., image or audio blobs) in a dedicated file thanks to its fragment-based design, thus balancing performance with storage size.

![](/assets/blog/from-bi-to-ai-lance-and-iceberg/multimodal_lakehouse.png)

### Flexible, zero-cost data evolution

A common need as the dataset scales in size is **data evolution,** i.e., changes to the table schema and adding, updating or removing columns and their associated data. These types of operations are especially common in ML/AI applications, where multiple developers working in parallel frequently add features, predictions or embeddings as new columns to an existing table. In Iceberg, data evolution comes with a non-trivial cost â€” adding data to a new column requires a **full table rewrite** since Parquet stores entire row groups together. This means that for very large tables, it's common to see multiple new feature columns being added in parallel by multiple teams in an organization -- which would require a table lock as new columns are being added, bottlenecking the feature engineering process.

In Lance, adding a new column **is essentially a zero-copy operation**. Lance's fragment design allows independent column files per fragment (though multiple columns can share a data file), meaning that adding or updating a column simply appends new column files without touching existing data. This avoids duplication on petabytes of data, as noted by [Netflix](https://lancedb.com/blog/case-study-netflix) as they built out their media data lake incorporating LanceDB.

![](/assets/blog/from-bi-to-ai-lance-and-iceberg/data_evolution.png)

The space savings can be tremendous -- say you have an existing table that's 100 GB in size. If
you update the table schema and add a new column that's only 1% this size (1 GB) -- in Iceberg, performing
a backfill operation on the new column would require a **full table copy** amounting to 101 GB of writes. In LanceDB, it would just be 1 GB of writes. The larger the dataset, the more this matters. The ability to continuously or incrementally add features, without duplicating or rewriting unaffected data, makes Lance a compelling choice for teams working with petabytes of data.

## When Iceberg is beneficial

Icebergâ€™s partition-based, catalog-centric approach can still be beneficial for traditional BI or analytics workloads, for the reasons listed below. In this section, we'll highlight some of them, as well as how Lance aims to address them in future versions.

### Optimized for analytical workloads

Icebergâ€™s hidden partitioning logic and its three-level metadata hierarchy enable efficient partition pruning for compute engines that are optimized for analytics workloads, where queries are naturally filtered on partition keys. Lance, in contrast, uses fragments (rather than partitions) as the organizational unit for data, so at present, the way Lance organizes data doesnâ€™t fit well with traditional OLAP-style compute engines that are heavily optimized for partition-based scans.

Newer methods like [liquid clustering](https://docs.databricks.com/aws/en/delta/clustering) (developed by Databricks) can, in the future, actively leverage Lanceâ€™s features, because they avoid hard-coded table layouts and adopt an adaptive clustering approach thatâ€™s optimized based on actual query patterns. However, partitioning is a concept thatâ€™s deeply baked into current-generation query engines, so until liquid clustering gains wider adoption in the ecosystem, Iceberg has several advantages for analytics workloads.

### Mature ecosystem integration

Iceberg has years of battle-hardening from production usage and is well-integrated with a mature ecosystem, including integrations with several compute engines and catalog services. In contrast, Lanceâ€™s compute engine integrations are still emerging (primarily Spark and Ray at present), with many more upcoming and in their nascent stages. There is strong community interest in adding Lance support to popular compute engines that are part of the Iceberg ecosystem, including Flink, StarRocks, and Trino. Expect this space to evolve over time.

### Centralized observability

Iceberg's catalog-dependent design means the catalog is aware of *all* table operations, enabling centralized monitoring and powerful automated optimization triggers. It also enable an easy-to-maintain unified audit log across all tables, with coordinated data lineage tracking.

Lance tables, like Delta Lake, can be **modified directly in storage** without catalog awareness. This storage-first design gives Lance a portability advantage but complicates activity tracking â€” downstream operations must rely on pull-based polling or storage event notifications (S3 Events, GCS Pub/Sub) rather than semantic catalog events. Lanceâ€™s approach to address this is through its managed offering, LanceDB Enterprise (which has knowledge of all read/write traffic), but in the future, there could be ways to onboard all operations onto open observability frameworks like OpenTelemetry for easy tracking in any tool that supports it.

## Takeaways from the comparison

The following table summarizes the reasons Lance is emerging as a **new standard for multimodal data and AI** workloads in the lakehouse.

| Feature | Lance | Iceberg |
| --- | --- | --- |
| **Metadata Structure** | Single-level manifest per version | Three-level hierarchy (metadata â†’ manifest list â†’ manifest) |
| **Metadata Growth** | Independent versions, no rollup | Metadata files accumulate snapshot history |
| **Data Organization** | Fragments (horizontal slices), global clustering/sorting | Partition specs with hidden partitioning, clustering/sorting within partition |
| **Row Address** | 64-bit addresses (fragment_id + offset) | file path + position tuple |
| **File Format** | Lance file format | Parquet/ORC/Avro |
| **Index Support** | Vector and full-text index, plus a standardized framework for new scalar index specifications | Puffin for simple NDV sketch, deletion vector |

Parquet and Iceberg, developed independently (in their own time frames), have led to an explosion of connectors, integrations and innovations up and down the layers of the lakehouse stack. However, a lot of these predate the age of AI, where the kinds of workloads and user requirements involved are changing at a blazing pace.

Lance is relatively new, and so it has had the opportunity to build and iterate rapidly from the ground up while learning from the successes and existing pain points of Iceberg/Parquet. The design features of Lance, as can be seen in the table above, incorporate several proven patterns while introducing new paradigms that aim to address the unique requirements of AI/ML workloads. Lance users can seamlessly interoperate across the various ML and data processing frameworks, from Pandas and Polars, to PyTorch and beyond.

## A unified data platform with Lance and Iceberg

Looking at the trade-offs involved when choosing between Lance and Iceberg, especially for analytics vs. ML/AI workloads, weâ€™re seeing a dual-format strategy in which large organizations are beginning to adopt Lance. Companies like Netflix are now [adopting LanceDB](https://lancedb.com/blog/case-study-netflix/) for their AI and multimodal workloads alongside Iceberg, which has long been their primary table format for BI and analytics workloads.

The figure below envisions how a unified lakehouse platform built on top of Lance and Iceberg might look, as more organizations build out their lakehouses on top of modern infrastructure.
The unification occurs at the compute layers both above (catalog services and compute engines) and below (i.e., the object stores) the storage formats.

![](/assets/blog/from-bi-to-ai-lance-and-iceberg/unified_lakehouse_platform.png)

Existing catalog specifications and metadata services like Glue, Hive metadata store (HMS), Unity REST catalog and Polaris are already integrated with Lance via [lance-namespace](https://lance.org/format/namespace/impls/), an open specification built on top of Lance that standardizes access to a collection of Lance tables.
On the compute engine side, there are numerous integrations in the [Lance format](https://github.com/lance-format) ecosystem (such as `lance-ray`, `lance-spark`, etc.) that are gaining adoption in open source.
The main takeaway from this section is that developers who do not want the burden of maintaining multiple catalog services can choose to build on top of Lance and leveraging its integration to the compute ecosystem, while developers who are already using Iceberg can interplay with Lance for use cases that benefit from the Lance format.

These emerging architectural patterns and open source projects reflect a broader trend: managing the separate needs of analytics and AI workloads with two distinct but interoperable formats â€” Iceberg for BI, and Lance for AI and multimodal data, bridging the best of both worlds.
```

---

## ðŸ“„ æ–‡ä»¶: geneva-feature-engineering.md

---

```md
---
title: "LanceDB's Geneva: Scalable Feature Engineering"
date: 2025-08-21
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/geneva-feature-engineering/preview-image.png
meta_image: /assets/blog/geneva-feature-engineering/preview-image.png
description: "Learn how to build scalable feature engineering pipelines with Geneva and LanceDB. This demo transforms image data into rich features including captions, embeddings, and metadata using distributed Ray clusters."
author: "Jonathan Hsieh"
author_avatar: "/assets/authors/jonathan-hsieh.jpg"
author_bio: "Software Engineer & Geneva Project Lead."
author_twitter: "jmhsieh"
author_github: "jmhsieh"
author_linkedin: "jonathan-m-hsieh"
---
When you start a machine learning project, your first challenge usually isnâ€™t training a model. Itâ€™s getting the data into a usable shape. 

> Raw data is messy: images are just pixels, audio is just waveforms, and text needs structure. Before you can do anything interesting, you need features. 

[LanceDB's Geneva](/docs/geneva/) is designed to take that pain away. Instead of writing ad hoc scripts that donâ€™t scale, you define feature transformations once as Python functions, run them locally or on a distributed cluster, and Geneva materializes the results as typed, queryable columns.

{{< admonition type="Note" title="Python Notebook" >}}
[All the code in this tutorial is in our Python notebook.](https://githubtocolab.com/lancedb/blog-lancedb/blob/main/content/docs/tutorials/geneva/geneva-feature-engineering.ipynb) We ran this from Google Vertex AI, so you will have to setup your own machine.
{{< /admonition >}}

In this walkthrough, youâ€™ll load [a dataset of cats and dogs](https://www.robots.ox.ac.uk/~vgg/data/pets/), define four feature extractors, and run them at scale: file size, dimensions, captions with BLIP, and embeddings with OpenCLIP. Along the way, youâ€™ll see how Geneva keeps the process consistent whether youâ€™re running on your laptop or across a Ray cluster.

**Figure 1: The Feature Engineering Workflow**

This diagram shows how Geneva transforms raw data into enriched features through UDFs, creating new columns for file size, dimensions, captions, and embeddings.

```mermaid
flowchart LR
    A[Raw dataset] --> B[Load into Geneva]
    B --> C[Define UDFs]
    C --> D[File size]
    C --> E[Dimensions]
    C --> F[Captions]
    C --> G[Embeddings]
    D --> H[New columns]
    E --> H
    F --> H
    G --> H
    H --> I[Enriched table]
```

{{< admonition type="Note" title="Why Geneva?" >}}
Geneva's basic promise is deceptively simple. Write Python like you normally would. Keep your functions pure. Geneva will serialize the code, ship the exact same environment to worker nodes, execute at scale, and persist results as new columns in LanceDB.
{{< /admonition >}}

## Watch the demo on YouTube

The demo is quite complex, so we recommend reading the article first.
The steps outlined in this blog will help guide you through the tutorial.

<div style="text-align: center;">
<iframe width="560" height="315" src="https://www.youtube.com/embed/4iKAOCw-_AA?si=pI98ifzKASgZ2vvT" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

### Step 1: Install and check your environment

Start by installing the required packages. Geneva integrates with PyTorch, Hugging Face Transformers, and OpenCLIP, so you can use state-of-the-art models right out of the box.

```python
!pip install --upgrade datasets pillow
!pip install transformers==4.51 torch accelerate     # BLIP captioning
!pip install open-clip-torch scikit-learn matplotlib # CLIP embeddings
```

If you have access to a GPU, Geneva will take advantage of it automatically. Itâ€™s worth checking:

```python
import torch
print("CUDA available:", torch.cuda.is_available())
```

This matters because GPU acceleration can dramatically reduce the time it takes to generate captions and embeddings. If CUDA isnâ€™t available, you can still run everything on CPU, which is fine for testing smaller datasets. The important thing is that the exact same code will work in either environment.

### Step 2: Load a dataset into Geneva

For this demo, weâ€™ll use the [Oxford-IIIT Pets dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) â€” a collection of cats and dogs with labels. 

![cats-and-dogs](/assets/blog/geneva-feature-engineering/dataset.png)

You can swap this for any dataset you like. You can ingest Arrow data into LanceDB tables, which makes it efficient to store, process, and query.

```python
from datasets import load_dataset
import pyarrow as pa
import io, shutil
from geneva.tqdm import tqdm

shutil.rmtree(GENEVA_DB_PATH, ignore_errors=True)

def load_images(frag_size: int = 25):
    dataset = load_dataset("timm/oxford-iiit-pet", split=f"train[:{NUM_IMAGES}]")
    batch = []
    for row in tqdm(dataset):
        buf = io.BytesIO()
        row["image"].save(buf, format="png")
        batch.append({"image": buf.getvalue(),
                      "label": row["label"],
                      "image_id": row["image_id"],
                      "label_cat_dog": row["label_cat_dog"]})
        if len(batch) >= frag_size:
            yield pa.RecordBatch.from_pylist(batch)
            batch = []
    if batch:
        yield pa.RecordBatch.from_pylist(batch)

db = geneva.connect(GENEVA_DB_PATH)
```

This function streams the dataset into batches instead of loading everything into memory at once. 

By breaking things into smaller batches, Geneva can process them in parallel more easily. If it was just one large partition, only one worker would take the work leaving the rest idle. By splitting into smaller batches, the batches can be farmed out to multiple workers for parallel processing. 

To make the table, just loop through the batches and add them:

```python
first = True
for batch in load_images():
    if first:
        tbl = db.create_table("images", batch, mode="overwrite")
        first = False
    else:
        tbl.add(batch)
```

At this point youâ€™ve got a table of raw images with labels. It doesnâ€™t do much on its own, but now youâ€™re ready to enrich it with features.

### Step 3: Add simple features (file size, dimensions)

Start small. Geneva lets you define UDFs as simple Python functions, and the results become new columns in your table. The following examples show how to return both scalars and structured data:

```python
from geneva import udf
import pyarrow as pa
from PIL import Image
import io

@udf
def file_size(image: bytes) -> int:
    return len(image)

@udf(data_type=pa.struct([
    pa.field("width", pa.int32()),
    pa.field("height", pa.int32())
]))
def dimensions(image: bytes):
    img = Image.open(io.BytesIO(image))
    return {"width": img.size[0], "height": img.size[1]}
```

The first UDF calculates the size of each image file in bytes. Itâ€™s a trivial example, but it demonstrates how easy it is to add scalar values. The second UDF extracts the width and height of each image and returns them as a structured record. With these two functions, you now have queryable columns that let you filter images by resolution or spot outliers based on file size. Even though these examples are simple, they highlight Genevaâ€™s flexibility in handling different data types.

### Step 4: Generate captions with BLIP

Now letâ€™s create something more useful. Geneva makes it easy to run expensive models at scale by letting you write stateful UDFs. That means the model is loaded once and reused across rows, instead of being reloaded every time. Hereâ€™s how you can generate captions using BLIP:

```python
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch, io

@udf(cuda=True)
class BlipCaptioner:
    def __init__(self): self.is_loaded = False
    def setup(self):
        self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device); self.is_loaded = True
    def __call__(self, image: bytes) -> str:
        if not self.is_loaded: self.setup()
        raw = Image.open(io.BytesIO(image)).convert("RGB")
        inputs = self.processor([raw], return_tensors="pt")
        inputs = {k: v.to(self.device) for k,v in inputs.items()}
        out = self.model.generate(**inputs, max_length=50)
        return self.processor.decode(out[0], skip_special_tokens=True)
```

With this UDF, each image in your dataset now gets a natural language description. Instead of just having raw pixels and labels, you can run queries like â€œshow me all the rows where the caption mentions a dog.â€ This transforms your dataset into something you can search and analyze in ways that werenâ€™t possible before. And because the model is cached, it runs efficiently even across large batches.

### Step 5: Create embeddings with OpenCLIP

Captions give you text, but embeddings give you the ability to search semantically. With embeddings, you can ask questions like â€œfind images most similar to this oneâ€ or â€œcluster my dataset into related groups.â€ Geneva makes it simple to generate these embeddings and store them as vector columns.

```python
import open_clip, numpy as np

@udf(data_type=pa.list_(pa.float32(), 512))
class GenEmbeddings:
    def __init__(self): self.is_loaded = False
    def setup(self):
        self.model, _, self.preprocess = open_clip.create_model_and_transforms(
            "ViT-B-32", pretrained="laion2b_s34b_b79k")
        self.is_loaded = True
    def __call__(self, image: bytes):
        if not self.is_loaded: self.setup()
        # preprocess â†’ forward pass â†’ normalize â†’ return 512-d vector
```

Once you run this, every image will have a 512-dimensional vector representation. That vector becomes the foundation for building similarity search, recommendations, or clustering workflows. Instead of just relying on labels or captions, you now have a mathematical representation of content that Geneva can index and query.

**Figure 2: Backfill Execution and Querying**

This diagram shows Geneva's execution workflow: UDFs are processed in batches with partial commits, then results become available for searching and filtering.

```mermaid
flowchart LR
    A[Define UDFs] --> B[Run Backfill]
    B --> C[Partial Commits]
    C --> D[Async Execution]
    D --> E[Table Updated]
    E --> F[Search/Filter]
    F --> G[Results]
```

### Step 6: Run backfills

Defining UDFs is only half the story â€” you need to run them against your dataset. Genevaâ€™s backfill API applies your UDFs across the table and writes the results into new columns. For lightweight tasks like file size, you can run them synchronously:

```python
tbl.backfill("file_size", batch_size=10)
tbl.backfill("dimensions", batch_size=10, commit_granularity=5)
```

For heavier tasks like embeddings, youâ€™ll want to run them asynchronously. This way, Geneva commits partial results as theyâ€™re processed, and you can monitor progress in real time:

```python
tbl.add_columns({"embedding": GenEmbeddings()})
fut = tbl.backfill_async("embedding", batch_size=10, commit_granularity=2)

while not fut.done(timeout=5):
    tbl.checkout_latest()
    done = tbl.search().where("embedding is not null").to_arrow()
    print(f"committed {len(done)} rows, version {tbl.version}")
```

This workflow makes a big difference when you scale. You donâ€™t have to wait hours for the entire dataset to finish before you can see results. Geneva will stream partial results as they come in, and because every version of the table is saved, you can safely retry or roll back if something fails.

#### A quick â€œbefore and afterâ€ view

This is just a simple example of the evolution in data you should see:

```bash
| image_id | label | image (bytes) |
|----------|-------|---------------|
| pet_001  | cat   | ...           |
| pet_002  | dog   | ...           |
| pet_003  | cat   | ...           |
```

UDFs take effect, your table grows with feature columns:

```bash
| image_id | label | file_size | dimensions        | caption                           | embedding        |
|----------|-------|-----------|-------------------|-----------------------------------|------------------|
| pet_001  | cat   | 15423     | {"w":128,"h":128} | "a small brown cat sitting down"  | [0.12, 0.33,...] |
| pet_002  | dog   | 28764     | {"w":256,"h":256} | "a black dog looking at camera"   | [0.42, 0.77,...] |
| pet_003  | cat   | 19287     | {"w":128,"h":128} | "a fluffy white kitten indoors"   | [0.09, 0.55,...] |
```

*(These tables are illustrative to show the shape of the data; in the notebook youâ€™ll see Arrow/Lance tables and DataFrame previews.)*

## Step 7: Query your results

Once your features are materialized, you can treat them just like any other database column. For example, you can run a SQL query to filter by captions:

```python
SELECT image_id, caption
FROM images
WHERE caption LIKE '%dog%'
```

Or you can query/filter for them in Python:

```
rows = tbl.search().where("caption is not null").to_arrow()
```

At this point, your dataset has evolved from a collection of raw images into a rich table with metadata, captions, and embeddings. You can search it, analyze it, or feed it into downstream pipelines without writing custom glue code.

## Why this workflow matters

What youâ€™ve built here is more than just a demo. By combining UDFs with Genevaâ€™s execution engine, youâ€™ve taken raw data and turned it into something structured, searchable, and scalable. 

- Scalars like file size show you that you can store simple numbers. 
- Structs like dimensions let you work with richer metadata. 
- Captions give you natural language descriptions that make the dataset human-readable, while embeddings let you run semantic search and clustering. 

Because Geneva backfills work asynchronously and are versioned, you donâ€™t have to worry about reruns or failures â€” you always know where you left off. 

And perhaps most importantly, you donâ€™t have to change your code when you scale: the same functions you tested on a handful of images locally will run across millions of rows in a Ray cluster. Thatâ€™s the difference between a quick prototype and a production-ready workflow.

## Your next step

1. Pick a dataset that matters to you. Maybe itâ€™s product images from your companyâ€™s website, or a collection of documents you want to index. 

2. Start with a small batch locally to make sure your UDFs behave as expected. 

3. Then, when youâ€™re ready, scale it out with a cluster. Geneva gives you a smooth path from experimentation to production, without the usual friction of rewriting pipelines for distributed systems. 

Once youâ€™ve experienced how quickly you can go from raw pixels to searchable features, youâ€™ll see why Geneva changes the way you approach feature engineering.

{{< admonition type="Note" title="Python Notebook" >}}
[All the code in this tutorial is in our Python notebook.](https://githubtocolab.com/lancedb/blog-lancedb/blob/main/content/docs/tutorials/geneva/geneva-feature-engineering.ipynb) We ran this from Google Vertex AI, so you will have to setup your own machine.
{{< /admonition >}}
```

---

## ðŸ“„ æ–‡ä»¶: geneva-twelvelabs.md

---

```md
---
title: "Building Semantic Video Recommendations with TwelveLabs and LanceDB"
date: 2025-09-16
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/geneva-twelvelabs/preview-image.png
meta_image: /assets/blog/geneva-twelvelabs/preview-image.png
description: "Build semantic video recommendations using TwelveLabs embeddings, LanceDB storage, and Geneva pipelines with Ray."
author: David Myriel
author_avatar: "/assets/authors/david-myriel.jpg"
author_bio: "Writer"
author_github: "davidmyriel"
author_linkedin: "davidmyriel"
---

{{< admonition tip "Notebook" >}}
The code snippets in this article are shortened to keep things clear and concise. If youâ€™d like to experiment with the full runnable version, you can find the complete notebook [here](https://colab.research.google.com/drive/1jZiMT1QFYGvPgrps2Vpge9CtlRKFY1L0?usp=sharing#scrollTo=046o2pt62413).
{{< /admonition >}}

Traditional recommendation engines usually depend on metadataâ€”titles, tags, or transcripts. While useful, those signals donâ€™t capture the deeper meaning of whatâ€™s happening inside a video. Imagine if your system could actually understand the visuals, audio, and context of the content itself.

Thatâ€™s exactly what weâ€™ll build in this tutorial: a semantic video recommendation engine powered by [TwelveLabs](https://www.twelvelabs.io/), [LanceDB](/), and [Geneva](/docs/geneva).

- [TwelveLabs](https://www.twelvelabs.io/) provides multimodal embeddings that encode the narrative, mood, and actions in a video, going far beyond keyword matching.
- [LanceDB](/) stores these embeddings together with metadata and supports fast vector search through a developer-friendly Python API.
- [Geneva](/docs/geneva), built on [LanceDB](/) and powered by [Ray](/blog/lance-namespace-lancedb-and-ray/), scales the entire pipeline seamlessly from a single laptop to a large distributed clusterâ€”without changing your code.

## Why this stack?

- [TwelveLabs](https://www.twelvelabs.io/): captures narrative flow and meaning, enabling natural queries like â€œa surfer riding a wave at sunsetâ€ to return relevant matches even without explicit tags.
- [LanceDB](/): a modern vector database built on [Apache Arrow](https://arrow.apache.org/), with:
	- A simple, intuitive Python interface.
	- Embedded operationâ€”no external services required.
	- Native multimodal support for video, images, text, and vectors in the same table.
- [Geneva](/docs/geneva): extends LanceDB for distributed processing. With [Ray](/blog/lance-namespace-lancedb-and-ray/) under the hood, it parallelizes embedding generation and large-scale searches.

## Loading and Materializing Videos

The first step is to **load your dataset** into LanceDB. Here, weâ€™re pulling in videos from [HuggingFace](https://huggingface.co/)â€™s [`HuggingFaceFV/finevideo`](https://huggingface.co/datasets/HuggingFaceFV/finevideo) dataset.

```python
def load_videos():
    dataset = load_dataset("HuggingFaceFV/finevideo", split="train", streaming=True)
    batch = []
    processed = 0

    for row in dataset:
        if processed >= 10:
            break

        video_bytes = row['mp4']
        json_metadata = row['json']

        batch.append({
            "video": video_bytes,
            "caption": json_metadata.get("youtube_title", "No description"),
            "youtube_title": json_metadata.get("youtube_title", ""),
            "video_id": f"video_{processed}",
            "duration": json_metadata.get("duration_seconds", 0),
            "resolution": json_metadata.get("resolution", "")
        })
        processed += 1

    return pa.RecordBatch.from_pylist(batch)
```

Here we stream the dataset to save memory and only process 10 rows for the demo. Each item stores the raw video bytes plus helpful metadata, producing a PyArrow RecordBatch that keeps video and metadata together.

Now we persist this dataset into **LanceDB using Geneva**:

```python
db = geneva.connect("/content/quickstart/")
tbl = db.create_table("videos", load_videos(), mode="overwrite")
```

`geneva.connect()` starts a local LanceDB instance, `create_table()` writes the dataset to `videos`, and `mode="overwrite"` resets the table.

At this point, we have a **LanceDB table of videos** ready for embedding and search.

## Embedding Videos with TwelveLabs

Next, we use **TwelveLabsâ€™ Marengo model** to generate embeddings from the raw video files.

```python
task = client.embed.tasks.create(
    model_name="Marengo-retrieval-2.7",
    video_file=video_file,
    video_embedding_scope=["clip", "video"]
)

status = client.embed.tasks.wait_for_done(task.id)
result = client.embed.tasks.retrieve(task.id)

video_segments = [seg for seg in result.video_embedding.segments
                  if seg.embedding_scope == "video"]

embedding_array = np.array(video_segments[0].float_, dtype=np.float32)
```

Here we submit an embedding job to TwelveLabs, request both clip and wholeâ€‘video embeddings, then convert the result to a NumPy array for storage and search.

With **Geneva**, embeddings are automatically stored as another column:

```python
tbl.add_columns({"embedding": GenVideoEmbeddings(
    twelve_labs_api_key=os.environ['TWELVE_LABS_API_KEY']
)})
tbl.backfill("embedding", concurrency=1)
```

`add_columns()` adds an `embedding` column powered by `GenVideoEmbeddings`, and `backfill()` computes it for all rows (increase `concurrency` in production).

At this stage, every video in our LanceDB table has a **semantic embedding** attached.

## Searching with LanceDB

With embeddings stored, LanceDB can run vector search queries.

```python
query = "educational tutorial"
query_result = client.embed.create(
    model_name="Marengo-retrieval-2.7",
    text=query
)
qvec = np.array(query_result.text_embedding.segments[0].float_)

lance_db = lancedb.connect("/content/quickstart/")
lance_tbl = lance_db.open_table("videos")

results = (lance_tbl
          .search(qvec)
          .metric("cosine")
          .limit(3)
          .to_pandas())
```

Here we embed the query into `qvec`, open the `videos` table, run `.search(qvec)` with cosine similarity, and return the top matches as a pandas DataFrame. This is semantic search in action.

## Summarizing with Pegasus

Embeddings alone provide similarity, but they donâ€™t explain *why* a result was returned. For better UX, TwelveLabs also provides **Pegasus**, a summarization model:

```
index = client.indexes.create(
    index_name=f"lancedb_demo_{int(time.time())}",
    models=[{"model_name": "pegasus1.2", "model_options": ["visual", "audio"]}]
)
```

`pegasus1.2` creates short multimodal summaries you can store alongside results to make recommendations easier to understand.

## Scaling with Geneva and Ray

Small datasets can be managed manually, but at **enterprise scale** you need automation. Geneva \+ Ray handle this:

| Concern | LanceDB only | With Geneva and Ray |
| ----- | ----- | ----- |
| Ingestion | Manual loaders | Declarative pipelines |
| Embeddings | Sequential | Parallel across many workers |
| Storage | Local tables | Distributed LanceDB tables |
| ML/Analytics | Custom scripts | Built-in distributed UDFs |

Here we declare the pipeline once and run it anywhere. Ray parallelizes the work so you can scale from a laptop to a large cluster without changing your code.

## Try it out

By combining [TwelveLabs](https://www.twelvelabs.io/), [LanceDB](/), and [Geneva](/docs/geneva) you can build a recommendation system that **understands video content directly**.

- [TwelveLabs Playground](https://playground.twelvelabs.io) â€“ Sign up for an API key and start generating video embeddings right away 
- [LanceDB Quickstart](/docs/quickstart) â€“ Install LanceDB locally and try your first vector search with Python 
- [Geneva Documentation](/docs/geneva) â€“ Learn how to scale pipelines and run distributed embedding jobs with Ray 
- [Complete Notebook](https://colab.research.google.com/drive/1jZiMT1QFYGvPgrps2Vpge9CtlRKFY1L0?usp=sharing#scrollTo=046o2pt62413) â€“ Explore the full runnable code with all the details
```

---

## ðŸ“„ æ–‡ä»¶: gpu-accelerated-indexing-in-lancedb-27558fa7eee5.md

---

```md
---
title: "GPU-Accelerated Indexing in LanceDB"
date: 2023-11-02
author: LanceDB
author_avatar: "/assets/authors/lancedb.jpg"
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/gpu-accelerated-indexing-in-lancedb-27558fa7eee5/preview-image.webp
meta_image: /assets/blog/gpu-accelerated-indexing-in-lancedb-27558fa7eee5/preview-image.webp
description: "Speed up vector index training in LanceDB with CUDA or Apple Silicon (MPS). See how GPUâ€‘accelerated IVF/PQ training compares to CPU and how to enable it in code."
---

Vector databases are extremely useful for RAG, RecSys, computer vision, and a whole host of other ML/AI applications. Because of the rise of LLMs, there has been a lot of focus on vector indices, query latency, as well as the tradeoffs between latency and recall of various indices. Whatâ€™s often neglected is the time it takes to *build* a vector index. Building a vector index is a very computationally intensive process that increases quadratically with the number of vectors or vector dimensions. As you scale up in production, this becomes a much bigger bottleneck for your AI stack.

Over the past few months, weâ€™ve made some pretty amazing improvements using CPUs to build vector indices. And now weâ€™re making a much bigger leap by supporting GPU acceleration for index building. So if you have access to an Nvidia GPU or a Macbook that supports [MPS](https://developer.apple.com/documentation/metalperformanceshaders), you can now take advantage of the unparalleled computing power of GPUs when training large scale vector indices.

## Training vector indices is expensive

Common indexing techniques like IVF (InVerted File index) or compression like PQ (Product Quantization) divide up the vectors into clusters. To find the cluster centroids, we have to use KMeans. While there are various techniques to improve the performance of KMeans, at the end of the day, it scales quadratically. This means that index training time quickly becomes prohibitively expensive at high scale.

The KMeans training algorithm is an iterative process where a ton of vector distance computations are performed to minimize the distance of vectors to their assigned clusters. It turns out that GPUs are amazingly good at this kind of math. Indexing libraries like FAISS, for example, support GPU-accelerated index training, but most vector databases have yet to add GPU support (and certainly havenâ€™t made it easy).

## LanceDB support for GPU-acceleration

Since the beginning of LanceDB, users managing embeddings at scale have asked for GPU-acceleration to speed up their index training. In the most recent release of the Python package of LanceDB (v0.3.3), backed by Lance (v0.8.10), you can now use either CUDA or MPS by simply specifying the â€œacceleratorâ€ parameter when calling `create_index` :

> Using a GPU in LanceDB is as simple as specifying the **accelerator parameter** on **create_index()**.

![](https://miro.medium.com/v2/resize:fit:770/1*lZQRY7ed3FDGw-h3Sd1rTg.png)
Creating index using Nvidia GPU (cuda)

![](https://miro.medium.com/v2/resize:fit:770/1*ANrOLSEGd2XXy5vv5h1_WQ.png)
Creating Index using Apple Silicon GPU (mps)

Under the hood, LanceDB uses [PyTorch](https://pytorch.org/) to train the IVF clusters, and passes the kmeans centroids to the Rust core for index serialization. Thanks to the high-quality support of Cuda and [MPS](https://pytorch.org/docs/stable/notes/mps.html) from the PyTorch community, it allows us to quickly deliver on two of the most popular developer platforms (Linux and Mac). Combined with other recent improvements in the LanceDB indexing process, for instance, out-of-memory shuffling, batched KMeans training in GPU, LanceDB can reliably train over tens of millions vectors without worrying about CPU or GPU Out of Memory (OOM).

{{< admonition >}}Make sure you have pytorch installed (with CUDA if applicable) to use GPU-acceleration in LanceDB.{{< /admonition >}}

## Results

To benchmark the performance improvement for KMeans training, we trained IVF_4096 (4096 clusters) using L2 euclidean distance on a 1-million vector dataset with OpenAI Ada2 embeddings (1536D). This was done on Linux and also on MacOS:

- Google Cloud **g2-standard-32 **instance, 32 logical cores, and 1 x Nvidia L4 GPU, Ubuntu 22.04, nvidia-driver-525
- Apple M2 Max Macbook Pro 14', with 64GB RAM, 30 GPU cores.

In general, GPU acceleration offers up to 20â€“26x speed up compared to their CPU counterparts:

- Linux VM: 323s on CPU, and 12.5s on GPU
- Macbook Pro: 397s on CPU, and 21s on GPU

![](https://miro.medium.com/v2/resize:fit:770/1*9tRrnjLVnasYS1E9d1PRvA.png)
IVF 4096 on 1 Million 1536D vectors. BLUE is CPU; RED is GPU.

## Whatâ€™s next

Currently the IVF KMeans training is only one part of the whole index training process. Weâ€™re going to be working to add GPU acceleration for PQ training and also assigning the vectors to the correct centroids. Once this is completed, youâ€™ll see even more drastic improvements in end-to-end index training time.

In addition, PyTorch offers LanceDB an easy path for large-scale distributed GPU training, as well as access to even more hardware accelerators (i.e., TPU via XLA) in the future. Imagine being able to train a vector index in minutes on billions of vectors, on any hardware.

Finally, now that we have a mechanism to use the GPU effectively, we could also use it for inference down the road.

## Try it out!

You can start to leverage CUDA and Apple Silicon GPU support today via `pip install lancedb`. If you found this useful or interesting, please show us some love by starring [LanceDB](http://github.com/lancedb/lancedb) and the [Lance format](http://github.com/lancedb/lance).

And if youâ€™re looking for a highly scalable, hosted vector database, check out our enterprise offering at [lancedb.com](https://lancedb.com/)!
```

---

## ðŸ“„ æ–‡ä»¶: graphrag-hierarchical-approach-to-retrieval-augmented-generation.md

---

```md
---
title: "GraphRAG: Hierarchical Approach to Retrieval-Augmented Generation"
date: 2024-03-25
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/graphrag-hierarchical-approach-to-retrieval-augmented-generation/graphrag-hierarchical-approach-to-retrieval-augmented-generation.png
description: "Explore GraphRAG: hierarchical approach to retrieval-augmented-generation with practical insights and expert guidance from the LanceDB team."
author: Akash Desai
author_avatar: "/assets/authors/community.jpg"
author_bio: ""
---

## What is RAG?

**Retrieval-Augmented Generation (RAG)** is an architecture that combines traditional information retrieval systems with large language models (LLMs). By integrating external knowledge sources, RAG enhances generative AI capabilities, allowing it to provide responses that are not only more accurate and relevant but also up to date. 

## How Does Retrieval-Augmented Generation (RAG) Work?

RAG operates through a few key steps to enhance the performance of generative AI:

1. **Retrieval and Pre-processing:** RAG employs advanced search algorithms to access external data from sources such as websites, knowledge bases, and databases. The retrieved information is then pre-processed â€” cleaned, tokenized, and filtered â€” to ensure it's ready for use.
2. **Generation:** The pre-processed data is integrated into the pre-trained LLM, enriching its context. This integration allows the LLM to generate responses that are more accurate, relevant, and informative.

These steps work together to make RAG a powerful tool for generating high-quality responses based on real-time information.

## Why Use RAG?

Retrieval-Augmented Generation (RAG) offers a range of compelling advantages over traditional text generation methods, making it particularly valuable in contexts where accuracy and relevance are paramount: are essential:

- **Access to Real-Time Data:** RAG ensures that responses are always up to date by seamlessly pulling in the latest information from diverse sources, allowing users to receive timely insights.
- **Guaranteed Accuracy:** It relies on verified sources to maintain factual correctness, building user trust.
- **Contextual Precision:** RAG customizes responses to align with the specific context of each query, enhancing relevance.
- **Consistency:** By grounding answers in accurate data, RAG minimizes contradictions and ensures reliability.
- **Efficient Retrieval:** Advanced vector databases allow RAG to quickly locate relevant documents, streamlining information access.
- **Enhanced Chatbots:** Integrating external knowledge elevates chatbot performance, providing more comprehensive and context-aware responses.

### Limitations of Baseline RAG

While RAG was designed to address various challenges in information retrieval and generation, its basic form has limitations in certain scenarios:

- **Difficulty in Connecting Information:** Baseline RAG often struggles with questions that require synthesizing disparate pieces of information into a cohesive answer, making it less effective for complex queries.
- **Challenges with Big Picture Understanding:** It has difficulty comprehensively understanding and summarizing large datasets or documents, which can hinder its ability to provide holistic insights.

This highlights the **need for advancements** in RAG to overcome these challenges and improve its overall effectiveness.

To overcome the limitations of baseline RAG, **Microsoft Research** introduced [**GraphRAG**](https://github.com/microsoft/graphrag). This method enhances the traditional RAG framework by constructing a dynamic knowledge graph from a given dataset. This graph serves as a structured representation of information, capturing not only the data itself but also the relationships and context between different pieces of information.

GraphRAG significantly enhances the ability to answer complex questions by effectively synthesizing and reasoning over diverse information, surpassing the performance of traditional methods.

## The GraphRAG Process ðŸ¤–

**GraphRAG** is an advanced iteration of RAG that leverages knowledge graphs to enhance both the retrieval and generation processes. By structuring data hierarchically, GraphRAG enables more effective reasoning over complex queries, which leads to improved accuracy and a deeper contextual understanding of responses. This structured approach allows the system to connect related information and deliver insights that are not only relevant but also nuanced.

### How GraphRAG Solves RAG Problems

GraphRAG effectively addresses the limitations of traditional RAG by introducing a structured approach to information retrieval. It constructs a dynamic knowledge graph from raw text, capturing essential entities, relationships, and key claims within the data. This structured representation allows GraphRAG to reason over multiple connections, enabling it to synthesize information more effectively. As a result, GraphRAG provides more accurate and comprehensive answers to complex queries, overcoming the challenges faced by baseline RAG in understanding intricate relationships among data points.

### How GraphRAG Works

**Indexing:** Raw text is divided into **TextUnits**, which are analyzed to extract entities and relationships. These **TextUnits** form the foundational elements for subsequent processing.

**Graph Construction:** Following indexing, a **knowledge graph** is created, organizing entities and relationships hierarchically. This graph is refined through hierarchical clustering using the **Leiden technique**, which groups related entities into communities. This approach aids in visualizing and understanding complex datasets holistically.

**Query Processing:** When a query is made, GraphRAG leverages the knowledge graph to retrieve relevant information through two primary modes:

- **Global Search:** This mode utilizes community summaries to provide answers to broad, holistic questions about the dataset, offering a comprehensive overview.
- **Local Search:** This mode concentrates on specific entities, exploring their neighbors and related concepts to facilitate detailed reasoning and more precise answers. It uses LanceDB as the default vector database for performing vector search to retrieve relevant entities.

We are going to see both in practice.

**LLM Integration:** After retrieving relevant data from the knowledge graph, it is fed into a large language model (LLM). The LLM generates coherent and contextually relevant responses by leveraging the structured knowledge, allowing it to address complex queries with precision.

**Prompt Tuning:** To achieve the best results with GraphRAG, it's recommended to fine-tune your prompts according to the guidelines provided in the Prompt Tuning Guide. This step ensures that the model is optimally tailored to your specific data and query needs.

### When to Use GraphRAG

**Complex Queries:** Ideal for scenarios where answers require reasoning across multiple steps or connections within the data.

**If High Precision Needed:** Essential in situations where factual accuracy is critical, such as in legal or scientific contexts.

**Large-Scale Knowledge Bases:** Perfect for managing and querying extensive, interconnected datasets efficiently.

**Dynamic Information:** Best suited for environments where data is frequently updated or constantly evolving.

## Example

In this section, we will walk through the practical steps of implementing RAG 

```python
import bs4
from langchain import hub
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import LanceDB
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from lancedb.rerankers import LinearCombinationReranker

# Load the text document
loader = TextLoader("/content/HTE_gst_scheme.txt")
documents = loader.load()

# Split the loaded documents into manageable chunks using RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1100, chunk_overlap=100)
splits = text_splitter.split_documents(documents)

# Initialize the reranker and create a vector store with the document splits
reranker = LinearCombinationReranker(weight=0.3)
vectorstore = LanceDB.from_documents(documents=splits, embedding=OpenAIEmbeddings(), reranker=reranker)

# Set up the retriever to fetch relevant snippets from the vector store
retriever = vectorstore.as_retriever()
prompt = hub.pull("rlm/rag-prompt")

# Function to format the retrieved documents for output
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Initialize the language model for response generation
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# Create the RAG chain to process the query with the context and question
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Invoke the RAG chain with a specific query and print the response
query = "tell me how to do export"
response = rag_chain.invoke(query)
print(response)
```

```
To export, one must first decide on the mode of shipping and choose an ocean freight forwarder. It is important to arrange for a customs clearance agent to ensure compliance with export processes. Quality control and meeting buyer's requirements are crucial steps in the export process.
```

## Implementing GraphRAG

To get started with GraphRAG, follow these steps:

### Step 1: Install GraphRAG

Begin by installing the GraphRAG package using the following command:

```bash
!pip install graphrag
```

### Step 2: Prepare Your Dataset

1. **Create and Organize Your Dataset**
   Start by creating your dataset and saving it in the following directory: `/content/rag_exim/input/info.txt`

2. **Supported File Types**
   Currently, GraphRAG supports only `.txt` and `.csv` file formats. If you need to add support for CSV files, you'll need to modify the `settings.yml` file accordingly to include the necessary configurations.

3. **Download and Format Your Data**
   Ensure that your dataset is properly downloaded and formatted for seamless processing by GraphRAG.

### Step 3: Initialize the Dataset for Indexing

To set up the initial structure for indexing your dataset, use the following command:

```bash
!python3 -m graphrag.index --init --root /content/rag_exim
```

### Step 4: Index the Dataset

To index your dataset, use the following command:

```bash
!python3 -m graphrag.index --root /content/rag_exim
```

### Step 5: Execute the Query

As we said, we have two options for executing queries in GraphRAG: **global** and **local query execution**.

#### Global Query Execution

To perform a global query, use the following command:

```bash
!python3 -m graphrag.query --root /content/rag_exim --method global "tell me how to do export?"
```

Using the global query method allows you to gain comprehensive insights and overarching themes from your dataset. After executing this command, review the output to see the responses generated by GraphRAG.

**Output:**

```
SUCCESS: Global Search Response: To successfully engage in exporting goods from India, businesses must navigate a series of regulatory, financial, and logistical steps designed to ensure compliance with both national and international standards. The process is multifaceted, involving the acquisition of necessary codes, compliance with legal requirements, financial planning, and the strategic use of available schemes to enhance competitiveness.

### Obtaining Necessary Codes and Compliance

First and foremost, obtaining an Importer-Exporter Code (IEC) from the Directorate General of Foreign Trade (DGFT) is mandatory for all entities engaged in import or export in India. This code is a fundamental requirement and serves as a primary identifier for businesses in the international trade arena [Data: Reports (40, 43)].

### Regulatory Compliance and Customs Clearance

Ensuring compliance with regulatory requirements is crucial. This includes adherence to the Goods and Services Tax (GST) regulations, fumigation, phyto-sanitary measures, and the Foreign Trade Policy (FTP). The FTP outlines legal and procedural guidelines critical for exports and imports, making understanding these regulations essential for smooth operations [Data: Reports (32, 27)].

The export goods must meet all required safety and legal standards, which is verified through a mandatory Export Customs Clearance process. This involves inspection and approval by customs agents to ensure that shipments comply with all legal and safety requirements before leaving the country. Engaging a Customs Broker can significantly manage this process, acting as an intermediary between the business and customs authorities [Data: Reports (35)].

### Financial Planning and Support Mechanisms

Financial planning is another critical aspect of the export process. Leveraging financial support mechanisms such as Post Shipment Finance and Packing Credit from commercial banks can meet working capital requirements for fulfilling export transactions. Additionally, schemes like the Advance Authorisation Scheme allow for duty-free import of inputs used in export production, reducing costs and enhancing competitiveness [Data: Reports (27, 17)].

### Leveraging Schemes and Certifications

Businesses should also consider collaborating with Export Oriented Units (EOUs) and Special Economic Zones (SEZs) for benefits such as exemptions on GST and customs duties. Obtaining certifications from the Export Inspection Council of India (EIC) can enhance product competitiveness and consumer trust in international markets. Utilizing schemes like the Export Promotion Capital Goods (EPCG) scheme enables the import of capital goods at zero customs duty, further reducing the cost of acquiring new technology and equipment for export production [Data: Reports (31, 21, 26)].

### Conclusion

In summary, exporting from India requires meticulous planning and adherence to a range of regulatory, financial, and logistical requirements. By obtaining the necessary codes, ensuring regulatory compliance, engaging in financial planning, and leveraging available schemes and certifications, businesses can navigate the complexities of international trade more effectively. This comprehensive approach not only ensures compliance with national and international standards but also enhances the competitiveness of Indian exports in the global market [Data: Reports (40, 43, 35, 32, 27, 17, 31, 21, 26, +more)].
```

Once you are familiar with global queries, you can also explore local query execution for more specific inquiries.

#### Local Query Execution

To perform a local query, use the following command:

```bash
!python3 -m graphrag.query --root /content/rag_exim --method local "tell me how to do export?"
```

Using the local query method allows you to drill down into specific details and obtain precise answers based on the context of the entities involved. After executing this command, review the output to see the responses generated by GraphRAG for your local query.

**Output:**

```
SUCCESS: Local Search Response: Exporting goods from India involves a comprehensive process that includes understanding legal requirements, obtaining necessary documentation, and ensuring compliance with both Indian and international trade regulations. Here's a detailed guide on how to export from India, based on the information provided in the data tables:

### 1. Understand Export Regulations and Policies

Firstly, familiarize yourself with India's export regulations and policies. The Foreign Trade Policy (FTP) outlines the legal and procedural guidelines for exports and imports in India. It's crucial to understand the FTP's provisions, as it regulates the trade environment [Data: Entities (85)].

### 2. Obtain Necessary Documentation

Exporters must prepare and obtain several key documents to comply with both Indian and international regulations:

- **Bill of Lading/Airway Bill**: Acts as a contract and receipt between the shipper and the carrier [Data: Sources (11)].
- **Commercial Invoice cum Packing List**: Merges two essential documents into one, detailing the product and its destination [Data: Sources (11)].
- **Shipping Bill/Bill of Export**: Required for customs clearance, issued by Indian Customs Electronic Gateway (ICEGATE) [Data: Sources (11)].
- **Certificate of Origin**: Indicates the country where the goods were manufactured [Data: Sources (11)].
- **Export License/IEC (Import-Export Code)**: Necessary for all exporters; it's a unique code issued by the DGFT [Data: Sources (11)].

### 3. Packaging and Labeling

Proper packaging and labeling are critical to ensure that goods are protected during transit and comply with international standards. Packaging should protect against breakage, moisture, and other risks, while labeling should provide essential information like contents, destination, and handling instructions [Data: Sources (11)].

### 4. Financial and Banking Arrangements

Exporters need to secure financing for their export operations. This can include obtaining pre-shipment and post-shipment finance from commercial banks at concessional rates. It's also important to present the export documents to the bank within 21 days for payment processing [Data: Sources (11)].

### 5. Engage with Export Promotion Councils and Authorities

Export Promotion Councils (EPCs) and authorities like the Agricultural and Processed Food Products Export Development Authority (APEDA) play a significant role in supporting exporters. They offer workshops, advice, and financial guidance to enhance global market contributions [Data: Entities (87), Relationships (108, 112, 144)].

### 6. Compliance with Export Obligations

Certain schemes, like the Advance Authorisation Scheme, require exporters to fulfill specific export obligations within a stipulated period. Understanding and complying with these obligations is crucial for exporters to benefit from such schemes [Data: Relationships (294)].

### 7. Utilize E-Commerce Platforms

For exporters focusing on goods and services sold via the internet, it's essential to comply with the regulatory framework provided by the DGFT and RBI. This includes ensuring that payment channels are authorized and meet the guidelines specified by the RBI [Data: Relationships (69, 176)].

### Conclusion

Exporting from India requires careful planning and adherence to a range of regulatory requirements. By following the steps outlined above and leveraging the support available from government and trade bodies, exporters can successfully navigate the complexities of international trade.
```

You can explore all the source code in our shared Colab notebook, available [here](https://colab.research.google.com/drive/14xuJ9dbdCEILRSg74fOHJijS699JW-J1?usp=sharing).

## Conclusion

GraphRAG provides significant advantages in generating deeper insights by utilizing graph nodes for complex reasoning, an area where traditional retrieval-augmented generation (RAG) may face limitations. However, this added depth comes with higher computational costs, as GraphRAG requires multiple calls to large language models (LLMs), leading to increased token usage.

Both GraphRAG and traditional RAG possess distinct strengths and trade-offs, making them suitable for different scenarios. For instance, GraphRAG shines when tackling complex, multi-step queries that demand a thorough understanding of context and relationships, while traditional RAG tends to be more efficient for straightforward tasks that require quick and direct answers.

To optimize performance and mitigate costs, various strategies can be employed. For GraphRAG, leveraging a local LLaMA model can be a promising alternative, potentially reducing dependency on cloud-based services. Meanwhile, traditional RAG can be enhanced through a range of optimization techniques. Ultimately, the optimal approach will depend on your specific use case and the empirical results of your experiments

For more information on optimizing both methods, check out our [vector recipe](https://github.com/lancedb/vectordb-recipes) repository for detailed techniques.
```

---

## ðŸ“„ æ–‡ä»¶: grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2.md

---

```md
---
title: RAG with GRPO Fine-Tuned Reasoning Model
date: 2025-03-24
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2/grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2.png
description: Explore rag with grpo fine-tuned reasoning model with practical insights and expert guidance from the LanceDB team.
author: Mahesh Deshwal
author_avatar: "/assets/authors/community.jpg"
author_bio: "ML Engineer and researcher specializing in reinforcement learning, fine-tuning techniques, and practical AI applications."
---

{{< admonition info "ðŸ’¡ Community Post" >}}
This is a community post by Mahesh Deshwal
{{< /admonition >}}

**Group Relative Policy Optimization** is the series of RL techniques for LLMs to guide them to specific goals. The process of creating a smart model these days is something like this:

1. Pre Training a model on a HUGE corpus to get a pre-trained model
2. Use the model created above on your data (usually Instructions or Que-Ans format) to fine-tune in an SFT (Supervised fine-tuning) manner.

Now let's say a question can be answered in 100 different ways by the above model, all of them are correct. For example, the answer can be one word, sarcastic, politically incorrect, harmful, etc. If you have to **GUIDE** the model to a specific set of rules for generation, you use the RL (Reinforcement Learning), where the popular techniques are PPO and DPO, and with DeepSeek, there comes GRPO.

### How does it work, roughly?

While using PPO or DPO, you don't have a ground truth label for the data. You generate more than 1 answer to a question. You then use a dedicated model (generally an LLM) that scores the answers and then based on the scores, you choose which answer is good. So it is basically selecting and rejecting similar answers based on some criteria.

![GRPO Process Overview](/assets/blog/grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2/image_fx_.jpg.png)

But in GRPO, there is no Critique or Scorer. What they do is simply decide multiple rules (python functions) to give scores to each answer. Then generate `G` responses to a question, calculate all the scores, and convert them to one single weighted score which is the `Normalized Z score`. Then given if an answer's score is more or less than that Z score, they accept or reject. Let's understand each and every component of the GRPO equation with intuition and line-by-line code.

### Components of GRPO

![GRPO Components](/assets/blog/grpo-understanding-and-fine-tuning-the-next-gen-reasoning-model-2/Screenshot-2025-03-06-at-1.30.07-AM--1--1.png)

Code snippets below are referred from [HuggingFace `GRPOTrainer`](https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py)

- **Overall Objective (ð’¥GRPO(Î¸))**: This is the GRPO Loss function we need to improve our model. Each components of this loss are defined below.

- **Question Sampling (q âˆ¼ P(Q))**: A question (AKA PROMPT) is randomly chosen from a pool of questions. This represents the problem or prompt that the model needs to answer.

- **Group of Outputs ({oi} with G outputs)**: For each question, we generate several answers (G in total, GRPOTrainer uses 8 by default) using the model from the previous training step. These outputs are like multiple attempts at answering the same question. This is the Crux of the equation because we don't have a Ground truth so we rate THESE answers based on some pre defined function and compute the score for EACH of the Answer and then see if an answer is above or below Average score of group

```python
self.sampling_params = SamplingParams(n=args.num_generations)
```

- **Reference Policy (Ï€ref)**: This is the model that we fine-tuned using SFT. It serves as a base to ensure that the model's updates do start giving weird answers.

```python
self.ref_model = AutoModelForCausalLM.from_pretrained(model_id, **model_init_kwargs)
```

- **Current Policy (Ï€Î¸)**: This is the model we are actively training and updating. During output generation, we compare how the current policy behaves relative to the old policy "LOGIT by LOGIT"Loss is computed Logit vise

```python
model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)
```

- **Old Policy (Ï€Î¸_old)**: Same model as above BUT this is the version of from the previous epoch. It also has "G" answers to the Prompts but for the "PREVIOUS" epoch

- **Token Advantage (Ä¤Ai,t)**: This term represents the normalized reward (or advantage) associated with "EACH TOKEN". It indicates how much better or worse the generation related to Z score. One thing is that score is Given for final Generation BUT it is replicated as same for EACH Logit in the answer

```python
advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)
```

- **Clipping Range (1 â€“ Îµ to 1 + Îµ)**: The ratio between the current and old policies is clipped within this range to prevent large, unstable updates. This ensures that each update is moderate and controlled.

```python
torch.clamp(coef_1, 1 - self.epsilon, 1 + self.epsilon)
```

- **Scaling Factor (Î²)**: This factor scales the penalty from the KL divergence. It controls how strongly the model is regularized.

```python
per_token_loss = per_token_loss + self.beta * per_token_kl
```

- **KL Divergence Penalty (ð”»KL[Ï€Î¸ || Ï€ref])**: This term measures the difference between the current policy and the reference policy. By penalizing large differences, it helps keep the updated model close to the original fine-tuned model, ensuring that improvements are made without losing previously learned skills. It is also implemented **PER TOKEN**

```python
per_token_kl = (torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1)
```

### Now let's fine tune some model on GRPO for Token Diversity and Length Constraints

You can use any model and tool of your choice but we'll use `PEFT: LoRA` using `HuggingFace TRL` to do it all on 1 GPU on Colab

```python
!pip install -qqq unsloth vllm --progress-bar off

import torch
import re
from datasets import load_dataset, Dataset
from trl import GRPOConfig, GRPOTrainer
import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer


max_seq_length = 512 # Can increase for longer reasoning traces
lora_rank = 8 # Larger rank = smarter, but slower

# Load model
model_id = "HuggingFaceTB/SmolLM-135M-Instruct" # VERY Small model
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype="auto",
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Load LoRA
lora_config = LoraConfig(
    task_type="CAUSAL_LM",
    r=8,
    lora_alpha=32,
    target_modules="all-linear",
)
model = get_peft_model(model, lora_config)
print(model.print_trainable_parameters())


# Load dataset
dataset = load_dataset("mlabonne/smoltldr")
print(dataset)
```

#### Reward Functions

Let's define some Reward functions. These will be applied to Each and Every generation and then used as weighted sum (default to 1 for each) to get advantage. You an use ANY reward function based on your choice but I'm just writing generic ones here

```python
def reward_len(completions, **kwargs):
    "Function to give rewards based on Length of the Answer"
    return [-abs(50 - len(completion)) for completion in completions]

def reward_token_diversity(completions, **kwargs):
    "Rewards completions with a higher ratio of unique tokens"
    rewards = []
    for completion in completions:
        tokens = completion.split()
        if tokens:
            diversity = len(set(tokens)) / len(tokens)
            rewards.append(diversity * 100)  # scaling factor for reward
        else:
            rewards.append(0)
    return rewards
```

Now just off to training the model as usual with very few modifications

```python
max_prompt_length = 512


training_args = GRPOConfig(
    learning_rate = 5e-6,
    adam_beta1 = 0.9,
    adam_beta2 = 0.99,
    weight_decay = 0.1,
    warmup_ratio = 0.1,
    lr_scheduler_type = "cosine",
    optim = "paged_adamw_8bit",
    logging_steps = 1,
    per_device_train_batch_size = 6,
    gradient_accumulation_steps = 1, # Increase to 4 for smoother training
    num_generations = 6, # Decrease if out of memory
    max_prompt_length = max_prompt_length,
    max_completion_length = 128,
    # num_train_epochs = 1, # Set to 1 for a full training run
    max_steps = 250,
    save_steps = 250,
    max_grad_norm = 0.1,
    report_to = "none", # Can use Weights & Biases
    output_dir = "outputs",
)


trainer = GRPOTrainer(
    model = model,
    processing_class = tokenizer,
    reward_funcs = [
        reward_len, reward_token_diversity,
    ],
    args = training_args,
    train_dataset=dataset["train"],
)

trainer.train()
```

### Model Inference

```python
from transformers import pipeline

prompt = """
# About the GOAT:

Lionel AndrÃ©s "Leo" Messi[note 1] (Spanish pronunciation: [ljoËˆnel anËˆdÉ¾es Ëˆmesi] â“˜; born 24 June 1987) is an Argentine professional footballer who plays as a forward for and captains both Major League Soccer club Inter Miami and the Argentina national team. Widely regarded as the greatest player of all time, Messi set numerous records for individual accolades won throughout his professional footballing career such as eight Ballon d'Or awards and four the Best FIFA Men's Player awards.[note 2] He is the most decorated player in the history of professional football having won 45 team trophies,[note 3] including twelve league titles, four UEFA Champions Leagues, two Copa AmÃ©ricas, and one FIFA World Cup. Messi holds the records for most European Golden Shoes (6), most goals in a calendar year (91), most goals for a single club (672, with Barcelona), most goals (474), hat-tricks (36) and assists (192) in La Liga, most assists (18) and goal contributions (32) in the Copa AmÃ©rica, most goal contributions (21) in the World Cup, most international appearances (191) and international goals (112) by a South American male, and the second-most in the latter category outright. A prolific goalscorer and creative playmaker, Messi has scored over 850 senior career goals and has provided over 380 assists for club and country.[16]

Born in Rosario, Argentina, Messi relocated to Spain to join Barcelona at age 13, and made his competitive debut at age 17 in October 2004. He gradually established himself as an integral player for the club, and during his first uninterrupted season at age 22 in 2008â€“09 he helped Barcelona achieve the first treble in Spanish football. This resulted in Messi winning the first of four consecutive Ballons d'Or, and by the 2011â€“12 season he would set La Liga and European records for most goals in a season and establish himself as Barcelona's all-time top scorer. The following two seasons, he finished second for the Ballon d'Or behind Cristiano Ronaldo, his perceived career rival. However, he regained his best form during the 2014â€“15 campaign, where he became the all-time top scorer in La Liga, led Barcelona to a historic second treble, and won a fifth Ballon d'Or in 2015. He assumed Barcelona's captaincy in 2018 and won a record sixth Ballon d'Or in 2019. During his overall tenure at Barcelona, Messi won a club-record 34 trophies, including ten La Liga titles and four Champions Leagues, among others. Financial difficulties at Barcelona led to Messi signing with French club Paris Saint-Germain in August 2021, where he would win the Ligue 1 title during both of his seasons there. He joined Major League Soccer club Inter Miami in July 2023.

An Argentine international, Messi is the national team's all-time leading goalscorer and most-capped player. His style of play as a diminutive, left-footed dribbler drew career-long comparisons with compatriot Diego Maradona, who described Messi as his successor. At the youth level, he won the 2005 FIFA World Youth Championship and gold medal in the 2008 Summer Olympics. After his senior debut in 2005, Messi became the youngest Argentine to play and score in a World Cup in 2006. Assuming captaincy in 2011, he then led Argentina to three consecutive finals in the 2014 FIFA World Cup, the 2015 Copa AmÃ©rica and the Copa AmÃ©rica Centenario, all of which they would lose. After initially announcing his international retirement in 2016, he returned to help his country narrowly qualify for the 2018 FIFA World Cup, which they would exit early. Messi and the national team finally broke Argentina's 28-year trophy drought by winning the 2021 Copa AmÃ©rica, which helped him secure his seventh Ballon d'Or that year. He then led Argentina to win the 2022 Finalissima, as well as the 2022 FIFA World Cup, his country's third overall world championship and first in 36 years. This followed with a record-extending eighth Ballon d'Or in 2023, and a victory in the 2024 Copa AmÃ©rica.
"""

messages = [
    {"role": "user", "content": prompt},
]


merged_model = trainer.model.merge_and_unload()
generator = pipeline("text-generation", model= merged_model, tokenizer=tokenizer)


generated_text = generator(messages)

print(generated_text)
```

### Using the model as a RAG with LanceDB

Smooth as a ðŸ° ðŸš¶.

Let's take a look at some interesting use cases. You are a screenplay writer and you have an idea about a movie but don't want the cliche scenario. Maybe you don't care about Older movies so you fetch the movies from the last 30 years or so using `PREFILTER`. Given we had lexical diversity AND length based rewards in GRPO, we're now not only creative in words but always to the point too. So let's test our ideas of "what not to write about" as:

```python
!pip install lancedb sentence-transformers -qqq

import pandas as pd
from sentence_transformers import SentenceTransformer
import lancedb
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import get_registry
import torch


df = pd.read_csv("hf://datasets/vishnupriyavr/wiki-movie-plots-with-summaries/wiki_movie_plots_deduped_with_summaries.csv").sample(10)
df.rename(columns = {"Release Year": "release_year", "Title": "title", "Plot": "movie_plot"},  inplace = True)
df = df.loc[:, ["release_year", "title", "movie_plot"]]


db = lancedb.connect("/tmp/db")
model = get_registry().get("sentence-transformers").create(name="BAAI/bge-small-en-v1.5", device="cuda" if torch.cuda.is_available() else "cpu")

class TableSchema(LanceModel):
    movie_plot: str = model.SourceField()
    vector: Vector(model.ndims()) = model.VectorField()
    title: str
    release_year: int

table = db.create_table("movie_plots", schema=TableSchema)
table.add(data=df)

query = "What's common in movies where the protagonist turns out to be the bad guy"
context = "\n\n".join([f"## Movie- {i+1}:\n{item['movie_plot']}" for i,item in enumerate(table.search(query).where(f"release_year < {1990}").limit(5).to_list())])

# NOW you pass this Query + Context to the model and get to the point results

messages = [
    {"role": "user", "content": f"Given the Query: {query}\n\nAnd the context\n#Context{context}\n\nAnswer concisely but creatively"}
    ]


generated_text = generator(messages)

print(generated_text)
```

Would really love to know what your result was for this query with your fine-tuned model ðŸ˜„
```

---

## ðŸ“„ æ–‡ä»¶: guide-to-use-contextual-retrieval-and-prompt-caching-with-lancedb.md

---

```md
---
title: "Implement Contextual Retrieval and Prompt Caching with LanceDB"
date: 2024-10-08
author: LanceDB
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/guide-to-use-contextual-retrieval-and-prompt-caching-with-lancedb/preview-image.png
meta_image: /assets/blog/guide-to-use-contextual-retrieval-and-prompt-caching-with-lancedb/preview-image.png
description: "Unlock about implement contextual retrieval and prompt caching with lancedb. Get practical steps, examples, and best practices you can use now."
---

### Decrease chunk retrieval failure rate by 35% to 50%

Have you ever worked on a task where your users search in one language and the context is in another? Tasks like text to code, text to music composition, text to SQL etc etc. When you work on these tasks, you have strings like code, and textual context that do not make sense with BM-25 matching and the query has some semantic sense where you cannot convert it to code. Let's see how we can tackle this problem with contextual retrieval.
![Linus quote](/assets/blog/guide-to-use-contextual-retrieval-and-prompt-caching-with-lancedb/linus.jpg)

Let's start with implementing a basic RAG with a code search and generation!

You can follow along with Colab:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Contextual-RAG/Anthropic_Contextual_RAG.ipynb)
```bash
pip install -U openai lancedb einops sentence-transformers transformers datasets tantivy rerankers -qq
```

```bash
# Get the data
wget -P ./data/ https://raw.githubusercontent.com/anthropics/anthropic-cookbook/refs/heads/main/skills/contextual-embeddings/data/codebase_chunks.json
```

```python
# IMPORT
import os, re, random, json
import pandas as pd
from datasets import load_dataset
import torch
import gc
import lancedb
import openai
from lancedb.embeddings import get_registry
from lancedb.pydantic import LanceModel, Vector
from tqdm.auto import tqdm
from openai import OpenAI

pd.set_option('max_colwidth', 400)

OAI_KEY = "sk-....."  # Replace with your OpenAI Key
os.environ["OPENAI_API_KEY"] = OAI_KEY

gpt_client = OpenAI(api_key=OAI_KEY)  # For Context text generation

model = get_registry().get("sentence-transformers").create(
    name="BAAI/bge-small-en-v1.5", device="cuda"
)  # For embedding
```

Now let's chunk some data and create a DB!

```python
def load_raw_data(datapath='/content/data/codebase_chunks.json', debugging=False):
    with open(datapath, 'r') as f:
        dataset = json.load(f)
    if debugging:
        print("Debugging Mode: Using few doc samples only")
        dataset = dataset[:5]  # just use a sample only

    data = []
    num_docs = len(dataset)
    total_chunks = sum(len(doc['chunks']) for doc in dataset)

    with tqdm(total=num_docs, desc=f"Processing {total_chunks} chunks from {len(dataset)} docs") as pbar:
        for doc in dataset:  # Full document
            for chunk in doc['chunks']:  # Each document has multiple chunks
                data.append({
                    "raw_chunk": chunk['content'],  # We won't make embedding from this; we'll create new context based on chunk and full_doc
                    "full_doc": doc["content"],
                    'doc_id': doc['doc_id'],
                    'original_uuid': doc['original_uuid'],
                    'chunk_id': chunk['chunk_id'],
                    'original_index': chunk['original_index'],
                })
                pbar.update(1)

    return data

raw_chunks = load_raw_data(debugging=True)  # Use the first few documents only for the tutorial
```

Let's create a basic RAG that we do and search a query to see the result right!

```python
class VanillaDocuments(LanceModel):
    vector: Vector(model.ndims()) = model.VectorField()  # Default field
    raw_chunk: str = model.SourceField()  # Column to embed
    doc_id: str
    original_uuid: str
    chunk_id: str
    original_index: int
    full_doc: str

db = lancedb.connect("./db")
vanilla_table = db.create_table("vanilla_documents", schema=VanillaDocuments)

vanilla_table.add(raw_chunks)  # Ingest docs with auto-vectorization
vanilla_table.create_fts_index("raw_chunk")  # Create an FTS index for BM25 later
```

> **QUERY = "implement corpus management with event handling"**

```python
vanilla_table.search(QUERY, query_type="hybrid").\
            limit(3).\
            to_pandas().\
            drop(["vector", "original_uuid"], axis=1)
```

![Hybrid search results](/assets/blog/guide-to-use-contextual-retrieval-and-prompt-caching-with-lancedb/Screenshot-2024-10-07-at-12.52.08-PM.png)

Umm!!! Looks weird, Some random functions are coming up! You see, `doc_2_chunk_0` -> `doc_3_chunk_0`  it means that the retriever is giving two different modules as chunks and if you use it as context, your user, program, ROI is going to suffer. This happens because there was no docstring and retriever had no idea of which function is exactly doing the asked task.

### Solution? Contextual Retrieval!

![Contextual retrieval diagram](/assets/blog/guide-to-use-contextual-retrieval-and-prompt-caching-with-lancedb/Screenshot-2024-10-08-at-8.21.21-PM.png) Diagram from [anthropic's blog](https://www.anthropic.com/news/contextual-retrieval)

Let's take the same example of a text-to-code generation from above. What can you do to improve the issue above? You'd either go in the past and ask the developer building the code to write detailed docstring and comments. Another solution could be to generate a doc-string for the whole function and module. Well! you're very close to what Anthropic mentioned in their research. The idea here is to do these things:

1. For each document, make chunks (Nothing new. Just like Vanilla RAG)
2. For each Chunk you created, as an LLM create a context of that Chunk (You see this is new!)
3. Append that context to the original chunk
4. Create BM-25 and Vector Index based on those chunks for Hybrid Search (New to you? See this [blog](__GHOST_URL__/hybrid-search-rag-for-real-life-production-grade-applications-e1e727b3965a/) on hybrid search)
5. Search as usual!

```python
def create_context_prompt(full_document_text, chunk_text):
      prompt = f"""
    <document>
    {full_document_text}
    </document>

    Here is the chunk we want to situate within the whole document
    <chunk>
    {chunk_text}
    </chunk>

    Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.
    Answer only with the succinct context and nothing else.
    """
    return prompt, gpt_client.chat.completions.create(
        model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}]
    ).choices[0].message.content.strip()

for chunk in raw_chunks:
    prompt, response = create_context_prompt(chunk["full_doc"], chunk["raw_chunk"])
    chunk["prompt"] = prompt
    chunk["chunk_context"] = response
    chunk["chunk_with_context"] = chunk["chunk_context"] + "\n" + chunk["raw_chunk"]
```

### What does the above code portion do?

What the above code does is ask an LLM to create the context for the chunk. How But the chunks here are portion of the some code from file. How would it create the whole context if the code if from in between?

This is where the first part of prompt `<document>` comes in. Along with the chunk, we are passing the whole document (full code in our case) so that the LLM has access to the entire code and write context for the code portion we want.

Wait! the whole document along with the chunk? What? If a document has 20 Chunks, it means we'll be passing the whole document 20 times? ***Wouldn't it be too costly on the input tokens I send to any LLM?***
![Prompt caching diagram](/assets/blog/guide-to-use-contextual-retrieval-and-prompt-caching-with-lancedb/prompt-1.png)

## Prompt Caching to the rescue

For a bigger knowledge base containing millions of chunks, it becomes very expensive to append context for each chunk. With prompt caching, you only need to load a document into the cache once instead of passing it in for every chunk. By structuring your prompts effectively, you can save 50-90% on costs through this feature available from LLM providers. If the same prefix appears in prompts within 5-10 minutes, you wonâ€™t be charged for those tokens again. We designed the prompts so that the document cost is incurred just once, reducing expenses for all subsequent chunks.
![](__GHOST_URL__/content/images/2024/10/prompt-1.png)Prompt Caching example by OpenAI. [Link](https://platform.openai.com/docs/guides/prompt-caching)
Let's make the new table and see the results. Remember, here we are creating index from the `text` field which is basically the **Chunk Context + Original Chunk**

```python
class Documents(LanceModel):
    vector: Vector(model.ndims()) = model.VectorField()
    text: str = model.SourceField()  # Column to embed
    doc_id: str
    raw_chunk: str
    full_doc: str
    original_uuid: str
    chunk_id: str
    original_index: int

KEYS = ["raw_chunk", "full_doc", "doc_id", "original_uuid", "chunk_id", "original_index"]

context_documents = []
for chunk in raw_chunks:
    temp = {"text": chunk["chunk_with_context"]}  # (Chunk_Context_i + Chunk_i)
    for key in KEYS:
        temp[key] = chunk[key]
    context_documents.append(temp)

context_table = db.create_table("added_context_table", schema=Documents)

context_table.add(context_documents)  # ingest docs with auto-vectorization
context_table.create_fts_index("text")  # Create an FTS index for BM25 later
```

In the final step, we boosted performance by querying Contextual Retrieval with hybrid search. In traditional RAG, vector search often retrieves many chunks from the knowledge base â€” of varying relevance.

Let's search and see the difference

```python
context_table.search(QUERY, query_type="hybrid").\
            limit(3).\
            to_pandas().\
            drop(["vector", "original_uuid"], axis=1)
```

![Contextual hybrid search results](/assets/blog/guide-to-use-contextual-retrieval-and-prompt-caching-with-lancedb/Screenshot-2024-10-07-at-1.12.23-PM.png)

As you can see from the above, both the top-2 codes are from the same document `doc_2_chunk_0` -> `doc_2_chunk_3`

You can evaluate them automatically using the [eval dataset](https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings/data) here.

Run it yourself:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Contextual-RAG/Anthropic_Contextual_RAG.ipynb)
## Rules of thumb

When implementing Contextual Retrieval, keep the following factors in mind:

1. **Chunk Boundaries**: How you split documents into chunks can affect retrieval performance. Pay attention to chunk size, boundaries, and overlap.
2. **Embedding Model**: While Contextual Retrieval improves performance across various models, some like Gemini and OpenAI may benefit more.
3. **Custom Prompts**: A generic prompt works well, but tailored prompts for your specific domainâ€”such as including a glossaryâ€”can yield better results.
4. **Number of Chunks**: More chunks in the context window increase the chance of capturing relevant information, but too many can overwhelm the model. We found that using 20 chunks performed best, though itâ€™s good to experiment based on your needs.

## **Conclusion**

Using the combinations of contextual retrieval, prompt caching, and LanceDB hybrid search, differences in Naive Retrieval and Contextual Retrieval shows both the top-2 codes are from the same document. To make this search results even better and refined [**LanceDB Reranking API**](https://lancedb.github.io/lancedb/reranking/)can be used.
```

---

## ðŸ“„ æ–‡ä»¶: how-to-reduce-hallucinations-from-llm-powered-agents-using-long-term-memory-72f262c3cc1f.md

---

```md
---
title: "Reduce Hallucinations from LLM-Powered Agents Using Long-Term Memory"
date: 2023-07-19
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/how-to-reduce-hallucinations-from-llm-powered-agents-using-long-term-memory-72f262c3cc1f/preview-image.png
meta_image: /assets/blog/how-to-reduce-hallucinations-from-llm-powered-agents-using-long-term-memory-72f262c3cc1f/preview-image.png
description: "Understand about reduce hallucinations from llm-powered agents using long-term memory. Get practical steps, examples, and best practices you can use now."
---

by Tevin Wang

## Introduction

A few weeks ago, I attended an AI Agents hackathon hosted in SF at the AGI house. There, I worked with a team on a medical multi-agent system that streamlined and automated a key clinical-research workflow â€” the collection and communication of clinical test data.

I learned that the development of these AI agent systems is extremely early stage, and although we have scaled exponentially in our AI tooling, infrastructure, and models, we have not seen many AI agents applied to real-world use cases. I believe that the biggest reason why is the** risk of hallucination**.

These systems may work in a large majority of runs, but hallucination and failure still happens. For many industries, like in the medical space, such risk is simply unacceptable. So, how can we work on improving our agents to reduce the risk of hallucination and increase probabilities of successful and safe run?

## Critique-based contexting

Weâ€™ve seen that using retrieval augmented generation can be a great way to add context to generative AI and reduce hallucination. I believe that we can use a similar concept within the context of AI agents. Letâ€™s say we have a simple fitness trainer agent that searches Google for information about a fitness routine for a particular person with specific interests. Additionally, such agent is able to review its past actions and develop critiques that could help improve further actions.

The key here is creating a context based on these critiques â€” what is called **critique-based contexting**. Critique-based contexting is when we use vector databases to store long-term memory of critiques for agents. For similar people with similar interests, we inserts past critique information into the context window to help improve the actions of the agent as well as ground such actions in relevant past results.

This system can improve its specialized answer by using long-term memory and thus generate better results. For instance, in the medical space, people with different backgrounds likely need different communication styles. By adding critique-based context, weâ€™ve enabled the system to better cater to those different backgrounds.

**Hereâ€™s a diagram of such system:**
![Critique-based contexting diagram](/assets/blog/how-to-reduce-hallucinations-from-llm-powered-agents-using-long-term-memory-72f262c3cc1f/Untitled--34-.png)
Critique-based contexting diagram, Credit: background by rawpixel.com
## Breakdown

1. A **LangChain agent **takes in client input that describes information about the client and the request. **LangChain **is an open source library/framework that simplifies building LLM-powered applications.
2. We use an **embedding model**, such as OpenAI `text-embedding-ada-002` model, to embed our client input into a vector embedding. **Vector embeddings** are high-dimensional representations of unstructured data generated from a trained model.
3. This embedding is now used to perform similarity search using [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) within a vector database, like LanceDB. **Vector databases **store vector embeddings.
4. We then retrieve these similar results, which comes with information about actions and critiques and **input them as context** into our LangChain agent.
5. The agent **decides, evaluates, and performs** its next actions based on this critique context.
6. Once itâ€™s done, it will evaluate and critique its actions. We take the embedding of the client input along with its actions and critiques to **insert into the vector database**.

## Fitness trainer example

Weâ€™ll now look at critique-based contexting via a simple example using LanceDB as a vector database.

LanceDB is a serverless vector database that makes data management for LLMs frictionless. It has multi-modal search, native Python and JS support, and many ecosystem integrations.

The full example is available in both Python (script and notebook) and JS [here](https://github.com/lancedb/vectordb-recipes/tree/main/examples/reducing_hallucinations_ai_agents/). Weâ€™ll be using Python in this post.

To start, you must have an OpenAI API key, which you can get for free by setting up an account [here](https://openai.com/blog/openai-api), and a SerpApi API key, which you can get for free by setting up an account [here](https://serpapi.com/).

Insert them as environment variables via a `.env` file:

```python
OPENAI_API_KEY = "<insert-key-here>"
SERPAPI_API_KEY = "<insert-key-here>"
```

Now letâ€™s install some required python libraries, as weâ€™ll be using OpenAI, LangChain, SerpApi for Google Search, and LanceDB. Weâ€™ll also need `dotenv` to retrieve our environment variables.

```python
pip install openai langchain google-search-results lancedb python-dotenv
```

Then, letâ€™s import the required libraries for our LangChain agent.

```python
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.chat_models import ChatOpenAI
```

Now letâ€™s import our environment variables via `load_dotenv()`.

```python
from dotenv import load_dotenv

load_dotenv()
```

Weâ€™ll now specify and connect to the path `data/agent-lancedb` to store our vector database.

```python
import lancedb

db = lancedb.connect("data/agent-lancedb")
```

To create embeddings out of the text, weâ€™ll call the OpenAI embeddings API (ada2 text embeddings model) to get embeddings.

```python
import openai

def embed_func(c):
    rs = openai.Embedding.create(input=c, engine="text-embedding-ada-002")
    return [record["embedding"] for record in rs["data"]]
```

Now, weâ€™ll create a LangChain tool that allows our agent to insert critiques, which uses a pydantic schema to guide the agent on what kind of results to insert.

In LanceDB the primary abstraction youâ€™ll use to work with your data is a **Table**. A Table is designed to store large numbers of columns and huge quantities of data! For those interested, a LanceDB is columnar-based, and uses Lance, an open data format to store data.

This tool will create a Table if it does not exist and store the relevant information (the embedding, actions, and critiques).

```python
from langchain.tools import tool
from pydantic import BaseModel, Field

class InsertCritiquesInput(BaseModel):
    info: str = Field(description="should be demographics or interests or other information about the exercise request provided by the user")
    actions: str = Field(description="numbered list of langchain agent actions taken (searched for, gave this response, etc.)")
    critique: str = Field(description="negative constructive feedback on the actions you took, limitations, potential biases, and more")

@tool("insert_critiques", args_schema=InsertCritiquesInput)
def insert_critiques(info: str, actions: str, critique: str) -> str:
    """Insert actions and critiques for similar exercise requests in the future."""
    table_name = "exercise-routine"
    if table_name not in db.table_names():
        tbl = db.create_table(table_name, [{"vector": embed_func(info)[0], "actions": actions, "critique": critique}])
    else:
        tbl = db.open_table(table_name)
        tbl.add([{"vector": embed_func(info)[0], "actions": actions, "critique": critique}])
    return "Inserted and done."
```

Similarly, letâ€™s create a tool for retrieving critiques. Weâ€™ll retrieve the actions and critiques from the top 5 most similar user inputs.

```python
class RetrieveCritiquesInput(BaseModel):
    query: str = Field(description="should be demographics or interests or other information about the exercise request provided by the user")

@tool("retrieve_critiques", args_schema=RetrieveCritiquesInput)
def retrieve_critiques(query: str) -> str:
    """Retrieve actions and critiques for similar exercise requests."""
    table_name = "exercise-routine"
    if table_name in db.table_names():
        tbl = db.open_table(table_name)
        results = tbl.search(embed_func(query)[0]).limit(5).select(["actions", "critique"]).to_df()
        results_list = results.drop("vector", axis=1).values.tolist()
        return "Continue with the list with relevant actions and critiques which are in the format [[action, critique], ...]:\n" + str(results_list)
    else:
        return "No info, but continue."
```

Letâ€™s now use LangChain to load our tools in. This includes our custom tools as well as a Google Search tool that uses SerpApi. We will use OpenAIâ€™s `gpt-3.5-turbo-0613` as our LLM.

```python
llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613")
tools = load_tools(["serpapi"], llm=llm)
tools.extend([insert_critiques, retrieve_critiques])
```

Before we run our agent, letâ€™s create a function that defines our prompt that we pass in to the agent, which allows us to pass in client information.

```python
def create_prompt(info: str) -> str:
    prompt_start = (
        "Please execute actions as a fitness trainer based on the information about the user and their interests below.\n\n"
        + "Info from the user:\n\n"
    )
    prompt_end = (
        "\n\n1. Retrieve using user info and review the past actions and critiques if there is any\n"
        + "2. Keep past actions and critiques in mind while researching for an exercise routine with steps which we respond to the user\n"
        + "3. Before returning the response, it is of upmost importance to insert the actions you took (numbered list: searched for, found this, etc.) and critiques (negative feedback: limitations, potential biases, and more) into the database for getting better exercise routines in the future. \n"
    )
    return prompt_start + info + prompt_end
```

Finally, letâ€™s create our run_agent function. Weâ€™ll use the `STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION` agent in order to allow us to use multi-input tools (since we need to add client input, actions, and critiques as arguments when inserting critiques).

```python
def run_agent(info):
    agent = initialize_agent(
        tools,
        llm,
        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
    )
    agent.run(input=create_prompt(info))
```

Letâ€™s run. Feel free to use your own input!

Notice that in the first run, there wouldnâ€™t be any critiques yet, since the database is empty. After the first run, critiques should appear.

```python
run_agent("My name is Tevin, I'm a 19 year old university student at CMU. I love running.")
```

Here are the results for a particular run when the context has been populated by previous runs:

```python
> Entering new  chain...
Action:
{
  "action": "retrieve_critiques",
  "action_input": "Tevin, 19, university student, running"
}
Observation: List with relevant actions and critiques which are in the format [[action, critique], ...]:
[["Searched for 'exercise routine for running enthusiasts', found 'The Ultimate Strength Training Plan For Runners: 7 Dynamite Exercises'", 'The routine provided is focused on strength training, which may not be suitable for all runners. It would be helpful to include some cardiovascular exercises and flexibility training as well.', 0.04102211445569992], ["Searched for 'exercise routine for running enthusiasts', found 'The Ultimate Strength Training Plan For Runners: 7 Dynamite Exercises'", 'The routine provided is focused on strength training, which may not be suitable for all runners. It would be helpful to include some cardiovascular exercises and flexibility training as well.', 0.04102211445569992], ["Searched for 'exercise routine for runners', found 'Core Work for Runners: 6 Essential Exercises'", 'The routine includes core exercises, which are beneficial for runners. However, it would be helpful to also include cardiovascular exercises and flexibility training.', 0.19422659277915955]]
Thought:Based on the user's information and interests, I need to research and provide an exercise routine that is suitable for a 19-year-old university student who loves running. I should also keep in mind the past actions and critiques to ensure a well-rounded routine. After that, I will insert the actions and critiques into the database for future reference.

Action:
{
  "action": "Search",
  "action_input": "exercise routine for university students who love running"
}
Observation: Work with maximum effort and intensity for 20 seconds and take an active rest for 10 seconds. Then when you are ready, begin again.
Thought:Based on my search, I found an exercise routine that may be suitable for you as a 19-year-old university student who loves running. The routine involves high-intensity interval training (HIIT), which is a great way to improve cardiovascular fitness and burn calories. Here's the routine:

1. Warm-up: Start with a 5-minute jog or brisk walk to warm up your muscles.
2. High Knees: Stand in place and lift your knees as high as possible, alternating legs. Do this for 20 seconds.
3. Active Rest: Take a 10-second break, walking or jogging in place.
4. Jumping Jacks: Stand with your feet together and arms by your sides. Jump your feet out to the sides while raising your arms overhead. Jump back to the starting position. Do this for 20 seconds.
5. Active Rest: Take a 10-second break, walking or jogging in place.
6. Burpees: Start in a standing position, then squat down and place your hands on the floor. Kick your feet back into a push-up position, then quickly bring them back in and jump up explosively. Do this for 20 seconds.
7. Active Rest: Take a 10-second break, walking or jogging in place.
8. Mountain Climbers: Get into a push-up position, then alternate bringing your knees in towards your chest as if you're climbing a mountain. Do this for 20 seconds.
9. Active Rest: Take a 10-second break, walking or jogging in place.
10. Repeat: Repeat the entire circuit (steps 2-9) for a total of 3-4 rounds.
11. Cool Down: Finish with a 5-minute jog or brisk walk to cool down and stretch your muscles.

Remember to listen to your body and modify the exercises as needed. Stay hydrated and take breaks if necessary. This routine can be done 2-3 times per week, alternating with your running sessions.

I hope you find this routine helpful! Let me know if you have any other questions.

Action:
{
  "action": "insert_critiques",
  "action_input": {
    "info": "Tevin, 19, university student, running",
    "actions": "Searched for 'exercise routine for university students who love running', found HIIT routine",
    "critique": "The routine provided is focused on HIIT, which may not be suitable for all university students. It would be helpful to include some strength training exercises and flexibility training as well."
  }
}

Observation: Inserted and done.
Thought:I have provided an exercise routine that involves high-intensity interval training (HIIT), which is suitable for a 19-year-old university student who loves running. However, I should note that the routine may not be suitable for all university students, as it focuses on HIIT. It would be beneficial to include some strength training exercises and flexibility training as well.

I have inserted the actions and critiques into the database for future reference. This will help improve the exercise routines provided in the future.

Let me know if there's anything else I can assist you with!

> Finished chain.
```

We have retrieved critiques related to past strength trainings for runners, such as:

```
    ["Searched for 'exercise routine for running enthusiasts', found 'The Ultimate Strength Training Plan For Runners: 7 Dynamite Exercises'", 'The routine provided is focused on strength training, which may not be suitable for all runners. It would be helpful to include some cardiovascular exercises and flexibility training as well.', 0.04102211445569992]
```
The critique given was that strength training might not be suitable for all runners, so the agent proceeded with a cardiovascular fitness routine that the agent thought was suitable:
```
    Based on my search, I found an exercise routine that may be suitable for you as a 19-year-old university student who loves running. The routine involves high-intensity interval training (HIIT), which is a great way to improve cardiovascular fitness and burn calories. Here's the routine:

    1. Warm-up: Start with a 5-minute jog or brisk walk to warm up your muscles.
    2. High Knees: Stand in place and lift your knees as high as possible, alternating legs. Do this for 20 seconds.
    3. Active Rest: Take a 10-second break, walking or jogging in place.
    4. Jumping Jacks: Stand with your feet together and arms by your sides. Jump your feet out to the sides while raising your arms overhead. Jump back to the starting position. Do this for 20 seconds.
    5. Active Rest: Take a 10-second break, walking or jogging in place.
    6. Burpees: Start in a standing position, then squat down and place your hands on the floor. Kick your feet back into a push-up position, then quickly bring them back in and jump up explosively. Do this for 20 seconds.
    7. Active Rest: Take a 10-second break, walking or jogging in place.
    8. Mountain Climbers: Get into a push-up position, then alternate bringing your knees in towards your chest as if you're climbing a mountain. Do this for 20 seconds.
    9. Active Rest: Take a 10-second break, walking or jogging in place.
    10. Repeat: Repeat the entire circuit (steps 2-9) for a total of 3-4 rounds.
    11. Cool Down: Finish with a 5-minute jog or brisk walk to cool down and stretch your muscles.
```
It then inserted some critiques backed into the database, now asking for flexibility and strength training.
```
    The routine provided is focused on HIIT, which may not be suitable for all university students. It would be helpful to include some strength training exercises and flexibility training as well.
```
You can see how when this process is refined, it can really help an agent improve its ability to perform the request and ground its request based on past results!

## Conclusion

I hope you got a chance to learn a bit more about critique-based contexting. Keep in mind that the use of critiques in prompt engineering is still very early stage and difficult to work with. However, the potential is there, and I believe AI agents can used in some really cool ways to impact society. Hereâ€™s a summary of this article:

- **Critique-based contexting** is the use of past critiques and past actions of similar in the context window of LLM prompts
- Such contexting can improve the actions taken by AI agents and **reduce hallucinations**
- We can use **vector databases **like LanceDB and **embedding models** like OpenAIâ€™s ada2 text embeddings model to store and retrieve these similar critiques for context

Thanks for reading! If you have questions, feedback, or want help using LanceDB in your app, donâ€™t hesitate to drop us a line at [contact@lancedb.com](mailto:contact@lancedb.com). And weâ€™d really appreciate your support in the form of a Github Star on our [LanceDB repo](https://github.com/lancedb/lancedb) â­.
```

---

## ðŸ“„ æ–‡ä»¶: hybrid-search-and-custom-reranking-with-lancedb-4c10a6a3447e.md

---

```md
---
title: "Hybrid Search and Custom Reranking with LanceDB"
date: 2024-02-19
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/hybrid-search-and-custom-reranking-with-lancedb-4c10a6a3447e/preview-image.png
meta_image: /assets/blog/hybrid-search-and-custom-reranking-with-lancedb-4c10a6a3447e/preview-image.png
description: "Combine keyword and vector search for higherâ€‘quality results with LanceDB. This post shows how to run hybrid search and compare rerankers (linear combination, Cohere, ColBERT) with code and benchmarks."
---

Search has historically been a complex problem in computer science. This is partly due to limitations in natural language or contextual understanding, and partly due to the absence of a universal ranking method that works well for all cases. Although the original premise still applies today, LLMs have vastly improved contextual reasoning and understanding. This allows searching for semantically similar points in a knowledge base and tests for ranking them based on a given context. Let's see how!

Broadly, this blog covers two classes of search â€” *Semantic* and *keyword-based* search.

### Semantic Search

Semantic Search or vector similarity search relies on vector embedding to search for the most relevant results. The intuition behind this method is that a well-trained deep learning model, which performs all internal operations (or â€œseesâ€ all data) as vector embeddings, has a good map of where a given query sits in its embedding space. Once the positionality of query embeddings is located, it finds the top-k nearest points to get the relevant search results.

![Embedding space illustration](/assets/blog/hybrid-search-and-custom-reranking-with-lancedb-4c10a6a3447e/1_qR4yGWg8LoO0g9vzLnrIkg.webp)

The position of an embedding in vector space captures some semantics of the data. Points close to each other in vector space are considered similar (or appear in similar contexts), and points far away are deemed dissimilar. This means that semantic search works on the extracted meaning of the given docs and query and not necessarily on the keywords themselves.

### **Keyword-based Search**

This is the well-known and traditional way of searching through a knowledge base. There are varied algorithms designed for some specific cases, but the general premise of keyword-based search is that it works on the lexical attributes of the query and not its semantics. This is useful in some cases, as we'll see next.

### Hybrid Search

![Hybrid search diagram](/assets/blog/hybrid-search-and-custom-reranking-with-lancedb-4c10a6a3447e/1_Zh4Jju6uiCYFO9HHvO5sIA.webp)

Hybrid Search is a broad (often misused) term. It can mean anything from combining multiple methods for searching, to applying ranking methods to better sort the results. In this blog, we use the definition of â€œhybrid searchâ€ to mean using a combination of keyword-based and vector search.

## The challenge of (re)ranking search results

Once you have a group of the most relevant search results from multiple search sources, you'd likely standardize the score and rank them accordingly. This process can also be seen as another independent step â€” reranking.

There are two approaches for reranking search results from multiple sources.

- **Score-based**: Calculate final relevance scores based on a weighted linear combination of individual search algorithm scores. Example â€” Weighted linear combination of semantic search & keyword-based search results.
- **Relevance-based**: Discards the existing scores and calculates the relevance of each search result â€” query pair. Example â€” Cross Encoder models

Even though there are many strategies for reranking search results, none works for all cases. Moreover, evaluating them itself is a challenge.

## Compare search and reranking methods easily with LanceDB

Hereâ€™s some evaluation numbers from experiment comparing these re-rankers on about 800 queries. It is modified version of an evaluation script from [llama-index](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb) that measures hit-rate at top-k.

**With OpenAI ada2 embeddings**

Vector Search baseline â€” `0.64`

![OpenAI ada2 evaluation - baseline](/assets/blog/hybrid-search-and-custom-reranking-with-lancedb-4c10a6a3447e/1*sIDZfRsg8CctGotZfZwq7w.png)

Vector Search baseline â€” `0.59`

![OpenAI ada2 evaluation - reranked](/assets/blog/hybrid-search-and-custom-reranking-with-lancedb-4c10a6a3447e/1*ItP95MwqIe43veGsCqe24Q.png)

With this context, let us now test hybrid search with different rerankers using LanceDB.

The dataset we'll query is Airbnb's financial report, which can be found [here](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001559720/8a9ebed0-815a-469a-87eb-1767d21d8cec.pdf). Itâ€™s inspired by GitHub user virattt's [GitHub gist](https://gist.github.com/virattt/9f41b2cd1e8f65e672127999ad443f15) comparing speeds of reranker models.

> Note: The Colab to reproduce the results below can be found here.

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/lancedb/blob/main/docs/src/notebooks/hybrid_search.ipynb)

## Ingestion

We'll start by ingesting the entire dataset by recursively splitting the text into smaller chunks and embedding it using the OpenAI embedding model.

Using LangChain makes this a single line process. Here's how your LanceDB table would look.

```python
table = ...
LanceDB.from_documents(docs, embedding_function, connection=table)
table.to_pandas().head()
```

![LanceDB table preview](/assets/blog/hybrid-search-and-custom-reranking-with-lancedb-4c10a6a3447e/1*Z8UCr_bNo3nCmKCafDHGlQ.png)
In the original example, tvectorDB is queried to retrieve the specific reasons as to why Airbnb's operating costs were high for that year.

```python
query = "What are the specific factors contributing to Airbnb's increased operational expenses in the last fiscal year?"
```

## Semantic Search

Let's first take a look at the semantic search results:

```python
vector_query = embedding_function.embed_query(query)
docs = table.search(vector_query).limit(5).to_pandas()

    In addition, the number of listings on Airbnb may decline as a result of a number of other factors affecting Hosts, including: the COVID-19 pandemic; enforcement or threatened
    enforcement of laws and regulations, including short-term occupancy and tax laws; private groups, such as homeowners, landlords, and condominium and neighborhood
    associations, adopting and enforcing contracts that prohibit or restrict home sharing; leases, mortgages, and other agreements, or regulations that purport to ban or otherwise restrict
    home sharing; Hosts opting for long-term rentals on other third-party platforms as an alternative to listing on our platform; economic, social, and political factors; perceptions of trust
    and safety on and off our platform; negative experiences with guests, including guests who damage Host property, throw unauthorized parties, or engage in violent and unlawful

    Made Possible by Hosts, Strangers, AirCover, Categories, and OMG marketing campaigns and launches, a $67.9 million increase in our search engine marketing and advertising
    spend, a $25.1 million increase in payroll-related expenses due to growth in headcount and increase in compensation costs, a $22.0 million increase in third-party service provider
    expenses, and a $11.1 million increase in coupon expense in line with increase in revenue and launch of AirCover for guests, partially offset by a decrease of $22.9 million related to
    the changes in the fair value of contingent consideration related to a 2019 acquisition.
    General and Administrative
    2021 2022 % Change
    (in millions, except percentages)
    General and administrative $ 836 $ 950 14 %
    Percentage of revenue 14 % 11 %
    General and administrative expense increased $114.0 million, or 14%, in 2022 compared to 2021, primarily due to an increase in other business and operational taxes of $41.3

    Our success depends significantly on existing guests continuing to book and attracting new guests to book on our platform. Our ability to attract and retain guests could be materially
    adversely affected by a number of factors discussed elsewhere in these â€œRisk Factors,â€ including:
    ...

    Table of Contents
    Airbnb, Inc.
    Consolidated Statements of Operations
    (in millions, except per share amounts)
    Year Ended December 31,
    2020 2021 2022
    Revenue $ 3,378 $ 5,992 $ 8,399
    Costs and expenses:
    Cost of revenue 876 1,156 1,499
    ...
```

The latency is very reasonable as can be seen below:

> 2.62 ms Â± 107 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)

These are pretty relevant results in the context of "operational expenses," but this can further be improved by reranking the top few results returned. Results 2 and 3 are more relevant as "specific factors that increase operational expense" , and should potentially appear on top.

Let us now compare this with hybrid search and reranking applied.

**Hybrid Search**

In LanceDB, you can easily run a hybrid search by passing the search string and simply setting the `query_type="hybrid"` keyword argument. The string query is automatically vectorized if you've passed the `EmbeddingFunction`.

```python
docs = table.search(query, query_type="hybrid").to_pandas()
```

> 71 ms Â± 25.4 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)

The results are shown below:

```
    â€œInitial Delivery Dateâ€); provided that the Pricing Certificate for any fiscal year may be delivered on any date following the Initial Delivery
    Date that is prior to the date that is 365 days following the last day of the preceding fiscal year, so long as such Pricing Certificate includes a
    certification that delivery of such Pricing Certificate on or before the Initial Delivery Date was not possible because (i) the information
    required to calculate the KPI Metrics for such preceding fiscal year was not available at such time or (ii) the report of the KPI Metrics Auditor,
    if relevant, was not available at such time (the date of the Administrative Agentâ€™s receipt thereof, each a â€œPricing Certificate Dateâ€). Upon
    delivery of a Pricing Certificate in respect of a fiscal year, (i) the Applicable Rate for the Loans incurred by the Borrower shall be increased or
    decreased (or neither increased nor decreased), as applicable, pursuant to the Sustainability Margin Adjustment as set forth in the KPI Metrics

    In addition, the number of listings on Airbnb may decline as a result of a number of other factors affecting Hosts, including: the COVID-19 pandemic; enforcement or threatened
    enforcement of laws and regulations, including short-term occupancy and tax laws; private groups, such as homeowners, landlords, and condominium and neighborhood
    associations, adopting and enforcing contracts that prohibit or restrict home sharing; leases, mortgages, and other agreements, or regulations that purport to ban or otherwise restrict
    home sharing; Hosts opting for long-term rentals on other third-party platforms as an alternative to listing on our platform; economic, social, and political factors; perceptions of trust
    and safety on and off our platform; negative experiences with guests, including guests who damage Host property, throw unauthorized parties, or engage in violent and unlawful

    (a) The Borrower may, at its election, deliver a Pricing Certificate to the Administrative Agent in respect of the most recently
    ended fiscal year, commencing with the fiscal year ended December 31, 2022, on any date prior to the date that is 270 days following the last
    day of such fiscal year (the
    -50-

    Made Possible by Hosts, Strangers, AirCover, Categories, and OMG marketing campaigns and launches, a $67.9 million increase in our search engine marketing and advertising
    spend, a $25.1 million increase in payroll-related expenses due to growth in headcount and increase in compensation costs, a $22.0 million increase in third-party service provider
    expenses, and a $11.1 million increase in coupon expense in line with increase in revenue and launch of AirCover for guests, partially offset by a decrease of $22.9 million related to
    ...

    Our success depends significantly on existing guests continuing to book and attracting new guests to book on our platform. Our ability to attract and retain guests could be materially
    adversely affected by a number of factors discussed elsewhere in these â€œRisk Factors,â€ including:
    â€¢events beyond our control such as the ongoing COVID-19 pandemic, other pandemics and health concerns, restrictions on travel, immigration, trade disputes, economic
    ...
```

When you don't specify any reranker, LanceDB uses the `LinearCombinationReranker` by default. This takes a weighted linear combination of the vector and full-text (keyword-based) search scores to produce the final relevance score.

**LinearCombinationReranker**

By default, this method assigns a vector search score a weighting factor of `0.7 `and full-text or FTS search score a weight of `0.3`.

```python
from lancedb.rerankers import LinearCombinationReranker

reranker = LinearCombinationReranker(weight=0.9)

docs = table.search(query, query_type="hybrid").rerank(reranker=reranker).to_pandas()
```

This method faster than other model-based rerankers, as no model inference or API requests are made during the reranking operation.

**CohereReranker**

This uses Cohere's [Rerank API](https://docs.cohere.com/docs/rerank-guide) to rerank the results. It accepts the reranking model name as a parameter. By default, it uses the `english-v3` model, but you can easily switch to a multi-lingual model if your data isnâ€™t just in English.

```python
from lancedb.rerankers import CohereReranker

reranker = CohereReranker()  # or CohereReranker(model_name="")
docs = table.search(query, query_type="hybrid").rerank(reranker=reranker).to_pandas()
```

> 605 ms Â± 78.1 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

**Results**: (Only showing first three lines of each doc for brevity)

```
    Increased operating expenses, decreased revenue, negative publicity, negative reaction from our Hosts and guests and other stakeholders, or other adverse impacts from any of the
    above factors or other risks related to our international operations could materially adversely affect our brand, reputation, business, results of operations, and financial condition.
    In addition, we will continue to incur significant expenses to operate our outbound business in China, and we may never achieve profitability in that market. These factors, combined
    with sentiment of the workforce in China, and Chinaâ€™s policy towards foreign direct investment may particularly impact our operations in China. In addition, we need to ensure that
    our business practices in China are compliant with local laws and regulations, which may be interpreted and enforced in ways that are different from our interpretation, and/or create

    Made Possible by Hosts, Strangers, AirCover, Categories, and OMG marketing campaigns and launches, a $67.9 million increase in our search engine marketing and advertising
    spend, a $25.1 million increase in payroll-related expenses due to growth in headcount and increase in compensation costs, a $22.0 million increase in third-party service provider
    expenses, and a $11.1 million increase in coupon expense in line with increase in revenue and launch of AirCover for guests, partially offset by a decrease of $22.9 million related to
    ...

    Table of Contents
    Airbnb, Inc.
    Consolidated Statements of Operations
    (in millions, except per share amounts)
    ...

    Our success depends significantly on existing guests continuing to book and attracting new guests to book on our platform. Our ability to attract and retain guests could be materially
    adversely affected by a number of factors discussed elsewhere in these â€œRisk Factors,â€ including:
    â€¢events beyond our control such as the ongoing COVID-19 pandemic, other pandemics and health concerns, restrictions on travel, immigration, trade disputes, economic
    ...

    In addition, the number of listings on Airbnb may decline as a result of a number of other factors affecting Hosts, including: the COVID-19 pandemic; enforcement or threatened
    enforcement of laws and regulations, including short-term occupancy and tax laws; private groups, such as homeowners, landlords, and condominium and neighborhood
    associations, adopting and enforcing contracts that prohibit or restrict home sharing; leases, mortgages, and other agreements, or regulations that purport to ban or otherwise restrict
    ...
```

Cohere Reranker better ranks the results as it is powered by a model designed for this task, i.e., calculating the relevance of given documents in relation to the given query. This is different from the linear combination as it relies only on the existing scores of individual search algorithms (vector search and FTS). Here are the relevance scores given to each of these docs by the cohere reranker. It is evident that the first two docs are highly relevant.

![Cohere Reranker relevance scores](/assets/blog/hybrid-search-and-custom-reranking-with-lancedb-4c10a6a3447e/1*0U6v0riXpGC2icbD4L0kIA.png)

## ColBERT Reranker

The ColBERT model powers Colbert Reranker. It uses the [Hugging Face implementation](https://huggingface.co/vjeronymo2/mColBERT) locally.

```python
from lancedb.rerankers import ColbertReranker

reranker = ColbertReranker()
docs = table.search(query, query_type="hybrid").rerank(reranker=reranker).to_pandas()["text"].to_list()
```

> 950 ms Â± 5.78 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

The results are below.

```
    Made Possible by Hosts, Strangers, AirCover, Categories, and OMG marketing campaigns and launches, a $67.9 million increase in our search engine marketing and advertising
    spend, a $25.1 million increase in payroll-related expenses due to growth in headcount and increase in compensation costs, a $22.0 million increase in third-party service provider
    expenses, and a $11.1 million increase in coupon expense in line with increase in revenue and launch of AirCover for guests, partially offset by a decrease of $22.9 million related to
    ...

    Our future revenue growth depends on the growth of supply and demand for listings on our platform, and our business is affected by general economic and business conditions
    worldwide as well as trends in the global travel and hospitality industries and the short and long-term accommodation regulatory landscape. In addition, we believe that our revenue
    growth depends upon a number of factors, including:
    ...

    Our success depends significantly on existing guests continuing to book and attracting new guests to book on our platform. Our ability to attract and retain guests could be materially
    adversely affected by a number of factors discussed elsewhere in these â€œRisk Factors,â€ including:
    â€¢events beyond our control such as the ongoing COVID-19 pandemic, other pandemics and health concerns, restrictions on travel, immigration, trade disputes, economic
    ...

    In addition, the number of listings on Airbnb may decline as a result of a number of other factors affecting Hosts, including: the COVID-19 pandemic; enforcement or threatened
    enforcement of laws and regulations, including short-term occupancy and tax laws; private groups, such as homeowners, landlords, and condominium and neighborhood
    associations, adopting and enforcing contracts that prohibit or restrict home sharing; leases, mortgages, and other agreements, or regulations that purport to ban or otherwise restrict
    ...

    Table of Contents
    Airbnb, Inc.
    Consolidated Statements of Operations
    ...
```

Colbert is similar to the Cohere reranker in that it doesn't use the existing scores from vector and full-text searches. The results are similar to Cohereâ€™s, except for the top 1 result.

LanceDB also supports other rerankers, including an experimental prompt-based [OpenAI reranker](https://github.com/lancedb/lancedb/blob/main/python/lancedb/rerankers/openai.py#L54), a Cross-encoder reranker, and others. Most of the results @ Top5 were found to be similar the ones shown above.

Subjectively, Cohere had a slight edge in getting the most relevant result (top 1), which the others missed, and considering it also included a network roundtrip, so the speed of 600ms is pretty impressive, too. This is also consistent with the evaluation results.

![Rerankers speed (ms) comparison](/assets/blog/hybrid-search-and-custom-reranking-with-lancedb-4c10a6a3447e/1*yWDh0Klw8Upsw1V54kkkdQ.png)

**Note**: Cohere is an API-based reranker, and the speed is subject to internet connection quality

## Bring your own reranker

Recognizing that ranking is a complex problem with varying requirements for every use case, hybrid Search in LanceDB is designed to be **very** flexible. You can easily plug in your own reranking logic. To do so, you can implement the base `Reranker` class as follows:

```python
from lancedb.rerankers import Reranker

class MyCustomReranker(Reranker):
    def rerank_hybrid(self, query: str, vector_results: pa.Table, fts_results: pa.Table) -> pa.Table:
        combined_results = self.merge(vector_results, fts_results)  # Or custom merge algo
        # Custom Reranking logic here

        return combined_results

reranker = MyCustomReranker()
table.search((vector_query, query)).rerank(reranker=reranker).to_pandas()
```

Weâ€™ve shown how you can apply reranking to significantly improve your retrieval quality with relatively small additions to your code base (by only adding a small additional latency). For RAG applications, better retrieval can significantly improve downstream generation, so itâ€™s worth spending some time trying out this feature on your existing LanceDB workflows. Weâ€™d love to hear back from the community wwhat you build with this exciting feature!

To learn more about LanceDB, hybrid search, or available rerankers, visit our [documentation page](https://lancedb.github.io/lancedb/hybrid_search/hybrid_search/) and chat with us on [Discord](https://discord.com/invite/zMM32dvNtd) about your use cases!
```

---

## ðŸ“„ æ–‡ä»¶: hybrid-search-combining-bm25-and-semantic-search-for-better-results-with-lan-1358038fe7e6.md

---

```md
---
title: "Hybrid Search: Combining BM25 and Semantic Search for Better Results with Langchain"
date: 2023-12-09
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/hybrid-search-combining-bm25-and-semantic-search-for-better-results-with-lan-1358038fe7e6/preview-image.png
meta_image: /assets/blog/hybrid-search-combining-bm25-and-semantic-search-for-better-results-with-lan-1358038fe7e6/preview-image.png
description: "Have you ever thought about how search engines find exactly what you're looking for?  They usually use a mix of looking for specific words and understanding the meaning behind them."
---

Have you ever thought about how search engines find exactly what you're looking for? They usually use a mix of looking for specific words and understanding the meaning behind them. This is called a hybrid search. Now, let's see how we can make a simple way to find documents using this mix.

## Understanding BM25:

[BM25](https://en.wikipedia.org/wiki/Okapi_BM25) is a ranking algorithm used in information retrieval systems to estimate the relevance of documents to a given search query.

- **What it does**: It looks at how often your search words appear in a document and considers the document length to provide the most relevant results.
- **Why itâ€™s useful:** Itâ€™s perfect for sorting through huge collections of documents, like a digital library, without bias towards longer documents or overused words.

## **Key elements of BM25:**

- **Term Frequency (TF)**: This counts how many times your search terms appear in a document.
- **Inverse Document Frequency (IDF)**: This gives more importance to rare terms, making sure common words donâ€™t dominate.
- **Document Length Normalization**: This ensures longer documents donâ€™t unfairly dominate the results.
- **Query Term Saturation**: This stops excessively repeated terms from skewing the results.

Overall

score(d, q) = âˆ‘(tf(i, d) ** idf(i) ** (k1 + 1)) / (tf(i, d) + k1 ** (1 - b + b ** (dl / avgdl)))

## **When is BM25/ Keyword search Ideal?**

- **Large Document Collections**: Perfect for big databases where you need to sort through lots of information.
- **Preventing Bias**: Great for balancing term frequency and document length.
- **General Information Retrieval**: Useful in various search scenarios, offering a mix of simplicity and effectiveness.

## Practical Application: Building a Hybrid Search System

Imagine youâ€™re crafting a search system for a large digital library. You want it not only to find documents with specific keywords but also to grasp the context and semantics behind each query. Hereâ€™s how:

- **Step 1**: BM25 quickly fetches documents with the search keywords.
- **Step 2**: VectorDB digs deeper to find contextually related documents.
- **Step 3**: The Ensemble Retriever runs both systems, combines their findings, and reranks the results to present a nuanced and comprehensive set of documents to the user.

## What Exactly is Hybrid Search?

Hybrid search can be imagined as a magnifying glass that doesnâ€™t just look at the surface but delves deeper. Itâ€™s a two-pronged approach:

- **Keyword Search:** This is the age-old method weâ€™re most familiar with. Input a word or a phrase, and this search hones in on those exact terms or closely related ones in the database or document collection.
- **Vector Search:** Unlike its counterpart, vector search isnâ€™t content with mere words. It works using semantic meaning, aiming to discern the queryâ€™s underlying context or meaning. This ensures that even if your words donâ€™t match a document exactly if the meaning is relevant, itâ€™ll be fetched.

![Hybrid search overview](/assets/blog/hybrid-search-combining-bm25-and-semantic-search-for-better-results-with-lan-1358038fe7e6/1*hfEfKvvNg2rVG-X_vr_V1g.png)
> Follow along with the Colab notebook:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Hybrid_search_bm25_lancedb/main.ipynb)
Letâ€™s get to the code snippets. Here weâ€™ll use langchain with LanceDB vector store

```python
# example of using BM25 & LanceDB - hybrid search

from langchain.vectorstores import LanceDB
import lancedb
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.schema import Document
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.document_loaders import PyPDFLoader

# Initialize embeddings
embedding = OpenAIEmbeddings()
```

and load a single PDF.

```python
# load single pdf

loader = PyPDFLoader("/content/Food_and_Nutrition.pdf")
pages = loader.load_and_split()
```

Create BM25 sparse **keyword matching retriever**

```python
# Initialize the BM25 retriever
bm25_retriever = BM25Retriever.from_documents(pages)
bm25_retriever.k = 2  # Retrieve top 2 results
```

Create a LanceDB vector store for dense **semantic search/retrieval.**

```python
db = lancedb.connect('/tmp/lancedb')
table = db.create_table("pandas_docs", data=[
    {"vector": embedding.embed_query("Hello World"), "text": "Hello World", "id": "1"}
], mode="overwrite")

# Initialize LanceDB retriever
docsearch = LanceDB.from_documents(pages, embedding, connection=table)
retriever_lancedb = docsearch.as_retriever(search_kwargs={"k": 2})
```

Now ensemble both retrievers, here you can assign the weightage to it.

```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, retriever_lancedb],
    weights=[0.4, 0.6],
)

# Example customer query
query = (
    "which food needed for building strong bones and teeth ?\n"
    "which Vitamin & minerals important for this?"
)

# Retrieve relevant documents/products
docs = ensemble_retriever.get_relevant_documents(query)
```

Using an ensemble retriever, it's trying to search each word in documents, such as strong bones & teeth as well as also searching using lancedb which will find the most similar documents based on similarity.

```python
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(openai_api_key="sk-yourapikey")

# if you want to use open source models such as Llama, Mistral see:
# https://github.com/lancedb/vectordb-recipes/blob/main/tutorials/chatbot_using_Llama2_&_lanceDB

qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=ensemble_retriever)

query = "what nutrition needed for pregnant women"
qa.run(query)
```

again, here it's searching the keyword â€” â€œ nutrition pregnant womenâ€ in the database using bm25 & returning the best matching results & similarly, at the same time, we are using lancedb for this. this is how it's more powerful to extract text.

![Hybrid search answers (ensemble)](/assets/blog/hybrid-search-combining-bm25-and-semantic-search-for-better-results-with-lan-1358038fe7e6/1*TEEE2ok0rvgMiK5-3vskGA.png)

below are answers from the traditional rag, you can check this in[ our repo](https://github.com/lancedb/vectordb-recipes/tree/main/tutorials/chatbot_using_Llama2_%26_lanceDB) the answers may vary based on different parameters, models, etc.

![Traditional RAG answers](/assets/blog/hybrid-search-combining-bm25-and-semantic-search-for-better-results-with-lan-1358038fe7e6/1*u37w8rgemau1bJN5Hw0RWA.png)

you can try this on Google Colab with your PDF & use case. This is how you can use hybrid search to improve your search quality.

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Hybrid_search_bm25_lancedb/main.ipynb)

**Explore More with Our Resources**

Discover the full potential of hybrid search and beyond in our lancedb repository, offering a setup-free, persisted vectordb that scales on on-disk storage. For a deep dive into applied GenAI and vectordb applications, examples, and tutorials, donâ€™t miss our [vectordb-recipes](https://github.com/lancedb/vectordb-recipes). From advanced RAG methods like Flare, Rerank, and HyDE to practical use cases, our resources are designed to inspire your next project.
```

---

## ðŸ“„ æ–‡ä»¶: hybrid-search-rag-for-real-life-production-grade-applications-e1e727b3965a.md

---

```md
---
title: "Hybrid Search: RAG for Real-Life Production-Grade Applications"
date: 2024-02-18
author: Mahesh Deshwal
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/hybrid-search-rag-for-real-life-production-grade-applications-e1e727b3965a/preview-image.png
meta_image: /assets/blog/hybrid-search-rag-for-real-life-production-grade-applications-e1e727b3965a/preview-image.png
description: "Get about hybrid search: rag for real-life production-grade applications. Get practical steps, examples, and best practices you can use now."
---

## What is Hybrid Search, and whatâ€™s the need for it?

With the increasing usage of LLMs in RAG setting, thereâ€™s a lot of focus on Vector (Embedding) search. Everything works well when youâ€™re dealing with toy problems or datasets, but you start to notice performance and evaluation constrains as the applications scale. What do I mean by that? Letâ€™s take an example of a simple search from a global company where you know user preferences like taste in music, geo-location, etc.

Now, if they love Rap music and search lyrics to a song, you wouldnâ€™t want to recommend something they hate or a song having the same meaning in a different language, at any cost. Another example of a bad result is that if they want to literally search a book named â€œElephant in the Room,â€ and you give them a paragraph providing the semantic meaning of it. Thatâ€™s a pretty good recipe for user churn. So whatâ€™s the solution?

## Hybrid search

There are a few common ways of searching for queries these days:

1. **Full-text search (FTS) â€” **Where you search the exact query in DB â€œsyntacticallyâ€ matching the words. Some famous algorithms used for these are BM-25 and LSH. This is a keyword based search
2. **Vector (Embedding) search** â€” This takes in an embedding of of text, video, audio etc and returns the â€œsemanticallyâ€ similar results. Some most used algorithms by vector databases are PQ, IVF-PQ, HNSW, etc
3. **SQL (or NoSQL) â€” **These use conditional filters where you filter the data based on things you already know.

Hybrid Search is a broad (often misused) term. It can mean anything from combining multiple methods for searching, to applying ranking methods to better sort the results. In this blog, we use the definition of â€œhybrid searchâ€ to mean using a combination of keyword-based and vector search.

Now that we have a general idea of how hybrid search works, letâ€™s take a look at a few methods that you can use to merge these together. Letâ€™s assume you have your Top-K results for both Semantic Search as well as Text Search. Now letâ€™s suppose you have some metadata from the user. For example user is searching for specific items or maybe user wants to search in particular genre or maybe user is from a particular geo-location. What you can do now is to use filters (SQL or Pandas) to remove the unwanted results to further improve your search.

## **1. Fusion / Merging of scores:**

In fusion or merging, you take both sets of results and combine them by some logic. *But keep in mind that usually BM-25 has unbounded scores and Vector search has scores between 0â€“1 as itâ€™s cosine similarity mostly. So, you need to â€œnormalizeâ€ the scores to keep them in the same range. You can apply Min-Max scaling.*

How you merge is up to you. If your task is more semantic-oriented, you can give more weight (say 0.75) to semantic search scores and the remaining (0.25) to normalized BM-25 scores or vice versa. You can choose to remove the results that are not common OR place them at the end OR sort according to their scaled score (multiplied by 0.75 or 0.25, depending on which category they belong to). FYI, there are also other ways of calculating new scores directly by using retrieved text and query pair, without using the existing scores. For example â€” Cross encoders or API based rerankers like Cohere.

## **2. Re-Ranking:**

In re-ranking, you increase the recall to fetch more than needed results. Now look at the result returned from both tables and then you shift the rank of results, which are common in both. If your task is more syntactic, then you look at the BM-25 results and sort that table, and if your task is more semantic, then you can re-rank the embedding results based on whether the same occurs in BM-25 or not. So here, you first increased recall, and then you reranked to increase precision.

## 3. Stacked & Filtered results:

You can get the BM-25 and Vector results based on different filter criteria. For example, you would want to get all the BM-25 results where the length of the text is < 150 AND the Language is English, but for the semantic search, youâ€™d want to get results in any language. Now how do you use those? One thing I used once was to â€œstackâ€ one after another. 1st final can come from BM-25 (or Vector search) 2nd comes from the Vector (or BM-25) and so on. You get a balance between these.

A study of how some of these actually work in practice is [given in this blog](https://opensourceconnections.com/blog/2023/02/27/hybrid-vigor-winning-at-hybrid-search/).

Apart from these approaches, you can use a Re-Ranker to re-score all of the Top-K results once again using a semantic reranker or some other approach like MinHash, SimHash, etc. To know more on Re-ranking, [read here](https://medium.com/etoai/simplest-method-to-improve-rag-pipeline-re-ranking-cf6eaec6d544).
![Hybrid search illustration](/assets/blog/hybrid-search-rag-for-real-life-production-grade-applications-e1e727b3965a/1*Rpx35CSEiQdIbTCKYvs59A.gif)
Letâ€™s look at some code.

LanceDB uses `tantivity` for the full text search. It also provides a Reranking API, where you can define your merge/fusion function **AND** make use of Re Ranking too. If you donâ€™t know about Re ranking, [Visit this blog.](https://medium.com/etoai/simplest-method-to-improve-rag-pipeline-re-ranking-cf6eaec6d544)

```python
import os
import lancedb
import re
import pandas as pd
import random

from datasets import load_dataset

import torch
import gc

import lance

import os

import lancedb
import openai
from lancedb.embeddings import get_registry
from lancedb.pydantic import LanceModel, Vector

os.environ["OPENAI_API_KEY"] = "sk-......."  # YOUR API
embeddings = get_registry().get("openai").create()
```

Now let us create a LanceDB table and load the the good old BEIR data. Remember, in this data, there is no metadata so we are creating our additional metadata as the Number of words for demo purpose

```python
queries = load_dataset("BeIR/scidocs", "queries")["queries"].to_pandas()
full_docs = load_dataset('BeIR/scidocs', 'corpus')["corpus"].to_pandas().dropna(subset="text")

docs = full_docs.head(64)  # random samples for faster embed demo
docs["num_words"] = docs["text"].apply(lambda x: len(x.split()))  # Insert Metadata for a more HYBRID search
docs.sample(3)
```

```python
class Documents(LanceModel):
    vector: Vector(embeddings.ndims()) = embeddings.VectorField()
    text: str = embeddings.SourceField()
    title: str
    num_words: int

data = docs.apply(lambda row: {"title": row["title"], "text": row["text"], "num_words": row["num_words"]}, axis=1).values.tolist()

db = lancedb.connect("./db")
table = db.create_table("documents", schema=Documents)

table.add(data)  # ingest docs with auto-vectorization
table.create_fts_index("text")  # Create a FTS index before the hybrid search

```
```text
_id  title                                           text                                        num_words
8804 d2fbc1a8bcc7c252a70c524cc96c14aa807c2345        Approximating displacement with the body... 128
6912 75859ac30f5444f0d9acfeff618444ae280d661d        Multibiometric Cryptosystems Based on Fe... 200
5797 90a2a7a3d22c58c57e3b1a4248c7420933d7fe2f        An integrated approach to testing complex... 332
```

Moment of truth? Letâ€™s search first giving more weight to Full Text Search

```python
from lancedb.rerankers import LinearCombinationReranker

reranker = LinearCombinationReranker(weight=0.3)  # 0: pure Text (BM25), 1: pure Semantic (Vector)

table.search("To confuse the AI and DNN embedding, let's put random terms from other sentences- automation training test memory?", query_type="hybrid").\
    rerank(reranker=reranker).\
    limit(5).\
    to_pandas()
```

![Hybrid results favoring text](/assets/blog/hybrid-search-rag-for-real-life-production-grade-applications-e1e727b3965a/1*ydtjwr-MneBC5y4p62VLmg.png)
You see, I confused the model with the query by putting random terms for text but the sense was different. Since it is `0.3` it favours more to the text search. Now letâ€™s full throttle to `0.7` and se the results for the same query
![Hybrid results favoring vector](/assets/blog/hybrid-search-rag-for-real-life-production-grade-applications-e1e727b3965a/1*8MyhbRq_96IGAWlm-EvoHw.png)
Whoohooo!!! Even the terms are there, it captured the meaning rather than the terms. Look at 3rd and 4th row interchanged.

Thatâ€™s about it. If you want to add your custom filtering and re ranking, learn more about it [here](https://lancedb.github.io/lancedb/hybrid_search/hybrid_search/#building-custom-rerankers).

```python
from typing import List, Union
import pandas as pd
import pyarrow as pa

class ModifiedLinearReranker(LinearCombinationReranker):
    def __init__(self, filters: Union[str, List[str]], **kwargs):
        super().__init__(**kwargs)
        filters = filters if isinstance(filters, list) else [filters]
        self.filters = filters

    def rerank_hybrid(self, query: str, vector_results: pa.Table, fts_results: pa.Table) -> pa.Table:
        combined_result = super().rerank_hybrid(query, vector_results, fts_results)
        df = combined_result.to_pandas()
        for filter in self.filters:
            # Implement your filters here. Hard-code or pass dynamically.
            df = df.query("(not text.str.contains(@filter)) & (num_words > 150)")

        return pa.Table.from_pandas(df)

modified_reranker = ModifiedLinearReranker(filters=["dual-band"])

table.search("To confuse the AI and DNN embedding, let's put random terms from other sentences- automation training test memory?", query_type="hybrid").\
    rerank(reranker=modified_reranker).\
    limit(5).\
    to_pandas()
```

![Custom reranker filtered results](/assets/blog/hybrid-search-rag-for-real-life-production-grade-applications-e1e727b3965a/1*aWiwhCMyag60AZYl9x4QQw.png)
This code will implement a custom filtering criteria where only the results are there where No of words are `>150`. You can also change the merging mechanism by inheriting from built-in Rerankers and adding some custom logic!

> Colab to reproduce the results:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Y9A7OCLjx1cm224xKB6Jbk1-qD68_YCj?usp=sharing)

To learn more about LanceDB, hybrid search, or available rerankers, visit our [documentation page](https://lancedb.github.io/lancedb/hybrid_search/hybrid_search/) and chat with us on [Discord](https://discord.com/invite/zMM32dvNtd) about your use cases!
```

---

## ðŸ“„ æ–‡ä»¶: implementing-corrective-rag-in-the-easiest-way-2.md

---

```md
---
title: "Implementing Corrective RAG in the Easiest Way"
date: 2024-03-04
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/implementing-corrective-rag-in-the-easiest-way-2/preview-image.png
meta_image: /assets/blog/implementing-corrective-rag-in-the-easiest-way-2/preview-image.png
description: "Even though text-generation models are good at generating content, they sometimes need to improve in returning facts.  This happens because of the way they are trained."
---

Even though text-generation models are good at generating content, they sometimes need to improve in returning facts. This happens because of the way they are trained. Retrieval Augmented Generation(RAG) techniques have been introduced to address this issue by fetching context from a knowledge base.

Corrective RAG is an additional step to ensure the model sticks to the information it gets. It corrects factual inaccuracies in real-time by ranking options based on how likely they fit the model and match the retrieved info. This helps ensure accurate corrections before completing the text.

[**Corrective Retrieval Augmented Generation paper**](https://arxiv.org/pdf/2401.15884.pdf)

![CRAG at Inference](/assets/blog/implementing-corrective-rag-in-the-easiest-way-2/unnamed.png)

CRAG at Inference ([https://arxiv.org/abs/2401.15884](https://arxiv.org/abs/2401.15884))

### **Top Level Structure**

CRAG has three main parts:

1. **Generative Model**: It generates an initial sequence.
2. **Retrieval Model**: It retrieves context based on the initial sequence from the knowledge base.
3. **Retrieval Evaluator**: It manages the back-and-forth between the generator and retriever, ranks options, and decides the final sequence to be given as output.

In CRAG, the **Retrieval Evaluator** links the Retriever and Generator. It keeps track of the text created, asks the generator for more, gets knowledge with updated info, scores options for both text fit and accuracy, and chooses the best one to add to the output at each step.

![Pseudocode of CRAG](/assets/blog/implementing-corrective-rag-in-the-easiest-way-2/crag-pseudocode.png)

Pseudocode of CRAG from paper

### **Implementation**

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/tutorials/Corrective-RAG-with_Langgraph/CRAG_with_Langgraph.ipynb)
Implementation will include giving ratings/scores to retrieved documents based on how well they answer a question:

**For Correct documents** -

1. If at least one document is relevant, it moves on to creating text
2. Before creating text, it cleans up the knowledge
3. This breaks down the document into â€œknowledge stripsâ€
4. It rates each strip and gets rid of ones that donâ€™t matter

**For Ambiguous or Incorrect documents** -

1. If all documents need to be more relevant or are unsure, the method looks for more information.
2. It uses a web search to add more details to what it found
3. The diagram in the paper also shows that they might change the question to get better results.

We will implement CRAG using Langgraph and LanceDB. LanceDB for lightning-fast retrieval from the knowledge base. Here is the flow of how it will work.

![Flow diagram](/assets/blog/implementing-corrective-rag-in-the-easiest-way-2/unnamed-1.png)

The next step to implement this flow will be:

1. Retrieve relevant documents
2. If a relevant document is not found, go for supplement retrieval with a web search(using Tavily API).
3. Query re-writing to optimize the query for web search.

Here is some pseudocode. For a complete implementation, please refer to the linked Colab notebook.

**Building Retriever**

We will use Jay Alammerâ€™s articles on Transformers as a knowledge base.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import LanceDB
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Using Jay Alammar's articles on Transformers, BERT, and retrieval
urls = [
    "https://jalammar.github.io/illustrated-transformer/",
    "https://jalammar.github.io/illustrated-bert/",
    "https://jalammar.github.io/illustrated-retrieval-transformer/",
]

docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

# document chunking
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=250, chunk_overlap=0
)
doc_splits = text_splitter.split_documents(docs_list)
```

**Define knowledge base using LanceDB**

```python
import lancedb

def lanceDBConnection(embed):
    db = lancedb.connect("/tmp/lancedb")
    table = db.create_table(
        "crag_demo",
        data=[{"vector": embed.embed_query("Hello World"), "text": "Hello World"}],
        mode="overwrite",
    )
    return table
```

**Embeddings Extraction**

We will use the OpenAI embeddings function and insert them in the LanceDB knowledge base for fetching context to extract the embeddings of documents.

```python
# OpenAI embeddings
embedder = OpenAIEmbeddings()

# LanceDB as vector store
table = lanceDBConnection(embedder)
vectorstore = LanceDB.from_documents(
    documents=doc_splits,
    embedding=embedder,
    connection=table,
)

# ready with our retriever
retriever = vectorstore.as_retriever()
```

**Define Langgraph**

We will define a graph for building Langgraph by adding nodes and edges as in the above flow diagram.

```python
from typing import Dict, TypedDict

from langchain_core.messages import BaseMessage

class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        keys: A dictionary where each key is a string.
    """

    keys: Dict[str, any]
```

This Graph will include five nodes: document retriever, generator, document grader, query transformer, and web search, and 1 edge will decide whether to generate or not.

The following graph shows the flow shown in the diagram.

```python
import pprint

from langgraph.graph import END, StateGraph

workflow = StateGraph(GraphState)

# Define the nodes
workflow.add_node("retrieve", retrieve)  # retrieve docs
workflow.add_node("grade_documents", grade_documents)  # grade retrieved docs
workflow.add_node("generate", generate)  # generate answers
workflow.add_node("transform_query", transform_query)  # transform_query for web search
workflow.add_node("web_search", web_search)  # web search

# Build graph
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "transform_query": "transform_query",
        "generate": "generate",
    },
)
workflow.add_edge("transform_query", "web_search")
workflow.add_edge("web_search", "generate")
workflow.add_edge("generate", END)

# Compile
app = workflow.compile()

# Run
query_prompt = "How Transformers work?"
inputs = {"keys": {"question": query_prompt}}
for output in app.stream(inputs):
    for key, value in output.items():
        # print full state at each node
        pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint.pprint("------------------------")

# Final generation
print("*"*5, " Generated Answer ", "*"*5)
pprint.pprint(value["keys"]["generation"])
```

Here is the generated answer output, illustrating each nodeâ€™s functioning and their decisions, ultimately resulting in the final generated output.

![Output of implemented CRAG](/assets/blog/implementing-corrective-rag-in-the-easiest-way-2/output.png)

Checkout the Colab for implementation:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/tutorials/Corrective-RAG-with_Langgraph/CRAG_with_Langgraph.ipynb)

### **Challenges and Future Work**

CRAG helps generate more accurate factual information out of the knowledge base, but still, Some Challenges remain for the widespread adoption of CRAG:

1. Retrieval quality depends on comprehensive knowledge coverage in the knowledge base.
2. Increased computation cost and latency compared to basic models.
3. The framework is sensitive to retriever limitations.
4. Balancing between fluency and factuality is challenging.
```

---

## ðŸ“„ æ–‡ä»¶: introducing-lance-namespace-spark-integration.md

---

```md
---
title: "Manage Lance Tables in Any Catalog using Lance Namespace and Spark"
date: 2025-08-08
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/introducing-lance-namespace-spark-integration/preview-image.png
meta_image: /assets/blog/introducing-lance-namespace-spark-integration/preview-image.png
description: "Access and manage your Lance tables in Hive, Glue, Unity Catalog, or any catalog service using Lance Namespace with the latest Lance Spark connector."
author: Jack Ye
author_avatar: "/assets/authors/jack-ye.jpg"
author_bio: "Software Engineer @ LanceDB"
author_github: "jackye1995"
author_linkedin: "https://www.linkedin.com/in/yezhaoqin"
---

Data management in AI and analytics workflows often involves juggling multiple systems and formats. 

Today, we're excited to introduce [Lance Namespace](https://lancedb.github.io/lance/format/namespace/), 
an open specification that standardizes access to collections of [Lance tables](/docs/concepts/tables/), 
making it easier than ever to [integrate Lance](/docs/overview/lance/) with your existing data infrastructure.

## What is Lance Namespace?

Lance Namespace is an open specification built on top of the storage-based Lance table and file format. 
It provides a standardized way for metadata services like Apache Hive MetaStore, Apache Gravitino, Unity Catalog, 
AWS Glue Data Catalog, and others to store and manage Lance tables. 
This means you can seamlessly use Lance tables alongside your existing [data lakehouse infrastructure](/blog/multimodal-lakehouse/).

### Why "Namespace" Instead of "Catalog"?

While the data lake world traditionally uses hierarchical structures with catalogs, databases, and tables, 
the ML and AI communities often prefer flatter organizational models like simple directories.
Lance Namespace embraces this flexibility by providing a multi-level namespace abstraction 
that adapts to your data organization strategy,
whether that's a simple directory structure or a complex multi-level hierarchy.

## Current Implementations and Building Your Own

Lance Namespace currently supports several implementations out of the box:

- **Directory Namespace**: Simple file-based organization
- **REST Namespace**: Connect to any server that is compliant with the [REST specification](https://lancedb.github.io/lance/format/namespace/impls/rest/), including [LanceDB Cloud and LanceDB Enterprise](https://lancedb.com/pricing/).
- **Hive 2.x and 3.x MetaStore**: Integration with Apache Hive
- **AWS Glue Catalog**: Native [AWS Glue](https://aws.amazon.com/glue/) support

### Building Custom Namespaces

You can build your own namespace implementation in two ways:

1. **REST Server**: Implement the Lance REST Namespace OpenAPI specification to create a standardized server that any Lance tool can connect to
2. **Native Implementation**: Build a direct implementation as a library

When deciding between building an adapter (REST server proxying to your metadata service) versus a native implementation, 
consider factors like multi-language support needs, tooling compatibility, security requirements, and performance sensitivity. 
See the [Lance REST Namespace documentation](https://lancedb.github.io/lance/format/namespace/impls/rest/#choosing-between-an-adapter-vs-an-implementation) for detailed guidance on this decision.

## Integration with Apache Spark

One of the most highly requested features in the Lance community that is enabled by Lance Namespace is 
seamless integration with Apache Spark, with the ability to use Lance not just as a data format plugin, 
but as a complete Spark table catalog that users can access and manage Lance tables in Spark, run proper SQL analytics,
and use Spark MLlib in the training process. 
Here we walk through how you can do that now with Lance Namespace.

## Getting Started: A Practical Example

Let's walk through a simple example of using Lance Namespace with Spark to manage and query Lance tables.

If you'd like to get started quickly without worrying about the setup, 
we've prepared a Docker image with everything pre-configured. 
Check out our [Lance Spark Connector Quick Start guide](https://lancedb.github.io/lance/integrations/spark/#quick-start) 
to get up and running in minutes.

### Step 1: Set Up Your Spark Session

First, configure Spark with the Lance Namespace catalog. Here's an example using a directory-based namespace:

```python
from pyspark.sql import SparkSession

# Create a Spark session with Lance catalog
spark = SparkSession.builder \
    .appName("lance-namespace-demo") \
    .config("spark.jars.packages", "com.lancedb:lance-spark-bundle-3.5_2.12:0.0.6") \
    .config("spark.sql.catalog.lance", "com.lancedb.lance.spark.LanceNamespaceSparkCatalog") \
    .config("spark.sql.catalog.lance.impl", "dir") \
    .config("spark.sql.catalog.lance.root", "/path/to/lance/data") \
    .config("spark.sql.defaultCatalog", "lance") \
    .getOrCreate()
```

This creates a Spark catalog `lance` that is configured to talk with the directory at `/path/to/lance/data`,
and also sets it as the default catalog in the current Spark session.

### Step 2: Create and Manage Tables

With the catalog configured, you can now create and manage Lance tables using familiar SQL commands:

```python
# Create a Lance table
spark.sql("""
    CREATE TABLE embeddings (
        id BIGINT,
        text STRING,
        embedding ARRAY<FLOAT>,
        timestamp TIMESTAMP
    )
    TBLPROPERTIES (
      'embedding.arrow.fixed-size-list.size'='3'
    )
""")

# Insert data into the table
spark.sql("""
    INSERT INTO embeddings 
    VALUES 
        (1, 'Hello world', array(0.1, 0.2, 0.3), current_timestamp()),
        (2, 'Lance and Spark', array(0.4, 0.5, 0.6), current_timestamp())
""")
```

Notice that when the user specifies an embedding column `embedding ARRAY<FLOAT>`,
with the table property `'embedding.arrow.fixed-size-list.size'='3'`,
it creates a fixed-size vector column in the underlying Lance format table that is optimized for [vector search performance](/docs/concepts/search/).

### Step 3: Query Your Data

Query Lance tables just like any other Spark table:

```python
# Query using SQL
results = spark.sql("""
    SELECT id, text, size(embedding) as dim
    FROM embeddings
    WHERE id > 0
""")
results.show()

# Or use the DataFrame API
df = spark.table("embeddings")
filtered_df = df.filter(df.id > 0).select("id", "text")
filtered_df.show()
```

### Step 4: Integration with ML Workflows

Lance's columnar format and vector support make it ideal for ML workflows:

```python
from pyspark.sql import functions as F

# Simulate generation of new embeddings
new_embeddings_df = spark.sql("""
    SELECT 
        3 as id,
        'Machine learning with Lance' as text,
        array(0.7, 0.8, 0.9) as embedding,
        current_timestamp() as timestamp
    UNION ALL
    SELECT 
        4 as id,
        'Vector databases are fast' as text,
        array(0.2, 0.4, 0.6) as embedding,
        current_timestamp() as timestamp
""")

# Append new embeddings to the Lance table
new_embeddings_df.writeTo("embeddings").append()

# Verify the combined dataset and compute embedding statistics
spark.sql("""
    SELECT 
        COUNT(*) as total_records,
        ROUND(AVG(aggregate(embedding, 0D, (acc, x) -> acc + x * x)), 3) as avg_l2_norm,
        ROUND(MIN(embedding[0]), 2) as min_first_dim,
        ROUND(MAX(embedding[0]), 2) as max_first_dim
    FROM embeddings
""").show()
```

## Advanced Namespace Configurations

Here are some other configuration examples for connecting to a few Lance namespace implementations:

### Directory Namespace on S3 Cloud Storage

```python
spark = SparkSession.builder \
    .config("spark.sql.catalog.lance.impl", "dir") \
    .config("spark.sql.catalog.lance.root", "s3://bucket/lance-data") \
    .config("spark.sql.catalog.lance.storage.access_key_id", "your-key") \
    .config("spark.sql.catalog.lance.storage.secret_access_key", "your-secret") \
    .getOrCreate()
```

### LanceDB Cloud REST Namespace

```python
spark = SparkSession.builder \
    .config("spark.sql.catalog.lance.impl", "rest") \
    .config("spark.sql.catalog.lance.uri", "https://your-database.api.lancedb.com") \
    .config("spark.sql.catalog.lance.headers.x-api-key", "your-api-key") \
    .getOrCreate()
```

### AWS Glue Namespace

```python
spark = SparkSession.builder \
    .config("spark.sql.catalog.lance.impl", "glue") \
    .config("spark.sql.catalog.lance.region", "us-east-1") \
    .config("spark.sql.catalog.lance.root", "s3://your-bucket/lance") \
    .getOrCreate()
```

## Benefits for AI and Analytics Teams

Lance Namespace with Spark integration brings several key benefits:

1. **Unified Data Management**: Manage Lance tables alongside your existing data assets
2. **Flexibility**: Choose the namespace backend that fits your infrastructure
3. **Performance**: Leverage Lance's table and file format with Spark's distributed processing
4. **Simplicity**: Use familiar SQL and DataFrame APIs
5. **Scalability**: Handle everything from local experiments to production workloads

For more information on [LanceDB's features](/docs/overview/features) and capabilities, check out our [comprehensive documentation](/docs/).

## What's Next?

Lance Namespace is designed to be extensible and community-driven. We're actively working on:

- Additional namespace implementations: Unity Catalog, Apache Gravitino, and Apache Polaris work in progress
- Enhanced [vector search capabilities](/docs/concepts/search/) within Spark
- Tighter integration with ML frameworks with features like data evolution
- Support for more compute engines beyond Spark

If you're interested in [getting started](/docs/quickstart/) with LanceDB or exploring our [enterprise features](/docs/overview/enterprise/), we have comprehensive guides available.

## Thank You to Our Contributors

We'd like to extend our heartfelt thanks to the community members who have contributed to making Lance Namespace 
and the Spark integration a reality:

- **Bryan Keller** from Netflix
- **Drew Gallardo** from AWS
- **Jinglun** and **Vino Yang** from ByteDance

Your contributions have been instrumental in making Lance Namespace a robust solution for the community.

## Get Involved

Lance Namespace is open source and we welcome all kinds of contributions! 
Whether you're interested in adding new namespace implementations, improving the Spark connector,
building integration with more engines, or just trying it out, we'd love to hear from you.

- **Documentation**: [Lance Namespace](https://lancedb.github.io/lance/format/namespace)
- **Documentation**: [Lance Spark Connector](https://lancedb.github.io/lance/integrations/spark)
- **Roadmap**: [Lance Namespace](https://github.com/lancedb/lance-namespace/issues/168)
- **Roadmap**: [Lance Spark Connector](https://github.com/lancedb/lance-spark/issues/47)

## Conclusion

Lance Namespace bridges the gap between modern AI workloads and traditional data infrastructure. 
By providing a standardized way to manage Lance tables and seamless integration with Apache Spark, 
it makes it easier than ever to build scalable AI and analytics pipelines.

Try it out today and let us know what you think! Whether you're building a recommendation system, 
managing embeddings for [RAG applications](/docs/tutorials/rag/), or analyzing large-scale datasets, 
Lance Namespace and Spark provide the foundation you need for success.
```

---

## ðŸ“„ æ–‡ä»¶: lance-community-governance.md

---

```md
---
title: "Building the Future Together: Introducing Lance Community Governance"
date: 2025-11-18
draft: false
featured: true
categories: ["Announcement"]
image: /assets/blog/lance-community-governance/preview-image.png
meta_image: /assets/blog/lance-community-governance/preview-image.png
description: "Announcing the formal governance structure for the Lance community, establishing clear pathways for contribution and leadership with a three-tier system of contributors, maintainers and PMC."
author: Jack Ye
author_avatar: "/assets/authors/jack-ye.jpg"
author_bio: "Software Engineer @ LanceDB"
author_github: "jackye1995"
author_linkedin: "yezhaoqin"
---

Over the last three years, Lance has created a truly unique community where data infrastructure engineers and AI/ML engineers come together to make lakehouse architectures work in the AI era. Today, weâ€™re excited to announce the formal governance structure for the Lance community to help it thrive, even further.

What started off as an idea to develop a better format for machine learning and AI has now become the fastest-growing open format with a vibrant community, spanning hundreds of contributors and dozens of companies. From top-level frontier model labs like RunwayML, Midjourney, Harvey.ai, and World Labs, to large enterprises building ML/AI applications like UBS, Netflix and Uber, to data infrastructure powerhouses like Databricks, AWS and ByteDance, Lance brings together diverse perspectives, united by a common goal: building the de facto standard format for multimodal AI.

As Lance continues to mature and its adoption accelerates, we believe it's time to establish clear pathways for community involvement and leadership that reflect our collaborative, inclusive values.

## A Community That Values All Contributions

One of the core principles of our governance is that **contributions may come in many forms, and all of them are valuable**. When we think about "contributions", we're not just talking about code. Bug reports, feature requests, documentation, design work, organizing meetups, giving talks, supporting users on Discord, and many other activities â€” all of these count as meaningful contributions to the Lance ecosystem.

To reflect this philosophy, we've established a three-tier governance structure:

- **Contributors**: Anyone who has made a contribution to Lance in any of the above-mentioned forms
- [**Maintainers**](https://lance.org/community/maintainers): Contributors who have made sustained and valuable contributions to the community
- [**Project Management Committee (PMC)**](https://lance.org/community/pmc): Maintainers who have demonstrated leadership and help guide the project's long-term direction

This structure resembles that of the projects in the [Apache Software Foundation](https://www.apache.org/), and what we described above is very much in line with [the Apache Way](https://www.apache.org/theapacheway/) and "Community over Code" spirit - which is not surprising if you check out the backgrounds of Lance maintainers and PMC members ðŸ˜‰. 

From that, we've actually gone a step further to **separate roles from permissions**. Maintainer status is a recognition of sustained, valuable contributions - not just code commits. Some maintainers are additionally granted write access to repositories (what's commonly known as "committers" in many other projects) - this is just an extra permission granted to help them better support the community's growth.

Similarly, other maintainers may be granted permissions to manage community spaces, handle trademarks, or oversee other aspects of the project. This separation ensures that we can recognize contributions of all types while distributing responsibilities based on individual strengths and interests. **You can advance through the governance tiers without writing a single line of code** - whether through community support, documentation, advocacy, or any other valuable contribution.

This isn't just a new principle for us - our [Lancelot program](https://github.com/lancedb/lancedb/wiki?ref=blog.lancedb.com) has been operating on this inclusive philosophy for over two years, recognizing contributions beyond code. That experience made it straightforward to identify our initial maintainers and PMC members, as we already had a track record of celebrating diverse contributions to the community.

## Consensus-Driven Decision Making

With this governance structure, we've formally defined our [voting process](https://lance.org/community/voting) for important matters including releases, changes to the maintainers and PMC, and future modifications to Lanceâ€™s governance. Any PMC member can veto a proposal, which requires consensus gathering to move the proposal forward - ensuring that no single entity can unilaterally make critical decisions.

We've heard from people in the community asking whether we plan to donate Lance to a foundation, often driven by concerns about potential license changes. With this governance model, **such a unilateral license change simply cannot happen** - any PMC member from any company can block it.

As for whether Lance should join a foundation, and which one - that's now a question the Lance community can thoughtfully consider, together. There are great examples like [Substrait](https://github.com/substrait-io/substrait), which has no foundation affiliation but maintains excellent community governance, and there are also projects within foundations that still have strong control by a single commercial entity. At the end of the day, it's the community and its *governance* that matter most, not an affiliation to a foundation. We're in no rush, and we'll let the community guide this decision, if and when the time comes.

## Empower Contributors in the AI Era

The AI and ML space moves fast, and our development infrastructure is designed to keep pace. The Lance community values **automation and rapid iteration** as first-class principles, both for releasing software and for making contributions.

Our [release process](https://lance.org/community/release) minimizes human intervention through GitHub Actions automation that handles version bumping, changelog generation, and artifact publishing. Maintainers can publish or preview beta releases at any time for early testing, while stable releases follow a streamlined voting process with clear timelines â€” as short as same-day approval for critical patches, 3 days for minor versions, and 1 week for major versions. This allows us to ship improvements quickly while maintaining quality through community verification and PMC oversight.

We're also embracing AI-assisted development by making our [contribution process](https://lance.org/community/contributing) AI-friendly. From code agent integrations (check out our `AGENTS.md` files) to AI-driven automations in our GitHub workflows, we're continuously working to lower friction for contributors. In a field where new frameworks and techniques emerge constantly, being able to contribute and release rapidly is crucial. Whether you're using AI coding assistants or contributing traditionally, we want the Lance codebase to be accessible and easy to work with.

## Fast Track for New Initiatives

Similar to how we learned from the Apache Software Foundation for designing our community governance hierarchy, we also consulted foundations like the [Cloud Native Computing Foundation](https://www.cncf.io/) for managing project portfolios. We've created a pathway for rapid experimentation and innovation through our [subproject structure](https://lance.org/community#subprojects). We recognize that as core projects mature and become more complex, it can become harder for newcomers to make their first meaningful contributions. At the same time, the AI/ML ecosystem is moving fast, with new integrations and use cases emerging constantly.

Our **subproject** model addresses this by providing a lower barrier to entry for new initiatives. Contributors can quickly start new subprojects with relaxed contribution requirements and the ability to move fast. For example, [lance-graph](https://github.com/lance-format/lance-graph) was started by Uber and is now drawing developers from companies like LinkedIn, while the [Lance data viewer](https://github.com/lance-format/lance-data-viewer) was started by an individual developer and is being extended for enterprise use cases based on community feedback. Subprojects can even grant write access to contributors who may not yet be maintainers, enabling rapid iteration.

When a subproject demonstrates maturity through proper CI/CD, testing standards, automated releases, production adoption, and sustained community contributions, it can graduate to become a core project through a PMC vote. Importantly, **graduation doesn't diminish the contributions of subproject contributors** - their work remains recognized regardless of the project's tier.

This dual-track approach maintains a healthy balance:

- Core projects like `lance` maintain high quality bars, ensuring that contributors who dive deep into complex format specifications, performance optimizations, and foundational innovations receive the recognition their challenging work deserves
- Subprojects provide accessible entry points and rapid experimentation grounds where new contributors can make immediate impacts

We believe this structure keeps Lance vibrant and accessible while protecting the depth and rigor needed in our core components. Whether you're building a new integration or optimizing the core formats, there's a place for your contributions.

## What About LanceDB?

You might wonder how this governance change impacts the LanceDB open source project. The short answer: it doesn't, at least not yet.

Architecturally, we want Lance to remain a pure format specification with its core SDK and engine connectors. LanceDB is a full lakehouse implementation on top of the Lance format to deliver an end-to-end AI-Native Multimodal Lakehouse. Similar to how Apache Iceberg and Apache Polaris are separate projects, or how Delta Lake and Unity Catalog maintain distinct boundaries, this separation between Lance and LanceDB makes sense from a technical perspective.

To make this distinction clear, we've created dedicated infrastructure for each project:

- Lance now has its own website ([lance.org](https://lance.org/)), GitHub organization ([lance-format](https://github.com/lance-format)), and even a Discord server [Lance Format](https://discord.gg/lance)
- LanceDB will still maintain its own website separately ([lancedb.com](https://lancedb.com/)), GitHub organization ([lancedb](https://github.com/lancedb)), and Discord server [LanceDB](https://discord.gg/zMM32dvNtd), catering to LanceDB's user community.

As for LanceDB's community development, we're taking it one step at a time. Right now, the LanceDB community is still growing, with by far the most contributions coming from LanceDB Inc. (the company). As the project matures and attracts more diverse contributors, the governance work we've done for Lance provides a clear pathway for how we can evolve LanceDB's community structure in the future.

## Grateful for Our Community

We want to express our deep appreciation for everyone who has contributed to getting Lance to this point. There are countless community members who have filed issues, answered questions, built integrations, adopted Lance in their projects, spread the word, and contributed in so many other ways. Every contribution matters, and we're grateful for all of them.

The [maintainers](https://lance.org/community/maintainers) and [PMC members](https://lance.org/community/pmc) you see in our governance documentation have each earned their positions through sustained, meaningful contributions - whether through code, community building, documentation, advocacy, or technical leadership. These are not ceremonial titles - each person listed has made real, valuable contributions that have shaped Lance into what it is today.

**What this governance structure provides is a formal pathway for recognition and leadership.** If you've been contributing to Lance but aren't yet on these lists, this framework now gives you a clear path forward. Whether you want to become a maintainer, join the PMC, or start a new subproject, there are now transparent guidelines and processes to help you take on a leadership role and receive formal recognition for your work.

## Get Involved

We're excited about this next chapter for Lance. If you're interested in getting more involved, here's how:

- Visit the new Lance documentation website at [lance.org](https://lance.org/)
- Join our new [Lance Format Discord server](https://discord.gg/lance) for day-to-day discussions
- Read the [Community Governance documentation](https://lance.org/community) to understand the structure and how to get involved
- Subscribe to the [community event calendar](https://calendar.google.com/calendar/u/0?cid=Y29tbXVuaXR5QGxhbmNlLm9yZw) to join upcoming meetups and discussions
- All Lance-related projects have been moved to the [lance-format GitHub organization](https://github.com/lance-format) â€” remember to update your bookmarks, and don't forget to give us a star!
- Start contributing today in whatever way fits your skills and interests

The future of Lance is community-driven, and we can't wait to see where we go together. Whether you're contributing code, documentation, designs, advocacy, or supporting the community in some way, you're helping build the de facto standard format for multimodal AI.

Thank you for being part of the Lance community!
```

---

## ðŸ“„ æ–‡ä»¶: lance-data-viewer.md

---

```md
---
title: "Introducing Lance Data Viewer: A Simple Way to Explore Lance Tables"
date: 2025-09-24
draft: false
featured: false
categories: ["Community"]
image: /assets/blog/lance-data-viewer/preview-image.png
meta_image: /assets/blog/lance-data-viewer/preview-image.png
description: "A lightweight open source web UI for exploring Lance datasets, viewing schemas, and browsing table data with vector visualization support."
author: "Gordon Murray"
author_avatar: "/assets/authors/gordon-murray.png"
author_bio: "Infrastructure Engineer with a passion for building on AWS and Terraform, and a growing interest in data engineering"
author_twitter: "gortron"
author_github: "gordonmurray"
author_linkedin: "gordonmurray"
---

As Iâ€™ve been learning Lance one thing I wanted was an easy way to see inside my datasets. Something like DataGrip but for Lance tables. That need led me to build the Lance Data Viewer, a lightweight open source web UI you can run in a container to browse your Lance datasets and view schemas.

## Getting Started with Lance

I first came across Lance through LinkedIn. What caught my attention was the idea of storing data in S3. Its scalable, low cost, and common in data lake architectures but I was curious whether S3-backed databases could operate
 quickly enough to be application or customerfacing.

Another feature that stood out was Lanceâ€™s ability to store vector data. I work with OpenSearch, and while powerful, it can be time-consuming to maintain: indexes, shards, latency tuning. I wondered whether Lance could avoid those challenges while still delivering fast search.

## Learning by Building

To explore Lance, I created a couple of small projects:

* Image search: A site to upload images, generate vector embeddings, and store them in Lance. I was able to search across the images immediately, without complicated pipelines or infrastructure.
* To-do list: A simple app where tasks and comments are stored in Lance with vectors. Related tasks naturally surfaced together, and performance stayed fast even when running on a small VM.

These projects showed me that Lance isnâ€™t just for vectors it can store normal data like strings, numbers, and timestamps as well.

##  The Need for a Viewer

As I experimented more, I was creating multiple Lance tables. With relational databases, I was used to being able to open a viewer like DataGrip to review data. I wanted something similar for Lance: a way to browse tables, check schemas, and quickly look at whatâ€™s inside.

Thatâ€™s how Lance Data Viewer started, simply as a tool to help me debug my own datasets.

![Lance Data Viewer Screenshot](/assets/blog/lance-data-viewer/lance_data_viewer_screenshot.png)

## What Lance Data Viewer Does Today

* List datasets in a folder
* Inspect schema (columns, types, version)
* Browse rows with pagination and column selection
* Compact previews for vectors - instead of showing hundreds of numbers, the viewer renders a small sparkline or heatstrip so you can spot anomalies visually
* Multiple Lance versions supported - originally it only worked with Lance 0.3.1, but I updated it so you can now pull container images for different Lance versions.

One challenge I encountered early was Lance version compatibility. Different versions of Lance use different data formats, which is why Lance Data Viewer supports multiple Lance versions through different container tags. If you're working with older datasets, you might need the lancedb-0.16.0 or lancedb-0.3.4 containers.

The app is deliberately minimal:

* Backend: Python FastAPI
* Frontend: plain HTML, CSS, and JS. No frameworks
* Containerized: run with Docker, mount a folder of Lance data, and open in your browser


```
docker pull ghcr.io/lancedb/lance-data-viewer:latest
docker run --rm -p 8080:8080 \
  -v /path/to/your/lance:/data:ro \
  ghcr.io/lancedb/lance-data-viewer:latest
```

## Community Feedback

When I first shared the viewer on LinkedIn, I was surprised at the response. A number of impressions and encouraging feedback from the community, including the Lance maintainers. That validation has been motivating: what started as a small debugging tool might actually fill a gap for others too.

## Whatâ€™s Next

There are plenty of ideas Iâ€™d like to explore:

* Import/export support (CSV, Parquet, etc.)
* Human-friendly views of transactions and indexes
* Guidance on which indexes make sense as data grows
* More advanced vector visualizations for spotting issues in embeddings

For now, Lance Data Viewer is read-only, but even in this simple state it has already helped me understand my data and modernize older projects.

## Closing

I built the Lance Data Viewer out of necessity while learning Lance through my side projects. Itâ€™s now open source, containerized, and supports multiple versions of Lance.

If you work with Lance and need a way to explore your tables, give it a try Iâ€™d love to hear your feedback and ideas.
```

---

## ðŸ“„ æ–‡ä»¶: lance-file-2-1-smaller-and-simpler.md

---

```md
---
title: "Lance File 2.1: Smaller and Simpler"
date: 2025-03-27
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/lance-file-2-1-smaller-and-simpler/lance-file-2-1-smaller-and-simpler.png
description: "Explore lance file 2.1: smaller and simpler with practical insights and expert guidance from the LanceDB team."
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "westonpace"
---

Almost a year ago I announced we were going to be embarking on a journey to build a new 2.0 version of our file format. Several months later, we released a beta, and last fall it became our default file format. Overall, I've been super pleased with how well it worked. As we've been working with the community and stressing the format in production, we've identified a number of areas for improvement. We've been working on a 2.1 format for a few months to address these insights.

As our 2.1 format enters beta I wanted to take some time and look back on the 2.0 format and talk about what worked, what didn't work, and what we're going to be doing differently in 2.1.

## 2.0: We Got Rid of Row Groups

The most direct success of the 2.0 format, by far, was getting rid of row groups. We have not missed them...ever...at all. We have not lost any opportunity for parallelism or performance and we have removed the single biggest foot gun from Parquet. I described my reasoning for this already [in an earlier post](/lance-v2/) so I don't need to go into details here.

## 2.0: We Filled the I/O Queue

The most important performance concept in modern I/O, whether it is NVMe or cloud storage, is queue depth. If you do not have enough concurrent I/O requests in flight, you will leave bandwidth on the floor. In fact, it's even better to have too many I/O requests even though you face overhead and fairness issues. In 2.0 we built a scheduling algorithm to hit whatever queue depth we wanted, in priority order, without going over our parallelism budget. So we don't even have to pay those overhead and fairness costs.

![Disk Utilization Comparison](/assets/blog/lance-file-2-1-smaller-and-simpler/Disk-Utilization.png)
*If your disk/NIC isn't screaming then we aren't working hard enough*

I discussed most of this in depth [in an earlier post](/file-readers-in-depth-parallelism-without-row-groups/). However, I was not prepared for just how effective this was. I had hoped, originally, to match Parquet's scan speed while introducing random access. I figured this meant we would be slower than Parquet until we figured out compression. However, I was surprised to find that Lance often beats Parquet in full scans. This turned out to be true even when the file was 2-3 times bigger!

## 2.0: Our Flexible Container Format um...Flexed?

The overall structure of the file format has worked great. Our encoding scheme for 2.0 evolved significantly as it was developed. In 2.1 we pretty much reworked everything related to encodings. The protobufs have been many, and they have been varied, and they have enabled extremely rapid prototyping. Through all of this, I have not once encountered any reason to change the overall file structure. Pages, column descriptors, and a file footer are a simple, consistent, and extremely flexible structure.

![Lance File Format Structure](/assets/blog/lance-file-2-1-smaller-and-simpler/High-Level-Structure.png)
*Lance file format in four words*

{{< admonition fun-fact "ðŸ’¡ Victory Tour Complete" >}}
Ok, the victory tour is over. I'm sure there's other cool 2.0 things I could talk about but this is a retrospective and I suspect everyone is really just here to read the juicy takes about what went wrong.
{{< /admonition >}}

## 2.1: We're Squeezing out Compression via Structural Encoding

The 2.0 format has very little compression, primarily limited to dictionary compression and a few other tricks. This often surprises people coming from Parquet, but the truth is, compression isn't as critical for us. Our primary goal is working well with multimodal data such as embeddings, tensors, audio, video images, etc. These types are typically compressed already (often with lossy compression) and they're so large that any compression of metadata isn't as significant as you might expect. As a result, compression was a lower priority and, as we started using 2.0 internally, and customers started adopting it in production, we realized we needed to tie things off and ship something stable.

That being said, we've found some areas where compression is significant. Compression metadata doesn't have much impact on overall storage costs, but it can significantly speed up things like prefiltering. Also, semantic text is a very common part of our user's datasets, and some of these text-heavy datasets (e.g. Common Crawl, Fineweb, Github Code) have massive text columns which need solid compression.

Integrating compression into Lance is trickier than it might seem. Compression is implicitly related to I/O scheduling. For example, if you delta encode your values, or you strip out NULLs during compression, then it can become difficult to extract a single value from the compressed output. Fortunately, while we were working on 2.0, we had two brilliant interns run various experiments into compression, and this really helped us nail down these integration points and refine our encoding traits. What we discovered is that we were mashing together two different concepts. In 2.1 we have split the generic step of "encoding" into **structural encoding** and **compressive encoding**.

I'm going to be writing an entire blog post on this topic but structural encoding tells us how an "array" is split into a series of "buffers". Compressive encoding then focuses on how those buffers can be compressed. For example, in Arrow, we have a well defined standard for splitting an array into buffers. But...there are too many buffers! (this turns out to be bad for random access). In Parquet, we have a completely different standard for splitting an array into buffers that uses far fewer buffers, but can introduce read amplification. In Lance 2.1 we now have two different strategies that we switch between depending on the size of the value.

![Structural Encoding Comparison](/assets/blog/lance-file-2-1-smaller-and-simpler/Structural-Encoding.png)
*Dividing structure into buffers can be important for random access*

In fact, there was one more lesson for us. Once we figured out structural encoding we suddenly discovered a way to create a taxonomy for compression algorithms. For example, **transparent** compression algorithms (like bitpacking) support random access while **opaque** compression algorithms (like delta encoding) do not. You may think we obviously just use transparent compression everywhere butâ€”and here is where the magic comes inâ€”the structural encoding that you choose controls which category of compressive encodings you are allowed. There are times in Lance 2.1 where opaque compression is perfectly valid! I promise I will write a lot more about this later.

## 2.1: We're Knocking Out the 1-2 IOP Challenge

As I was working on 2.0 encodings I would often refer to something I called the "1-2 IOP challenge" (it's a punchy name ðŸ˜‰). The dream was to come up with an encoding so that I could access any element in an array in a single IOP (for fixed width data types) or 2 IOPS (for variable width data types). This sounds simple at first. We all know how to grab a string/binary value. First we grab the offsets and then we grab the value.

Unfortunately, it seems that someone has told customers about fancy data types. Also, customers love NULL values. As an example, let's look at what happens if you have a list of strings (e.g. tags, facets, etc.). This one array has many pieces of information.

What does this mean? Well, if you've been keeping up on your Arrow column format homework you know that we now have a struct validity buffer, a list validity buffer, a list offsets buffer, a string offsets buffer, a string validity buffer, and a string values buffer. In Lance 2.0 this, unfortunately, meant that we needed to perform 6 IOPS to fetch a single value. We have failed the 1-2 IOP challenge.

![Bad Tag Access Pattern](/assets/blog/lance-file-2-1-smaller-and-simpler/Bad-Tag-Access.png)
*In 2.0 we spend too many IOPS to read a single value*

In Lance 2.1 it turns out the answer is once again related to the idea of structural encodings. Recall that the structural encoding tells us how an array is split into buffers (that's important because it controls our IOPS). What's more, the structural encodings all have a concept known as a **repetition index**. The union of these two concepts turns out to be exactly what we need for the 1-2 IOP challenge and I am proud to say that we have solved it. No matter what data type you have, no matter how complex it is, no matter how many layers of validity you've stashed away, we can access any value in 1 or 2 IOPS. I'll definitely be writing more on this concept soon.

![Good Tag Access Pattern](/assets/blog/lance-file-2-1-smaller-and-simpler/Good-Tag-Access.png)
*In Lance 2.1 we only need 2 IOPS (and sometimes just 1) for the same data!*

## 2.1: We're Pushing Statistics so Far Down they Fall Out

During the 2.0 development process we had a working prototype of pushdown filtering. I didn't love it but it worked. It then broke. We fixed it. Then it broke again. As I write this that defunct prototype is still lurking in the code, but I've come to figure out why it just didn't stick.

There's a lot of reasons for this but I'll boil them down to "introducing all this compute complexity in the file format, when you have a fully functional and complex query engine just sitting there, is just a lot of work for little gain". What's more, keeping statistics decoupled from encoding can be [helpful for performance too](https://vldb.org/cidrdb/papers/2025/p19-prammer.pdf).

It turns out the answer, for us, is that statistics based pushdown is "just another index" (technically a *primary* index) and it is much easier for us to store this information **outside the file** (just like we do with all our other indices). This concept may sound strange but it has existed for a very long time, it just had a different name, the [zone map](https://docs.oracle.com/en/database/oracle/oracle-database/21/dwhsg/using-zone-maps.html#GUID-BEA5ACA1-6718-4948-AB38-1F2C0335FDE4) (just not the one you use for planting your garden).

![External Indices Flow](/assets/blog/lance-file-2-1-smaller-and-simpler/External-Indices.png)
*Once you can read arbitrary ranges from files then many things become possible*

By storing these indices outside the file we can pick which columns we want to index after we've already written the file. We can also retrain the index with a different block size without rewriting the file. What's more, we just so happen to have infrastructure for managing external indices lying around.

I can already hear you asking, "what if I don't have a query engine lying around?". Well, in that case we will give you one. We take some kind of compute engine like [Datafusion](https://datafusion.apache.org) (or possibly just arrow-rs) and shove it into a plugin. This plugin calculates the index during write and stores it in the file as a free global buffer. Then, during read, the plugin will apply the filter.

## 2.1 We're Experiencing Deja Vu with I/O Scheduling

One of the things we got right in 2.0 was I/O scheduling. It turns out this is one of the things we also got wrong. We could saturate the bandwidth on full scans but random access on NVMe is tricky. A good NVMe can perform close to a million reads a second (the $200 disk I have at home for playing games daily development can achieve ~800K reads/s). This means an average latency of almost 1 microsecond.

![Synchronous I/O Issues](/assets/blog/lance-file-2-1-smaller-and-simpler/Sync-I-O.png)
*These gaps can be larger than the time it takes for the I/O to run!*

At the risk of stating the obvious, this is not a lot of time. In 2.0 our poor little read request requires two thread transfers. First we put it in a queue to the I/O thread. Then we put the response back into a queue for the decode thread. As a result we were spending 2-3 microseconds of overhead for a request that only takes a single microsecond. Adding a fully synchronous blocking API brought our file reader from 40K values/second to 400K values/second in some low-level benchmarking! As part of the stabilization for 2.1 I'm hoping to look into this further, and make up new terms like "semi-synchronous I/O".

## 2.1 Timeline

The 2.1 format is now officially in beta. There are a few known limitations but it should work for most types. Follow our work and track its completion [here](https://github.com/lancedb/lance/milestone/4). If you want to run experiments you can enable the 2.1 format in a few ways:

### Dataset Level

```python
import lance
import pyarrow as pa

data = pa.table({"x": range(1000)})
ds = lance.write_dataset(data, "/tmp/test_ds", data_storage_version="2.1")
print(ds.data_storage_version)  # Should be 2.1
```

The data_storage_version is set when a new dataset is created and will control all data written by that dataset.

### File Level

```python
from lance.file import LanceFileReader, LanceFileWriter
import pyarrow as pa

data = pa.table({"x": range(1000)})
with LanceFileWriter("/tmp/test_file.lance", version="2.1") as writer:
    writer.write_batch(data)

reader = LanceFileReader("/tmp/test_file.lance")
print(reader.metadata())
```

If you'd prefer to work directly with Lance files for experimentation or lower-level access we have python bindings for a file reader/writer.

{{< admonition warning "âš ï¸ Beta Warning" >}}
By "beta" we mean "files you write with the beta may not be readable in the future". Please do NOT use unstable / beta versions with production data.
{{< /admonition >}}

Over the next few months we will be adding new tests, fixing some remaining todos, dogfooding, and tuning the performance. We encourage you all to try it out and do your own experiments. Feel free to start filing bugs about 2.1 and we will take a look. I'll also be writing additional blog posts going over the new features at a much lower level.

---

**Join the Conversation!** ðŸ‘©â€ðŸ’»

We're always happy to chat more on our [Discord](https://discord.gg/G5DcmnZWKB) and on [Github](https://github.com/lancedb/lance). Feel free to ask for more details or help us find better ways to do things! If something doesn't work or could be faster then let us know too.
```

---

## ðŸ“„ æ–‡ä»¶: lance-file-2-1-stable.md

---

```md
---
title: "Lance File 2.1 is Now Stable"
date: 2025-10-03
draft: false
featured: true
categories: ["Engineering"]
image: /assets/blog/lance-file-2-1-stable/preview-image.png
description: "The 2.1 file version is now stable, learn what that means for you and what's coming next."
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "westonpace"
---

The 2.1 version of the Lance file format has been [in beta](/blog/lance-file-2-1-smaller-and-simpler/) for a while now and we're excited to announce that it is now stable. This means we've [documented the spec](https://lancedb.github.io/lance/format/file/encoding/) and any potential breaking changes will now be part of 2.2 and we are committing to backwards compatibility of 2.1.

## Compression Without Impacting Random Access

A [recent paper](https://dl.acm.org/doi/10.1145/3749163) measured random access performance and stated:

> **Lance is the fastest** because it does not have cascading encoding or compression like the others, enabling direct
> offset calculation for certain types (e.g., integers) and minimizing read amplification."

This lack of compression was a significant limitation of 2.0. That's why the primary reason behind the 2.1 format
has been to introduce cascading encoding and compression _without sacrificing random access performance_. We wrote
more about the [structural encodings](/blog/columnar-file-readers-in-depth-structural-encoding/) that enable this feature and
studied this in more depth in our [research paper](https://arxiv.org/abs/2504.15247). I'm happy to report that we
were able to achieve our goal and avoid impacts to random access performance.

**I guess this means we're still the fastest.**

### Other Benefits

In addition to the compression benefits, there are a few other minor benefits added in the 2.1 format:

- Fewer IOPS when reading nested data (lists and structs)
- Support for distinguishing between null structs and null values.
- Optional repetition index caching to further reduce IOPS of variable-width data at the expense of more memory usage.

## How to Upgrade

The data storage version is set on a per-dataset basis. If you are happy with your dataset performance with 2.0, there
is no push to upgrade. We will of course continue to maintain 2.0. If you would like to take advantage of the new
features in 2.1 then you will need to make a copy of your dataset. The simplest way to do this is to do something
like this:

```python
import lance

ds = lance.dataset("my_2_0_dataset")
lance.write_dataset(ds, "my_2_1_dataset", data_storage_version="2.1")
```

## Should I Upgrade?

Some workflows will not significantly benefit. Vector embeddings, images, and audio are all pre-compressed and
often make up the majority of the data in a dataset, so there may not be much total impact. Compressing the smaller
columns will still speed up scans of those columns but not all workflows rely on scans if they make good use of
secondary indices.

The most likely workflows to benefit will be those that scan smaller columns as these workloads
are typically bound by the disk bandwidth. You may want to try converting a subset of your data to see if there is a
meaningful reduction in size or performance.

## Ensuring a Smooth Transition (Even if you Don't Upgrade)

The 0.38.0 release of Lance is the first release to fully support reading 2.1 files. You could potentially
run into trouble if you are reading dataset with older versions of Lance while writing 2.1 files with newer
versions of Lance.

As a result, we recommend upgrading all of your software to 0.38.0 or higher before you start writing 2.1
files. To facilitate this, we are not making 2.1 the default file format in 0.38.0. You will still need to
opt-in to 2.1 when writing a dataset:

```python
# In 0.38.0 you still need to opt-in to 2.1
lance.write_dataset(data, "my_2_1_dataset", data_storage_version="2.1")
```

We will be changing the default in the near future (potentially the next release). If you are planning on
keeping old versions of Lance around for some time you should ensure you explicitly set the 2.0 version:

```python
# You should explicitly set the data storage version to 2.0 if you
# plan on running a mixed environment with both older and newer versions
# of Lance.
lance.write_dataset(data, "my_2_0_dataset", data_storage_version="2.0")
```

In both cases this is only a concern for creating new datasets. Adding data to an existing dataset or updating
data will always use the data storage version of the dataset.

## What's Next?

Work on 2.2 has already begun. It is too early to say what will be in it for sure but we are excited to share
some previews of ideas we are working on.

### We want to make it simpler to migrate

We expect 2.1 to be the last version that will require a dataset copy to upgrade. 2.1 has established an overall
structure for file readers that will be consistent regardless of what new encodings are added. As a result,
we are hoping to support [mixed-version datasets](https://github.com/lancedb/lance/issues/4870) by the time we
release 2.2.

### Some cases need better compression

Our goal in 2.1 was to establish the overall strategy for compression and define nice easy-to-implement traits for
compression algorithms. We also implemented a number of popular lightweight compression techniques. However, there
are still gaps in our compression coverage that we hope to fill in 2.2. If you love columnar compression and are
interested in contributing, then a lot of these gaps might be nice starting issues. Look for some of the
`good_first_issue` tags in the [2.2 milestone](https://github.com/lancedb/lance/milestone/9).

### We want to fully support struct packing

We have supported struct packing for fixed-width fields for a while now. However, without support for variable-width
fields it is difficult to use the packing feature to it's fullest potential. We hope to
[add support](https://github.com/lancedb/lance/issues/2862) for variable-width fields soon. This will allow for
flexible trade-offs between row-major and column-major storage. This is important for use cases like model
training from cloud storage which can be dominated by random access read patterns on smaller materialized subsets
of the data.

### We plan to investigate better JSON encoding

We've been ramping up our [JSON support](https://github.com/lancedb/lance/discussions/3841) in the table format
with the addition of JSON indexes. We are also exploring how we can best store JSON data in the file format.
Common examples include JSONB and the new Parquet Variant data type. We hope to have more details on this in the future.

## Join the Conversation

We're always happy to chat more on our [Discord](https://discord.gg/G5DcmnZWKB) and on [Github](https://github.com/lancedb/lance). Feel free to ask for more details or help us find better ways to do things! If something doesn't work or could be faster then let us know too.
```

---

## ðŸ“„ æ–‡ä»¶: lance-namespace-lancedb-and-ray.md

---

```md
---
title: "Productionalize AI Workloads with Lance Namespace, LanceDB, and Ray"
date: 2025-09-04
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/lance-namespace-lancedb-and-ray/preview-image.png
meta_image: /assets/blog/lance-namespace-lancedb-and-ray/preview-image.png
description: "Learn how to productionalize AI workloads with Lance Namespace's enterprise stack integration and the scalability of LanceDB and Ray for end-to-end ML pipelines."
author: Jack Ye
author_avatar: "/assets/authors/jack-ye.jpg"
author_bio: "Software Engineer @ LanceDB"
author_github: "jackye1995"
author_linkedin: "https://www.linkedin.com/in/yezhaoqin"
---

In our [previous post](https://lancedb.com/blog/introducing-lance-namespace-spark-integration), 
we introduced [Lance Namespace](https://lancedb.github.io/lance/format/namespace/) and its integration with Apache Spark. 
Today, we're excited to showcase how to **productionalize your AI workloads** by combining:
- **Lance Namespace** for seamless enterprise stack integration with your existing metadata services
- **Ray** for data ingestion and feature engineering at scale
- **LanceDB** for efficient [vector search](/docs/search/vector-search/) and [fullâ€‘text search](/docs/search/full-text-search/)

This powerful combination enables you to build production-ready AI applications that 
integrate with your existing infrastructure while maintaining the scalability needed for real-world deployments.

## What's New

### Lanceâ€“Ray Integration

The [lance-ray](https://pypi.org/project/lance-ray/) package has now evolved into its own independent subproject, 
bringing seamless integration between Ray and Lance. It enables distributed read, write, and data evolution operations 
on Lance datasets using Ray's parallel processing capabilities, making it simple to handle large-scale data transformations 
and feature engineering workloads across your compute cluster.

### Lance Namespace Python and Rust SDKs

Lance Namespace now provides native Python and Rust SDKs that enable seamless enterprise integration across languages. 
This is what enables integration with both `lance-ray` and LanceDB.

## Building an End-to-End AI Pipeline

Let's walk through a complete example using real data from Hugging Face to build a question-answering system. 
We'll use the [BeIR/quora](https://huggingface.co/datasets/BeIR/quora) dataset to demonstrate the entire workflow.

### Step 1: Setting Up the Environment

First, install the required packages:

```bash
pip install lance-ray sentence-transformers datasets
pip install --no-deps lancedb==0.25.0
pip install --no-deps lance-namespace==0.0.14
```

Initialize your Ray cluster and import the necessary libraries:

```python
import ray
import pyarrow as pa
from lance_ray import write_lance, read_lance, add_columns
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import numpy as np

# Initialize Ray with sufficient resources for parallel processing
ray.init()

# Load the embedding model (we'll use it later)
model = SentenceTransformer('BAAI/bge-small-en-v1.5')
```

### Step 2: Initialize Lance Namespace

Lance Namespace provides a unified interface to store and manage your Lance tables across different metadata services. 
Depending on your enterprise environment requirements, you can choose from various supported catalog services:

```python
import lance_namespace as ln

# Example 1: Directory-based namespace (for development/testing)
namespace = ln.connect("dir", {"root": "./lance_tables"})

# Example 2: Hive Metastore (for Hadoop/Spark ecosystems)
# namespace = ln.connect("hive", {"uri": "thrift://hive-metastore:9083"})

# Example 3: AWS Glue Catalog (for AWS-based infrastructure)
# namespace = ln.connect("glue", {"region": "us-east-1"})

# Example 4: Unity Catalog (for Databricks environments)
# namespace = ln.connect("unity", {"url": "https://your-workspace.cloud.databricks.com"})
```

For this example, we'll use a directory-based namespace for simplicity, 
but you can seamlessly switch to any of the above options based on your infrastructure. 
See the [namespace implementations documentation](https://lancedb.github.io/lance/format/namespace/impls) 
for detailed configuration options of each integrated service.

### Step 3: Distributed Data Ingestion with Ray

Now let's load the Quora dataset and ingest it into [Lance format](/docs/overview/lance/) using Ray's distributed processing:

```python
# Load Quora dataset from Hugging Face
print("Loading Quora dataset...")
dataset = load_dataset("BeIR/quora", "corpus", split="corpus[:10000]", trust_remote_code=True)

# Convert to Ray Dataset for distributed processing
ray_dataset = ray.data.from_huggingface(dataset)

# Define schema with proper types
schema = pa.schema([
    pa.field("_id", pa.string()),
    pa.field("title", pa.string()),
    pa.field("text", pa.string()),
])

# Write to Lance format using namespace
print("Writing data to Lance format via namespace...")
write_lance(
    ray_dataset,
    namespace=namespace,
    table_id=["quora_questions"],
    schema=schema,
    mode="create",
    max_rows_per_file=5000,
)

print(f"Ingested {ray_dataset.count()} documents into Lance format")
```

### Step 4: Feature Engineering with Lanceâ€“Ray

Now we'll use Ray's distributed processing to generate embeddings for all documents. 

```python
def generate_embeddings(batch: pa.RecordBatch) -> pa.RecordBatch:
    """Generate embeddings for text using sentence-transformers."""
    from sentence_transformers import SentenceTransformer
    
    # Initialize model (will be cached per Ray worker)
    model = SentenceTransformer('BAAI/bge-small-en-v1.5')
    
    # Combine title and text for better semantic representation
    texts = []
    for i in range(len(batch)):
        title = batch["title"][i].as_py() or ""
        text = batch["text"][i].as_py() or ""
        combined = f"{title}. {text}".strip()
        texts.append(combined)
    
    # Generate embeddings
    embeddings = model.encode(texts, normalize_embeddings=True)
    
    # Return as RecordBatch with fixed-size list field
    return pa.RecordBatch.from_arrays(
        [pa.array(embeddings.tolist(), type=pa.list_(pa.float32(), 384))],
        names=["vector"]
    )

# Add embeddings column using distributed processing with namespace
print("Generating embeddings using Ray...")
add_columns(
    None, # no static URI
    namespace=namespace,
    table_id=["quora_questions"],
    transform=generate_embeddings,
    read_columns=["title", "text"],  # Only read necessary columns
    batch_size=100,  # Process in batches of 100
    concurrency=4,  # Use 4 parallel workers
    ray_remote_args={"num_gpus": 0.25} if ray.cluster_resources().get("GPU", 0) > 0 else {}
)

print("Embeddings generated successfully!")
```

The `add_columns` functionality in Ray allows ML/AI scientists to quickly start feature engineering
with a local or remote Ray cluster.
For more advanced feature engineering capabilities such as lazy materialization, partial backfill, 
fault-tolerant execution, check out [LanceDB's Geneva](https://lancedb.com/docs/geneva/) - 
our feature engineering framework that provides schema enforcement, versioning, and complex transformations. 
You can also follow our [multimodal lakehouse tutorial](https://lancedb.com/docs/tutorials/mmlh/) for comprehensive examples.

### Step 5: Vector Search with LanceDB

Now let's connect to our Lance dataset through [LanceDB](/docs) 
using the same namespace and perform vector similarity search:

```python
import lancedb
from sentence_transformers import SentenceTransformer

# Connect to LanceDB using the same namespace
db = lancedb.connect_namespace("dir", {"root": "./lance_tables"})
table = db.open_table("quora_questions")

# Create [vector index](/docs/indexing/vector-index/) for fast similarity search
print("Creating vector index...")
table.create_index(
    metric="cosine",
    vector_column_name="vector",
    index_type="IVF_PQ",
    num_partitions=32,
    num_sub_vectors=48,
)

# Perform vector similarity search
query_text = "How do I learn machine learning?"
model = SentenceTransformer('BAAI/bge-small-en-v1.5')
query_embedding = model.encode([query_text], normalize_embeddings=True)[0]

vector_results = (
    table.search(query_embedding, vector_column_name="vector")
    .limit(5)
    .to_pandas()
)

print("\n=== Vector Search Results ===")
print(f"Query: {query_text}\n")
for idx, row in vector_results.iterrows():
    print(f"{idx + 1}. {row['title']}")
    print(f"   {row['text'][:150]}...")
    print()
```

### Step 6: Full text search wit LanceDB

Now let's also do a full text search against the `text` column:

```python
print("Creating full-text search index...")
table.create_fts_index("text")

# Example 1: Fullâ€‘Text Search
keyword_results = (
    table.search("machine learning algorithms", query_type="fts")
    .limit(5)
    .to_pandas()
)

print("\n=== Full-Text Search Results ===")
print("Keywords: 'machine learning algorithms'\n")
for idx, row in keyword_results.iterrows():
    print(f"{idx + 1}. {row['title']}")
    print(f"   {row['text'][:150]}...")
    print()
```

### Step 7: Beyond the Examples

Now, you can continue playing around with the dataset.
You can add more feature columns with python functions through Ray.
LanceDB also allows [hybrid search](/docs/search/hybrid-search/) that combines the semantic understanding of [vector search](/docs/search/vector-search/) 
with the precision of [keyword matching](/docs/search/full-text-search/).
You can also load data into tools like PyTorch and LangChain for other AI activities.

## Real-World Use Cases

This integration pattern is particularly powerful for:

1. **RAG Applications**: Ingest documents, generate embeddings, and serve semantic search
2. **Recommendation Systems**: Process user interactions and build vector indices at scale
3. **Multimodal Search**: Process images and text together using Ray's distributed computing
4. **Feature Stores**: Transform and store ML features with versioning via Lance Namespace
5. **Real-time Analytics**: Combine batch processing with low-latency search

## Getting Started Today

Ready to scale your AI workloads? Here's how to get started:

1. **Install the packages**: `pip install lance-ray lancedb`
2. **Read the documentation**: [Lanceâ€“Ray](https://lancedb.github.io/lance/integrations/ray/), [LanceDB](/docs), [Vector Search](/docs/search/vector-search/), [Fullâ€‘Text Search](/docs/search/full-text-search/), [Hybrid Search](/docs/search/hybrid-search/), [Vector Indexing](/docs/indexing/vector-index/), [FTS Indexing](/docs/indexing/fts-index/), [Filtering](/docs/search/filtering/), [Reranking](/docs/reranking/), [Quickstart](/docs/quickstart/basic-usage/), [LanceDB Geneva](/docs/geneva/)
3. **Join the community**: [Discord](https://discord.gg/zMM32dvNtd) and [GitHub Discussions](https://github.com/lancedb/lance/discussions)

## Thank You to Our Contributors

We'd like to extend our heartfelt thanks to the community members who have contributed to 
making this integration a reality, shoutout to:

- **Enwei Jiao** from Luma AI
- **Bryan Keller** from Netflix
- **Jay Narale** from Uber
- **Jay Ju** from ByteDance  
- **Jiebao Xiao** from Xiaomi

Your contributions, feedback, and real-world use cases have been instrumental 
in shaping this integration to meet the needs of production AI workloads.

## Conclusion

The combination of [Lance Namespace](https://lancedb.github.io/lance/format/namespace/), Ray, and [LanceDB](/docs) provides a complete solution for productionalizing AI workloads. 
[Lance Namespace](https://lancedb.github.io/lance/format/namespace/) ensures seamless integration with your existing enterprise metadata services, 
Ray delivers the distributed computing power needed for data ingestion and feature engineering at scale, 
and LanceDB provides efficient [vector search](/docs/search/vector-search/), [fullâ€‘text search](/docs/search/full-text-search/), and [hybrid search](/docs/search/hybrid-search/) capabilities for serving your AI applications.

This integrated approach bridges the gap between experimentation and production, 
enabling you to build AI systems that not only scale but also fit naturally into your existing infrastructure. Get started with the [Quickstart](/docs/quickstart/basic-usage/) or explore [indexing](/docs/indexing/vector-index/) options.

Whether you're building a [RAG system](/docs/tutorials/rag/), recommendation engine, or [multimodal search](/docs/tutorials/mmlh/) application, 
this powerful trio gives you the enterprise integration, scalability, and performance you need for production deployments.

Try it out today and let us know what you build! We're excited to see how you use [Lance Namespace](https://lancedb.github.io/lance/format/namespace/), Ray, and [LanceDB](/docs) 
to productionalize your AI workloads.
```

---

## ðŸ“„ æ–‡ä»¶: lance-v2.md

---

```md
---
title: "Lance v2: A New Columnar Container Format"
date: 2024-04-13
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/lance-v2/lance-v2.png
description: "Explore lance v2: a new columnar container format with practical insights and expert guidance from the LanceDB team."
author: Weston Pace
author_avatar: "/assets/authors/weston-pace.jpg"
author_bio: "Data engineer from the open source space, working on LanceDB, Arrow, Substrait."
author_twitter: "westonpace"
author_github: "westonpace"
author_linkedin: "westonpace"
---

## Why a New Format?

Lance was invented because readers and writers for existing column formats did not handle AI/ML workloads efficiently. Lance v1 solved some of these problems but still struggles in a number of cases. At the same time, others ([btrblocks](https://github.com/maxi-k/btrblocks), [procella](https://research.google/pubs/procella-unifying-serving-and-analytical-data-at-youtube/), [vortex](https://github.com/fulcrum-so/vortex)) have found similar issues with Parquet in their own use cases. I'd like to talk about a new format, Lance v2, that will solve these issues, but first let me describe the various use cases we have learned about by working with modern workloads.

### Point Lookups

A point lookup is a query that accesses a small set of rows. This is essential whenever you are using a secondary index. For example, both semantic search and full text search end up as point lookups in LanceDB. Parquet's main challenge with point lookups is that its encodings are not designed to be "sliceable" and you typically need to load an entire page of data to access a single row. This is especially bad for multi-modal workloads because our values are typically quite large and coalescing is much harder.

![Parquet Point Lookups Challenge](/assets/blog/lance-v2/parquet-point-lookup.png)
*Parquet faces challenges satisfying point lookups*

### Wide Columns

Wide columns are columns where each value is very large. Traditional db workloads use fairly small columns (floats, doubles, etc.) Strings are often the largest column but even they are usually quite small in practice. In ML workloads we often want to store tensors like semantic search embeddings (e.g. 4KiB CLIP embeddings) or even images (much larger).

![Wide Columns Challenge](/assets/blog/lance-v2/wide-columns-grouping.png)
*Picking a good row group size is impossible when a file has a wide column*

### Very Wide Schemas

Many user workloads involve very wide schemas. These can range from finance workloads (which sometimes have a column per ticker) to feature stores (where there can be thousands of features for a given record). Parquet and other columnar formats help by giving us powerful column projection but we still need to load the schema metadata for all columns in the file. This is a significant cost for low-latency workloads and potentially memory intensive when caching metadata across many files.

![Wide Schemas Performance](/assets/blog/lance-v2/wide-schemas.png)
*Many Parquet readers do not perform well on very wide schemas, even with highly selective column projection*

### Flexible Encodings

Parquet supports a powerful set of encodings, but it doesn't keep up with the development of new, sometimes purpose-specific, encodings. Adding a new encoding requires modifying the file reader itself. This is difficult as there are many Parquet implementations and development of Parquet has slowed now that the technology has matured.

### Flexible Metadata

In Parquet, an encoding can only control what goes into the data page. This means that encodings have no access to the column or file metadata. For example, consider dictionary encoding, where we know the dictionary will be constant throughout a column. We would ideally like to put the dictionary in the column metadata but we are instead forced to put it into every single row group. Another use case is skip tables for run length encoded columns. If we can put these in the column metadata then we can trade slightly larger metadata for much faster point lookups into RLE encoded columns.

![Metadata Flexibility Issues](/assets/blog/lance-v2/flexible-metadata-parquet.png)
*A lack of metadata flexibility forces some unfortunate encoding decisions in various cases*

### And More

We have considered more use cases, some of them perhaps a bit esoteric, when building Lance v2, and I don't want to bore you by listing them exhaustively. Instead, here is a short list of remaining things we have considered and incorporated that don't merit further explanation:

- Storing columns that do not have the same # of rows (e.g. non-tabular data)
- Writing data one array at a time, instead of one record batch at a time
- Configurable and more flexible alignment, for example, to allow pages to be aligned to 4KiB sectors for direct I/O
- Writing extremely large cells (e.g. 50MiB videos) by treating the videos as "blobs" (they don't go inside pages) and only storing the blob locations/lengths in a page.

## The Format

Now let me describe the [Lance v2 format](https://github.com/westonpace/lance/blob/821eb0461e7e474155485db32ac589b1933ef251/protos/file2.proto) (take a look, it's less than 50 lines of protobuf) and explain how it solves the various use cases I have mentioned above.

![Lance v2 Format Overview](/assets/blog/lance-v2/lance-v2-format.png)
*A high level overview of the Lance v2 format*

ðŸ¥±

The rest of this article goes into detail about the file format's design decisions. If your goal is just to understand what problems we are solving then you can skip it or feel free to come back and read later when you're well rested.

## Feature 1: Encodings are Extensions

Lance v2 does not have encodings. These are handled entirely by extensions. The file readers and writers are completely unaware of any kind of encoding (even plain encoding). The data producer's choice of what encoding to use and the details of how that encoding works are determined by plugins. New encodings can be written without any modification to the file format, the file writer, or the file reader. This is done by storing all encoding descriptions as protobuf "any" messages. This is why we refer to Lance v2 as a "columnar container format" (inspired by [container formats](https://en.wikipedia.org/wiki/Container_format) used for storing video).

### Corollary 1: There is No Type System

The Lance format itself does not have a type system. From Lance's perspective, every column is simply a collection of pages (with an encoding) and each page a collection of buffers. Of course, the Lance readers and writers will actually need to convert between these pages into typed arrays of some kind. The readers and writers we have written use the [Arrow type system](https://arrow.apache.org/docs/format/Columnar.html).

The primary benefit is that it keeps the specification of the format simple. I do not have to tell you what types are supported and waste time describing them. It also helps avoid creating "yet another type system"

![Lance v2 Conceptual Overview](/assets/blog/lance-v2/Lance-v2-Conceptual-Overview.png)
*Conceptually, Lance v2 is quite simple*

### Corollary 2: Empowering Encoding Developers

This makes it extremely easy to add a new encoding to Lance and helps to make the ecosystem fragmentation that Parquet has seen (e.g. mixed support for page lookups, delta encoding, etc.) more manageable. If I develop some new encoding then I just need to publish a .proto file to standardize how the encoding is described and write at least one implementation of the encoder/decoder. If a user tries to read a file written with this encoding, and their reader does not support it, then they will receive a helpful error "this file uses encoding X and the reader has not been configured with a decoder". They can then figure out how to install the decoder (or implement it if need be). All of this happens without any change to the Lance format itself.

For example, take a look at our [current set of encodings](https://github.com/lancedb/lance/blob/v0.10.10/protos/encodings.proto), which encode Arrow data in a fairly plain format.

## Feature 2: Abolish Row Groups

Row groups (or stripes) are a clever idea that have outlived their usefulness. Some days, it feels like I keep [writing](https://stackoverflow.com/a/76782501) the same [book](https://lists.apache.org/thread/4nhbwplpj351g1xjqh5wqtb0nz3vw30k) on finding the proper balance for this finicky parameter. If the row group size is too small then you end up with excess metadata, poor performance, and most importantly, you end up with runt pages which are below the optimal read size for a filesystem (and you can't coalesce your way out of this problem because the pages you want are pretty much guaranteed to be maximally spread throughout the file).

If the row group size is too large then, at a minimum, you are going to require a significant amount of RAM to be buffered in your file writer. However, many existing Parquet readers (e.g. pyarrow) treat the row group size as the unit of computation. This means that you end up using way too much RAM when reading a file too.

![Row Group Conundrum](/assets/blog/lance-v2/Row-Group-Conundrum.png)

Row groups are a bad choice for multi-thread parallelism (within a process). Imagine you have 10 cores and so you try to read 10 row groups at once. Each row group reader now has to read (let's say) 5 columns. This means you will launch 50 IOPS simultaneously. First, this is possibly over-scheduling your I/O. Second, in the average / expected case you won't start compute until roughly all 50 of these IOPS complete. There are much better ways to do multi-thread parallelism (pipeline parallelism instead of data parallelism).

![Row Group Multithreading Issues](/assets/blog/lance-v2/Row-Group-Multithreading.png)

Row groups are a reasonable choice for multi-process parallelism. Butâ€¦so are files. From our perspective, a file with 10 row groups is no different than 10 files. If you need multi-process parallelism then just make 10 files. This will be easier to manage anyways (e.g. sharding)

Lance v2 does away with row groups, and the best part is that there are zero tradeoffs. You will get maximally sized data pages with minimal RAM buffering in the writer and reader. The trick is fairly simple, *we do not require pages within a column to be next to each other*. When a column writer has accumulated enough data it will flush a page to disk. Since pages should be larger than the minimum read of the filesystem there is no penalty to performance when reading a single column in entirety. Some writers will write lots of pages. Some writers will only write a single page throughout the entire file.

This may sound fantastical or exotic but this is actually very close to what happens in parquet-rs today when the file has only a single row group and a page lookup table is used. We are simply making this the norm and, because we do that, we can be a bit more flexible in the way we write the file.

### Corollary 1: Ideal Page Sizes

Each column writer will have its own buffer that it uses when writing pages. These can all be configured to match the ideal page size for your filesystem (e.g. 8MiB is a good choice for S3). The only time a page will be less than 8MiB is when it is the last page for that column within the file.

Imagine you are writing 10Mi rows to a file using 8MiB pages. One column is 8-byte doubles and so you want to write 10 pages (probably less with compressive encodings). Another column is a boolean column and so you only need to write 1 page (you can fit 64Mi bools into an 8MiB page).

### Corollary 2: Decoupling I/O and Compute

Now, imagine we are reading that file, let's pretend there are only those two columns. First we will read the first double page and the boolean page (there is only one). As soon as those two 8MiB reads complete now have 1Mi values of the double column and 10Mi values of the boolean column.

We get to choose the batch size for our read, completely independent of our I/O size. Let's assume we decided we want to process 10Ki rows at a time. Once those two pages arrive we can launch 100 decode tasks, one for each 10Ki rows we have. While those decode tasks are running we can fetch the remaining double pages.

![Lance Read Architecture](/assets/blog/lance-v2/Lance-Read-Architecture-2-.png)
*Lance uses a two-thread read algorithm to decouple I/O parallelism from compute parallelism*

## Feature 3: Flexibility

Columns in Lance do not need to be the same length. When writing a Lance file, it is possible to write an "array at a time" instead of writing a "batch at a time" (or even a mixture of the two approaches). Data can be placed in a page buffer, a column buffer, or a file buffer. This allows the format to be more flexible and opens up the door to a whole new branch of use cases that Parquet is not eligible for.

### Corollary 1: "True" Column Projection

The Lance metadata is structured in such a way that each column's metadata is in a completely independent block. Partly this is possible because there is no "schema" (though you can store one in the file-wide metadata if you want) since there are no types. What this means is that you can read a single column without reading any metadata from any other column. It should be possible to have hundreds of thousands or even millions of columns in a Lance file with no impact on performance.

### Corollary 2: Fluidity Between Data & Metadata

Back when Arrow-cpp was initially being developed there was some confusion on how dictionary encoding should work. Dictionary encoding encodes an array into "indices" and a "dictionary". The indices take the normal spot in the data page...but what about the dictionary? Should the dictionary be considered *data* (and part of the record batches) or *metadata* (and part of the schema)? Both approaches have pros and cons. Eventually, since Parquet chose to store dictionaries as data then Arrow followed suit (although the IPC format occupies a strange middle ground). On the other hand, Lance v1 chose the opposite approach because putting dictionaries in the data pages gave point lookups unreasonable latency.

Lance v2 allows the writer to choose the most appropriate location. If the dictionary can be provided up front then the writer should put it in the column metadata. If it differs from page to page then the writer should write page specific dictionaries. This fluidity goes way beyond dictionaries however. Skip tables for RLE and zone maps (discussed further soon) can also be placed in column metadata. Even nullability information can be stored in the column metadata (e.g. if a column only has a few nulls the put the nullability in the metadata to avoid another IOP on point lookups).

![Flexible Metadata Approaches](/assets/blog/lance-v2/Flexible-Metadata-1-.png)
*Lance supports all approaches, allowing the encoder to choose which is most appropriate*

What's more, any piece of information that is truly file-wide (e.g. the arrow schema of the data or a common dictionary shared by several columns) can be stored into a space for file-wide buffers. Encodings can make this decision (where to store a piece of data) on-the-fly based on the data they are writing.

### Corollary 3: Statistics are Encoding Metadata

In Parquet, file statistics (e.g. zone maps) are a 1st-class entity and part of the top-level format. However, this means a format change is required to support a new kind of statistics. It turns out, there are many ways to store statistics (e.g. skip tables in RLE, histograms, bloom filters, etc.) and so this is kind of inconvenient. Also, making them part of the top-level format means your zone map resolution is tied to your data page size, encouraging either small pages or large zones (this is not as bad as it seems, as tiny pages in big column chunks can be coalesced, unlike tiny pages in tiny column chunks discussed earlier, but the two concepts still don't need to be related).

![Statistics as Metadata](/assets/blog/lance-v2/StatsAsMetadata.png)
*Traditional min/max/null count zone maps become a column metadata buffer*

In Lance v2, statistics are just a part of the encoding process. A zone map is just a proxy layer in the encoding and the resulting zone maps can be stored in the column metadata (in the same way any piece of information can be stored in the column metadata). These can then be utilized on read to apply pushdown filters the same way zone maps are used today. Switching to a different statistics format is simply another encoding and doesn't require a change in the format.

## We Can Use Your Help

We currently have an [initial implementation](https://lancedb.github.io/lance/api/python/lance.html#module-lance.file). This only uses simple encodings and doesn't yet use offer most of the features discussed, but it adds the groundwork for the format. We have verified that performance is on par with the best Parquet readers (currently this is only true when data is not very compressible but we expect this will be true in general as we add better encodings). The one thing we do not need at the moment is development help. However, we need help in just about every other way.

- If you have additional use cases for a file format that we have not covered yet then please contact us. The best way to contact us is to join our [Discord server](https://discord.gg/G5DcmnZWKB).
- Let us know if there are any benchmarks you are interested in testing the new format against. We plan to develop a few of our own but would be happy to work with you to run against pre-existing benchmarks.
- We could also use help coming up with example integration tests or integrating with existing Parquet integration suites.
- Feel free to start using the initial drafts of the format and let us know what issues you run into.

```python
import pyarrow as pa

from lance.file import LanceFileReader, LanceFileWriter

# It's still early days so doing too much more than this will
# probably break :)
table = pa.table({
    "my_list": [[1, 2, 3], [4, 5]],
    "my_struct": [
        { "x": 1, "y": 1 }, { "x": 2, "y": 2 }
    ],
    "my_col": [11.0, 12.0]
})

# No surprise in reader/writer API I hope
with LanceFileWriter("/tmp/foo.lance", schema=table.schema) as writer:
    # Currently only support writing a batch at a time, array at a
    # time API to come later
    writer.write_batch(table)

reader = LanceFileReader("/tmp/foo.lance", schema=table.schema)
# Can read whole table (to_table) or batch at a time (to_batches)
assert table == reader.read_all().to_table()

# Perhaps the most interesting bit is we can print the metadata to
# see what encodings are used and how big the metadata is
print(reader.metadata())
```

- We will eventually want to add ecosystem integrations. Let us know if you have an integration in mind or if you have bandwidth to help make this happen.
- If you're also developing a new format, or working on new encodings, then contact us (see discord above), and let's see where we can collaborate (we have done very little work so far on encodings, focusing mostly on the format), especially if you're working in Rust.
- We will have bindings in Rust and Python but if you're interested in building bindings for another language then let us know.
```

---

## ðŸ“„ æ–‡ä»¶: lance-windows-windows-lance.md

---

```md
---
title: "Lance, Windows. Windows, Lance"
date: 2023-05-31
author: Chang She
author_bio: CEO and Co-founder of LanceDB
author_github: changhiskhan
author_twitter: changhiskhan
author_avatar: "/assets/authors/chang-she.jpg"
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/lance-windows-windows-lance-3328ff6129b/preview-image.webp
meta_image: /assets/blog/lance-windows-windows-lance-3328ff6129b/preview-image.webp
description: "It was Spring of 2012.  After being an avid user for 2+ years, I finally decided to join Wes Mckinney and work on pandas full time."
---

It was Spring of 2012. After being an avid user for 2+ years, I finally decided to join Wes Mckinney and work on pandas full time. We worked out of an apartment in Brooklyn on pandas by day. By night Wes was working on the first edition of Python for Data Analysis while I was working on what probably was the very first python package for quantitative portfolio management.

ðŸ‘· In the days before github actions, distributing your python package was a very manual affair. We had a Mac Mini that we used to build the mac wheel, and our own linux desktops to build the linux wheel. We had no windows machines and didnâ€™t have the bandwidth to deal with yet another toolchain.

ðŸš‘ And thatâ€™s when Christoph came to the rescue. I knew him best by his academic url [https://www.lfd.uci.edu/~gohlke](https://www.lfd.uci.edu/~gohlke), where he build windows binaries for pandas, along with a lot of other packages in the scientific python stack that didnâ€™t build windows wheels (or sometimes no wheels at all). Prior to each release we would send an email to Christoph and magically windows wheels would show up in a few days.

ðŸ˜¢ I was saddened to come across this [old Reddit post](https://www.reddit.com/r/Python/comments/vcaibq/christoph_gohlkes_windows_wheels_site_is_shutting/) saying that Christophâ€™s lab has lost funding and the windows binaries would stop updating. I sincerely hope that work is able to continue elsewhere and heâ€™s able to find funding elsewhere. Without his contributions to each and every package he maintained, the python data community would be largely devoid of windows users.

## Windows support is coming to Lance

Christoph and so many unsung heroes of the python OSS community have been keeping the whole scientific python stack standing all these years. It is with this feeling of immense gratitude that weâ€™re announcing windows support in Lance starting v0.4.0.

## Installation and usage

Installing Lance on Windows is the same as other OS: `pip install pylance` . Windows wheels are only available starting v0.4.0.

Using Lance in python is also the same with `import lance` , `lance.dataset(uri)` , and `lance.write_data(data, uri)` .

## Non-user facing changes

Required changes for supporting windows came in two flavors:

1. Importing different libraries based on OS. This is no different than say using accelerate on MacOS vs openblas on Linux. Sometimes if MacOS and Linux behavior should be the same but different from windows, you can use `cfg(target_os = â€œunixâ€)` as the flag.
2. Where the rubber meets the operating system. This usually manifested as dealing with file paths, tildeâ€™s, and backwards vs forward slashes. Fortunately Rust crates like `shellexpand` takes care most of this as well.

## Packaging and testing

Turns out most of the time and effort was spent in testing changes, setting up the right dev / test environments, and also including the required dllâ€™s with the final python wheel.

The testing loop for github actions is slow so adding the required GHA for CI and CD are particularly frustrating. Manually installing protoc, configuring vcpkg, caching, etc etc etc. Each problem / task would take a long time.

## Limitations

In this first milestone for windows support, we donâ€™t yet link to LAPACK, which means that the OPQ option for the vector index in Lance is disabled on Windows. Weâ€™ll be addressing this soon.

## Acknowledgements

Windows support would not be possible without the great work of Giancarlo Silvestrin, with additional contributions from Dennis Collinson.

## Feedback welcome

If youâ€™re on windows and interested in working with data on python, you can try out lance with `pip install -U pylance` . Weâ€™d love to hear your feedback. For more details, please check us out [on github](http://github.com/eto-ai/lance) and donâ€™t forget to give us a ðŸŒŸ if you like us!

Last but not least â€” Thank you Chistoph!
```

---

## ðŸ“„ æ–‡ä»¶: lancedb-x-continue.md

---

```md
---
title: "Developers, Ditch the Black Box: Welcome to Continue"
date: 2024-05-23
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/lancedb-x-continue/preview-image.png
meta_image: /assets/blog/lancedb-x-continue/preview-image.png
description: "Remember flipping through coding manuals?  Those quickly became relics with the rise of Google and Stack Overflow, a one-stop shop for developer queries."
---

Remember flipping through coding manuals? Those quickly became relics with the rise of Google and Stack Overflow, a one-stop shop for developer queries. Now, the tides are turning again as LLMs are rapidly becoming the primary source for coding assistance.

Existing closed-source API-based LLM developer tools are frustratingly opaque. We copy-paste from ChatGPT, which does not disclose its sources or thought process. Copilot throws out suggestions, but on what basis? Meanwhile, as we use these tools, we contribute valuable code and implicit feedback that trains their LLMs. Yet, this data remains private.

Surely, there needs to be a better way. Developers deserve **transparency, hackability, and control**. We need to understand the "why" behind LLM suggestions, have the freedom to tinker, and own the data that fuels these powerful models and their outputs.

That's where **Continue** steps in. A new generation of LLM developer tools puts control back in the hands of developers.

## Continue: A modular AI software development system

The basic use-case of continue is that it removes the need of repetitive copy/pasting from ChatGPT or similar services. Instead, you can simply highlight the context and ask questions in the sidebar or have an edit streamed directly to your editor.

Continue is available as open-source [VSCode](https://marketplace.visualstudio.com/items?itemName=Continue.continue) and [JetBrains](https://plugins.jetbrains.com/plugin/22707-continue-extension) extensions.
![Continue extension screenshot](/assets/blog/lancedb-x-continue/Screenshot-2024-05-22-at-4.26.28-PM.png)
Continue also provides powerful tools for managing context. For example, you can type â€˜`@issue`â€™ to quickly reference a GitHub issue as you are prompting the LLM, â€˜`@README.md`â€™ to reference such a file, or â€˜`@google`â€™ to include the results of a Google search.

Continue works with any LLM, including local models using ggml or open-source models hosted on your own cloud infrastructure, allowing you to remain 100% private if security is a concern.
![Continue context tools](/assets/blog/lancedb-x-continue/Screenshot-2024-05-12-at-3.09.33-PM.png)
It automatically gathers information about how you code. This includes things like the steps you take and the choices you make. This data is stored locally on your machine (in a folder called "`.continue/dev_data`").

By combining this data with the code you commit, Continue can actually improve the Large Language Model (LLM) used by your team. This means the AI coding assistant everyone uses will get smarter and more helpful over time, based on your contributions!

## Continue â¤ï¸ LanceDB
![Continue + LanceDB](/assets/blog/lancedb-x-continue/Screenshot-2024-05-12-at-3.18.44-PM.png)
As Continue offers local-first IDE extensions, most of the codebase is written in Typescript, and the data is stored locally in the `~/.continue` folder. The tooling choices are made such that there are no separate processes required to handle database operations.  It uses SQLite as a transactional database because there are libraries with great Typescript support, it is embedded, and is stored gracefully as a single file. Continue's codebase retrieval features are powered by LanceDB, as it is the only vectorDB with an embedded Typescript library that is capable of fast lookup times while being stored on disk and also supports SQL-like filtering.

## Further customizations

Continue offers a ton of room for further customization. You can write your own:

- Slash commands (e.g., â€˜`/commit`â€™ to write a summary and commit message for staged changes; â€˜`/docs`â€™ to grab the contents of a file and update documentation pages that depend on it; â€˜`/ticket`â€™ to generate a full-featured ticket with relevant files and high-level instructions from a short description)
- Context sources (e.g., GitHub issues, Jira, local files, StackOverflow, documentation pages)
- Templated system message (e.g., â€œAlways give maximally concise answers. Adhere to the following style guide whenever writing code: `{{ /Users/nate/repo/styleguide.md }}`â€
- Tools (e.g., add a file, run unit tests, build, and watch for errors)

Learn more about using Continue: [Customization overview](https://docs.continue.dev/customization/overview).
```

---

## ðŸ“„ æ–‡ä»¶: late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index.md

---

```md
---
title: "Late Interaction & Efficient Multi-modal Retrievers Need More Than a Vector Index"
date: 2024-09-18
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index.jpg
description: "Explore late interaction & efficient multi-modal retrievers need more than a vector index with practical insights and expert guidance from the LanceDB team."
author: Ayush Chaurasia
author_avatar: "/assets/authors/ayush-chaurasia.jpg"
author_bio: "ML Engineer and researcher focused on multi-modal AI systems and efficient retrieval methods."
author_twitter: "ayushchaurasia"
author_github: "ayushchaurasia"
author_linkedin: "ayushchaurasia"
---

A lot has happened in the last few months in AI. Let's look at what's new with document retrieval. 

Try this through on Colab: [https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb)

## Late Interaction

Retrieval models rely on embedding similarity between query and corpus(documents)

![Retrieval Architecture](/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-12-at-11.19.05-AM.png)

**Representation-focused rankers** independently compute an embedding for q and another for d and estimate relevance as a single similarity score between two vectors. Think of cosine similarity on bi-encoder outputs of doc and query.

**Query-Document rankers** Instead of summarizing q and d into individual embeddings, these rankers model word- and phrase-level relationships across q and d and match them using a deep neural network (e.g., with CNNs/MLPs). In the simplest case, they feed the neural network an interaction matrix that reflects the similarity between every pair of words across q and d.

**All-to-all Interaction rankers** is a more powerful interaction-based paradigm, which models the interactions between words within as well as across q and d at the same time, as in BERT's transformer architecture. **While interaction-based models tend to be superior for IR tasks, a representation-focused modelâ€”by isolating the computations among q and dâ€”makes it possible to precompute document representations,** greatly reducing the computational load per query. 

**Late interaction rankers** offer a new architecture where every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. This paradigm allows ColBERT to exploit deep LM-based representations while shifting the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents.

**ColBert creates a vector index of all documents in the corpus offline, then computes the MaxSim during the query time.**

## Late Interaction + VLMs = Vision Retriever!

![Final Architecture](/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/final_architecture.png)

ColPali is a visual retriever model that combines the following:

- PaliGemma - A VLM combines [**SigLIP-So400m/14 vision encoder**](https://huggingface.co/google/siglip-so400m-patch14-384) and [**Gemma-2B**](https://unfoldai.com/gemma-2b/) language model. It also introduces projection layers to map language model inputs to 128-dim vectors
- A late interaction mechanism based on ColBert

Like ColBert, ColPali works in 2 phases:

**Offline:**

- Each document input is processed through the vision encoder and in patches. Each patch is then passed through the projection layer to get its vector representation.
- Then these vectors are stored as multi-vector representations of the documents for retrieval at the query time
- Each document is divided into 1030 patches with each patch resulting in a 128-dim vector.

**Online:**

- At query time, the user input is encoded using a language model
- Using the late interaction mechanism, MaxSims are calculated between query and already embedding document patches.
- The similarity score of each patch is summer across the pages & the top K pages with maximum similarity score are returned as the final result

## Let's Try It Out!

In this example, we'll make retrieval challenging by ingesting documents of very different genres:

- Investor relations/ Financial reports of Q2 2024 from:
  - Apple
  - Amazon
  - Meta
  - Alphabet
  - Netflix
  - Starbucks

- Naturo Volume 72
- Arabian Nights 
- Children's short story collection.
- InraRed Cloud report
- Short Stories for Children by Cuentos para la clase de InglÃ©s

You can also run it yourself in this colab walkthrough. The code is based on original work by [Kyryl Truskovskyi](https://x.com/truskovskiy) which can be found [here](https://kyrylai.com/2024/09/09/remove-complexity-from-your-rag-applications/)

### Need for an Efficient Retriever

In this pipeline, building an efficient retriever is of utmost importance as each query is compared to each document, which is stored in patches. Moreover, efficiently storing and retrieving the document images allows us to store the data and metadata in the same place to retrieve it when passing it on to the generator in case of a RAG setting. Efficient multi-modal data storage and retrieval is where LanceDB shines. It offers strong compute-storage separation, providing fast random access for retrieval while being persisted in storage. 

The codebase is a derivative work on [vision-retriever](https://github.com/kyryl-opens-ml/vision-retrieval). It adds support for better ingestion through batch iterators which allows ingesting large datasets into Lance without running out of memory, meaning you can store your documents, vectors, and metadata in the same place without running OOM and also get fast retrieval and added features that we'll see next.

### Searching for Relevant Docs

Let's take a look at some of the query and retrieved document pairs to get an idea of ColPali's performance.

Retrieving the docs from the table and then performing MaxSims operation on them with a query will look something like this with LanceDB:

```python
from colpali_engine.trainer.retrieval_evaluator import CustomEvaluator

def search(query: str, model, processor, top_k: int = 3):
    qs = get_query_embedding(query=query, model=model, processor=processor)

    # Retrieve all documents
    r = table.search().limit(None).to_list()

        
    def process_patch_embeddings(x):
        patches = np.reshape(x['page_embedding_flatten'], x['page_embedding_shape'])
        return torch.from_numpy(patches).to(torch.bfloat16)
    
    all_pages_embeddings = [process_patch_embeddings(x) for x in r]
    
    retriever_evaluator = CustomEvaluator(is_multi_vector=True)
    scores = retriever_evaluator.evaluate_colbert([qs["embeddings"]], all_pages_embeddings)

    top_k_indices = torch.topk(scores, k=top_k, dim=1).indices

    results = []
    for idx in top_k_indices[0]:
        page = r[idx]
        pil_image = base64_to_pil(page["image"])
        result = {"name": page["name"], "page_idx": page["page_idx"], "pil_image": pil_image}
        results.append(result)
    return results
```

The device used for this experiment:

- GPU - Nvidia V100 16GB
- RAM - 30GB

Total documents pages ingested - 556

## Retrieval

In this section, we'll see:

- Performance of ColPali in retrieving the correct document. In this case, it'll use MaxSim operation across ALL ingested document pages.
- Optimizing lookup time by reducing search space using LanceDB FTS (**ColPali as a FTS reranker**)
- Optimizing lookup time by reducing search space using LanceDB Semantic search (**ColPali as a vector search reranker**)

1. First, we'll ask a question about the trends in model training costs. This should be covered in the infraRed cloud report doc.

```python
query = "How do model training costs change over time?"
def search()
search_result = search(query=query, model=colpali, processor=processor, top_k=3)
search_result["image"][0]
```

Time taken - 34 seconds

![Training Costs Result](/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-15-at-10.39.27-PM.png)

It got the exact doc that describes the training cost trend as a graph. This is especially interesting because we've ingested many documents from similar genres (financial reports of big tech and other organizations). Any decent VLM can interpret this to form a final response if it's an RAG setting.

While late interaction with ColPali allows you to compute the document patch-embedding offline ahead of query time, it still needs to perform MaxSim operation to calculate the **similarity between a query and ALL documents, which is an expensive operation.** As the number of documents grows, the total time taken will also increase proportionally. Of course, using a more powerful GPU will result in better performance but it'll still be relative.

Let's take a look at another example and then move on to some optimizations that can allow this workflow to be used in a real-world setting.

```python
query = "What did the queen ask her magic mirror everyday?"
```

Time taken - 30.50 seconds

![Magic Mirror Result](/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-16-at-12.32.11-AM.png)

Again, it got to the exact page where the evil queen from snow white asks the given question to her magic mirror.

### Limitations

Using ColPali as a 1-step vision retriever greatly simplifies the process of document retrieval but without any additional optimizations, as the document index grows, the retrieval operation at query time will keep getting expensive. Let us now look at some ways to speed up retrieval by reducing the search space.

### ColPali MaxSims as a Reranking Step

There are a couple of hacks that can be used to reduce the search space by filtering out some results. 

1. **If text field is available**

In case you're able to extract the text from the pdf (Here we're not referring to OCR. Some pdf encodings allow reading text directly from the file), you can create an FTS index on the text column to try and reduce the search space. This will effectively make the ColPali MaxSims operation a reranking step for FTS. In this example, we'll get the top 100 FTS matches, which brings down the search space to about 1/5th. 

![FTS Optimization](/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-17-at-5.28.22-PM.png)

This is how the implementation would look like:

```python
def search(query: str, table_name: str, model, processor, db_path: str = "lancedb", top_k: int = 3, fts=False, limit=None):
    qs = get_query_embedding(query=query, model=model, processor=processor)
  
    # Search over all dataset 
    if fts:
        # set 100 as the max limit if filtering / reducing 
        # search space
        limit = limit or 100 
        r = table.search(query, query_type="fts").limit(limit)
    else:
        r = table.search().limit(limit)
    
    r = r.to_list()
    
    ...
    # Same as described above
    ...
    
    return results
```

Let's take a look at the result of the 2nd query.

```python
query = "What did the queen ask her magic mirror everyday?"
```

Time taken - 6.30 seconds

![Magic Mirror FTS Result](/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-16-at-12.32.11-AM-2.png)

We get the expected result with the latency reduced to about 1/5th.

2. **Reducing the search space using a similarity search**

A vision retriever pipeline ideally shouldn't be dependent on being able to parse the text from the documents as it defeats the purpose of being a 1-shot retrieval method. 

**Hypothesis**

Remember, 128 dim vector projections are derived from the language model part of ColPali. Although query dim (in this case 25x128) isn't the same as the doc path embedding dim (1030x128). However because these are the representations of the same model, they might be able to capture the similarity between the query and the doc patches. So we can flatten them out, zero pad the query embeddings to match the doc patch embeddings, and run vector search to filter out the top_k and reduce the search space for ColPali MaxSims reranking.

![Vector Search Optimization](/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-17-at-5.36.44-PM.png)

This'll look something like this:

```python
def search(query: str, model, processor, top_k: int = 3, fts=False, vector=False, limit=None):
    qs = get_query_embedding(query=query, model=model, processor=processor)

    # Search over all dataset
    if vector and fts:
        raise ValueError("can't filter using both fts and vector")
        
    if fts:
        limit = limit or 100
        r = table.search(query, query_type="fts").limit(limit)
    elif vector:
        limit = limit or 100
        vec_q = flatten_and_zero_pad(qs["embeddings"],table.to_pandas()["page_embedding_flatten"][0].shape[0])
        r = table.search(vec_q.float().numpy(), query_type="vector").limit(limit)
    else:
        r = table.search().limit(limit)
    if where:
        r = r.where(where)
    
    r = r.to_list()
    
    ...
    # Same as before
    ...
    
    return results
```

**Results:**

Let's try the previous query and then a new one:

```python
query = "What did the queen ask her magic mirror everyday?"
```

Time taken = ~6 seconds

![Magic Mirror Vector Result](/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-16-at-12.32.11-AM-2-2.png)

Let's run a couple of new tests and also verify the results are the same with and without pre-filtering with vector search.

Now, let's a question from one of the stories from the Arabian Nights, where a lady scatters some water around in a lake and the fishes turn into humans. Let's see if it can find that document.

Time taken ~ 6 seconds

```python
query = "How did the fish become men, women, and children"
```

![Arabian Nights Result](/assets/blog/late-interaction-efficient-multi-modal-retrievers-need-more-than-just-a-vector-index/Screenshot-2024-09-16-at-3.39.06-PM.png)

It does find the exact document!

### Other Strategies for Reducing the Search Space

There are more obvious ways of reducing the search space with some prior information. For example, if you already know the document from which the query is requested, you can simply use filters. 

Also, with a little bit of more processing on the offline indexing side, you classify the document genre and add that as another property when ingesting the docs. 

### The Challenge of Large-Scale Vision Retrieval

ColPali is definitely a step function change in the document retrieval process. It greatly simplifies the process of indexing documents. OCR is a complex modeling challenge and can be a point of failure on its own. Chunking and embedding the texts from the OCR model again is another hyper-parameter to take care of. There are [dedicated tools](https://github.com/Layout-Parser/layout-parser) to analyze the visual layout of the contents of docs for better retrieval. ColPali as a vision retriever allows the elimination of these individual, error-prone methods while still being relatively efficient. 

But making vision retrieval work on a large scale requires more than just a traditional vector index. Because of the high dimensionality of patch embeddings (1030x128), creating a vector index across a large-scale dataset is not feasible for an in-memory DB. Moreover, reducing search space by using BM25/FTS or efficient filtering, storing and retrieving actual image data is also required. 

**LanceDB shines when it comes to large-scale multi-modal applications**. LanceDB is fundamentally different from other vector databases in that it is built on top of [Lance](https://github.com/lancedb/lance), an open-source columnar data format designed for performant ML workloads and fast random access. 

- Due to the design of Lance, LanceDB's indexing philosophy adopts a primarily *disk-based* indexing philosophy. It can also simply store and retrieve objects so you can have your vectors, metadata, as well **patch-level** dataset files in the same place.
- Comes with native support for FTS and semantic search which allows efficient retrieval for multi-modal datasets as we've seen in this blog.

### Future Work

Vision retrieval with VLMs is a new retrieval technique and experiment results still pouring in. Multi-vector patch-level embeddings can capture document visual layout and semantic meaning of the text but indexing large vectors can be an expensive operations. In this blog, we've seen some methods of reducing the search space to speed up the retrieval process. Some other ways to optimize this would be the binarization of document embeddings to reduce the dimensionality of the vectors by 26 to 32 times for efficient storage and faster search, as covered in this detailed blog post by Jo from Vespa - [https://blog.vespa.ai/retrieval-with-vision-language-models-colpali/](https://blog.vespa.ai/retrieval-with-vision-language-models-colpali/)

Try out this example - [https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/ColPali-vision-retriever/colpali.ipynb)

Or others on [vectorDB recipes](https://github.com/lancedb/vectordb-recipes).
```

---

## ðŸ“„ æ–‡ä»¶: memgpt-os-inspired-llms-that-manage-their-own-memory-793d6eed417e.md

---

```md
---
title: "MemGPT: OS Inspired LLMs That Manage Their Own Memory"
date: 2023-12-11
author: Ayush Chaurasia
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/memgpt-os-inspired-llms-that-manage-their-own-memory-793d6eed417e/preview-image.png
meta_image: /assets/blog/memgpt-os-inspired-llms-that-manage-their-own-memory-793d6eed417e/preview-image.png
description: "Explore about memgpt: os inspired llms that manage their own memory. Get practical steps, examples, and best practices you can use now."
---

In the landscape of artificial intelligence, large language models (LLMs) have undeniably reshaped the game. However, a notable challenge persists â€” their restricted context windows limit their effectiveness in tasks requiring extended conversations and thorough document analysis.

[MemGPT](https://github.com/cpacker/MemGPT), an open source python package aims to solve this problem by using a concept drawing inspiration from traditional operating systemsâ€™ hierarchical memory systems. This technique optimizes data movement between fast and slow memory, providing the illusion of expansive memory resources.

![](https://miro.medium.com/v2/resize:fit:770/1*RvDrywTt1wbW2j0uKZ9-wQ.png)

MemGPT is a system that tackles the limited context window of traditional LLMs by allowing them to manage their own memory. It does this by adding a tiered memory system and functions to a standard LLM processor. The main context is the fixed-length input, and MemGPT analyzes the outputs at each step, either yielding control or using a function call to move data between the main and external contexts. It can even chain function calls together and wait for external events before resuming. In short, MemGPT gives LLMs the ability to remember and process more information than their usual limited context allows. This opens up new possibilities for tasks that require long-term memory or complex reasoning.
## Conversational agent with virtually unlimited memory!

[MemGPT](https://github.com/cpacker/MemGPT) can update context and search for information from its previous interactions when needed. This allows it to perform as a powerful conversational agent with unbound context.

The authors assess MemGPT, on these two criteria:

â€¢ Does MemGPT leverage its memory to improve conversation consistency? Can it remember relevant facts, preferences, and events from past interactions to maintain coherence?

â€¢ Does MemGPT produce more engaging dialogue by taking advantage of memory? Does it spontaneously incorporate long-range user information to personalize messages?
![](https://miro.medium.com/v2/resize:fit:770/1*kZhTIVAmMLlPM5CGqiwm4A.png)The above example illustrates a deep memory retrieval task. The user asks a question that can only be answered using information from a prior session (no longer in-context). Even though the answer is not immediately answerable using the in-context information, MemGPT can search through its recall storage containing prior conversations to retrieve the answer.
## External Data Sources

MemGPT supports pre-loading data into archival memory. In order to make data accessible to your agent, you must load data and then attach the data source to your agent.

External data sources are vectorized and stored for the agent to perform semantic search when user queries require assistance

## Built-in support for LanceDB

![](https://miro.medium.com/v2/resize:fit:770/1*8kW8NQPW25PJn5vFEAGbWg.png)

MemGPT uses [lancedb](http://lancedb.com/) as the default archival storage for storing and retrieving external data. It not only provides a seamless setup-free experience but the persisted HDD storage allows you scale from gigabytes to terabytes to petabytes without blowing out your budget or sacrificing performance.

## MemGPT in Action

After installing MemGPT (*mymemgpt* on pypi), you configure it using *memgpt configure *command.

Hereâ€™s an example that configures an agent and simply adds something to the archival memory. Then, it asks something related to it and memGPT understands what to return

## Using external data source

Letâ€™s ingest the intro of memGPT docs as an external data source and ask question about it. The best part is that once you load an external data it stays available for you to load it in any other agent too. And you can load multiple data sources for an agent.

![](https://miro.medium.com/v2/resize:fit:770/1*H0djUI2u0uinm50VuhWXRg.png)

You can use special commands followed by a slash to perform specific actions. For example here in this example, Iâ€™ve used the `/attach` command to attach an external vectorized data source.

## Customizations

MemGPT allows you to customize it to your needs. You can [create your own presets ](https://memgpt.readthedocs.io/en/latest/presets/)by setting a system prompt tuned to your use case.

It also supports various LLMs like OpenAI, Azure, Local LLMs including LLama.cpp and also custom LLM servers.

## Give it a try!

To give memGPT a try, you can follow the installation steps in the [quickstart](https://github.com/cpacker/MemGPT). The github repo also provides and up-to-date future roadmap of the tool and links to discord community if youâ€™d like to get involved.

Learn more about the features and roadmap of MemGPT on their [GitHub](https://github.com/cpacker/MemGPT). Donâ€™t forget for drop a ðŸŒŸ.
```

---

## ðŸ“„ æ–‡ä»¶: modified-rag-parent-document-bigger-chunk-retriever-62b3d1e79bc6.md

---

```md
---
title: "Modified RAG: Parent Document & Bigger Chunk Retriever"
date: 2023-12-15
author: Mahesh Deshwal
author_avatar: /assets/authors/mahesh-deshwal.jpg
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/modified-rag-parent-document-bigger-chunk-retriever-62b3d1e79bc6/preview-image.png
meta_image: /assets/blog/modified-rag-parent-document-bigger-chunk-retriever-62b3d1e79bc6/preview-image.png
description: "Get about modified rag: parent document & bigger chunk retriever. Get practical steps, examples, and best practices you can use now."
---

In case youâ€™re interested in modifying and improving retrieval accuracy of RAG pipelines, you should check [Re-ranking post](https://medium.com/p/cf6eaec6d544).

## Whatâ€™s it about?

There are some cases when your users want to have a task done by providing just a couple of lines input or even worse, couple of words. In this example, letâ€™s say I have a â€œSequelâ€ song generation task given a line or two as input. Now if itâ€™s a Part-2 of something, the tone, writing style, story etc are supposed to be related to the previous song so given the line â€œI am whatever I amâ€, my LLM should generate something related to the previous song not a mixture of 10 different songs and artists. If you use a vanilla RAG here, youâ€™d be getting multiple results which might not be from same song, artist or even genre. If you use only the first match, you lose a lot of context as a smaller chunk wonâ€™t give the full context of the song.

## Solution?

There are 2 approaches to tackle that. Letâ€™s go one by one from theory to code starting from **Parent Document Retriever**.

![Parent Document retriever diagram](/assets/blog/modified-rag-parent-document-bigger-chunk-retriever-62b3d1e79bc6/1*0JHJPrAuvalxOsywuNxJqg.jpg)

Given a text, find the most related chunk first (you can fetch N and then use additional logic based on Count etc too). Then instead of passing that chunk, get the Parent Document itself whose part was this chunk and pass THAT to the LLM as context. Letâ€™s jump to the code quickly. Install and get all the required imports like LanceDB, LangChain etc. For the Embedding function, Iâ€™m using BAAI encoder but you can use any one.

```bash
pip install -U "langchain==0.0.344" openai tiktoken lark datasets sentence_transformers FlagEmbedding lancedb -qq
```

```python
from langchain.vectorstores import LanceDB
from langchain.retrievers import ParentDocumentRetriever

# Text Splitting
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.storage import InMemoryStore
from langchain.docstore.document import Document

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

import os
from datasets import load_dataset

from langchain.embeddings import HuggingFaceBgeEmbeddings
import lancedb

os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY_HERE"  # Needed if you run LLM experiment below

# Embedding Functions
model_name = "BAAI/bge-small-en-v1.5"  # Open Source and effective embedding
encode_kwargs = {"normalize_embeddings": True}  # set True to compute cosine similarity
bge_embeddings = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs={"device": "cuda"}, encode_kwargs=encode_kwargs)

# Data Chunking Functions
small_chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=512)  # Split documents into small chunks
big_chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=2048)   # Another level of bigger chunks

# LanceDB Connection. Load if exists else create
my_db = lancedb.connect("./my_db")
```

For the demo data, Iâ€™m using [Eminem Song lyrics dataset present here](https://huggingface.co/huggingartists/eminem). Extract that data, split it into chunks and make embedding of those. Save all of those into a LanceDB table.

```python
# Load a sample data here
long_texts = load_dataset("huggingartists/eminem")["train"].to_pandas().sample(100)["text"]  # Data of huge context length. Use 100 random examples for demo

# Convert to LangChain Document object
docs = [Document(page_content=content, doc_id=_id, metadata={"doc_id": _id}) for (_id, content) in enumerate(long_texts)]

if "small_chunk_table" in my_db.table_names():
    small_chunk_table = my_db.open_table("small_chunk_table")
else:  # NOTE: 384 is the size of BAAI Embedding and -999 because it's a dummy data so invalid Embedding
    small_chunk_table = my_db.create_table("small_chunk_table", data=[{"vector": [-999] * 384, "text": "", "doc_id": "-1"}], mode="overwrite")

small_chunk_table.delete('doc_id = "-1"')

vectorstore = LanceDB(small_chunk_table, bge_embeddings)  # Index child chunks
store = InMemoryStore()  # Storage layer for the parent documents

full_doc_retriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=small_chunk_splitter)

full_doc_retriever.add_documents(docs, ids=None)  # Add all the documents
```

Now that everythingâ€™s been done, retrieve some text. Weâ€™ll first retrieve a smaller chunk and then weâ€™ll jump on to get the parent document itself.

```python
# Fetch 3 most similar Smaller Documents
sub_docs = vectorstore.similarity_search("I am whatever you say I am and if I wasn't why would you say I am", k=3)

print(sub_docs[0].page_content)  # This is a smaller chunk

full_docs = full_doc_retriever.get_relevant_documents("I am whatever you say I am and if I wasn't why would you say I am", k=3)
print(full_docs[0].page_content)  # Parent document returned after matching the smaller chunks internally
```

```text
Girls Lyrics
Ayo, dawg, I got some *** on my ****** chest
That I need to get off cause if I dont
Ima ****** explode or somethin
Now, look, this is the story about......
```

If you look at both outputs, youâ€™ll see that the first documents are the child and parent ones. You can use the Parent Document for the task described. **BUT there might be a PROBLEM with that**. If you have your parent documents, very big and short context length of LLM, then? You canâ€™t summarise lyrics like a good old solution. Can you? So? Thereâ€™s a middle ground too.

## Bigger Chunk Retrieval

To get around the problem of larger size of Parent document, what you can do right now is to make bigger chunks along with smaller ones. For example, if your smaller chunks are of 512 tokens and your Parent Documents are of 2048 tokens on average, you can make chunks of size 1024. Now during retrieval, itâ€™ll match as the previous one above BUT this time, instead of parent document, itâ€™ll fetch the Bigger chunk and pass it to LLM. This way youâ€™ll lose some text for sure but not completely. You could use 2 verses instead of the original 4 to help the model understand writing style and context while staying within limits. Good thing, you just have to change 1 line from the previous one.

```python
if "big_chunk_table" in my_db.table_names():
    big_chunk_table = my_db.open_table("big_chunk_table")
else:
    big_chunk_table = my_db.create_table(
        "big_chunk_table",
        data=[{"vector": [-999] * 384, "text": "", "doc_id": "-1"}],
        mode="overwrite",
    )

big_chunk_table.delete('doc_id = "-1"')

vectorstore = LanceDB(big_chunk_table, bge_embeddings)
store = InMemoryStore()

big_chunk_retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=small_chunk_splitter,
    parent_splitter=big_chunk_splitter,  # retrieves the larger chunk instead of Parent Document
)

big_chunk_retriever.add_documents(docs, ids=None)  # Add all the documents

big_chunks_docs = big_chunk_retriever.get_relevant_documents(
    "I am whatever you say I am and if I wasn't why would you say I am", k=3
)
print(big_chunks_docs[0].page_content)  # BIG chunks (in place of Parent Document)
```

```text
.........But as soon as someone calls you out
You put your tail between your legs and bow down
Now, I dont ask nobody to share my beliefs
To be involved in my beefs
Im a man, I can stand on my feet
So if you dont wanna be in em, all I ask
Is that you dont open your mouth with an opinion
And I wont put you in em
Cause I dont ask nobody to share my beliefs...........
```

## Using it with OpenAI

```python
qa = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=big_chunk_retriever,
)

query = "I am whatever you say I am and if I wasn't why would you say I am? So who is Em?"
qa.run(query)
```

You can [find the whole code (and lot more examples like this) HERE.](https://github.com/lancedb/vectordb-recipes#examples)

Until next time, happy parenting :)

![Bigger chunk retriever diagram](/assets/blog/modified-rag-parent-document-bigger-chunk-retriever-62b3d1e79bc6/1*lkDfUDQXYMUsnje8fM331Q.jpg)
```

---

## ðŸ“„ æ–‡ä»¶: multi-document-agentic-rag-a-walkthrough.md

---

```md
---
title: "Multi Document Agentic RAG: a Walkthrough"
date: 2024-08-28
author: Vipul Maheshwari
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/multi-document-agentic-rag-a-walkthrough/preview-image.png
meta_image: /assets/blog/multi-document-agentic-rag-a-walkthrough/preview-image.png
description: "Unlock about multi document agentic rag: a walkthrough. Get practical steps, examples, and best practices you can use now."
---

Agentic RAG (Retrieval-Augmented Generation) brings a notable improvement in how information is managed. While traditional RAG systems focus on retrieving relevant data and delivering it to a language model (LLM) for generating responses, Agentic RAG goes beyond that by introducing a higher level of autonomy. The system now does more than just gather data; it can also make decisions and take action independently.

This represents a move from basic tools to more sophisticated, intelligent systems. With Agentic RAG, the process evolves from passive to active, enabling AI logic to pursue specific objectives without constant supervision.

### How Does an Agentic RAG Work?

To understand Agentic RAG, it's important to first grasp the concept of an "agent." An agent is essentially a smart system capable of making independent decisions. When presented with a question or task, it determines the best approach by breaking the task into smaller steps and utilizing the appropriate tools to accomplish it.
![Multi-Document Agentic RAG with LanceDB](/assets/blog/multi-document-agentic-rag-a-walkthrough/image-5.png)
Multi-Document Agentic RAG with LanceDB
Agentic RAG is built on this same idea. Unlike regular RAG systems that just retrieve information, Agentic RAG uses intelligent strategies to ensure the system provides the best response. It goes beyond offering a basic, context-aware answer. These agents carefully analyze the query, choose the best approach, and deliver a more nuanced and well-thought-out reply.

### How to use it?

Hereâ€™s how we can build a specialized RAG agent for automotive needs. Imagine youâ€™re facing car troubles and need guidanceâ€”this agent would help you to:

1. **Diagnose Issues**: Understand and analyze your car's symptoms.
2. **Provide Solutions**: Offer possible causes and suggest fixes.
3. **Organize Maintenance**: Keep track of your car's upkeep needs.

To help our agent quickly access and use the information, weâ€™ll store pieces of data from six JSON files. Hereâ€™s a summary of what each file contains:

- **car_maintenance.json**: Maintenance schedules and tasks based on mileage.
- **car_problems.json**: Descriptions of common car issues, including required parts and repair time.
- **car_parts.json**: A list of parts, categorized by type, brand, and other attributes.
- **car_diagnosis.json**: Diagnostic steps, potential causes, and recommended actions.
- **car_cost_estimates.json**: Cost estimates for repairs based on identified problems.
- **car_models.json**: Specific issues common to certain car models.

You can get your JSON files here: [vectordb-recipes: multi-document-agentic-rag/json_files](https://github.com/lancedb/vectordb-recipes/tree/main/examples/multi-document-agentic-rag/json_files)
### How to: Tech Stack

To build our Agentic agent, weâ€™ll be using a few key tools to make things run smoothly:

1. [**LlamaIndex**](https://www.llamaindex.ai/)**:** This framework will serve as the core of our agent. It helps manage the agent's logic and operations. While the details might seem vague now, their role will become clearer as we implement the system.
2. **Memory Management:** Our agent will need to remember past interactions. Currently, it processes each question in isolation without tracking previous conversations. To fix this, we'll use memory to record the chat history. This history will be stored in a conversational memory buffer, managed by LlamaIndex, allowing the agent to refer back to both past and current interactions when making decisions.
3. **Vector Databases:** We'll use VectorDBs to retrieve information. Queries will be embedded and matched semantically with the relevant VectorDB through our retrievers.
4. LLM Integration: On the language model side, weâ€™ll go with OpenAIâ€™s GPT-4 for generating responses. For embeddings, weâ€™re using `sentence-transformers/all-MiniLM-L6-v2`

Make sure your `.env` file includes the `OPENAI_API_KEY`. You can adjust and use different LLMs and embedding models as needed. The key is to have an LLM for reasoning and an embedding model to handle data embedding

### Follow along with the Colab

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/multi-document-agentic-rag/main.ipynb)
### Step 1: Creating our DBs

Let's set up our vector database to store the data. Hereâ€™s an example of creating a singleton LanceDB vector store with multiple tables, each storing different types of embedded data.

```python
# Vector store setup
problems_vector_store = LanceDBVectorStore(
    uri='./lancedb',
    table_name='problems_table',
    mode="overwrite",
)
```

Since we're dealing with different types of information, we'll use multiple tables to organize and retrieve our data efficiently. Just as we created a separate table for our problem file, we'll do the same for our other data files. Now that our LanceDB table is set up, the next step is to create a retriever object that will focus on a specific table to access the relevant information.

```python
def load_and_index_document_from_file(file_path: str, vector_store: LanceDBVectorStore) -> VectorStoreIndex:
    """Load a document from a single file and index it."""
    with open(file_path, 'r') as f:
        data = json.load(f)
        document = Document(text=json.dumps(data))

    parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)
    nodes = parser.get_nodes_from_documents([document])
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    return VectorStoreIndex(nodes, storage_context=storage_context)

def create_retriever(index: VectorStoreIndex) -> VectorIndexRetriever:
    """Create a retriever from the index."""
    return index.as_retriever(similarity_top_k=5)

# Load and index documents directly from file paths
problems_index = load_and_index_document_from_file("/content/drive/MyDrive/multi-document-rag/problems.json", problems_vector_store)

# Creating our retriever
problems_retriever = create_retriever(problems_index)
```

Each retriever is linked to a specific LanceDB table, with one index for each table. Each Vector DB offers a retriever instanceâ€”a Python object that returns a list of documents matching a query. For instance, the `problems_retriever` will get documents about car problems, while the `cars_retriever` will find common issues faced by vehicle owners.

In case the bot misses information or seems to hallucinate, it might be due to missing data in our JSON files.

### Step 2: Creating our Agentic tools

Well, LlamaIndex agents are built to process natural language inputs and execute actions, rather than just generating responses. The effectiveness of these agents depends on how well we abstract and utilize tools. But what does 'tool' mean in this context? Think of tools as the specialized equipment a warrior uses in battle. Just as a warrior selects different weapons based on the situation, tools for our agent are specialized API interfaces that enable the agent to interact with data sources or reason through queries to provide the best possible responses.
In LlamaIndex, tools come in various types.

There is this one `FunctionTool`, which turns any user-defined function into a tool, making it capable of understanding the functionâ€™s schema and usage. These tools are crucial for enabling our agents to reason about queries and take effective actions.

As we use different tools, itâ€™s important to clearly describe its purpose and functionality so the agent can use it effectively. To get started, weâ€™ll create tools that leverage the retriever objects we defined earlier. For reference, letâ€™s create our first tool, which will help us retrieve relevant problems for a specific car.

```python
max_context_information = 200

def retrieve_problems(query: str) -> str:
    """Searches the problem catalog to find relevant automotive problems for the query."""
    docs = problems_retriever.retrieve(query)
    information = str([doc.text[:max_context_information] for doc in docs])
    return information

retrieve_problems_tool = FunctionTool.from_defaults(fn=retrieve_problems)
```

Similarly, we can create such tools for all our retrievers.

```python
parts_index = load_and_index_document_from_file("/content/drive/MyDrive/multi-document-rag/parts.json", parts_vector_store)
cars_index = load_and_index_document_from_file("/content/drive/MyDrive/multi-document-rag/cars_models.json", cars_vector_store)
diagnostics_index = load_and_index_document_from_file("/content/drive/MyDrive/multi-document-rag/diagnostics.json", diagnostics_vector_store)
cost_estimates_index = load_and_index_document_from_file("/content/drive/MyDrive/multi-document-rag/cost_estimates.json", cost_estimates_vector_store)
maintenance_schedules_index = load_and_index_document_from_file("/content/drive/MyDrive/multi-document-rag/maintenance.json", maintenance_schedules_vector_store)

parts_retriever = create_retriever(parts_index)
cars_retriever = create_retriever(cars_index)
diagnostics_retriever = create_retriever(diagnostics_index)
cost_estimates_retriever = create_retriever(cost_estimates_index)
maintenance_schedules_retriever = create_retriever(maintenance_schedules_index)
```

With the retriever tools now set up, our agent can effectively select the appropriate tool based on the query and fetch the relevant contexts. Next, we'll create additional helper tools that will complement the existing ones, providing the agent with more context and enhancing its reasoning capabilities. Here is the description of the helper tools that we are going to use

1. **`comprehensive_diagnosis`**: Generates a detailed diagnosis report based on car symptoms, including possible causes, estimated repair costs, and required parts.
2. **`plan_maintenance`**: Develop a maintenance plan based on the car's mileage and details, outlining common issues and estimated time for tasks.
3. **`create_calendar_invite`**: Creates a simulated calendar invite for a car maintenance or repair event, scheduling it for the next week.
4. **`coordinate_car_care`**: Integrates diagnosis, maintenance planning, and scheduling based on user queries to provide a comprehensive car care plan.

Refer to the Colab notebook for complete code and detailed implementation:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/multi-document-agentic-rag/main.ipynb)
Additionally, weâ€™ll implement some helper functions that, while not tools themselves, will be used internally within the tools to support the logic and enhance their functionality. Like a tool that can get us information about a car based on the mileage, its care make year, model, and all.

Here are the additional tools in their complete form

```python
comprehensive_diagnostic_tool = FunctionTool.from_defaults(fn=comprehensive_diagnosis)
maintenance_planner_tool = FunctionTool.from_defaults(fn=plan_maintenance)
calendar_invite_tool = FunctionTool.from_defaults(fn=create_calendar_invite)
car_care_coordinator_tool = FunctionTool.from_defaults(fn=coordinate_car_care)
retrieve_car_details_tool = FunctionTool.from_defaults(fn=retrieve_car_details)
```

Now, let's combine all these tools into a comprehensive tools list, which we will pass to our agent to utilize.

```python
tools = [
    retrieve_problems_tool,
    retrieve_parts_tool,
    diagnostic_tool,
    cost_estimator_tool,
    maintenance_schedule_tool,
    comprehensive_diagnostic_tool,
    maintenance_planner_tool,
    calendar_invite_tool,
    car_care_coordinator_tool,
    retrieve_car_details_tool
]
```

### Step 4: Creating the Agent

Now that weâ€™ve defined the tools, weâ€™re ready to create the agent. With LlamaIndex, this involves setting up an Agent reasoning loop. Basically, this loop allows our agent to handle complex questions that might require multiple steps or clarifications. Essentially, our agent can reason through tools and complete tasks across several stages.

LlamaIndex provides two main components for creating an agent: `AgentRunner` and `AgentWorkers`.

The `AgentRunner` acts as the orchestrator, like in a symphony, managing the overall process. It handles the current state, conversational memory, and tasks, and it runs steps for each task while providing a high-level user interface on what's going on. On the other hand, `AgentWorkers` are responsible for the operational side. They select and use the tools and choose the LLM to interact with these tools effectively.

Now, let's set up both the `AgentRunner` and `AgentWorker` to bring our agent to life.

```python
# Function to reset the agent's memory
def reset_agent_memory():
    global agent_worker, agent
    agent_worker = FunctionCallingAgentWorker.from_tools(
        tools,
        llm=llm,
        verbose=True
    )
    agent = AgentRunner(agent_worker)

# Initialize the agent
reset_agent_memory()
```

Every time you call `reset_agent_memory()`, a new, fresh agent is created, ready to reason through and act on the user's query.

With everything now set upâ€”our tools, an agent for reasoning, and databases for retrieving relevant contextâ€”letâ€™s test to see if our agent can handle simple questions effectively.

### Step 5: D-Day

Let's ask the agent a straightforward question related to car maintenance based on the mileage count and see how well it handles it.

```python
response = agent.query(
    "My car has 60,000 miles on it. What maintenance should I be doing now, and how much will it cost?"
)
```

and the response I got is

```python
Added user message to memory: My car has 60,000 miles on it. What maintenance should I be doing now, and how much will it cost?
=== Calling Function ===
Calling function: get_maintenance_schedule with args: {"mileage": 60000}

=== Calling Function ===
Calling function: estimate_repair_cost with args: {"problem": "Oil and filter change"}

=== Calling Function ===
Calling function: estimate_repair_cost with args: {"problem": "Tire rotation"}

=== Calling Function ===
Calling function: estimate_repair_cost with args: {"problem": "Air filter replacement"}

=== Calling Function ===
Calling function: estimate_repair_cost with args: {"problem": "Brake inspection"}

=== LLM Response ===
At 60,000 miles, the recommended maintenance tasks for your car are:

1. Oil and filter change: This typically costs around $250.
2. Tire rotation: The average cost for this service is around $50.
3. Air filter replacement: This usually costs about $70.
4. Brake inspection: The cost for this can vary, but it's typically around $100.

Please note that these are average costs and can vary based on your location and the specific make and model of your car. It's always a good idea to get a few quotes from different service providers.
```

Well, this is amazing! The agent effectively understood the query and provided an excellent response. Notice how it was first called the `maintenance_schedule_tool`, which utilized the `get_maintenance_schedule` retriever object to gather context on the relevant maintenance schedule, including different tasks based on the car's mileage. This context was then used by the `cost_estimator_tool`

The best part is that it passed the relevant parametersâ€”problems extracted from the `maintenance_schedule_tool`â€”to the cost estimator tool, deciding on its own based on the user query. Finally, with all the gathered context, it produced a comprehensive response that perfectly addresses the user's needs.

Btw, If you want the agent to retain the context of previous conversations, replace `.query` them with `.chat` to ensure context is preserved. Keep in mind that the context size is limited by the information you provide when calling the retrievers. Watch out for the `max_context_information` parameters in the retrievers to avoid exceeding the token limits for the LLMs.

And that's it! You've successfully created an agentic RAG that not only understands the user's query but also delivers a well-reasoned and contextually accurate answer. Here is the Colab for your reference:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/multi-document-agentic-rag/main.ipynb)
```

---

## ðŸ“„ æ–‡ä»¶: multimodal-lakehouse.md

---

```md
---
title: "What is the LanceDB Multimodal Lakehouse?"
date: 2025-06-23
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/multimodal-lakehouse/preview-image.png
description: "Introducing the Multimodal Lakehouse - a unified platform for managing AI data from raw files to production-ready features, now part of LanceDB Enterprise."
author: David Myriel
author_avatar: "/assets/authors/david-myriel.jpg"
author_bio: "Writer."
author_github: "davidmyriel"
author_linkedin: "davidmyriel"
---

## Multimodal Is No Longer Optional

> Multimodality is no longer a niche capability. It is the foundation of every AI workflow that aims to operate in the real world.

Modern enterprises are already working with multimodal data, even if they don't call it that. PDFs, slide decks, contracts, sales calls, emails, and dashboards are part of daily operations. These inputs span formats: text, audio, images, structured metadata, and more. 

*Even if you're not generating media with AI, you're almost certainly consuming it.*

Building AI that delivers real business value means connecting and interpreting information across these modalities. Though vector embeddings are powerful tools for comparison and retrieval, they aren't the whole picture. 

You still need access to raw content and structured signals such as filenames, timestamps, captions, and bounding boxes to build applications that truly understand the data they're working with. 

## From Storage to Scalable Processing
![Multimodal Lakehouse Architecture](/assets/blog/multimodal-lakehouse/multimodal-lakehouse-1.jpg)

If you've worked with our [high-performance columnar format - Lance](https://github.com/lancedb/lance), you're already familiar with the foundation. Also, if you've used our [vector database - LanceDB](https://github.com/lancedb/lancedb), then you've experienced how it can efficiently store and retrieve both structured and multimodal data. 

However, as more teams build complex AI systems, they also need to transition seamlessly from rapid experimentation to large-scale production. In practice, many organizations get bogged down by brittle preprocessing scripts and fragmented pipelines.

Some of our largest customers posed essential questions:
- How can we automate and accelerate feature engineering?
- How do we scale across data modalities? 
- How do we transform raw data into training-ready datasetsâ€”without relying on complex orchestration tools or rebuilding infrastructure from scratch?

## The Multimodal Lakehouse is Our Answer

LanceDB started as a vector database company, designed to handle both embeddings and diverse data types. You could run [vector search](http://lancedb.com/documentation/guides/search/vector-search.html) and [full-text search](http://lancedb.com/documentation/guides/search/full-text-search.html) for a [complete hybrid search](http://lancedb.com/documentation/guides/search/hybrid-search.html) experience relevant to search engines, RAG chatbots or all flavors of agentic systems.

**Figure 1:** [LanceDB Cloud](https://accounts.lancedb.com/sign-up) will still offer a full vector search engine experience with added features such as clustering and dimensionality reduction.

![Multimodal Lakehouse Architecture](/assets/blog/multimodal-lakehouse/lancedb-cloud.png)

This is where things start to diverge. In LanceDB Enterprise, Search is now one part of a broader platform.

As of June 24th, 2025, we are introducing the Multimodal Lakehouse suite of products into LanceDB Enterprise. The LanceDB Enterprise offering now consists of four features: Search, Exploratory Data Analysis, Feature Engineering and Training. 

![Multimodal Lakehouse Architecture](/assets/blog/multimodal-lakehouse/lancedb-enterprise.png)


On top of Lance and LanceDB, [The Multimodal Lakehouse](https://lancedb.com) adds a distributed serving engine, UDF-based feature engineering, materialized views, and SQL-based data exploration. 

The accompanying open-source Python package `geneva`, brings this vision to life with a simple, developer-friendly API.

### Centralized Multimodal Data Management

[The Multimodal Lakehouse](https://lancedb.com) acts as a flexible abstraction layer that connects to your existing LanceDB datasets. It empowers you to transform raw assets into usable AI-ready features without having to manage pipelines manually. 

By centralizing data transformations, versioning, and distributed execution, the lakehouse becomes a shared foundation for AI teams working across modalitiesâ€”video, image, text, audio or structured metadata.

**Figure 2:** **Traditional Data Lakes** are modular but fragmented, requiring teams to stitch together multiple systems, one for each kind of query. **The Multimodal Lakehouse** is cohesive and hybrid-query-native, offering vector, full-text, and SQL capabilities, with direct integration into modern ML and data tools.

![Traditional Data Lakes vs Multimodal Lakehouse](/assets/blog/multimodal-lakehouse/multimodal-lakehouse.png)

{{< admonition info "The Single Source of Truth" >}}
Whether you're prototyping in a Jupyter notebook or orchestrating large backfills across GPUs, the same interface and abstractions apply. Teams across departments can align around a single source of truth for features, metrics, and data logic.
{{< / admonition >}}

This architecture shift is what enables LanceDB to act not just as a vector database, but as a foundation for building AI-native platforms at scale.

## Declarative Feature Engineering with Python UDFs

What makes the [Multimodal Lakehouse](https://lancedb.com) unique is its deep integration with how data scientists and ML engineers already work. 

Using the lakehouse starts with installing the open-source geneva Python package:
```
pip install geneva
```
#### Connect to your LanceDB table 

After connecting to a LanceDB table, users can define feature logic as standard Python functions and apply them to their datasets via a simple API.

```python
import geneva as gv
table = gv.connect("imdb-movies")
```
Traditionally, moving from experimentation to production involves porting notebook code into brittle DAGs, replicating environments, and managing task execution across compute nodes.

#### Define feature engineering functions

The lakehouse eliminates these steps. Users can write Python functionsâ€”decorated as UDFsâ€”that are then scheduled, versioned, and executed across distributed infrastructure. 

```python
@udf
def gen_caption(image: bytes) -> str:
    return call_image_caption_model(image)

table.add_columns({"caption": gen_caption})
table.backfill("caption")
```

Behind the scenes, the platform packages your environment, deploys your code, and handles all the complexity of data partitioning, checkpointing, and incremental updates.

#### Query with hybrid search

Finally, you can easily go through your captions by either leveraging vector search, full-text search, SQL-based exploration or a hybrid combination.

```python
results = view.search("sunset over mountains")
    .limit(10)
    .to_pandas()
```

**Note:** These operations are incremental by default. You can backfill only the rows that meet specific conditions and refresh materialized views as new data arrives. Whether working interactively or scheduling batch jobs, the same code paths are used, reducing complexity and increasing reliability.

The system supports scalar UDFs for per-row computation, batched UDFs for performance optimization using Apache Arrow, and stateful UDFs that can load models or external clients, which is ideal for embedding generation or inference tasks.

## Scalable Compute with Ray and Kubernetes

The Multimodal Lakehouse doesn't just simplify developmentâ€”it also scales. Through its integration with [Ray](https://www.ray.io) and [KubeRay](https://github.com/ray-project/kuberay), workloads can be distributed across clusters of CPUs and GPUs, either on-prem or in the cloud.

Compute resources can be provisioned dynamically and matched to your workload, whether it's CPU-bound document parsing or GPU-heavy model inference. Features like workload identity, custom Docker images, and execution control over concurrency and batch sizes make it easy to run high-throughput jobs securely and efficiently.

This infrastructure allows for massive scale-out of feature engineering, training data preparation, and inference preprocessing, which are all within the same declarative Python environment.

## What Can You Do With the Multimodal Lakehouse?
![Multimodal Lakehouse Architecture](/assets/blog/multimodal-lakehouse/multimodal-lakehouse-2.jpg)

The Multimodal Lakehouse supports a wide variety of AI workflows, making it a versatile tool for any modern ML organization. Some common use cases include:

| Use Case | Description |
|----------|-------------|
| **LLM Training Pipelines** | Extract, clean, and embed large text corpora for transformer-based models |
| **Multimodal Vision + Language Systems** | Generate features across images, audio, and metadata for contrastive or fusion models |
| **Semantic Search Engines** | Build rich hybrid search pipelines with embeddings, captions, thumbnails, and metadata |
| **Recommender Systems** | Generate vector representations from logs, clicks, and metadata for nearest neighbor retrieval |
| **AI Data Platforms** | Maintain end-to-end pipelines from raw data ingestion to clean, versioned, training-ready datasets |

## Focus on Data, Not Infrastructure

The core value of the Multimodal Lakehouse is letting teams focus on their data, not on DevOps. Engineers are no longer burdened with writing custom pipeline orchestration, debugging DAG dependencies, or manually scaling compute clusters. Instead, they can iterate on feature logic, monitor outputs, and let the platform handle execution.

The shift from building infrastructure to building data products unlocks faster experimentation, better collaboration, and more robust systems. It's a fundamentally different way to manage AI development.

> This architecture marks the beginning of a new era for LanceDB. Instead of building disconnected tools, we're converging on a single, unified system for managing AI data from raw files to production-ready features.

Whether you're building a state-of-the-art semantic search system, a multimodal LLM training pipeline, or just want to simplify your feature engineering infrastructure, the multimodal lakehouse offers a production-ready, open-source solution with first class developer experience.

Looks like the sunâ€™s almost up. Weâ€™re excited to see what youâ€™ll discover and what great things youâ€™ll build.

![Multimodal Lakehouse Architecture](/assets/blog/multimodal-lakehouse/multimodal-lakehouse-3.jpg)
```

---

## ðŸ“„ æ–‡ä»¶: multimodal-myntra-fashion-search-engine-using-lancedb.md

---

```md
---
title: "Multimodal Myntra Fashion Search Engine Using LanceDB"
date: 2024-03-20
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/multimodal-myntra-fashion-search-engine-using-lancedb/preview-image.png
meta_image: /assets/blog/multimodal-myntra-fashion-search-engine-using-lancedb/preview-image.png
description: "Build a multimodal fashion search engine with LanceDB and CLIP embeddings. Follow a stepâ€‘byâ€‘step workflow to register embeddings, create the table, query by text or image, and ship a Streamlit UI."
---

In this comprehensive guide, we aim to explain the process of creating a multi-modal search app and break it down into manageable steps. The procedure involves several phases, including registering clip embeddings, setting up a data class and table, and executing a search query.

Using a vector database like LanceDB allows us to efficiently handle large volumes of data, paving the way for robust and efficient vector search. Clip embeddings, an integral part of our process, convert the images into embeddings that can be recognized by the vector database.

We'll also create a Streamlit app, which offers a more user-friendly GUI to access and use the search engine.

By the end of this blog, you should have a clear understanding of how to use LanceDB to create your own search engine, regardless of your dataset or use case. With LanceDB, you can see how simple it is to set up and query a vector database. Most of the boilerplate code is handled by LanceDB itself. Let's dive in.

To create the search engine, these are the steps we will be following -

1. **Define the Embedding Function** - We will use OpenAI CLIP to create the embeddings using `EmbeddingFunctionRegistry` for the images.
2. **Define the Schema** - We define the table schema.
3. **Create the Table** - We create the vector database using the embedding function and schema.
4. **Search the Query** - We can now do a multimodal vector search in the database.
5. **Streamlit App** - For a GUI

## Download the data

First, we will download the [Myntra Fashion Product Dataset](https://www.kaggle.com/datasets/hiteshsuthar101/myntra-fashion-product-dataset) from Kaggle and store it in the `input` folder. The folder shall contain the `Images` directory along with the `Fashion Dataset.csv` file. The dataset contains about 14.5K images, for the purpose of this blog we only take a 1000 samples out of it. For a more comprehensive search, you can increase the number of samples.

## 1. Register CLIP embedding

To store the images in the form of a vector database, we first need to convert them to embeddings. There are many ways to do this within LanceDB.

LanceDB supports 3 methods of working with embeddings.

1. You can manually generate embeddings for the data and queries. This is done outside of LanceDB.
2. You can use the built-in [embedding functions](https://lancedb.github.io/lancedb/embeddings/embedding_functions/) to embed the data and queries in the background (this is explained below)
3. For Python users, you can define your own [custom embedding function](https://lancedb.github.io/lancedb/embeddings/custom_embedding_function/), which extends the default embedding function.

For this project, we use the embedding function. We define a global embedding function registry with the OpenAI CLIP model. With the `EmbeddingFunctionRegistry`, you can generate embeddings with just a few lines of code.

```python
from lancedb.embeddings import EmbeddingFunctionRegistry

def register_model(model_name):
    registry = EmbeddingFunctionRegistry.get_instance()
    model = registry.get(model_name).create()
    return model
```

## 2. Define the schema

The embedding function defined above abstracts model and dimension details required to define the schema. A schema tells LanceDB the structure of the table.

For this project, we will define the schema with two fields -

1. Vector Field - `VectorField` tells LanceDB to use the CLIP embedding function to generate query embeddings for the `vector` column
2. Source Field - `SourceField` ensures that when adding data, we automatically use the specified embedding function to encode `image_uri`

```python
from PIL import Image
from lancedb.pydantic import LanceModel, Vector

clip = register_model("open-clip")

class Myntra(LanceModel):
    vector: Vector(clip.ndims()) = clip.VectorField()
    image_uri: str = clip.SourceField()

    @property
    def image(self):
        return Image.open(self.image_uri)
```

## 3. Create the table

Now that we have our embedding function and schema, we will create the table.

```python
import lancedb
import pandas as pd
from pathlib import Path
from random import sample
import argparse

def create_table(database, table_name, data_path, schema=Myntra, mode="overwrite"):

    # Connect to the LanceDB database
    db = lancedb.connect(database)

    # Check if the table already exists in the database
    if table_name in db:
        print(f"Table {table_name} already exists in the database")
        table = db[table_name]

    # if it does not exist then create a new table
    else:

        print(f"Creating table {table_name} in the database")

        # Create the table with the given schema
        table = db.create_table(table_name, schema=schema, mode=mode)

        # Define the Path of the images and obtain the Image uri
        p = Path(data_path).expanduser()
        uris = [str(f) for f in p.glob("*.jpg")]
        print(f"Found {len(uris)} images in {p}")

        # Sample 1000 images from the data
        # Select more samples for a wider search
        uris = sample(uris, 1000)

        # Add the data to the table
        print(f"Adding {len(uris)} images to the table")
        table.add(pd.DataFrame({"image_uri": uris}))
        print(f"Added {len(uris)} images to the table")
```

One thing to note is that you do not need to create the embeddings separately when using the embedding function; LanceDB takes care of that automatically.

## 4. Query the table

Now you can open and query the table

```python
# Connect to the Database and open the table
db = lancedb.connect("~/.lancedb")
table = db.open_table(table_name)
```

The OpenCLIP query embedding function supports querying via both text and images. We will make this project multimodal by providing text and image search.

```python
def run_vector_search(database, table_name, schema, search_query, limit=6, output_folder="output"):
    if os.path.exists(output_folder):
        for file in os.listdir(output_folder):
            os.remove(os.path.join(output_folder, file))
    else:
        os.makedirs(output_folder)

    db = lancedb.connect(database)
    table = db.open_table(table_name)
    rs = table.search(search_query).limit(limit).to_pydantic(schema)

    for i in range(limit):
        image_path = os.path.join(output_folder, f"image_{i}.jpg")
        rs[i].image.save(image_path, "JPEG")
```

## 5. Build the Streamlit app

Once this is done, we create a Streamlit app to build an interface.

```python
import os
import argparse
import streamlit as st
from PIL import Image

def main(args):
    # Define the title and sidebar options
    st.sidebar.title('Vector Search')
    table_name = st.sidebar.text_input('Name of the table', args.table_name)
    search_query = st.sidebar.text_input('Search query', args.search_query)
    limit = st.sidebar.slider('Limit the number of results', args.limit_min, args.limit_max, args.limit_default)
    output_folder = st.sidebar.text_input('Output folder path', args.output_folder)

    # Image Based Search
    # Add an option for uploading an image for query
    uploaded_image = st.sidebar.file_uploader('Upload an image')
    if uploaded_image is not None:
        image = Image.open(uploaded_image)
        st.sidebar.image(image, caption='Uploaded Image', use_column_width=True)
        search_query = image  # Set the search query as the uploaded image

    # Run the vector search when the button is clicked
    if st.sidebar.button('Run Vector Search'):
        run_vector_search("~/.lancedb", table_name, Myntra, search_query, limit, output_folder)

    # Initialize session state for image index if it doesn't exist
    if 'current_image_index' not in st.session_state:
        st.session_state.current_image_index = 0

    # Display images in output folder
    if os.path.exists(output_folder):
        image_files = [f for f in os.listdir(output_folder) if f.endswith('.jpg') or f.endswith('.png')]
        if image_files:
            # Ensure the current index is within the bounds of available images
            num_images = len(image_files)
            st.session_state.current_image_index %= num_images
            image_file = image_files[st.session_state.current_image_index]
            image_path = os.path.join(output_folder, image_file)
            image = Image.open(image_path)
            st.image(image, caption=image_file, use_column_width=True)

            # Navigation buttons for previous and next images
            col1, col2 = st.columns(2)
            with col1:
                if st.button('Previous'):
                    st.session_state.current_image_index = (st.session_state.current_image_index - 1) % num_images
            with col2:
                if st.button('Next'):
                    st.session_state.current_image_index = (st.session_state.current_image_index + 1) % num_images
        else:
            st.write("No images found in the output folder.")
```

## Demo

<iframe width="560" height="315" src="https://www.youtube.com/embed/okoR_sts858?si=erCSFYEH66FIJxji" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Conclusion

In conclusion, using LanceDB to create a multimodal fashion search engine for Myntra simplifies the process and improves efficiency. By taking advantage of capabilities such as vector search, automatic handling of embeddings, and the capacity to handle large volumes of data, we can streamline the development process and create a powerful tool for fashion search. Whether you're a seasoned programmer or a beginner, this guide offers a comprehensive walkthrough of the process, making it accessible. It underlines the power and simplicity of using LanceDB and serves as a stepping stone for creating your own search engines for different datasets or use cases.

## Resources

- [GitHub Repository](https://github.com/ishandutta0098/lancedb-multimodal-myntra-fashion-search-engine)

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1qnaNasUy6aJOcaUYw9lMX_s4SWWGq3jY?usp=sharing)

## Contact

For queries and doubts, you can reach out to me on

- [LinkedIn](https://www.linkedin.com/in/ishandutta0098/)
- [Twitter](https://x.com/ishandutta0098)
```

---

## ðŸ“„ æ–‡ä»¶: my-simd-is-faster-than-yours-fb2989bf25e7.md

---

```md
---
title: "My SIMD Is Faster than Yours"
date: 2023-04-24
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/my-simd-is-faster-than-yours-fb2989bf25e7/preview-image.webp
meta_image: /assets/blog/my-simd-is-faster-than-yours-fb2989bf25e7/preview-image.webp
description: "An untold story about how we make LanceDB vector search fast. Get practical steps and examples from 'My SIMD is faster than Yours'."
---

> An untold story about how we make LanceDB vector search fast.

In January, the engineering team here at Eto [made the decision to rewrite the Lance columnar format in Rust](https://github.com/lancedb/lance/pull/497). While this decision may seem like the usual â€œ***Rewrite-X-In-Rust***â€ trend, there is an untold story behind it: Our goal was to achieve excellent performance in the recently announced SSD-resilient vector search in [LanceDB](https://github.com/lancedb/lancedb). Rustâ€™s native code-generation, great toolchain for performance tuning ( `cargo asm` / `cargo flamegraph` ), as well as the vast amount of first-class CPU intrinsics/SIMD support in the standard library (`std::arch`), heavily influenced our decision.

The most fundamental query in vector search is â€œ***finding K-nearest-neighbors (KNN) in a high-dimensional vector space***â€. To execute this search, the distances between vectors must be computed. In this article, we will use the classic Euclidean Distance (L2) as an example. Other popular distance measures include Cosine Distance, Dot Product, and Hamming Distance.

![](https://miro.medium.com/v2/resize:fit:770/0*MPpRZ0O3tt4BJBg0.png)
Euclidean (L2) Distance

## First attempt: Naive implementation and let the compiler do it

L2 distance is quite straightforward to implement in Rust.

```rust
pub fn l2(x: &[f32], y: &[f32]) -> f32 {
    x.iter()
        .zip(y.iter())
        .map(|(a, b)| (a - b).powi(2))
        .sum::<f32>()
}
```

Computing L2 between one query vector (1024 dimensions) to 1 million vectors takes `1.25s` on an AWS compute-optimized instance (`C6in.4xlarge`).

Rust compiler `rustc (1.68)`does a pretty decent code-generation for the above code, as shown in the [Compiler Explorer](https://rust.godbolt.org/) with `RUSTFLAGS=-C opt-level=3 -C target-cpu=icelake-server -C target-feature=+avx2`: loop-unrolling is kicked in:

```rust
/// RUSTFLAGS="-C opt-level=3 -C target-cpu=icelake-server"
...
.LBB0_5:
    vmovss  xmm1, dword ptr [rdi + 4*rsi]
    vmovss  xmm2, dword ptr [rdi + 4*rsi + 4]
    vsubss  xmm1, xmm1, dword ptr [rdx + 4*rsi]
    vmulss  xmm1, xmm1, xmm1
    vaddss  xmm0, xmm0, xmm1
    vsubss  xmm1, xmm2, dword ptr [rdx + 4*rsi + 4]
    vmulss  xmm1, xmm1, xmm1
    vaddss  xmm0, xmm0, xmm1
    vmovss  xmm1, dword ptr [rdi + 4*rsi + 8]
    vsubss  xmm1, xmm1, dword ptr [rdx + 4*rsi + 8]
    vmulss  xmm1, xmm1, xmm1
    vaddss  xmm0, xmm0, xmm1
    vmovss  xmm1, dword ptr [rdi + 4*rsi + 12]
    vsubss  xmm1, xmm1, dword ptr [rdx + 4*rsi + 12]
    vmulss  xmm1, xmm1, xmm1
    vaddss  xmm0, xmm0, xmm1
    vmovss  xmm1, dword ptr [rdi + 4*rsi + 16]
    vsubss  xmm1, xmm1, dword ptr [rdx + 4*rsi + 16]
    vmulss  xmm1, xmm1, xmm1
    vaddss  xmm0, xmm0, xmm1
    vmovss  xmm1, dword ptr [rdi + 4*rsi + 20]
    vsubss  xmm1, xmm1, dword ptr [rdx + 4*rsi + 20]
    vmulss  xmm1, xmm1, xmm1
    vaddss  xmm0, xmm0, xmm1
    vmovss  xmm1, dword ptr [rdi + 4*rsi + 24]
    vsubss  xmm1, xmm1, dword ptr [rdx + 4*rsi + 24]
    vmulss  xmm1, xmm1, xmm1
    vaddss  xmm0, xmm0, xmm1
    vmovss  xmm1, dword ptr [rdi + 4*rsi + 28]
    lea     rax, [rsi + 8]
    vsubss  xmm1, xmm1, dword ptr [rdx + 4*rsi + 28]
    vmulss  xmm1, xmm1, xmm1
    vaddss  xmm0, xmm0, xmm1
    mov     rsi, rax
    cmp     rcx, rax
    jne     .LBB0_5
...
```

People often say, â€œlet the compiler optimize the code for youâ€. Unfortunately, `vsubss/vaddss/vmulss` are the scalar instructions on X86_64. It appears that LLVM does not auto-vectorization the loop. [It has been discovered](https://stackoverflow.com/questions/73118583/auto-vectorization-with-rust) that it would be quite challenging for LLVM/GCC to vectorize loops without a static length at compiling time. But our vector index ***MUST*** support any dimension of user vectors -- did we just leave some performance on the table?

## Second attempt: Arrow compute kernel

LanceDB is built on top of Apache Arrow (Rust), can we do better by using Arrow compute kernels directly?

```rust
// RUSTFLAGS="-C target-cpu=native -C target-feature=+avx2"
// Use arrow arith kernels.
use arrow_arith::arithmetic::{multiply, subtract};
use arrow_arith::arity::binary;

#[inline]
fn l2_arrow_1(x: &Float32Array, y: &Float32Array) -> f32 {
    let s = subtract(x, y).unwrap();
    let m = multiply(&s, &s).unwrap();
    sum(&m).unwrap()
}

#[inline]
fn l2_arrow_2(x: &Float32Array, y: &Float32Array) -> f32 {
    let m: Float32Array = binary(x, y, |a, b| (a - b).powi(2)).unwrap();
    sum(&m).unwrap()
}
```

Running on the same `c6in.4xlarge` machine with `RUSTFLAGS=â€-C target-cpu=native -C target-feature=+avx2â€` , we got `2.81s` and `2.16s` respectively. Surprisingly, using arrow compute kernel is slower than our naive implementation. The reason being that, unlike the native L2 function that has zero memory allocation and only scan `x` and `y` once, which only invalid the L1 cache when it is absolutely necessary. `l2_arrow_1` requires scanning over `x, y, m, s`array once, invaliding L1/L2 cache twice, and two extra memory allocations for `s` and `m`. `l2_arrow_2` scans `x`, `y`once, thus better cache behavior, and only requires one extra memory allocation.

## Third attempt: BLAS to the rescue?

Many scientific computing libraries are using BLAS underneath to accelerate the matrix / vector compute. *LanceDB actually uses BLAS in some places too!*

Especially, numpy is well-regard as well-optimized for linear algebra algorithms. We use Numpy + Intel MKL, one of the fastest BLAS library out there to benchmark BLAS performance.

```
>>> import numpy as np
>>> np.show_config()
blas_armpl_info:
    NOT AVAILABLE
blas_mkl_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/home/ubuntu/miniconda3/envs/np-mkl/lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/home/ubuntu/miniconda3/envs/np-mkl/include']
blas_opt_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/home/ubuntu/miniconda3/envs/np-mkl/lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/home/ubuntu/miniconda3/envs/np-mkl/include']
lapack_armpl_info:
    NOT AVAILABLE
lapack_mkl_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/home/ubuntu/miniconda3/envs/np-mkl/lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/home/ubuntu/miniconda3/envs/np-mkl/include']
lapack_opt_info:
    libraries = ['mkl_rt', 'pthread']
    library_dirs = ['/home/ubuntu/miniconda3/envs/np-mkl/lib']
    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]
    include_dirs = ['/home/ubuntu/miniconda3/envs/np-mkl/include']
Supported SIMD extensions in this NumPy install:
    baseline = SSE,SSE2,SSE3
    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2,AVX512F,AVX512CD,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL
    not found = AVX512_KNL,AVX512_KNM
```

Running a simple python script on the same AWS `c6in` instance takes `5.498s`
```python
import numpy as np
import time

x = np.random.random((1, 1024))
y = np.random.random((1024*1024, 1024))

total = 10
start = time.time()
for i in range(total):
    d = np.linalg.norm(y - x, axis=1)
print("time: {}s".format((time.time() - start) / total))
```

## Last attempt: Can we squeeze every penny out of a modern CPU?

> Our philosophy of building LanceDB, a serverless vector database, is to **fully** utilize modern hardware capabilities.

Modern CPUs are extraordinarily capable. We strongly believe that we can do better to utilize the best that CPUs have to offer.

We re-write the L2 distance computation using Intel AVX-2 instructions in `std::arch::x86_64`

```rust
use std::arch::x86_64::*;

#[inline]
pub(crate) fn l2_f32(from: &[f32], to: &[f32]) -> f32 {
    unsafe {
        // Get the potion of the vector that is aligned to 32 bytes.
        let len = from.len() / 8 * 8;
        let mut sums = _mm256_setzero_ps();
        for i in (0..len).step_by(8) {
            let left = _mm256_loadu_ps(from.as_ptr().add(i));
            let right = _mm256_loadu_ps(to.as_ptr().add(i));
            let sub = _mm256_sub_ps(left, right);
            // sum = sub * sub + sum
            sums = _mm256_fmadd_ps(sub, sub, sums);
        }
        // Shift and add vector, until only 1 value left.
        // sums = [x0-x7], shift = [x4-x7]
        let mut shift = _mm256_permute2f128_ps(sums, sums, 1);
        // [x0+x4, x1+x5, ..]
        sums = _mm256_add_ps(sums, shift);
        shift = _mm256_permute_ps(sums, 14);
        sums = _mm256_add_ps(sums, shift);
        sums = _mm256_hadd_ps(sums, sums);
        let mut results: [f32; 8] = [0f32; 8];
        _mm256_storeu_ps(results.as_mut_ptr(), sums);

        // Remaining unaligned values
        results[0] += l2_scalar(&from[len..], &to[len..]);
        results[0]
    }
}
```

This time, it only takes `0.325s` to compute L2 distances over 1 million vectors, a **350%** speed up to the closest alternatives!

## We do not forget you, Apple Silicon!

Our team shares a love for Apple Silicon M-series Macbooks. We are committed to making LanceDB the best-in-class performance option on the machines we use every single day.

Apple's latest chips are based on Arm `aarch64` architecture, with `NEON` instructions. Unfortunately, Arm Scalable Vector Extension (SVE) is not available on these chips yet. So, we have only implemented the NEON version of `L2 distance.`

```rust
use std::arch::aarch64::*;

#[inline]
pub(crate) fn l2_f32(from: &[f32], to: &[f32]) -> f32 {
    unsafe {
        let len = from.len() / 4 * 4;
        let buf = [0.0_f32; 4];
        let mut sum = vld1q_f32(buf.as_ptr());
        for i in (0..len).step_by(4) {
            let left = vld1q_f32(from.as_ptr().add(i));
            let right = vld1q_f32(to.as_ptr().add(i));
            let sub = vsubq_f32(left, right);
            sum = vfmaq_f32(sum, sub, sub);
        }
        let mut sum = vaddvq_f32(sum);
        sum += l2_scalar(&from[len..], &to[len..]);
        sum
    }
}
```

This NEON-accelerate L2 implementation only takes `0.299s` to compute 1 million distances on a M2 Max MacBook Pro.

For the sake of completeness in benchmarking, we also ran the same numpy script on a Macbook Pro using the Apple Accelerate Framework for BLAS. Weâ€™ve seen similar 3â€“15x speed ups by manually tuning the SIMD instructions across the CPU architectures ( `x86_64` and `apple-silicon (aarch64)` ).

![](https://miro.medium.com/v2/resize:fit:770/1*eylFuCRjMSnm2MPyN7t4MA.png)Relative speed-up on X86_64(AVX2) and Apple Silicon (NEON). Numpy uses intel MKL on AWS EC2 c6in.4xlarge instance, and Apple Accelerate Framework on a M2 Max Macbook Pro.

## What the future of LanceDB will look like

Even though weâ€™ve already significantly accelerated vector computations using our SIMD implementation, thereâ€™s still much more we can do to make LanceDB run even faster:

- Add AVX-512 routines for widen bit-width vectorization.
- Support `bf16, f64`floats vectors.
- CUDA and Apple Silicon GPU acceleration.
- Improve PQ / OPQ code cache efficiency.

Once the `std::simd` / portable SIMD project is available in stable Rust, we would definitely migrate to use `std::simd` for simplicity.

Additionally, because LanceDB is an SSD-resilient and serverless Vector Database, there are many more I/O path optimizations that can be done specially for NVME SSDs.

The rich Rust ecosystem gives us enormous power to write cross-platform hardware-accelerate with ease, would especially thank the Rust community in:

- `std::arch::{x86_64, aarch64}` covers majority of our use cases in the stable Rust toolchain.
- `cargo bench` + `pprof` ([https://github.com/tikv/pprof-rs](https://github.com/tikv/pprof-rs)) make benchmarks easy to maintain and inspect. It is the foundation of our continuous benchmarking effort.
- [cargo asm](https://github.com/pacak/cargo-show-asm) and [cargo-show-asm](https://github.com/pacak/cargo-show-asm) to check the code-generation on both `x86_64` and `aarch64` CPUs, so we can check whether auto-vectorization and other compiler optimizations are active.
- `Compiler Explorer` ([https://godbolt.org/](https://godbolt.org/)) is another tool for us to understand compiler optimizations better.
- `cargo flamegraph` ([https://github.com/flamegraph-rs/flamegraph](https://github.com/flamegraph-rs/flamegraph)) makes it super easy to work with `flamegraph` . If you have used `flamegraph.pl` in C/C++ project before , I am sure you will appreciate `flamegraph-rs` project sincerely.

## Give LanceDB a try!

If you like the idea of `LanceDB` , you can try it today by simply running `pip install lancedb` . And please give us a ðŸŒŸ on [Github](https://github.com/lancedb/lancedb)!
```

---

## ðŸ“„ æ–‡ä»¶: my-summer-internship-experience-at-lancedb-2.md

---

```md
---
title: "My Summer Internship Experience at LanceDB"
date: 2024-08-15
author: Raunak Sinha
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/my-summer-internship-experience-at-lancedb-2/preview-image.png
meta_image: /assets/blog/my-summer-internship-experience-at-lancedb-2/preview-image.png
description: "I'm Raunak, a master's student at the University of Illinois, Urbana-Champaign.  This summer, I had the opportunity to intern as a Software Engineer at LanceDB, an early-stage startup based in San Francisco."
---

I'm Raunak, a master's student at the University of Illinois, Urbana-Champaign. This summer, I had the opportunity to intern as a Software Engineer at LanceDB, an early-stage startup based in San Francisco.

LanceDB is a database company, specializing in the storage and retrieval of multimodal data at scale. I worked on the open-source file format ([Lance](https://github.com/lancedb/lance)). The main codebase is in Rust, built on top of Apache Arrow, with a user-facing API in Python. Working at Lance has been a great learning experience that has broadened my perspective and increased my confidence as an engineer.

I worked on several [features](https://github.com/lancedb/lance/commits/main/?author=raunaks13) intended to improve the extent of compression and reduce the read time of data, for both full scans and random access. I got extensive hands-on experience working within the Apache Arrow ecosystem, doing asynchronous programming in Rust, and was able to delve deep into the internals of a real database system, allowing me to understand things at a fundamental level.

A significant portion of my work involved implementing compressive encodings that intelligently processed data at a bit level. For instance, previously strings were encoded using a logical encoding - more specifically, as a `List` of bytes. My initial goal was to add dictionary encoding to improve string compression. This required first re-implementing the basic binary (string) encoding as a physical encoder from scratch ([PR](https://github.com/lancedb/lance/pull/2426)). In the new encoder, I encoded the bytes and offsets of the string array separately. For example:

```text
StringArray: ["abc", "d", "efg"]
Physical Binary Encoding:
  Bytes: ['a', 'b', 'c', 'd', 'e', 'f', 'g']
  Offsets: [0, 3, 4, 7]
```

 Once this was done, I added dictionary encoding ([PR](https://github.com/lancedb/lance/pull/2409)). Since a large dictionary size can be suboptimal, this encoding is only used for columns with low cardinality.

```text
StringArray: ["abc", "d", "abc", "d", "efg"]
Dictionary Encoding:
  BinaryEncoding(["abc", "d", "efg"])
  Indices: [0, 1, 0, 1, 2]
```

When applied, these encodings significantly improved Lanceâ€™s performance on string data. On the TPCH dataset, the size of the relevant string columns was reduced by** 10â€“20x**, and the read time was lowered by **2â€“3x**. Here is a comparison of how the read times reduced, along with the performance of parquet on the same data:

![String columns read time reduction](/assets/blog/my-summer-internship-experience-at-lancedb-2/Screenshot-2024-08-13-at-9.53.04-AM.png)

Later, I also added a fixed-size binary encoding ([PR](https://github.com/lancedb/lance/pull/2707)). If the strings are of fixed size, we donâ€™t need to store all the offsets. Storing the byte width is enough. This can help save an I/O call (IOP) and result in extra compression when decoding.

```text
StringArray: ["abc", "def", "ghi"]
FixedSizeBinary Encoding:
  Bytes: ['a','b','c','d','e','f','g','h','i']
  Byte width: 3
```

On running a benchmark on 10M random strings with a fixed size of 8, the file sizes were reduced by around **2x**. The read time reduced by almost** 2x** as well:

![Fixed-size binary performance](/assets/blog/my-summer-internship-experience-at-lancedb-2/Screenshot-2024-08-13-at-3.58.20-PM.png)

Above: performance of default encoding, Below: performance of fixed-size encoding
Since most workflows are built on top of the file format, optimizing read and write time for maximum performance is critical. Getting such results was also a consequence of benchmarking low-level Rust code, identifying bottlenecks in performance, and optimizing code by using zero-copy operations, all skills that I honed during the internship.

I also worked on a somewhat nontraditional encoding for structs. Previously, retrieving data from a specific sub-field of a struct resulted in a separate IOP. Thus, retrieving data from multiple struct fields incurred multiple IOPs, which was bad for random access. The packed struct encoding ([PR](https://github.com/lancedb/lance/pull/2593)) changes this so that the data for all struct fields in a single row can be packed together.

![Packed struct layout](/assets/blog/my-summer-internship-experience-at-lancedb-2/Screenshot-2024-08-12-at-1.42.56-PM.png)

For example, a struct with 3 fields of different types will be packed like this
Thus, retrieving even multiple struct fields only consumes a single IOP. This makes random access much faster. On [benchmarking](https://github.com/lancedb/lance/blob/main/python/python/benchmarks/test_packed_struct.py) a random struct column, we were able to achieve over **3x** speedup in average read time:

![Packed struct read time improvement](/assets/blog/my-summer-internship-experience-at-lancedb-2/Screenshot-2024-08-12-at-1.51.44-PM.png)

Using this encoding has a trade-off since retrieving an individual struct field will now be slower than before. It should only be applied when the user wants to retrieve multiple struct fields together. This can be indicated by adding a `packed: true` flag to the struct column metadata.

Implementing such encodings was tricky at first - I had to wrap my head around how bits across multiple nullable arrays were arranged and figure out which bits to decode depending on which rows were queried. Dealing with all the buffer and byte slicing operations while debugging Rust lifetime issues did not make it any easier :) However, my manager was very helpful throughout this process and answered any questions I had. Over time, things got easier. I was able to make faster progress and see improvements in the quality of my work.

Apart from encodings, it was also fun to do some work on the indexing side of things. For example, I added a scalar bitmap index to help search through a column quickly ([PR](https://github.com/lancedb/lance/pull/2560)). This was useful for users interested in fast pre-filtering of data. For example, when running a query like `SELECT COUNT(*) WHERE age > 30`, using a bitmap index on the `age` column can help speed things up. I also added tooling to help split, shuffle, and load IVF-PQ vector indices for very large-scale datasets ([PR1](https://github.com/lancedb/lance/pull/2657), [PR2](https://github.com/lancedb/lance/pull/2670), [PR3](https://github.com/lancedb/lance/pull/2681)). These can be useful for vector search, which is an important use case for enterprises, especially those dealing with storage and retrieval (e.g. RAG) in the GenAI space.

{{< admonition type="note" title="Open-source impact" >}}
One of the most rewarding aspects of this internship was the opportunity to contribute to an open-source project within a corporate setting. Since the code is public, I can see how the impact of my work (and the project as a whole) evolves over time as the community grows and more developers contribute.
{{< /admonition >}}

For instance, our dictionary encoding implementation sped up even further after a community contribution, when [a user used the hyperloglog approximation](https://github.com/lancedb/lance/pull/2555) for cardinality estimation (this is used in a check to decide whether to use dictionary encoding or not).

Another example was when we found and fixed a bug reported by a user - previously, when randomly accessing multiple rows, each request for a row was scheduled separately. In this [PR](https://github.com/lancedb/lance/pull/2636), I coalesced multiple read requests based on how close the requested rows were, reducing the number of decode tasks being created. When fixed, this helped recover a **10x** speed gap on random access.

An open-source format also makes outside collaborations possible. Earlier in June, I presented a research paper at the SIGMOD 2024 conference, where I got a chance to speak with the authors of [FastLanes](https://www.vldb.org/pvldb/vol16/p2132-afroozeh.pdf). This eventually led to discussions on integrating FastLanes with Lance. This is still a work in progress, but it can lead to the addition of optimized versions of encodings like delta, frame-of-reference, bit-packing, and more - all of which can significantly improve the performance of the Lance file format.

There are a lot of exciting directions that the Lance file format can take in the future. There are more extensive random access benchmarks that can be added. There are many more encodings we can add to improve performance. Some promising options include a delta encoding, a run-length encoding, or a packed struct encoding for variable-length fields. Experimenting with ALP (Adaptive Lossless Floating-Point Compression) also offers potential benefits.

Another interesting direction is allowing the user to encode a column using multiple encodings to support different use cases. For example, automatically using a packed struct encoding for random access, and an unpacked encoding for full table scans. This would result in faster performance at the cost of some extra storage.

Ultimately, interning at an early-stage startup is a fun experience in itself. The team is extremely efficient, and the culture encourages you to be independent, move fast, and get things done. It was a great opportunity to interact with leaders in the industry and get a glimpse of what really goes into building a tech company.

It's been an exciting 3 months! I've learned a lot, and everyone has been very supportive. Exploring SF and the Bay Area has also been fun. Next, I will be returning to UIUC to complete the final year of my master's degree. I'm looking forward to the adventures that lie ahead!
```

---

## ðŸ“„ æ–‡ä»¶: netflix-mediadata-lake-coderabbit-case-study-lance-namespace.md

---

```md
---
title: "Netflixâ€™s Media Data Lake â¤ï¸ LanceDB, CodeRabbit ðŸ’¼ Case Study, Lance Namespace"
date: 2025-09-08
draft: false
featured: false
categories: ["Newsletter"]
image: "/assets/blog/netflix-mediadata-lake-coderabbit-case-study-lance-namespace/image5.png"
description: "Our September newsletter highlights LanceDB powering Netflix's Media Data Lake, a case study on CodeRabbit's AI-powered code reviews, and updates on Lance Namespace and Spark integration."
author: "Jasmine Wang"
author_avatar: "/assets/authors/jasmine-wang.png"
author_bio: "Ecosystem Engagement, Partnership, Community, DevRel"
author_github: "onigiriisabunny"
author_linkedin: "jasminechenwang"
---


## [LanceDB Powers Netflix Media Data Lake](https://lancedb.com/blog/case-study-netflix/) 

> To enable the next generation of media analytics and machine learning, we are building the **Media Data Lake** at Netflix â€” a data lake designed specifically for media assets at Netflix using [LanceDB](https://lancedb.com/). We have partnered with our data platform team on integrating LanceDB into our [Big Data Platform](https://netflixtechblog.com/all?topic=big-data). 
> 
> - **Media ML Data Engineering, Netflix**

[![](/assets/blog/netflix-mediadata-lake-coderabbit-case-study-lance-namespace/image11.png)](https://lancedb.com/blog/case-study-netflix/)  

A deep dive on Netflixâ€™s Media ML Data Engineering, a new specialization that bridges the gap between traditional data engineering and the unique demands of media-centric machine learning, and how they build the Media Data Lake with LanceDB. 

Read the Netflix Tech Blog [From Facts & Metrics to Media Machine Learning: Evolving the Data Engineering Function at Netflix](https://netflixtechblog.com/from-facts-metrics-to-media-machine-learning-evolving-the-data-engineering-function-at-netflix-6dcc91058d8d)

## [ðŸ’¼ Case Study: How CodeRabbit Leverages LanceDB for AI-Powered Code Reviews](https://lancedb.com/blog/case-study-coderabbit/)

[![](/assets/blog/netflix-mediadata-lake-coderabbit-case-study-lance-namespace/image3.png)](https://lancedb.com/blog/case-study-coderabbit/)

> "LanceDB transformed how we handle context at scale. While other vector databases hit cost and performance walls, LanceDB scales effortlessly with our growthâ€”from startup to enterprise. Its multimodal capabilities and deployment flexibility were game-changers, enabling us to deliver the depth of analysis our customers expect while maintaining sub-second response times across millions of code reviews."
> 
> **[Rohit Khanna, VP of Engineering ](https://www.linkedin.com/in/rrkhanna/), [CodeRabbit](https://www.coderabbit.ai/)**

## [Manage Lance Tables in Any Catalog using Lance Namespace and Spark](https://lancedb.com/blog/introducing-lance-namespace-spark-integration/)

Lance Namespace is an open specification built on top of the storage-based Lance table and file format. It provides a standardized way for metadata services like Apache Hive MetaStore, Apache Gravitino, Unity Catalog, AWS Glue Data Catalog, and others to store and manage Lance tables. This means you can seamlessly use Lance tables alongside your existing [**data lakehouse infrastructure**](https://lancedb.com/blog/multimodal-lakehouse/) .  

[![](/assets/blog/netflix-mediadata-lake-coderabbit-case-study-lance-namespace/image4.png)](https://lancedb.com/blog/introducing-lance-namespace-spark-integration/)

## ðŸŽ¤ Event Recap\!

LanceDB made a small tour around the world in Aug. Started with a workshop with dltHub at Berlin PyData Con. Then a stop in Amsterdam to present at the inaugural Open Lakehouse Meetup with Databricks and DuckDB, followed by a keynote at AI\_Dev Con. Our last stop was London started with a meetup generously hosted by AWS London, and we wrapped up the tour with our VLDB workshop on the [Lance paper](https://arxiv.org/abs/2504.15247).        

![](/assets/blog/netflix-mediadata-lake-coderabbit-case-study-lance-namespace/image15.png) 


## ðŸ“£ Coming up in Sep: 


Join us on September 25 for the live webinar: [ð—”ð—½ð—®ð—°ð—µð—² ð—¦ð—½ð—®ð—¿ð—¸â„¢ ð—®ð—»ð—± ð—Ÿð—®ð—»ð—°ð—² ð—¦ð—½ð—®ð—¿ð—¸ ð—–ð—¼ð—»ð—»ð—²ð—°ð˜ð—¼ð—¿\! ðŸš€](https://www.linkedin.com/events/apachespark-andlancesparkconnec7363659816340258816/theater/)
Lance Spark Connector brings Lanceâ€™s AI-native multimodal storage to Spark. Weâ€™ll cover how Spark can work efficiently with embeddings, images, videos, and documents using Lanceâ€™s random access, indexing, and vector/blob support.  

[![](/assets/blog/netflix-mediadata-lake-coderabbit-case-study-lance-namespace/image12.png)](https://www.linkedin.com/events/apachespark-andlancesparkconnec7363659816340258816/theater/)

## ðŸ“š Good Reads

[Feature Engineering with Geneva](https://lancedb.com/blog/geneva-feature-engineering/)  
[![](/assets/blog/netflix-mediadata-lake-coderabbit-case-study-lance-namespace/image8.png)](https://lancedb.com/blog/geneva-feature-engineering/)  


[Columnar File Reader in Depth â€“ Structural Encoding](https://lancedb.com/blog/columnar-file-readers-in-depth-structural-encoding/)  
[![](/assets/blog/netflix-mediadata-lake-coderabbit-case-study-lance-namespace/image14.png)](https://lancedb.com/blog/columnar-file-readers-in-depth-structural-encoding/) 


[LanceDB WikiSearch: Native Full-Text Search on 41M Wikipedia Docs](https://lancedb.com/blog/feature-full-text-search/)  
[![](/assets/blog/netflix-mediadata-lake-coderabbit-case-study-lance-namespace/image2.png)](https://lancedb.com/blog/feature-full-text-search/)

## ðŸ—žï¸ LanceDB Enterprise Product News

| Feature | Description |
| :---- | :---- |
| Faster and more accurate Full-Text Search (FTS) | Complex FTS queries (50â€“100 terms) now run 3â€“8x faster with improved relevance and ranking. |
| Simpler data loading | Insert, merge, and create tables seamlessly without worrying about dataset size or batch tuning. |
| Flexible search results | Support for \`limit\` and \`offset\`  in both vector and full-text search allows easy pagination of large result sets. |
| Better observability for \`merge\_insert\`  | Use \`explain\_plan\` and \`analyze\_plan\` to visualize execution and identify performance bottlenecks. |

## ðŸ¤ Community contributions

GEO Data Type support coming to Lance\! ([git](https://github.com/lancedb/lance/discussions/4482)) Thanks to the contributions from our community [@ddupg](https://github.com/ddupg) and [@jaystarshot](https://github.com/jaystarshot), **Lance now supports Geo type**. Geo index and query optimizations are coming soon too\! A shoutout for the individual contributors from **Bytedance** and **Uber** for making this possible\!  

A heartfelt thank you to our community contributors of lance and lancedb this past month:   [@majin1102](https://github.com/majin1102) [@fangbo](https://github.com/fangbo) [@wojiaodoubao](https://github.com/wojiaodoubao) [@pimdh](https://github.com/pimdh)[@ebyhr](https://github.com/ebyhr) [@yanghua](https://github.com/yanghua) [@HaochengLIU](https://github.com/HaochengLIU) [@imededin](https://github.com/imededin) [@HubertY](https://github.com/HubertY) [@chenghao-guo](https://github.com/chenghao-guo) [@lorinlee](https://github.com/lorinlee) [@vlovich](https://github.com/vlovich) [@adrian-wang](https://github.com/adrian-wang) [@ddupg](https://github.com/ddupg) [@LeoReeYang](https://github.com/LeoReeYang) [@emmanuel-ferdman](https://github.com/emmanuel-ferdman) [@adi-ray](https://github.com/adi-ray) [@yuvalif](https://github.com/yuvalif) [@Heisenberg208](https://github.com/Heisenberg208) [@mocobeta](https://github.com/mocobeta)  [@MarkMcCaskey](https://github.com/MarkMcCaskey) [@reedloden](https://github.com/reedloden)
 

## ðŸ”¦ Open Source Releases Spotlight 

| LanceDB | 0.22.0 | Integration with Lance Namespace, support multi-level namespace management.  |
| :---- | :---- | :---- |
| Lance | 0.35.0 | JSONB data type and index support, Apache OpenDAL integration, lance-tools CLI command, contains\_tokens UDF for full text search |
| | 0.34.0 | Shallow clone support, zone map index support, row level conflict resolution for Delete, metadata diff API |
| | 0.33.0 | File format 2.1 official release (2.1 files written with earlier versions of the library may not be readable due to breaking changes during development). Java transaction commit API for all commit types. |
| Lance Namespace | 0.0.6 \- 0.0.14 | Python and Rust SDK release |
| Lance Ray | 0.0.1 \- 0.0.5 | Integration with Lance Namespace |
| Lance Spark | 0.0.2 \- 0.0.11 | Support CREATE TABLE with fixed size vector column, support UPDATE and DELETE. |
```

---

## ðŸ“„ æ–‡ä»¶: newsletter-august-2025.md

---

```md
---
title: "âš–ï¸ Harveyâ€™s Enterprise-Grade RAG on LanceDB, ðŸ’¼ Dosu Case Study, Minimax&LumaLabsâ¤ï¸Lance-Ray"
date: 2025-08-05
draft: false
featured: false
categories: ["Newsletter"]
image: "/assets/blog/newsletter-august-2025/JulyNews.png"
description: "Our August newsletter features a new case study with Dosu, recaps from events with Harvey and Databricks, and the latest product and community updates."
author: "Jasmine Wang"
author_avatar: "/assets/authors/jasmine-wang.png"
author_bio: "Ecosystem Engagement, Partnership, Community, DevRel"
author_github: "onigiriisabunny"
author_linkedin: "jasminechenwang"
---

## ðŸ’¼ LanceDB Powers the Heart of AI at Dosu

- 90% label accuracy
- 70% less manual triage
- Millisecond search on millions of vectors

Those are the numbers that became real the moment the team switched to LanceDB inside Dosu. Learn more about how Dosu uses LanceDB as their living knowledge for codebases in this new [Case Study: Meet Dosu - the Intelligent Knowledge Base for Software Teams and Agents](https://lancedb.com/blog/case-study-dosu/)

> "LanceDB powers the heart of our AI at [Dosu](https://www.dosu.ai). Its lightning-fast search, built-in versioning, and effortless scalability let us move faster, ship smarter, and deliver cutting-edge intelligence to millions of developers without compromise."
>
> **[Devin Stein, Founder & CEO](https://www.linkedin.com/in/devstein/), [Dosu](https://dosu.dev/)**


![Dosu Case Study](/assets/blog/newsletter-august-2025/Dosucase.png)



## ðŸŽ¤ Event Recap!

In case you missed some of the cool events in June, the recordings are finally out! Check out the joint talks we did with Harvey and Databricks on how LanceDB helps scale enterprise AI systems in production!

-   **<span style="color: #ff6f1a; font-weight: bold;">[Scaling Enterprise-Grade RAG: Lessons from Legal Frontier](https://youtu.be/W1MiZChnkfA)</span>**: A joint talk with [Calvin Qi](https://www.linkedin.com/in/calvinqi/) from [Harvey](https://www.harvey.ai) at the AI Engineer World's Fair.
    {{< youtube W1MiZChnkfA >}}
-   **<span style="color: #ff6f1a; font-weight: bold;">[Lakehouse Architecture for AI Data: Search, Analytics, Processing, Training](https://youtu.be/SvXhXIJM7hA)</span>**: Our session at the <span style="color: #ff6f1a; font-weight: bold;">Databricks</span> DATA+AI SUMMIT with [Chang She](https://www.linkedin.com/in/changshe/) and [Zhidong Qu](https://www.linkedin.com/in/zhidong-qu/).
    {{< youtube SvXhXIJM7hA >}}




## ðŸ“° Product News: New Enterprise Features

| Feature                                      | Description                                                                                                                         |
| -------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| **Seamless streaming ingestion**             | Automatically optimize indexes during streaming writes, providing consistently fast performance even under continuous data flow.     |
| **Blazing-fast range queries**               | Range filters like "value >= 1000 and value < 2000" now can execute hundreds of times faster, supercharging vector search with metadata filtering. |
| **Multivector support in JavaScript SDK**    | LanceDB JavaScript/TypeScript SDK enables you to store and search multiple vector embeddings for a single item.                      |
| **Easy data exploration from LanceDB Cloud UI** | Filter table data with SQL and select specific columns for a customizable view of your LanceDB datasets.                             |

## Community Contributions

- [@jiaoew1991](https://github.com/jiaoew1991) Enwei Jiao previously worked on Data Engine at Minimax and now at Luma.ai. He has made multiple contributions to support Lance-Ray integration: `add_column` supports Ray to add columns to existing tables in a distributed manner [PR1](https://github.com/lancedb/lance-ray/pull/21), and `add_dataset_options parameter` supports exposing more native parameters when reading lance tables based on version and tag [PR2](https://github.com/lancedb/lance-ray/pull/27) .
- [@geruh](https://github.com/geruh) Drew Gallardo from AWS worked on AWS Glue integration with Lance [PR1](https://github.com/lancedb/lance-namespace/pull/167), [PR2](https://github.com/lancedb/lance-namespace/pull/158)
- [@kazuhitoT](https://github.com/kazuhitoT) Kazuhito Takeuchi for native integration with Lindera tokenizer [PR1](https://github.com/lancedb/lance/pull/3932), [PR2](https://github.com/lancedb/lance/pull/4144)
- A heartfelt thank you to our community contributors of Lance and LanceDB this past month: [@yanghua](https://github.com/yanghua) [@fangbo](https://github.com/fangbo) [@emmanuel-ferdman](https://github.com/emmanuel-ferdman) [@adi-ray](https://github.com/adi-ray) [@ddupg](https://github.com/ddupg) [@chenghao-guo](https://github.com/chenghao-guo) [@LeoReeYang](https://github.com/LeoReeYang) [@majin1102](https://github.com/majin1102) [@HaochengLIU](https://github.com/HaochengLIU) [@Jay-ju](https://github.com/Jay-ju) [@SaintBacchus](https://github.com/SaintBacchus) [@Sbargaoui](https://github.com/Sbargaoui) [@lalitx17](https://github.com/lalitx17) [@allenanswerzq](https://github.com/allenanswerzq) [@bjurkovski](https://github.com/bjurkovski) [@TaoKevinKK](https://github.com/TaoKevinKK) [@wojiaodoubao](https://github.com/wojiaodoubao) [@KazuhitoT](https://github.com/KazuhitoT) [@niyue](https://github.com/niyue) [@kilavvy](https://github.com/kilavvy) [@b4l](https://github.com/b4l) [@Dig-Doug](https://github.com/Dig-Doug) [@xhwhis](https://github.com/xhwhis) [@tristanz](https://github.com/tristanz) [@chenkovsky](https://github.com/chenkovsky) [@aniaan](https://github.com/aniaan) [@yihong0618](https://github.com/yihong0618) [@Kilerd](https://github.com/Kilerd) [@CyrusAttoun](https://github.com/CyrusAttoun) [@kemingy](https://github.com/kemingy) [@wengh](https://github.com/wengh) [@geruh](https://github.com/geruh)

## Open Source Releases Spotlight

| Project | Version | Updates |
| ------- | ------- | ------- |
| LanceDB | [v0.21.2](https://github.com/lancedb/lancedb/releases/tag/v0.21.2) | Various improvements including support for [ngram tokenizer](https://github.com/lancedb/lancedb/pull/2507), [multivector for JavaScript](https://github.com/lancedb/lancedb/pull/2527), [session support in Python and JavaScript](https://github.com/lancedb/lancedb/pull/2530) |
| LanceDB | [v0.21.1](https://github.com/lancedb/lancedb/releases/tag/v0.21.1) | Various improvements including [batch Ollama embed calls](https://github.com/lancedb/lancedb/pull/2453) |
| Lance | [v0.32.0](https://github.com/lancedb/lance/releases/tag/v0.32.0) | Breaking changes: use [FilteredReadExec in the planner](https://github.com/lancedb/lance/pull/3813) & [consolidated index cache](https://github.com/lancedb/lance/pull/4047) |
| Lance | [v0.31.1](https://github.com/lancedb/lance/releases/tag/v0.31.1) | New documentation website built with mkdocs |
```

---

## ðŸ“„ æ–‡ä»¶: newsletter-june-2025.md

---

```md
---
title: "June 2025: $30M Series A, Multimodal Lakehouse Launch & Product Updates"
date: 2025-07-08
draft: false
featured: false
categories: ["Announcement"]
image: /assets/blog/newsletter-june-2025/social_preview.png
description: "LanceDB's June 2025 newsletter covering latest company news, product updates, open source releases, and community highlights."
author: David Myriel
author_avatar: "/assets/authors/david-myriel.jpg"
author_bio: "Writer"
author_github: "davidmyriel"
author_linkedin: "davidmyriel"
---

## LanceDB is Now a Series A Company

Over the past year, we have witnessed [the Lance columnar format](https://lancedb.com) become the new standard for multimodal data. As of June 2025, [Lance remains the fastest growing format](https://github.com/lancedb/lance) across the data ecosystem. During this period, our open source packages have been downloaded more than 20 million times. 

![Series A Funding Announcement](/assets/blog/series-a-funding/preview-image.png)

This milestone represents a significant validation of our vision to democratize multimodal AI development. The $30M Series A funding will accelerate our mission to build the most efficient and scalable data platform for AI applications. This investment will fuel our continued innovation in multimodal data processing, expand our enterprise offerings, and strengthen our global community of developers and data scientists.

{{% admonition Blog %}}
[Announcement from our Cofounder & CEO Chang She](https://lancedb.com/blog/series-a-funding/)
{{% /admonition %}}

## Introducing the LanceDB Multimodal Lakehouse

As of June 24th, 2025, along with the celebration of LanceDBâ€™s Series A, we are introducing the [Multimodal Lakehouse Suite of Products](https://lancedb.com/multimodal-lakehouse.html) into LanceDB Enterprise. 

![Multimodal Lakehouse Architecture](/assets/blog/multimodal-lakehouse/lancedb-enterprise.png)

The LanceDB Enterprise offering now consists of four features: Search, Exploratory Data Analysis, Feature Engineering and Training.

![LanceDB Multimodal Lakehouse Architecture](/assets/blog/multimodal-lakehouse/preview-image.png)

The Multimodal Lakehouse represents a breakthrough in unified data management for AI applications, seamlessly handling text, images, audio, and video data in a single platform. This comprehensive solution eliminates the complexity of managing multiple data silos and provides enterprises with the tools they need to build, train, and deploy multimodal AI models at scale. 

With built-in support for the latest AI frameworks and optimized performance for large-scale datasets, the Multimodal Lakehouse is designed to accelerate the development of next-generation AI applications.

{{% admonition Blog %}}
[What is the LanceDB Multimodal Lakehouse?](https://lancedb.com/blog/multimodal-lakehouse/)
{{% /admonition %}}

## Product News: New Enterprise Features

| Feature | Description |
|---------|-------------|
| **Richer full-text search capabilities** | Unlock advanced FTS with boolean logic, flexible phrase matching, and autocomplete-ready prefix queries. |
| **Blazing-fast full-text search** | Optimized FTS engine now delivers P99 latencies under 50ms on 40M-row tables, lightning speed at scale. |
| **Streamlined Kubernetes deployments** | Native Helm chart support makes BYOC setups faster and easier to manage. A deployment can be up and running in a couple of hours. |
| **Smarter vector search with tight filters** | Fine-tune recall with new minimum_nprobes and maximum_nprobes controls for better results on queries with highly selective filters. |

## Lance & LanceDB OSS Releases:

| Project | Version | Updates |
|---------|---------|----------|
| Lance | [v0.31.0](https://github.com/lancedb/lance/releases/tag/v0.31.0) | Breaking changes: [refactor Dataset config api and expose it via pylance](https://github.com/lancedb/lance/pull/4041). |
| Lance | [v0.30.0](https://github.com/lancedb/lance/releases/tag/v0.30.0) | Breaking changes: [auto-remap indexes before scan](https://github.com/lancedb/lance/pull/3971) & [move file metadata cache to bytes capacity](https://github.com/lancedb/lance/pull/3949). |
| LanceDB | [v0.21.0](https://github.com/lancedb/lancedb/releases/tag/v0.21.0) | Various improvements to native full text search and native full text search is now the default <br> New documentation site: lancedb.github.io/documentation <br> Lance Trino connector and PostgreSQL extension|

## Events and Community Recap

### From Text to Video: A Unified Multimodal Data Lake for Next-Generation AI

[Ryan Vilim from Character AI](https://character.ai) shares how their Data & AI Platform team builds self-service tools and infrastructure to power LLM training and research. He explains how they leverage data lakes, Spark, Trino, Kubernetes, and Lance to prepare, annotate, and serve massive multimodal datasetsâ€”while keeping workflows fast and researcher-friendly.

<iframe width="560" height="315" src="https://www.youtube.com/embed/P6GdGMvZG74?si=9WU7pjSNOp5l_iu5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

The talk also covers why Lanceâ€™s open multimodal lakehouse format fits their needs, enabling unified storage, search, and analytics at scale. Packed with practical insights on managing AI data pipelines, this session is perfect for anyone building or scaling AI systems.

### Building a Data Foundation for Multimodal Foundation Models

[Ethan Rosenthal from Runway](https://runwayml.com) delivers an in-depth exploration of the unsung heroes behind generative models: data pipelines. Skipping over models and flashy applications, he dives into the nuts and bolts of handling massive, unstructured datasetsâ€”video, text, embeddings, and metadataâ€”used to train and iterate on state-of-the-art generative video and image systems. 

Drawing on his experience at Square and Runway, Ethan outlines the evolution from structured fraud-detection data to the complexities of multimodal AI, demonstrating why robust data infrastructure is the true backbone of model performance.

<iframe width="560" height="315" src="https://www.youtube.com/embed/6kf58xD5s3Q?si=OcRRt-dWPU3TM7SR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Throughout the talk, Ethan shares pragmatic lessons on system designâ€”covering topics like columnable storage formats (moving beyond tarballs and Parquet), schema evolution with LanceDB, efficient video decoding, asynchronous data loading to eliminate GPU bottlenecks, and on-the-fly augmentation for flexible experimentation. 

With candid insights and practical trade-offs, this session is essential for engineers and researchers building scalable, researcher-friendly pipelines for the next generation of generative AI.

## A Thank You to Our Valued Contributors:

A heartfelt thank you to our community contributors of Lance and LanceDB OSS projects this past month:

@renato2099, @wojiaodoubao, @Jay-ju, @b4l, @yanghua, @HaochengLIU, @ddupg, @bjurkovski  @kilavvy@wojiaodoubao, @leaves12138, @majin1102, @leopardracer, @luohao, @KazuhitoT, @frankliee
```

---

## ðŸ“„ æ–‡ä»¶: newsletter-october-2025.md

---

```md
---
title: ðŸŽ¨ Semantic.Art, ðŸ’¾ Stable Lance 2.1, ðŸŽ¥ Ray+LanceDB powers Netflix
date: 2025-10-31
draft: false 
featured: false 
categories: ["Newsletter"]
image: "/assets/blog/newsletter-october-2025/newsletter-october-2025.png"
description: "Our October newsletter highlights Semantic.Art, Lance File 2.1, RaBitQ Quantization, upcoming events, latest product and community updates."
author: "Jasmine Wang"
author_avatar: "/assets/authors/jasmine-wang.png"
author_bio: "Ecosystem Engagement, Partnership, Community, DevRel"
author_github: "onigiriisabunny"
author_linkedin: "jasminechenwang"
---

## ðŸŽ¨ Semantic.Art

Meet [semantic.art](https://lancedb.com/blog/semanticdotart/), a multi-feature multi-index retrieval art discovery engine that lets you search with a feeling or intent rather than matching keywords.

Powered by [LanceDB](https://www.linkedin.com/company/lancedb/). The core of the system involves dynamically understanding and inspecting a query's intent. It then chooses the best search path on the fly, blending vector, FTS, hybrid search, filtering, and custom rerankers.

<iframe width="560" height="315" src="https://www.youtube.com/embed/TqFO9fS94to?si=NE-sdFQ1bBCLUe1b" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

[![SemanticDotArt: Rethinking Art Discovery with LanceDB](/assets/demos/sda_hero.jpg)](https://lancedb.com/blog/semanticdotart/)

## ðŸ’¾ Lance File 2.1 Is Now Stable

Big news from the LanceDB team â€” Lance File Format 2.1 is officially stable!

This release solves one of the biggest challenges from 2.0: ðŸ‘‰ adding compression without sacrificing random access performance.

[![Lance File 2.1 Stable](/assets/blog/lance-file-2-1-stable/preview-image.png)](https://lancedb.com/blog/lance-file-2-1-stable/)

## LanceDB's ðŸ°RaBitQ Quantization for Blazing Fast Vector Search âš¡ï¸

A new quantization technique in LanceDB engineering for high-performance search on large-scale, high-dimensional data.

[![LanceDB's RaBitQ Quantization for Blazing Fast Vector Search](/assets/blog/feature-rabitq-quantization/preview-image.png)](https://lancedb.com/blog/feature-rabitq-quantization/)

## â–¶ï¸ Recordings you might've missed!

<iframe width="560" height="315" src="https://www.youtube.com/embed/ga-hj7byOHw?si=8bJ9P7IwD6Q2Pqgw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> 

## ðŸ“… Upcoming Events

[![Ray Summit 2025](/assets/blog/newsletter-october-2025/ray-summit-2025.png)](https://www.anyscale.com/ray-summit/2025/agenda)

[Ray Summit 2025](https://www.anyscale.com/ray-summit/2025/agenda) | Wednesday, November 5, 4:00 PM

LanceDB and Netflix are joining forces at Ray Summit to do a deep dive on what the Netflix Media Data Lake team is building.

[![LanceDB at PyData](/assets/blog/newsletter-october-2025/pydata.png)](https://pydata.org/seattle2025/schedule)

Look forward to three sessions from the LanceDB team at [PyData Seattle](https://pydata.org/seattle2025/schedule)!

 ðŸŽ¤ **Data Loading for Data Engineers**<br>
Weston Pace, Senior Software Engineer @ LanceDB | Friday, November 7, 10:10â€¯AM

ðŸŽ¤ **Keynote Session: Never Send a Human to do an Agentâ€™s Retrieval**<br>
Chang She, Co-Founder & CEO @ LanceDB | Saturday, November 8, 9:00 AM

ðŸŽ¤ **Supercharging Multimodal Feature Engineering with Lance and Ray**<br>
Jack Ye, Senior Software Engineer @ LanceDB | Saturday, November 8, 11:40 AM

[![LanceDB at KubeCon](/assets/blog/newsletter-october-2025/kubecon-2025.png)](https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/)

Check out our two sessions at [KubeCon + CloudNativeCon North America 2025](https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/)!

ðŸŽ¤ [**Highly Scalable AI Search Engine and AI Data Lake With Kubernetes and LanceDB**](https://kccncna2025.sched.com/event/27FXF/highly-scalable-ai-search-engine-and-ai-data-lake-with-kubernetes-and-lancedb-lu-qiu-chanchan-mao-lancedb)<br>
Lu Qiu, Database Engineer & ChanChan Mao, Developer  @ LanceDB | Tuesday, November 11, 5:45 PM

ðŸŽ¤ [**Building AI/ML Pipelines on Kubernetes**](https://kccncna2025.sched.com/event/27FdU/building-aiml-pipelines-on-kubernetes-susan-wu-ian-chakares-google-lu-qiu-lancedb-anant-vyas-lucy-sweet-uber)<br>
Lu Qiu (LanceDB), Susan Wu & Ian Chakares (Google), Anant Vyas & Lucy Sweet (Uber) | Thursday, November 13, 11:45 AM

[![Open Lakehouse + AI Mini Summit](/assets/blog/newsletter-october-2025/open-lakehouse-meetup.png)](https://luma.com/OLMS-1113)

[Open Lakehouse + AI Mini Summit](https://luma.com/OLMS-1113) | Thursday, November 13, 12:00 PM - 4:30 PM

Dive into the key capabilities of LanceDB's Multimodal Lakehouseâ€”fast random-access at scale, vector + full-text search, and optimized schema primitivesâ€”so you can iterate rapidly without blowing your budget. 

Register: [https://luma.com/OLMS-1113](https://luma.com/OLMS-1113)

## ðŸ“Š LanceDB Enterprise Product News

| Feature | Description |
| :------ | :---------- |
| Clusters and manifests for Geneva | <ul><li>We have web pages and REST APIs for cluster management and manifests in Geneva</li></ul> | 
| Performance improvements | <ul><li>Optimized filtered reads with limit pushdown for better query efficiency.</li><li>Reduced index cache size for memory stability in high-load query nodes.</li></ul> |
| Operational improvements | <ul><li>Reduced **Docker image sizes** for faster downloads and deployments.</li><li>Improved **concurrent request monitoring** with gauges and adjusted logging performance. </li><li>Enhanced **PE metrics** for remote filtered reads and request performance.</li><li>Removed **torch and CUDA dependencies** from indexer image to reduce image size.</li><li>Added **Geneva metrics dashboards** and endpoint response code monitoring.</li><li>Introduced **admin commands** in <code>lancedb-cli</code>, starting with <code>health</code>.</li></ul> |

## ðŸ«¶ Community Contributions

- [Multi-path support](https://github.com/lancedb/lance/pull/4765) for Lance datasets, thanks to [@jaystarshot](https://github.com/jaystarshot) from Uber. 
- [Lance-Ray now supports distributed compaction](https://github.com/lancedb/lance-ray/pull/53) of Lance table, thanks to [@yingjianwu98](https://github.com/yingjianwu98) from Netflix.
- [Spark Data evolution](https://github.com/lancedb/lance-spark/pull/91) feature and [JSON document support in FTS index](https://github.com/lancedb/lance/pull/4752), thanks to [@fangbo](https://github.com/fangbo) & [@wojiaodoubao](https://github.com/wojiaodoubao) from Bytedance

A heartfelt thank you to our community contributors of lance and lancedb this past month:    

[@lyang24](https://github.com/lyang24)
[@wojiaodoubao](https://github.com/wojiaodoubao)
[@jaystarshot](https://github.com/jaystarshot)
[@yanghua](https://github.com/yanghua)
[@chunshao90](https://github.com/chunshao90)
[@ddupg](https://github.com/ddupg)
[@majin1102](https://github.com/majin1102)
[@yingjianwu98](https://github.com/yingjianwu98)
[@xloya](https://github.com/xloya)
[@ColdL](https://github.com/ColdL)
[@niebayes](https://github.com/niebayes)
[@yingjianwu98](https://github.com/yingjianwu98)
[@morales-t-netflix](https://github.com/morales-t-netflix)
[@wayneli-vt](https://github.com/wayneli-vt)
[@steFaiz](https://github.com/steFaiz)
[@zhangyue19921010](https://github.com/zhangyue19921010)
[@timsaucer](https://github.com/timsaucer)
[@wayneli-vt](https://github.com/wayneli-vt)
[@chenghao-guo](https://github.com/chenghao-guo)
[@jtuglu1](https://github.com/jtuglu1)
[@huyuanfeng2018](https://github.com/huyuanfeng2018)
[@valkum](https://github.com/valkum)
[@naaa760](https://github.com/naaa760)
[@edrogers](https://github.com/edrogers)
[@tlamarre91](https://github.com/tlamarre91)

## ðŸŒŸ Open Source Releases Spotlight

| Feature | Version | Description |
| :------ | :------ | :---------- |
| LanceDB | 0.22.2 | Allow bitmap indexes on large-string, binary, large-binary, and bitmap, support namespace database in rust.|
| Lance | 0.38.3 | Support multi-base write, allowing one Lance dataset to span across multiple buckets. Support zstd/lz4 compression. Support custom stop word in FTS index. Support shallow clone and branching in Python and Java. Support Blob API in Java. Support index against nested field. Support change data feed for inserted and updated rows. Better logging and auditing across code execution.|
| | 0.38.2 | Fix forward compatibility of FTS index changes in 0.38.0. |
| | 0.38.1 | Support fragment level column update. Support faster IVF-PQ indexing on GPU.
| | 0.38.0 | â— As of this release, the 2.1 version of the file format is considered stable. There will be no more breaking changes and the format should be readable by future versions of lance. In an upcoming release (possibly the next release) the 2.1 version will become the default. <br><br> Blob encoding for v2.1 file format. Performance improvement of FTS index (reduces the P95 latency by 32.3%). FTS index now supports specialized JSON document tokenizer. |
| Lance Namespace | 0.0.17-0.0.20 | Add support for Azure OneLake, refactor of rust library into main lance repo for better integration |
| Lance Ray | 0.0.7-0.0.8 | Support distributed compaction of Lance table |
| Lance Spark | 0.0.14-0.0.15 | Data Evolution in Apache Spark: users can run ALTER COLUMN FROM to add column and backfill column data at the same time |
```

---

## ðŸ“„ æ–‡ä»¶: newsletter-september-2025.md

---

```md
---
title: ðŸ›¡ï¸ Newly Knighted Lancelot, â–¶ï¸TwelveLabs Semantic Video Recommendations, ðŸ§ Cognee's AI Memory Layer with LanceDB
date: 2025-10-08
draft: false
featured: false
categories: ["Newsletter"]
image: "/assets/blog/newsletter-september-2025/september-newsletter-image-3.png"
description: "Our September newsletter welcomes new Lancelot members, highlights TwelveLabs semantic video recommendations, Cogneeâ€™s AI memory layer, and shares the latest product and community updates."
author: "Jasmine Wang"
author_avatar: "/assets/authors/jasmine-wang.png"
author_bio: "Ecosystem Engagement, Partnership, Community, DevRel"
author_github: "onigiriisabunny"
author_linkedin: "jasminechenwang"
---

## ðŸ›¡ï¸ Meet the Newly Knighted Lancelot

In May, we announced 3 new members to join our [Lancelot Round Table](https://github.com/lancedb/lancedb/wiki?ref=blog.lancedb.com). It is time again for us to welcome **four new noble members** to the Roundtable from Netflix, RunwayML, Luma AI, and Seven Research! A huge thank you to each of them for their continued support and contributions to `lance` and `lancedb`. For the full list of all Lancelot, please check our [github wiki page](https://github.com/lancedb/lancedb/wiki#lancelot-round-table).

âš”ï¸ ðŸŽ Hail to the Knights of the Lancelot Roundtable!

![Alt text for Knights of the Lancelot Roundtable image](/assets/blog/newsletter-september-2025/september-newsletter-image-6.png)

## [Building Semantic Video Recommendations with TwelveLabs and LanceDB](https://lancedb.com/blog/geneva-twelvelabs/)

[![Alt text for Semantic Video Recommendations image](/assets/blog/newsletter-september-2025/september-newsletter-image-1.png)](https://lancedb.com/blog/geneva-twelvelabs/)

Tutorial: a semantic video recommendation engine powered by [TwelveLabs](https://www.twelvelabs.io/) , [LanceDB](https://lancedb.com/) , and [Geneva](https://lancedb.com/docs/geneva) .

*   [TwelveLabs](https://www.google.com/url?q=https://www.twelvelabs.io/&sa=D&source=editors&ust=1759910620606865&usg=AOvVaw3apCGFkGlAhOVK7b8Hrlbp) provides multimodal embeddings that encode the narrative, mood, and actions in a video, going far beyond keyword matching.
*   [LanceDB](https://www.google.com/url?q=https://lancedb.com/&sa=D&source=editors&ust=1759910620607076&usg=AOvVaw2-S5VIKyCqUPmtYGlg-a1M) stores these embeddings together with metadata and supports fast vector search through a developer-friendly Python API.
*   [Geneva](https://lancedb.com/docs/geneva), built on [LanceDB](https://www.google.com/url?q=https://lancedb.com/&sa=D&source=editors&ust=1759910620607367&usg=AOvVaw1n4tTHcknb-XQ1n4MfGz8L) and powered by [Ray](https://www.google.com/url?q=https://lancedb.com/blog/lance-namespace-lancedb-and-ray/&sa=D&source=editors&ust=1759910620607442&usg=AOvVaw2pkOhKVL4dLnwYqoNYVA8j) , scales the entire pipeline seamlessly from a single laptop to a large distributed clusterâ€”without changing your code.

## [Productionalize AI Workloads with Lance Namespace, LanceDB, and Ray](https://lancedb.com/blog/lance-namespace-lancedb-and-ray/)

Most of the work in AI isnâ€™t in the model. Itâ€™s in the data pipeline. Organizing billions of rows of data for indexing and retrieval isn't easy. This is where Ray and LanceDB come together:

*   Ray takes care of the heavy lifting: distribute data ingestion, embedding generation, and feature engineering across clusters.
*   LanceDB makes the results instantly queryable: fast vector search, hybrid search, and analytics at production scale.

[![Alt text for Lance Namespace, LanceDB, and Ray image](/assets/blog/newsletter-september-2025/september-newsletter-image-8.png)](https://lancedb.com/blog/lance-namespace-lancedb-and-ray)


## ðŸŽ¤ Watch the Recordings!


<iframe width="560" height="315" src="https://www.youtube.com/embed/y5g7u3sWyrk?si=yobin3RePEsmnKlS" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


<iframe width="560" height="315" src="https://www.youtube.com/embed/mVN1P1HvpKg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


*   [Apache Spark Connector for Lance](https://www.linkedin.com/events/apachespark-andlancesparkconnec7363659816340258816/theater/)


![Alt text for Apache Spark Connector for Lance image](/assets/blog/newsletter-september-2025/spark-connector.png)

*   [OpenSource Startup Podcast](https://creators.spotify.com/pod/profile/ossstartuppodcast/episodes/E181-Why-Multimodal-Is-the-Future-of-AI-Data-Workloads-e3813id/a-ac59kq5)

<iframe width="560" height="315" src="https://www.youtube.com/embed/AJodQHXGnCA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## ðŸ“š Good Reads

#### [Setup Real-Time Multimodal AI Analytics with Apache Fluss (incubating) and Lance](https://lancedb.com/blog/fluss-integration/)

Real-time multimodal AI is finally practical! With [Apache Fluss (Incubating)](https://www.linkedin.com/company/apachefluss/) as the streaming storage layer and Lance as the AI-optimized lakehouse, you can stream data in right now, tier it automatically, and query it instantly for RAG, analytics, or training.

[![Alt text for Apache Fluss and Lance image](/assets/blog/newsletter-september-2025/september-newsletter-image-2.png)](https://lancedb.com/blog/fluss-integration/)

## ðŸ’¼ Case Study

#### [How Cognee Builds AI Memory Layers with LanceDB](https://lancedb.com/blog/case-study-cognee/)

[![Alt text for Cognee Case Study image](/assets/blog/newsletter-september-2025/september-newsletter-image-5.png)](https://lancedb.com/blog/case-study-cognee/)

>"LanceDB gives us effortless, truly isolated vector stores per user and per test, which keeps our memory engine simple to operate and fast to iterate.â€
>
>-Vasilije Markovic, Cognee CEO



#### [Distributed Training with LanceDB and Tigris | Tigris Object Storage](https://www.tigrisdata.com/blog/lancedb-training/)

Training multimodal datasets at scale is hard. They grow fast, they donâ€™t fit on a single disk, and traditional workarounds (like network filesystems) introduce complexity and cost. But what if you could treat storage as infinite and let your infrastructure handle the hard parts for you?

Thatâ€™s exactly what [Xe iaso](https://www.linkedin.com/in/xe-iaso/) shows in this new guide: how [LanceDB](https://www.linkedin.com/company/lancedb/) and [Tigris Data](https://www.linkedin.com/company/tigrisdata/) make large-scale distributed training simple. No more worrying about local storage limits, manual sharding, or egress feesâ€”just seamless scaling from research to production.

[![Alt text for Distributed Training with LanceDB and Tigris image](/assets/blog/newsletter-september-2025/september-newsletter-image-4.png)](https://www.tigrisdata.com/blog/lancedb-training/)

## ðŸ“Š LanceDB Enterprise Product News

| Feature                                     | Description                                                                                                                                                                                                                             |
| :------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| RabitQ quantization for vector indices      | RabitQ is an alternative to PQ that is more memory- and compute-efficient. See [https://arxiv.org/abs/2405.12497](https://arxiv.org/abs/2405.12497) for more details.                                                               |
| Significantly-reduced latencies for full-text search | Full-text search P95 latencies are now reduced by up to 32.3%. Cold start P95 latencies are now reduced by up to 18.7%.                                                                                                  |
| Scalar indices for JSON columns with type-aware indexing | LanceDB now has the ability to create JSON scalar indices that are type-aware, instead of assuming all extracts are strings. This should resolve the class of issues that are a result of no type awareness.                |
| KMeans algorithm runs significantly faster  | We improved the implementation of our KMeans algorithm to run ~30x faster than before, with even more gains at large k. This results in IVF-based vector indices being built more quickly. |

## ðŸ«¶ Community contributions

#### [Introducing Lance Data Viewer: A Simple Way to Explore Lance Tables](https://lancedb.com/blog/lance-data-viewer/)

When exploring LanceDB for his own projects, Gordon realized there was no simple way to peek inside Lance tables, something many of us take for granted with relational databases. Instead of waiting, he built it. **Lance Data Viewer**, an open-source, containerized web UI for browsing Lance datasets:

[![Alt text for Lance Data Viewer image](/assets/blog/newsletter-september-2025/september-newsletter-image-7.png)](https://lancedb.com/blog/lance-data-viewer/)

A heartfelt thank you to our community contributors of lance and lancedb this past month: [ @majin1102](https://github.com/majin1102) [ @wayneli-vt](https://github.com/wayneli-vt) [ @wojiaodoubao](https://github.com/wojiaodoubao) [ @chenghao-guo](https://github.com/chenghao-guo)[ @huyuanfeng2018](https://github.com/huyuanfeng2018)[ @ColdL](https://github.com/ColdL)[ @jtuglu1](https://github.com/jtuglu1)[ @xloya](https://github.com/xloya)[ @yanghua](https://github.com/yanghua)[ @steFaiz](https://github.com/steFaiz)[ @morales-t-netflix](https://github.com/morales-t-netflix)[ @felix-schultz](https://github.com/felix-schultz)[ @timsaucer](https://github.com/timsaucer) [ @HaochengLIU](https://github.com/HaochengLIU)[ @beinan](https://github.com/beinan)[ @fangbo](https://github.com/fangbo)[ @jaystarshot](https://github.com/jaystarshot)[ @lorinlee](https://github.com/lorinlee) [ @manhld0206](https://github.com/manhld0206)[ @naaa760](https://github.com/naaa760)[ @vlovich](https://github.com/vlovich)[ @pimdh](https://github.com/pimdh)

## ðŸŒŸ Open Source Releases Spotlight

|           |       |  |
| :-------- | :---------- | :-------------------------------------------------------------------------------------------------------------------- |
| LanceDB          | 0.22.1      | Shallow clone support, mTLS, custom request header support for remote database, Mean reciprocal rank reranker support |                                                                                                     |
| Lance     | 0.37        | New table metadata and incremental metadata update support, S3 throughput optimization, distributed compaction in Java, FTS UDTF support in DataFusion, scalar and vector index support for nested struct fields |
|           | 0.36        | Bloom filter index support, scalar index and compaction APIs in Java                                                  |
|           | 0.35        | New lance-tools CLI, JSONB read write support, JSON UDFs, scalar index for JSON, FTS index support for contains_token function, Apache OpenDAL support |
| Lance Namespace | 0.0.15-0.0.16 | Apache Gravitino, Apache Polaris, Google Dataproc support, simplified basic operations for REST namespace implementers |
| Lance Ray | 0.0.6       | Distributed build for FTS index, btree index, fragment level operations for customized write workflows        |
| Lance Spark | 0.0.12-0.0.13 | COUNT(\*) support, BINARY type with Lance Blob encoding read and write support                          |
```

---

## ðŸ“„ æ–‡ä»¶: november-feature-roundup.md

---

```md
---
title: November Feature Roundup
date: 2024-12-03
draft: false
featured: false
image: /assets/blog/november-feature-roundup/november-feature-roundup.png
description: Explore november feature roundup with practical insights and expert guidance from the LanceDB team.
author: Will Jones
---
## Lance

On November 13th, we released Lance 0.19.2. This release includes several new features and improvements, including:

- Flexible handling of data for inserts
- An experimental storage type for video columns and other large blobs
- Full-text search includes not-yet-indexed data

A full list of changes can be found in [the release notes](https://github.com/lancedb/lance/releases/tag/v0.19.2).

### **Inserting data, without the footguns**

Up until now, Lance has been pretty strict about schemas. When you appended to a table:

- You had to provide all the fields in the schema.
- You had to provide them in the same order as the dataset schema.
- You had to match the exact nullability for each field in the schema. If a field was non-nullable in the dataset, providing data with a nullable schema would fail, even if the actual values were never null.

This all changes starting in 0.19.2. Appending data is much more flexible. Now when you append new data:

- You can omit fields from the schema, even nested fields.
- You can provide fields in any order.
- You can provide different but compatible nullability. For example, if a field  is non-nullable in the dataset, you can insert data with a nullable field as  long as the actual values aren't null.

Users could already create tables with missing fields in some rows, as most tools know how to insert nulls:

```python
    import lance
    import pyarrow as pa
    
    data = [
        {"vec": [1.0, 2.0, 3.0], "metadata": {"x": 1, "y": 2}},
        {"metadata": {"x": 3}},
        {"vec": [2.0, 3.0, 5.0], "metadata": {"y": 4}},
    ]
    table = pa.Table.from_pylist(data)
    ds = lance.write_dataset(table, "./demo")
    ds.to_table().to_pandas()

                   vec               metadata
    0  [1.0, 2.0, 3.0]   {'x': 1.0, 'y': 2.0}
    1             None  {'x': 3.0, 'y': None}
    2  [2.0, 3.0, 5.0]  {'x': None, 'y': 4.0}

But what's new is you can create a table that is completely missing some fields and then insert data into the table. Notice how we can omit not only the `vec` field, but also the nested `metadata.x` field:

    new_data = [
        {"metadata": {"y": 6}},
    ]
    new_table = pa.Table.from_pylist(new_data)
    ds = lance.write_dataset(new_table, "./demo", mode="append")
    ds.to_table().to_pandas()

                   vec               metadata
    0  [1.0, 2.0, 3.0]   {'x': 1.0, 'y': 2.0}
    1             None  {'x': 3.0, 'y': None}
    2  [2.0, 3.0, 5.0]  {'x': None, 'y': 4.0}
    3             None    {'x': None, 'y': 6}
```

User feedback has taught us that this development is important for schema evolution. 

In many cases, an existing ETL (Extract, Transform, Load) process is designed to work with a specific schema. If you add a new field to the schema, you must carefully update the ETL process to handle the new field. If not, the ETL process might fail because it doesn't recognize the new field.

With the new ability to insert subschemas, you can now add new fields to the schema without breaking the ETL process. This means you can evolve your schema more easily and with less risk of errors.

This feature was developed by LanceDB engineers Weston Pace (@westonpace) and Will Jones (@wjones127). ([https://github.com/lancedb/lance/pull/3041](https://github.com/lancedb/lance/pull/3041), [https://github.com/lancedb/lance/pull/2467](https://github.com/lancedb/lance/pull/2467))

On top of this feature, we are planning on supporting inserting subschemas using merge insert. (This is tracked in [#2904](https://github.com/lancedb/lance/issues/2904).) Earlier this year, we already added support for updating subschemas using merge insert.

### **Fast analytical queries for video datasets**

0.19.2 introduced an experimental feature to solve a tricky problem: balancing file sizes when column values are very different in size. This is common with unstructured data like videos.

For example:

- An integer column might have values that are only 4 bytes each. With compression, they can be even smaller.
- A video column can have values that are megabytes each and not compressible.

This difference makes it hard to decide how many rows to put in each data file.
Column typeSize per valueRows for 10GBSize of 1 million rowsint324 B2.5 billion4 MBImage (100KB)100 KB100,00010 GBVideo (10MB)10 MB1,00010 PB
For the best performance, we want millions of rows per file for integer columns. A million rows would be just 4 MB without compression. But for video columns, a million rows would be 10 PB per file, which is too large for most storage systems. This means that previously, you had to choose a suboptimal file size for analytical columns if you had any very large columns.

The new balanced storage feature aims to solve this problem. Large columns can be assigned to a new "storage class" called "blob". The blob storage class is kept in separate files from main storage. The main storage will be limited by the default maximum of 1 million rows per file, while the blob storage will be limited by a maximum file size of 90 GB. This allows excellent scan performance for small columns, even if your dataset contains large unstructured data.

To opt into this feature for a particular column, you must add a metadata field to the schema. For example, here's how to construct the PyArrow schema:

```python
    import pyarrow as pa
    
    schema = pa.schema([
        pa.field("int_col", pa.int32()),
        pa.field("video_col", pa.large_binary(), metadata={"lance-schema:storage-class": "blob"}),
    ])
```

This feature is experimental and made available for prototyping. Additional work and testing will be done in the next few months before we will recommend it for production use.

This feature was developed by LanceDB engineer Weston Pace (@westonpace).

### Eliminating the lag to see new data in full text search

Up until now, searching with full text search only included data that was in the table when the index was created or updated. If you added new data to the table, those new rows would be missing from the search results. You could update the index and the results would appear, but this meant a delay between inserting data and being able to search it. It's also different than all other indices in Lance: vector search and scalar indices could both automatically search new data. With 0.19.2, full text search now includes not-yet-indexed data.

```python
    data = pa.table({"animal": ["wild horse", "domestic rabbit"]})
    ds = lance.write_dataset(data, "./demo_fts", mode="overwrite")
    ds.create_scalar_index("animal", index_type="INVERTED")
    ds.to_table(full_text_query="domestic rabbit and horse").to_pandas()

                animal    _score
    0  domestic rabbit  1.386294
    1       wild horse  0.693147

    new_data = pa.table({"animal": ["wild rabbit"]})
    ds = lance.write_dataset(new_data, "./demo_fts", mode="append")
    ds.to_table(full_text_query="domestic rabbit and horse").to_pandas()

                animal    _score
    0  domestic rabbit  1.386294
    1      wild rabbit  0.871385
    2       wild horse  0.693147
```

Eventually, you will still want to update the index to include the new data. This is partially for performance reasons. But importantly it also will improve search results. The search on new data only works for terms that have already been indexed. So if you have a new term that wasn't in any documents previously, searching that term will not return any results until you update the index.

This feature was developed by LanceDB engineer Yang Cen (@BubbleCal). ([#3036](https://github.com/lancedb/lance/pull/3036)). 

## LanceDB

On November 15th, we released LanceDB 0.13.0 (Rust/Node) and LanceDB Python 0.16.0. In addition to the features from Lance 0.19.2, these releases include several improvements to feature parity between the Python, Node, and Rust SDKs.

A full list of changes can be found in the release notes:

- [LanceDB 0.13.0](https://github.com/lancedb/lancedb/releases/tag/v0.13.0)
- [LanceDB Python 0.16.0](https://github.com/lancedb/lancedb/releases/tag/python-v0.16.0)

### Features from Lance

LanceDB 0.13.0 includes all the features from Lance 0.19.2. This includes then ability to insert subschemas and the full-text search improvements.

### Improvements in feature parity

We've made several more improvements to feature parity between the Python, Node, and Rust SDKs this release, which include:

- [Added `Table.optimize` to Python synchronous API](https://github.com/lancedb/lancedb/pull/1769)
- [`fast_search` query added in Python and Node APIs](https://github.com/lancedb/lancedb/pull/1623)
- [`post_filter` is now an option for full-text search queries in Python](https://github.com/lancedb/lancedb/pull/1783)
- [`with_row_id` query option support in Python and Node](https://github.com/lancedb/lancedb/pull/1784)
- [Pass multiple vectors to vector search query](https://github.com/lancedb/lancedb/pull/1811)

### New Rust-based remote client

v0.19.2 also included a major milestone in feature parity: the LanceDB remote client (used to connect to LanceDB Cloud and Enterprise) has been consolidated into a single Rust-based client. Previously Python, Node, and Rust had independent implementations, supporting different features. Now they all use the same underlying implementation, ensuring feature parity now and in the future.
```

---

## ðŸ“„ æ–‡ä»¶: python-package-to-convert-image-datasets-to-lance-type.md

---

```md
---
title: Python Package to convert image datasets to lance type
date: 2024-12-09
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/python-package-to-convert-image-datasets-to-lance-type/python-package-to-convert-image-datasets-to-lance-type.png
description: Explore python package to convert image datasets to lance type with practical insights and expert guidance from the LanceDB team.
author: Vipul Maheshwari
author_avatar: "/assets/authors/community.jpg"
author_bio: ""
---

A few months ago, I wrote two articles on how the Lance format can supercharge your machine-learning workflows. In the first, I showed how Lance's columnar storage can make handling large image datasets much more efficient for ML workflows. Then, I followed up with a guide on converting datasets like [cinic](https://www.kaggle.com/datasets/vipulmaheshwarii/cinic-10-lance-dataset) and [mini-imagenet](https://www.kaggle.com/datasets/vipulmaheshwarii/mini-imagenet-lance-dataset) into Lance format using a custom Python script in [Google Colab](https://colab.research.google.com/drive/12RjdHmp6m9_Lx7YMRiat4_fYWZ2g63gx?usp=sharing). While that worked well, it was a bit manual.

Some of my friends are lazy but excited enough to run the Colab and use the Lance formatted datatype for some of their experiments. Being a good friend, I'm excited to share a much easier solution: the `lancify` Python package. 

It's literally just running one command and boomâ€”your image datasets are in Lance format, ready to go. And, just between us, it makes my life a lot easier, too.

![Lancify Package Overview](/assets/blog/python-package-to-convert-image-datasets-to-lance-type/image-6.png)

## Installing the package

Before diving into the conversion process, let's install the `lancify` package. You can easily install it via pip:

```bash
pip install lancify
```

## Converting Your Image Dataset to Lance

Once you've installed the package, converting any image dataset to the Lance format is as simple as running the following Python code. The `lancify` package abstracts away the complexity of running the Colab notebooks manually:

```python
from lancify.converter import convert_dataset

# Define the path to your image dataset
image_dataset_path = 'cards-classification'
resize_dimensions = (256, 256)  # Example resize dimensions
splits = ["train", "test", "valid"]

convert_dataset(
    dataset_path=image_dataset_path,
    batch_size=10,  # You can adjust the batch size as needed
    resize=resize_dimensions,  # Pass resize dimensions if you want to resize images
    splits=splits
)
```

For this demonstration, I have used this [dataset](https://www.kaggle.com/datasets/gpiosenka/cards-image-datasetclassification), which provides flexibility in terms of image resizing and dataset splits. The image resizing is optional; by default, the images are processed with their original dimensions. However, if needed, you can specify a target size, such as 256x256, by passing the desired dimensions. If you prefer to keep the original size, simply pass `None` for the resize parameter. Regarding dataset splits, if the dataset includes predefined divisions like training, testing, and validation sets, you can pass a list specifying the relevant splits.

For [datasets](https://www.kaggle.com/datasets/jehanbhathena/weather-dataset) that do not have predefined splits, the images are organized by classification labels. In such cases, you only need to provide the dataset path and a single lance file will be generated, containing all the images with their corresponding labels. This makes sure that the various kinds of image datasets are handled properly whether they include splits or not.

```python
from lancify.converter import convert_dataset

image_dataset_path = 'weather-classification-data'

convert_dataset(
    dataset_path=image_dataset_path,
    batch_size=10,  # You can adjust the batch size as needed
)
```

The `convert_dataset` function automatically handles the following:

1. **Reading the image data**: It reads image files and their metadata (filename, category, data split).
2. **Converting to Lance**: The images are converted into the Lance format with the proper schema.
3. **Saving the Lance files**: Lance files are saved for each dataset split (train, test, validation) if there are splits in the dataset, if not then a single Lance file is saved with the combined data with an adequate schema to segregate the data with the respective labels.

This method is far more concise than manually iterating over directories, creating schemas, and writing to lance files as we did in the previous version using raw Google Colab.

## Install CLI SDK

In addition to using the `lancify` package programmatically through the imported function, you can also leverage the CLI SDK to convert your image datasets. The SDK offers a CLI for the `lancify`.

To use the CLI, all you need to do is install the package with `pip install lancify` and then run the `lancify` command in your terminal and follow the args.

This is how it looks:

*CLI usage for lancify*

## What's happening behind the scenes?

To give you a better understanding, here's a brief overview of what happens when you use `lancify`:

- **Image Data**: The package reads images from your dataset directory and converts them into a binary format.
- **Metadata Extraction**: Metadata such as the image's filename, category (label), and data split (train/test/validation) are automatically extracted.
- **PyArrow RecordBatch**: The image data and metadata are packaged into a PyArrow `RecordBatch` for efficient columnar storage.
- **Lance Dataset Creation**: These `RecordBatch` objects are then written to Lance datasets optimized for performance and storage.

This process mirrors the manual steps we previously took but in a much more user-friendly manner, significantly reducing the boilerplate code that was necessary before when you had to manually handle the [colab](https://colab.research.google.com/drive/12RjdHmp6m9_Lx7YMRiat4_fYWZ2g63gx?usp=sharing#scrollTo=93qlCg6TpcW-).

## Loading your dataset into pandas

Once your image dataset has been converted into the lance format, you can seamlessly load it into Pandas data frames to do all kinds of stuff. Here's how to do it for the `card-classification` training lance file.

```python
import lance
import pandas as pd

# Load Lance dataset
ds = lance.dataset('cards-classification/cards-classification_train.lance')
table = ds.to_table()

# Convert Lance table to Pandas dataframe
df = table.to_pandas()
print(df.head())
```

This is a simple and efficient way to convert your image datasets to the lance format using the `lancify` package and it integrates smoothly into your deep-learning projects.

Switching to the Lance format speeds up and improves your data pipelines, especially when handling large image datasets. Install the package and run a simple script to convert your datasetsâ€”it's easy and straightforward.

![Lance Format Benefits](/assets/blog/python-package-to-convert-image-datasets-to-lance-type/image-5.png)

Just this small change can really speed up your machine learning workflowsâ€”data loading and processing become much quicker, which means your models train faster. If you need a reference, this is a [quickie](https://vipul-maheshwari.github.io/2024/06/26/train-a-cnn-with-lancedataset) on how to use the lance formatted image datasets for training your deep learning models. And if you're looking for more ideas, there are plenty of other [deep-learning recipes](https://github.com/lancedb/lance-deeplearning-recipes) built on Lance. 

Trust me, it's worth it! ðŸ¤—
```

---

## ðŸ“„ æ–‡ä»¶: scalable-computer-vision-with-lancedb-voxel51-d8b65066d5f6.md

---

```md
---
title: "Scalable Computer Vision with LanceDB & Voxel51"
date: 2023-07-13
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/scalable-computer-vision-with-lancedb-voxel51-d8b65066d5f6/preview-image.png
meta_image: /assets/blog/scalable-computer-vision-with-lancedb-voxel51-d8b65066d5f6/preview-image.png
description: "Explore about scalable computer vision with lancedb & voxel51. Get practical steps, examples, and best practices you can use now."
---

## Importance of dataset analysis in CV

Dataset analysis is an essential step in any computer vision project. It allows us to understand the data that we are working with, identify any potential problems, and make sure that the data is as representative as possible of the real world. This is especially important once the model architecture is fixed due to deployment constraints, as the best way to improve performance is via iteration on datasets.

Voxel51 is a set of open-source tools for computer vision. The FiftyOne query language allows you to explore and analyze your data in a variety of ways. You can use the query language to find specific images, objects, or features.

With this integration with LanceDB, Voxel51 can leverage its fast, robust, and setup-free vector search capabilities.
![](/assets/blog/scalable-computer-vision-with-lancedb-voxel51-d8b65066d5f6/1*RFtus4W8XVeoG0Bc08IHdg.jpg)
## LanceDB: The serverless vectorDB

LanceDB is an open-source, serverless, multi-modal vector database.

- It is built with persistent storage, which greatly simplifies retrieval, filtering, and management of embeddings.
- It supports vector similarity search, full-text search, and SQL.
- Native support for JavaScript/TypeScript

To enable the LanceDB backend in Voxel51, simply pass `backend="lancedb"` in supported operations

```python
import fiftyone.brain as fob

fob.compute_similarity(
    ...
    backend="lancedb",
)
```

## **Example use cases**

## Sort dataset by similarity

![](/assets/blog/scalable-computer-vision-with-lancedb-voxel51-d8b65066d5f6/1*WIKrLZEtkYGwlXaCDdctJQ.gif)
## Get similar images to a given id

```python
import fiftyone as fo
import fiftyone.brain as fob

fob.compute_similarity(
    ...
    backend="lancedb",
)
id = dataset.first().id
view = dataset.sort_by_similarity(id, k=30)

session = fo.launch_app(view)
session.wait()
```

## Search by Text queries

```python
query = "a photo of a dog"
view = dataset.sort_by_similarity(query, k=10, brain_key="lancedb_index")
```

The integration allows various customizations that can be found in the docs.

## LanceDB interface integrates into the Python data ecosystem

LanceDB is compatible with the Python data ecosystem and can be used with pandas, NumPy, and Arrow.

```python
lancedb_index = fob.compute_similarity(â€¦)
table = lancedb_index.table

# Integration with Python data ecosystem
df = table.to_pandas()  # get the table as a pandas dataframe
pa = table.to_arrow()   # get the table as an arrow table
```

More use cases and examples using voxel51 with LanceDB backend are available in the [integration docs](https://docs.voxel51.com/integrations/lancedb.html#lancedb-integration).

Visit LanceDB [repo](https://github.com/lancedb/lancedb) and [docs](https://lancedb.github.io/lancedb/) to see more examples and integration.
```

---

## ðŸ“„ æ–‡ä»¶: search-within-an-image-331b54e4285e.md

---

```md
---
title: "Search Within an Image with Segment Anything "
date: 2023-12-12
author: Kaushal Choudhary
author_avatar: /assets/authors/kaushal-choudhary.jpg
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/search-within-an-image-331b54e4285e/preview-image.png
meta_image: /assets/blog/search-within-an-image-331b54e4285e/preview-image.png
description: "Get about search within an image with segment anything. Get practical steps, examples, and best practices you can use now."
---

## Introduction

[SAM](https://github.com/facebookresearch/segment-anything)** **(Segment Anything) model by FAIR, has set a benchmark in field of Computer Vision. It seamlessly segments objects image with zero-shot classification. Whereas [**CLIP**](https://github.com/openai/CLIP)** **(Contrastive Language Image Pretraining) model by OpenAI, which is trained on numerous (image, text) pairs is really useful in Q&A with images.

We are going to leverage both of these models to create a â€œ**Search Engine**â€ for an image. We will be using both the models in symphony to create a search engine which can effectively search within a given image and a given natural language query.

## Semantic Searching with Natural Language

Enabling semantic search within an image requires multiple steps. This process is akin to developing a basic search engine, involving steps such as indexing existing entities, calculating the distance between the user query and all entities, and then returning the closest match.

Hereâ€™s a visual representation of the process

![Architecture for Search Engine](/assets/blog/search-within-an-image-331b54e4285e/1*32siuOjQamsOWzxmH1ZxDg.png)

The process consists of four key steps:

1. **Instance Segmentation:** Utilizing the** *SAM*** model, we extract entities from the image through segmentation.
2. **Embedding Entities:** [**open_clip**](https://github.com/mlfoundations/open_clip)is employed to convert the identified entities into embeddings.
3. **Embedding User Prompts:** *open_clip* is also utilized to convert the provided text prompt into embeddings.
4. **Finding and Highlighting the Closest Entity:** ***LanceDB*** is used to locate the entity closest to the text prompt. Subsequently,*** OpenCV*** is applied to highlight the edge of the identified entity within the original image.

## Implementing the Search functionality

Follow along with the Colab notebook:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/search-within-images-with-sam-and-clip/main.ipynb)

Now, letâ€™s dive right into creating it.

**I. Create the Segmentation to extract the entities.**

Download the Image

```python
url = 'https://w0.peakpx.com/wallpaper/600/440/HD-wallpaper-john-wick-with-mustang.jpg'
img_uuid = download_image(url)
```

We will be using [**Open_Clip**](https://github.com/mlfoundations/open_clip), which can be install using `pip install open-clip-torch`.

Load the weights for SAM.

```python
import requests

url = 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'
response = requests.get(url)

with open('sam_vit_h_4b8939.pth', 'wb') as f:
    f.write(response.content)
```

Get the Image segmentation

```
    #load SAM
    from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
    sam = sam_model_registry["vit_h"](checkpoint="sam_vit_h_4b8939.pth")

    #extract the segmentation masks from the image
    def get_image_segmentations(img_path):
        input_img = cv2.imread(img_path)
        input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)
        mask_generator = SamAutomaticMaskGenerator(sam)
        masks = mask_generator.generate(input_img)
        return masks
```

Display the Segmentation Masks
![Segmentation Masks](/assets/blog/search-within-an-image-331b54e4285e/1*P-c2e5YZk2Z_AUnmyMsnfw.png)
**II**. **Convert the entities into embeddings**

To convert the entities into embeddings, we will be using **CLIP** model.Get the embeddings of the segmented images.

```
    def get_image_embeddings_from_path(file_path):
        image = preprocess(Image.open(file_path)).unsqueeze(0)
        # Encode the image
        with torch.no_grad():
            embeddings = model.encode_image(image)
        embeddings = embeddings.squeeze() # to squeeze the embeddings into 1-dimension
        return embeddings.detach().numpy()
```

**III. Convert the text prompt into embeddings**.

```python
text = tokenizer(user_query)
k_embedding = model.encode_text(text).tolist()  # Use tolist() instead of to_list()
# Flatten k_embedding to a List[float]
k_embedding_list = flatten_list(k_embedding)
```

**IV. Find the Closest match and highlight.**

```python
# initialize the database
uri = "data/sample-lancedb"
db = lancedb.connect(uri)

# composite call to the functions to create the segmentation mask, crop it, embed it and finally index it into LanceDB
def index_images_to_lancedb(img_uuid):
    img_path = img_uuid + '/index.jpg'
    source_img = cv2.imread(img_path)
    segmentations = get_image_segmentations(img_path)  # get the segmentations

    for index, seg in enumerate(segmentations):
        # crop the image by bbox
        cropped_img = crop_image_with_bbox(crop_image_by_seg(source_img, seg['segmentation']), seg['bbox'])
        c_img_path = img_uuid + '/{}.jpg'.format(index)
        cv2.imwrite(c_img_path, cropped_img)
        embeddings = get_image_embeddings_from_path(c_img_path)  # embed the image using CLIP
        seg['embeddings'] = embeddings
        seg['img_path'] = c_img_path
        seg['seg_shape'] = seg['segmentation'].shape
        seg['segmentation'] = seg['segmentation'].reshape(-1)

    seg_df = pd.DataFrame(segmentations)
    seg_df = seg_df[['img_path', 'embeddings', 'bbox', 'stability_score', 'predicted_iou', 'segmentation', 'seg_shape']]
    seg_df = seg_df.rename(columns={"embeddings": "vector"})
    tbl = db.create_table("table_{}".format(img_uuid), data=seg_df)  # index the images into table
    return tbl
```

### Search Function

```python
# find the image using natural language query
def search_image_with_user_query(vector_table, img_id, user_query):

    text = tokenizer(user_query)
    k_embedding = model.encode_text(text).tolist()  # Use tolist() instead of to_list()
    # Flatten k_embedding to a List[float]
    k_embedding_list = flatten_list(k_embedding)

    target = vector_table.search(k_embedding_list).limit(1).to_df()
    segmentation_mask = cv2.convertScaleAbs(target.iloc[0]['segmentation'].reshape(target.iloc[0]['seg_shape']).astype(int))

    # Dilate the segmentation mask to expand the area
    dilated_mask = cv2.dilate(segmentation_mask, np.ones((10, 10), np.uint8), iterations=1)

    # Create a mask of the surroundings by subtracting the original segmentation mask
    surroundings_mask = dilated_mask - segmentation_mask

    # Create a highlighted version of the original image
    path = '{}/index.jpg'.format(img_id)
    highlighted_image = cv2.imread(path)
    highlighted_image[surroundings_mask > 0] = [253, 218, 13]

    cv2.imwrite('{}/processed.jpg'.format(img_id), highlighted_image)

    # Display the image
    display(Image(filename='{}/processed.jpg'.format(img_id)))
```

Letâ€™s search in the Image with a user query

```python
# index the downloaded image and search within the image
tbl = index_images_to_lancedb(img_uuid)
search_image_with_user_query(tbl, img_uuid, 'a dog')
```

![Result](/assets/blog/search-within-an-image-331b54e4285e/1*Ob_qPZfmCfVX-J3SAXYTCA.png)

Visit our [LanceDB](https://github.com/lancedb/lancedb) and if you wish to learn more about LanceDB python and Typescript library.
For more such applied GenAI and VectorDB applications, examples and tutorials visit [**VectorDB-Recipes.**](https://github.com/lancedb/vectordb-recipes/tree/main)** **Donâ€™t forget to leave a star at the repo.

Lastly, for more information and updates, follow our** **[**LinkedIn**](https://www.linkedin.com/company/lancedb/) and [**Twitter**](https://twitter.com/lancedb)
```

---

## ðŸ“„ æ–‡ä»¶: second-dinners-secret-weapon-lancedb-powered-rag-for-faster-smarter-game-development.md

---

```md
---
title: "Second Dinner's Secret Weapon: LanceDB-Powered RAG for Faster, Smarter Game Development"
date: 2025-03-25
draft: false
featured: false
categories: ["Case Study", "Gaming"]
image: /assets/blog/second-dinners-secret-weapon-lancedb-powered-rag-for-faster-smarter-game-development/second-dinners-secret-weapon-lancedb-powered-rag-for-faster-smarter-game-development.png
description: "Discover how Second Dinner, creators of Marvel Snap, leveraged LanceDB Cloud to transform game development workflows, reducing prototyping time from months to hours and automating QA test generation with 81% better results."
author: Qian Zhu
author_avatar: "/assets/authors/qian-zhu.jpg"
author_bio: "Software Engineer at LanceDB specializing in cloud infrastructure, game development workflows, and AI-powered development tools."
author_twitter: "qianzhu56"
author_github: "qianzhu56"
author_linkedin: "qianzhu56"
aliases: ["/blog/blog/second-dinners-secret-weapon-lancedb-powered-rag-for-faster-smarter-game-development/"]
---

{{< admonition info "Case Study Collaboration" >}}
This is a case study contributed by the Second Dinner team and the LanceDB team.
{{< /admonition >}}

*Second Dinner's innovative game development setup showcasing their AI-powered workflow transformation.*

![Second Dinner Setup](/assets/blog/second-dinners-secret-weapon-lancedb-powered-rag-for-faster-smarter-game-development/Screenshot-2025-03-20-at-10.21.46-PM.png)

[Second Dinner](https://seconddinner.com/) teamed up with LanceDB Cloud's serverless architecture to turbocharge their game development workflow. Overnight, system designers went from waiting months to spinning up prototypes in hours, slashing endless cross-team coordination. QA engineers, meanwhile, swapped week-long testing marathons for a single API call, achieving superior test coverage and pinpoint accuracy. The kicker? [**LanceDB Cloud**](https://cloud.lancedb.com/) **went from zero to production-ready in just two weeks**, proving that cutting-edge innovation doesn't have to come with a headache.

## Introduction

In the fast-paced world of game development, Second Dinner stands as a powerhouse of creativity and innovation. Founded in 2018 by Hearthstone masterminds Hamilton Chu and Ben Brode, this studio is not just crafting gamesâ€”it's redefining them. With a legacy of expertise and a hunger for fresh horizons, Second Dinner is built to be a gaming studio where the pursuit of excellence is fueled by collaboration and the freedom to think big.

Central to its innovation is a strategic embrace of next-generation AI, enhancing both creativity and efficiency. From accelerating concept iteration to QA workflows, these tools amplify the studio's ability to deliver groundbreaking experiences without compromising artistic vision.

## The Challenge

### Revolutionizing Game Design

Despite their success, two major roadblocks were slowing down game development:

#### Streamlining Prototyping for New Features

System designers dreamed up new features all the time. However, turning scribbles into code required assembling multiple teamsâ€”engineers, UX wizards, and QA testersâ€”leading to months of meetings, drained budgets, and stalled ideas.

#### Time-Consuming and Inconsistent QA Testing

Every new game card needed testing against thousands of interactions. QA engineers had to manually compose test suites, a process that could stretch out for an entire week. Despite team expansion, this manual approach strains the small workforce while risking missed edge cases and failure to meet stringent quality benchmarks.

{{< admonition warning "Critical Requirements" >}}
Second Dinner needed a game-changing solution that could:
- Enable quick prototyping from a prompt
- Automate comprehensive QA test generation via API
- Seamlessly handle multimodal data, from Jira tickets to design docs
- Scale effortlessly via a serverless architecture
{{< /admonition >}}

Second Dinner needed a game-changing solutionâ€”one that could harness LLMs to accelerate both prototyping and QA, reducing delays and allowing designers and engineers to focus on crafting unforgettable gaming experiences. They needed a solution that seamlessly supports vector retrieval, handles multimodal data, and serves as the core data platform for both RAG and Agentic workflows.

## The Solution: LanceDB as the AI Backbone

To bring their vision to life, Second Dinner sought a vector database that was powerful, flexible, Python-friendly, and cost-effective. Their ideal solution comes with rich functionalities yet provides great flexibility for experimenting with various scenarios, is capable of supporting multimodal data and is cost-effective. On top of all those, the solution must deliver a frictionless developer experience for engineers.

After evaluating available options, Second Dinner chose LanceDB Cloud to power their internal LLM-based tools and AI-driven development workflow. The decision was strategicâ€”LanceDB Cloud aligned with their "no-ops" philosophy, offering a fully managed platform that met all their technical needs while being 3â€“5x more cost-effective than alternatives.

### Implementation Architecture

Second Dinner's AI team converted their proprietary codebase, design documents, Jira tickets etc. as vectors and stored them with LanceDB. Minimal setup was requiredâ€”the AI team simply ingested embeddings and created indices, while LanceDB Cloud handled the heavy lifting, including serverless scaling, on-the-fly optimizations (compacting datasets, incremental indexing, and full index rebuilds), and robust retrieval featuresâ€”all without disrupting live services.

{{< admonition tip "Rapid Deployment" >}}
From the moment they gained access, LanceDB Cloud was production-ready in just two weeks, making it as simple as working in a standard Python environment without the typical DevOps headaches.
{{< /admonition >}}

*Second Dinner leverages LanceDB to power an internal Slackbot for streamlined workflow automation.*

![Autogenerated Mockup](/assets/blog/second-dinners-secret-weapon-lancedb-powered-rag-for-faster-smarter-game-development/unnamed.png)

### For System Designers: From Idea to Prototype in Hours

Now, when a designer has a new ideaâ€”like "What would a card that steals mana from an opponent's pool to amplify your own spells look like?"â€”they simply ask in Slack.

Within hours, the system fetches relevant mechanics from previous cards using LanceDB-powered Retrieval-Augmented Generation (RAG). A webpage mockup is automatically generated, allowing the designer to quickly gather internal feedback. The result? No need to assemble an entire engineering team or risk launching an untested feature.

*An autogenerated webpage mockup from system designer's idea, showcasing rapid prototyping capabilities.*

![Second Dinner Slackbot](/assets/blog/second-dinners-secret-weapon-lancedb-powered-rag-for-faster-smarter-game-development/unnamed-1.png)

### For QA Engineers: Automated Test Generation

Instead of spending a full week manually writing test cases, QA engineers now generate comprehensive test suites in minutes through intuitive API calls or Slack commands. Powered by LLMs, this tool acts as a force multiplierâ€”the autogenerated tests outperform human-generated ones 81% of the time, ensuring better coverage and consistency.

Notably, 20.5% of AI-generated tests prove "considerably better" in terms of styling or functional requirements than human-written counterparts. By offloading repetitive validation work, the lean QA team now prioritizes high-impact strategic initiativesâ€”transforming from executors to visionaries.

## Results & Impact

Integrating LanceDB Cloud-powered AI tooling has transformed Second Dinner's development workflow for QA and early prototyping:

### Performance Metrics

- **Faster Prototyping**: Cycle times are slashed from months to hours, enabling designers to juggle 5x more ideas in parallel
- **Cost Efficiency**: Engineers focus on high-impact tasksâ€”like optimization and polishâ€”instead of repetitive coding
- **Creative Freedom**: Breakthrough features emerged from risk-free experimentation, thanks to serverless scaling and flexible retrieval capabilities

### Quality Improvements

- **81% Better Test Coverage**: AI-generated tests outperform human-generated ones in most cases
- **20.5% Superior Quality**: AI-generated tests prove "considerably better" in styling or functional requirements
- **Strategic Focus**: QA team transformed from executors to visionaries, focusing on high-impact initiatives

> "LanceDB isn't just a toolâ€”it's our cheat code. We're shipping crazier ideas, faster, with minimal operational overhead, and our players love it."
> 
> â€” Xiaoyang Yang, VP of AI, Data, and Security @ Second Dinner

## The Future of Game Development

Second Dinner's next move? More AI-powered tooling to speed up developmentâ€”all powered by LanceDB. Because in gaming, the only rule is: Never stop playing.

Game over? Nope. Game on. ðŸŽ®

## Acknowledgments

{{< admonition thanks "Special Thanks" >}}
Special thanks to the following team members who contributed to this case study:
- [Xiaoyang Yang](https://www.linkedin.com/in/xyyang/), VP of AI, Data, Security @ Second Dinner
- [Eric Del Priore](https://www.linkedin.com/in/ericdelpriore/), Principal Technical Producer II @ Second Dinner
- [Tal Puhov](https://www.linkedin.com/in/tal-puhov/), AI Research Engineer @ Second Dinner
- [Qian Zhu](https://www.linkedin.com/in/qianzhu56/), Software Engineer @ LanceDB
{{< /admonition >}}
```

---

## ðŸ“„ æ–‡ä»¶: semanticdotart.md

---

```md
---
title: "SemanticDotArt: Rethinking Art Discovery with LanceDB"
date: 2025-10-16
author: Ayush Chaurasia
categories: ["Applications"]
draft: false
featured: true
image: /assets/demos/sda_hero.jpg
meta_image: /assets/demos/sda_hero.jpg
description: "SemanticDotArt turns art discovery into a multimodal search experience, matching feelings, phrases, and images with LanceDB's hybrid retrieval."
---

# SemanticDotArt: Rethinking art discovery with LanceDB

[Try it out â†’](https://www.semantic.art/)

In an age of infinite scroll, we can browse more art than any real-world gallery could hold--yet finding a piece that feels right could still take minutes, or hours. **SemanticDotArt** began with a hunch: meaning lives not in pixels or tags, but in the mood of a painting, the rhythm of a brushstroke, or the metaphors that tie them together. From that intuition grew a multimodal retrieval system we built, with LanceDB at its core. This post traces the journey of how words meet images, and how we taught search to feel a little more human.

## The Vision

Art discovery often begins with a mood, rather than a checklist of traits. We imagine what we're looking for through thoughts like: â€œfind me something restless but hopeful,â€ â€œshow me a painting that feels like a quiet storm.â€ SemanticDotArt is built for that kind of language. It lets you search not just with literal phrases, but with poetry, prose, or the emotions they stir in you. Sometimes, _how_ you ask is part of what youâ€™re looking for.

![LanceDB table overview](/assets/demos/sda_2.jpg)

We wanted:
- A unified art corpus that continually grows across museums, marketplaces, and open archives.
- Metadata which captures both literal content and emotional subtext.
- A retrieval engine that can handle large amounts of text and images, and knows when a query is poetic, prosaic, artistic or literal.
- An interface that feels exploratory, rather than transactional.

LanceDB is used as the multimodal foundation for this system, because it offers the following key features:
- On disk index which allow building large scale multi-feature multi-index retrieval system
- First-class hybrid & full-text search support, fast SQL-style filtering
- Built-in support for various multimodal embedding models, and hooks for creating custom rerankers. 
- Being truly multimodal, it doubles as an object store, every artworkâ€™s vectors, text, and JPEG bytes live in the same source table, preventing chaos as we keep adding new features.

# How this works

As with any other retrieval system, the workflow is broken down into two main parts: ingestion and querying.

## Multi-representation

The guiding principle is simple: we create multiple representation of the artworks so the we have different ways of looking at it. For any given piece, we record multiple perspectives: poetic impressions, literal descriptions, mood tags, color palettes, and even stylistic fingerprints. Some become vector columns, some remain as text, and others live on as raw media. LanceDB lets us stitch all of that into a single row of a table so we can keep adding features as the dataset evolves without reindexing the world. The main idea is to offer maximum flexibility at query time, so that we can experiment with different search paths dynamically, as we'll see in the next section. 

### A single painting, many views

![van gogh painting](/assets/demos/sda_van_gogh.jpg)

Take Van Gogh's **Path Through a Field with Willows**. We keep several parallel interpretations so that whichever language a visitor uses, there is an index ready to meet it. Here are examples of some:

- **Poetic caption**
  > A path winds on beneath a vibrant sky, where sun-warmed grasses whisper secrets. Brushstrokes dance with restless energy, quiet fields hold deep intensity, and a lonely journey is bathed in golden light. Nature breathes both calm and wild, colors sing a song of solitude, and hope lingers where the track ascends.
- **Natural caption**
  > A path meanders under a bright sky as sun-warmed grasses softly rustle and energetic brushstrokes of light and shadow play across quiet fields. The solitary journey glows with golden light, nature around it feels tranquil yet untamed, and the colors evoke solitude while hope follows the upward slope of the path.
- **Mood keywords** â€” `nature`, `solitude`, `dream`, `deep`, `track`, `intensity`, `path`, `field`, `willows`, `van_gogh`

These ingredients seed separate full-text, vector, and keyword indexes. The corpus keeps expanding, but because the representations belong to the same row, we can add new features, such as palette embeddings, brushstroke fingerprints, provenance signals, without having to refactor our storage solution.

![Ingestion workflow diagram](/assets/demos/sda_3.png)

## Semantic Routing: Matching Feelings with Features dynamically

Because each piece of artwork is over-represented in the data, retrieval turns into a choose-your-own-adventure task. A session might start with text, an image, or both. Semantic routing inspects that intent and helps us choose the dynamic search path that fits poetic vectors when the request feels lyrical, with natural-language embeddings for straightforward descriptions, and visual features when a user starts with image/pixels. Along the way, new features can be added in as they become available. When we blend or rewrite the query, mood hint keywords are used with LanceDBâ€™s SQL-style prefilters to narrow down the search space. Finally, a custom reranker weights the results to surface pieces that echo the emotional signature of the request.

This is our rendition of classic retrieval strategies like query understanding, query rewriting, and multi-index routing. The agent classifies how the visitor is describing the art, rephrases the prompt so it aligns with the chosen representation, and finally selects which LanceDB indices to use. Every new representation we add becomes another branch the router can learn to take. From there, we can switch tactics on the fly depending on the query type and the column that seems most relevant.

With LanceDB, a complex hybrid search with prefiltering and reranking looks is simply:

```python
results = (
    table.search(query_type="hybrid", vector_column_name="poetic_vector")
         .vector(query_embedding)
         .text(query_text)
         .where(prefilter_clause, prefilter=True)
         .limit(10)
         .rerank(CustomKeywordRanker(keywords))
)
```


![Semantic routing paths](/assets/demos/sda_4.png)


Here's how one of those pathways can unfold when someone uploads an image and adds a short poem:

1. **Classify the intent** â€“ Label the request as `poetic`, `natural`, or another style so downstream steps know which feature column to favor.
2. **Caption the image** â€“ If pixels are present, synthesize a caption in the same style as the intent so image and text signals travel together.
3. **Rewrite the prompt** â€“ Blend the visitor's words and the generated caption into a single rewritten query that preserves both mood and literal anchors.
4. **Extract mood keywords** â€“ Pull out a keyword set that reflects the emotional signature sitting inside the rewritten query.
5. **Prefilter via SQL support** â€“ Apply a LanceDB filter using those keywords so the search space collapses to artworks that share at least one mood.
6. **Choose search technique** â€“ Switch between full-text, vector, or hybrid search depending on length and style -- we over-fetch by roughly 2Ã— so the reranker has room to maneuver.
7. **Rerank** â€“ Score the candidates with a custom keyword ranker that weights overlap between query and artwork moods, then surface the pieces that best echo the request.

The reranker leans on a weighted blend of recall and precision over the keyword sets:

\[
\text{score} = w \cdot \left( 0.7 \cdot \frac{\text{matches}}{|B|} + 0.3 \cdot \frac{\text{matches}}{|A|} \right)
\]

Where `matches` is the overlap count between the artwork keyword set (`A`) and the query keyword set (`B`), so the recall term is represented by:

\[
\frac{|A \cap B|}{|B|}
\]

and the precision term by:

\[
\frac{|A \cap B|}{|A|}
\]



This keeps the responses feeling both relevant and surprising without drifting into uncanny matches. LanceDB supports custom rerankers natively, so we can plug in new ranking strategies as the dataset and features evolve.


## Conclusion

AI-generated images are everywhere, but the thrill of discovery still belongs to human-made art. SemanticDotArt uses AI as a bridge: drop in an image a model just imagined, or a photo you took, and it will lead you to the paintings and sculptures shaped by people who _felt_ that idea before you did. Whether you search with a poetic cue like â€œ_a quiet optimism painted in the sky_â€ or with a literal description, the path ends in the same place: human creations that echo your feeling.

![A quiet optimism](/assets/demos/sda_5.jpg)

[Try SemanticDotArt â†’](https://www.semantic.art/)



## Tools used

<img width="794" height="116" alt="Screenshot 2025-10-15 at 10 55 17â€¯PM" src="https://github.com/user-attachments/assets/9c4e10e6-c6ce-499f-b6be-2970496a7984" />

- **LanceDB** â€“ Core multimodal store and retrieval engine: vectors, captions, mood keywords, and original JPEGs share one table, with hybrid search and custom rerankers stacking on top.
- **Google Gemini** â€“ Multimodal model powering poetic rewrites, intent classification, and on-demand captioning to keep text and image evidence aligned.
- **Modal Labs** â€“ Managed the backend services and high-throughput batch ingestion pipelines without building new infrastructure.


## Credits

This project was created by Bryan Bischof, Ayush Chaurasia, and Chang She. Kelly Chong and Pavitar Saini designed and implemented the UI. Adam Conway and Mischa Lamoureux assisted with the website.
```

---

## ðŸ“„ æ–‡ä»¶: series-a-funding.md

---

```md
---
title: "LanceDB Raises $30M Series A to Build the Multimodal Lakehouse"
date: 2025-06-24
draft: false
featured: true
categories: ["Announcement"]
image: /assets/blog/series-a-funding/preview-image.png
meta_image: /assets/blog/series-a-funding/preview-image.png
description: "We have closed another funding round to accelerate development of the Multimodal Lakehouse - a unified platform for AI data infrastructure."
author: Chang She
author_avatar: "/assets/authors/chang-she.jpg"
author_bio: "CEO and Co-Founder of LanceDB."
author_twitter: "changhiskhan"
author_github: "changhiskhan"
author_linkedin: "changshe"
---

Today, I am proud to announce that LanceDB has closed a **$30 million Series A** round led by **Theory Ventures**, with additional participation from **CRV, YCombinator, Databricks Ventures, RunwayML, Zero Prime, Swift**, and many other amazing investors, all of whom share our vision for multimodal data.

## The New Open Source Standard for AI Data

Over the past year, we have witnessed Lance become the new standard for multimodal data. As of June 2025, **Lance remains the fastest growing format** across the data ecosystem. During this period, our open source packages have been downloaded more than 20 million times.

Four years ago my co-founder **Lei** and I asked a simple question: Why is working with embeddings, images, and video still so difficult, when compared to tabular data? The truth was simple: the industry was building on foundations meant for yesterday's data. We decided to start from first principles, which meant discarding formats like Parquet and WebDataset. This moment of clarity became the seed of LanceDB.

Now, **LanceDB Enterprise** runs at massive scale within leading Generative AI companies such as Runway, Midjourney, and Character.ai. These teams are searching over tens of billions of vectors and managing petabytes of training data for AI models. With LanceDB, they are moving faster, scaling more efficiently, and dramatically reducing infrastructure complexity and cost.

## The Game Has Changed

Since we started LanceDB, AI's rise has been nothing short of meteoric. In two decades of building data tools, I've never seen technology spread faster. Our customers taught us three things:

1. **Data variety** now includes embeddings, documents, images, and video, a much richer set than the tabular data seen previously.  
2. **Multimodal data volume** is 3-9 orders of magnitude larger than tabular data, surprising even small teams with petabytes to manage.  
3. **The velocity** of data generation is no longer gated on manual human action; LLMs and VLMs create new data at thousands of tokens per second and are getting faster all the time.

The generation and use of multimodal data is growing at an unprecedented pace, driven by the need to capture the full richness of meaning.

By 2025, an estimated 156 zettabytes (ZB) of data will be videoâ€”representing 90% of all data generated. Thatâ€™s nearly three times more than in 2022 (53.6 ZB) and over four times more than in 2018 (37 ZB).

![multimodal-data-growth](/assets/blog/series-a-funding/data-size.jpg)

Source: [Accelerating Model Development and Fine-Tuning on Databricks with TwelveLabs](https://www.youtube.com/watch?v=gdIrpWvPD1M)

The data infrastructure we have today is not enough to meet this new challenge. Traditional data lakes are great for tabular data and BI tasks, but not great for AI workloads, online serving, and multimodal data. Simultaneously, vector databases are optimized for online search on vector data, but not appropriate for other needs. 

Tasks like feature engineering, data exploration, and training that are critical for building great AI are not well served by any of the existing infrastructure tooling.

The result is that engineers today need to stitch together multiple tools, spending all their time with plumbing instead of experimenting, shipping features, or improving the cognitive layer in the agent they're building.

## Introducing the Multimodal Lakehouse

Imagine you're building a complex AI application or training a large model. The ideal foundation would let you put your data from embeddings and documents to images and videos all in one place. On top of this **single source of truth**, you'd be able to run all the workloads you need from search to training without multiple systems. 

As you scale from experimentation to petabytes of data and tens of thousands of queries per second, you wouldn't need to worry about the underlying infrastructure. Everything would simply work.

**This is exactly what we're creating at LanceDB, and it's called the Multimodal Lakehouse.** Built on open-source Lance format, it is a unified engine designed specifically for AI, allowing you to store, retrieve, and compute over all your multimodal data.

![Multimodal Lakehouse Architecture](/assets/blog/series-a-funding/lancedb-enterprise.png)

The Multimodal Lakehouse combines the best of data lakes and vector databases into a single platform optimized for AI workloads, eliminating the need to manage multiple disparate systems.

To learn more about the Multimodal Lakehouse, [read our latest engineering blog](http://lancedb.com/blog/multimodal-lakehouse/).

## A Major Milestone

This investment marks a pivotal step in our mission to redefine data infrastructure for the AI era. We will accelerate our work on the Multimodal Lakehouse, deepen our commitment to the Lance open-source community, and help teams everywhere build AI applications faster, cheaper and easier.

Early Lakehouse features are already driving real-world production for our largest LanceDB Enterprise customers. To discover how it works for them, and what's coming next, [download our technical report on the Multimodal Lakehouse](/download/) with real-world learnings. 

With this funding, we're doubling down on:

- **Expanding the Multimodal Lakehouse** with new capabilities for feature engineering and model training
- **Growing our open-source community** around the Lance format
- **Scaling our enterprise platform** to support even larger AI workloads
- **Building partnerships** with leading AI companies and research institutions

> Let's build what's next, together. </br>
> Chang She
```

---

## ðŸ“„ æ–‡ä»¶: the-case-for-random-access-i-o.md

---

```md
---
title: "The Case for Random Access I/O"
date: 2024-08-20
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/the-case-for-random-access-i-o/preview-image.png
meta_image: /assets/blog/the-case-for-random-access-i-o/preview-image.png
description: "One of the reasons we started the Lance file format and have been investigating new encodings is because we wanted a format with better support for random access."
---

One of the reasons we started the Lance file format and have been investigating new encodings is because we wanted a format with better support for random access.  Random access I/O has traditionally been ignored by columnar formats, and with reasonably good arguments...or at least, arguments that sound reasonably good.  In this post we break down the old reasoning and explain why it falls short.

## The case against random access

Let's start with the status quo which boils down to:

- Random access is slower
- Random access is more expensive
- If you want random access you're better off with a row-based format

That seems like a pretty solid case but it's not quite so simple.  Let's break down each point in more detail.

## Problem: Random access is slower

Random access is slower than linear access.  This depends on the storage mechanism (NVME and RAM don't suffer too much) but it is exceptionally true for cloud storage.  Cloud storage, by default, typically offers around 4-5 thousand IOPs per second **per bucket**.  You can increase these limits in various ways but it isn't very easy, requires a lot of fine-tuning, and probably multiple instances.  On the other hand, cloud instances can easily achieve 30-40Gibps or more of bandwidth.  Fully profiling cloud storage is a difficult task but from just these numbers we can build a *very rough* rule of thumb:

ðŸ’¡

Cloud storage supports 4K IOPs per second and 4G bytes per second

These numbers aren't perfect.  It's very hard to get 5GBps on cloud storage and sometimes we get more than 4K IOPs but they are probably within a factor of 2.  The important part is the ratio.  If we break this down that means we need to read 1MB per IOP to saturate the bandwidth (4K * 1M == 4G)!  This suggests we can have quite a lot of read amplification and still get the same performance.  Or, in parquet terms, if each data page is 1MB then we can afford to read an entire data page and still get the same throughput.

### Rebuttal: Latency vs Bandwidth

The first thing to point out with this reasoning is that we are only looking at bandwidth.  In cloud storage, even though we are limited in how many requests we can make, the latency of those requests decreases significantly as the request size decreases, all the way down to somewhere around 4 to 32KiB.

In other words, a cloud storage page-at-a-time parquet system and a random-access lance system might both be limited to the same searches per second (throughput) but the queries in the random-access system will have significantly lower latency than the queries in the page-at-a-time system, unless your data pages are 4KiB (not practical for current parquet readers but we might be moving Lance to use 4KiB pages for scalar types in the future ðŸ˜‰).
![Results of a simple benchmark on Google's cloud storage](/assets/blog/the-case-for-random-access-i-o/Throughput---Latency-vs.-Request-Size.png)
Results of a simple benchmark on Google's cloud storage
### Rebuttal: Row size matters

Note the difference in units. 4K **rows** per second and 4B **bytes** per second.  As I already pointed out, once a single row reaches 1MB the random access penalty is gone.  Now, even with multi-modal data, we are unlikely to reach 1MB per row (unless working with video).  However, it is not entirely uncommon to have something like 128KB per row (compressed images, HTML pages, source code documents, etc.)  With rows this wide, the random access penalty is much smaller and the old rationales start to quickly fall apart.  For example, modern parquet readers are starting to add late materialization for large columns because, suddenly, random access isn't so frightening anymore.

### Rebuttal: Future proofing

The other problem here is that this logic is mostly predicated on a "cloud storage" way of thinking that has been around for quite some time.  My personal prediction is that this IOPs per second limitation will be disappearing from cloud storage offerings in the future (possibly in the form of a "bulk get").  Even if it doesn't, data engineers can bypass these limits with their own services.  In our enterprise product we already have caching layers that allow us to blast past this limitation.  If you change the above analysis to be based on NVME instead of cloud storage it completely falls apart.  As soon as your read amplification forces you to read from many (4KiB) disk sectors instead of a single disk sector then you are losing throughput.

## Problem: Random access is more expensive

Another common refrain is that random access is more expensive.  Once again, this reasoning is usually based in the cloud storage world.  The classic reasoning basically boils down to "cloud storage charges per IOP and random access needs more IOPs".  The typical use case considered is something like "SELECT x,y FROM table WHERE x IS TRUE" and we start to talk about things like "selectivity thresholds", "early materialization", and "coalesced I/O".  For example, a common claim might be "*if x is true 1% of the time then it is cheaper to scan the entire y column than it is to pick the values matching the filter using random access*" and this is true.

### Rebuttal: Selectivity thresholds don't always scale

The first problem here is that we're relying on this 1% selectivity threshold which means the # of rows that matches our filter scales with the dataset.  This is important for the original argument.  For example, if we select 10 rows from 1M potential rows there is a high chance we will coalesce.  If we select 10 rows from 1B potential rows there is almost no chance we will coalesce.
![The odds you can coalesce I/O depend on the # of rows you want and the # of rows you have](/assets/blog/the-case-for-random-access-i-o/Coalesce-vs-Selectivity.png)
The odds you can coalesce I/O depend on the # of rows you want and the # of rows you have
Of course, it seems obvious to assume that a filter scales with our data.  However, there are many operations where *the filter does not scale with the data*.  For example, workflows such as find by id, update by id, upsert, find or create, vector search and full text search all target a very narrow range of data that does not grow with dataset size.  Existing formats struggle with these workflows because they rely on coalescing for random access instead of optimizing the random access itself.

### Rebuttal: You can have your cake and eat it too

The second problem with the above argument is a simple one.  We can absolutely coalesce I/O and support random access at the same time (Lance coalesces I/O within a page).  In fact, I would argue that Lance is even more likely to coalesce I/O because its default page size is very large (8MiB).
![Large page sizes (thanks to no row groups) => lots of coalescing, even while supporting random I/O](/assets/blog/the-case-for-random-access-i-o/Coalesce-vs-Page-size.png)
Large page sizes (thanks to no row groups) => lots of coalescing, even while supporting random I/O
### Rebuttal: It costs CPU cycles to throw data away

The final problem with the cost argument is that it focuses on the I/O cost (cost per IOP) and ignores the CPU cost.  For example, let's pretend we've stored 8GB of 8-byte integer data and used some kind of compression algorithm to drop that down to 6GB.  Then, we want to select 1% of our data.  We only want to return 80MB of data (10M values).  However, we need to load and decompress 8GB of data.


> In theory you could push your selection vector into the decompression algorithm but I'm not aware of any parquet reader that does this (and I don't even know if decompression algorithms would support it). In fact, many parquet readers are not even able to push the selection into the decode process so it's an even worse situation since you need to decode 8B rows as well as decompress all that data.

To give you a basic idea I ran a simple experiment with the pyarrow parquet reader.  This isn't as rigorous as I'd normally like since the pyarrow parquet reader has no way of pushing down selection vectors into the decode but it should give a rough rule of thumb.  With Lance I randomly accessed 1,000 values of the `l_partkey` (an integer) column from a scale factor 10 TPC-H dataset.  With parquet I fully read in the `l_partkey` column into memory (~60M rows, snappy compression, 30 row groups, 1MB page size).  I then repeated this query 1,000 times (to remove initialization noise).  The random access approach required 71B instructions.  The full scan approach required 1.34T instructions (approx. 19 times higher CPU cost).

Or, to put this into dollars, if we assume a core speed of 4B instructions per second, and a core cost of $0.0000124/second (based on current pricing of a `c7i.16xlarge` instance) then *we get about *4,544*,000 random access queries per dollar and about *241*,000 linear scan queries per dollar of CPU.*

**But aren't we paying more for I/O? And doesn't I/O dominate cost?**

Not necessarily.  As I mentioned above, we are coalescing I/O for random access in Lance.  Another way to think of this is, in Lance, we throw away bytes after reading but before decompression.  In Parquet we don't throw bytes away until after decompression (and, with pyarrow, after decoding)  Both formats are using nice 8MiB pages and so we end up with a minimum of 30 IOPs per request.  With 1,000 rows across 30 pages we are going to get a lot of coalesced I/O.  In S3 we pay $0.0004 per thousand requests.  Let's do a bunch of math to calculate the cost of 1000 queries.
FormatCPU costI/O costTotal costParquet$0.00410$0.0012$0.00530Lance$0.00022$0.0012$0.00142
As explained above, as the scale of our dataset increases, the difference in price will only grow (assuming this is a case where our filter is independent of dataset size).

## Problem: Just use a row-based format

The final argument against random access is that you're better off with a row-based format if you want random access.  This is a pretty simple argument and it's not wrong.  A row-major format is going to give you better access than a column-major format if you are querying multiple columns of data.  However, it's not that simple.

### Rebuttal: You probably want both

Most workflows, at least, most workflows we see at LanceDB, need both types of access.  For example, an extremely common workflow is AI model training & inference.  Researchers start with a big list of some large data type (e.g. images, or common crawl HTML pages, or prose documents).  They slowly grow new columns of data onto that page doing analysis and extraction of the data (this is very much a columnar operation).  This process can end up with thousands of columns.  Using a row-based format would be awful for performance here.

Then, once the model is built, they need to run inference against the data, and now they suddenly want random access.  These random access queries typically only load a small set of columns and its always the same set of columns (e.g. source image and caption).  Suddenly we need to support random access patterns.

We could, of course, store two copies of the data.  This has the obvious problem of doubling your storage costs but there is the less obvious problem in that it tends to greatly increase your data team's engineering costs to manage the complexities of two different storage solutions.  This need for both is exactly why we aim to develop a format that can do both processes efficiently.

### Rebuttal: Cake, eat it, keep it, do both

Once again, the second rebuttal is simply that you can support both access patterns with a single format.  In Lance we've already developed a "packed struct" encoding that can be turned on, at will, to pack the fields in a struct (with all fixed length fields) into a row-based format.
![Packed struct encoding](/assets/blog/the-case-for-random-access-i-o/Packed-Struct.png)
You can opt-in to this encoding on a per-struct basis using field metadata.  This allows you to selectively pack fields that are frequently accessed together.  In addition, we are planning a packed-row encoding which will allow you to pack together any set of fields (fixed or variable length) into a single variable-length data type.  Once this is in place, we can even add a boolean flag to change the top-level encoding from columnar to row which would allow Lance to be used as a 100% row-based format.

This allows for simple and coarse grained switching between row & column formats as well as fine grained selection on a per-column basis.  You can now fine-tune your format to maximize performance on any set of query patterns.  If you really need both patterns for some columns then you can selectively duplicate those columns without creating a duplicate of all your other data (most importantly, avoiding duplication of expensive string and image data).

## TANSTAAFL: Too good to be true?

Yes, it turns out we don't get everything for free.  There are some legitimate drawbacks to support random access that we've encountered

- We plan on zipping definition and repetition levels with the values themselves, this has some small increase to repdef sizes since it means we cannot use RLE on the repdef levels (though we could potentially make use of it with the combined values).  There's also a CPU cost associated with zipping and unzipping.
- In order to support random access we need to materialize null values (e.g. how arrow does it) instead of storing those arrays sparsely (e.g. how parquet handles it).  Mostly sparse arrays will take up more space on disk (and thus more time to query).
- In order to support random access we can't support certain encodings (e.g. generalized block compression or delta encoding) which impacts our compressibility.

However, we believe many of these obstacles can be overcome.  Clever use of run length encoding on values may tackle the first two bullet points for most real-world data sets (and there are ways we can do *(mostly)* random access with run length encoding ðŸ˜Ž).  Meanwhile, using small (4KiB) chunks for some data types can allow us to use encodings like delta without sacrificing random access performance.  Or, we can find encodings like FSST which can give us compression without sacrificing random access.

These drawbacks have not been significant so far (indeed, we believe we make up for these drawbacks already with our smarter scheduling algorithms) and should only shrink with time as we get more sophisticated.

## TL;DR:

- Random access has worse throughput on cloud storage but caching layers can avoid this and the improved latency is worth it in many situations.
- Supporting random access does not necessarily cost anything and can lead to cheaper and less CPU intensive workflows.
- The Lance format will eventually support both row and column storage patterns in a single format, giving your great flexibility in query patterns without adding to your development overhead or storage costs.
```

---

## ðŸ“„ æ–‡ä»¶: the-future-of-ai-native-development-is-local-inside-continues-lancedb-powered-evolution.md

---

```md
---
title: "The Future of AI-Native Development is Local: Inside Continue's LanceDB-Powered Evolution"
date: 2025-04-16
draft: false
featured: false
categories: ["Case Study"]
image: /assets/blog/the-future-of-ai-native-development-is-local-inside-continues-lancedb-powered-evolution/the-future-of-ai-native-development-is-local-inside-continues-lancedb-powered-evolution.png
description: "Discover how Continue revolutionized AI-native development with LanceDB's embedded TypeScript library, enabling lightning-fast semantic code search while maintaining complete developer privacy and offline capability."
author: Ty Dunn
author_avatar: "/assets/authors/ty-dunn.jpg"
author_bio: ""
---

As Continue offers user-controlled IDE extensions, most of the codebase is written in TypeScript, and the data is stored locally in the `~/.continue` folder. The tooling choices are made such that there are no separate processes required to handle database operations. Continue's codebase retrieval features are powered by [LanceDB](https://github.com/lancedb/lancedb), as it is the only vector database with an embedded TypeScript library that is capable of fast lookup times while being stored on disk and also supports SQL-like filtering.

Continue seamlessly integrated LanceDB to transform codebase search - deploying a production-ready solution in under a day. This rapid implementation not only accelerated development but inherently aligned with Continue's foundational principles: a local-first architecture that prioritizes developer privacy and offline capability, ensuring sensitive code never leaves the user's machine.

## Introduction

*Agent Mode in Continue: Demonstrating AI-powered code assistance that understands context and semantics beyond traditional keyword matching.*

![Agent Mode in Continue](/assets/blog/the-future-of-ai-native-development-is-local-inside-continues-lancedb-powered-evolution/agent.gif)

Continue reimagines how developers harness AI, transforming it from a rigid tool into an extension of your workflow. With open-source extensions for VS Code and JetBrains, Continue empowers developers to **build, customize**, and **deploy AI coding assistants** tailored to your team's unique patterns, preferences, and codebases. Integrate models, prompts, rules, and documentation into a unified toolkit - all within your IDE, and all under your control.

While Continue operates locally by default - storing data securely in your `~/.continue` directory - it's built to transcend individual setups, scaling effortlessly into server or cloud environments for teams. Organizations can extend its core Retrieval Augmented Generation (RAG) system through a flexible context provider API, integrating proprietary databases, internal documentation, or legacy codebases to create tailored AI assistants.

{{< admonition tip "Enterprise Scalability" >}}
By leveraging LanceDB's high-performance vector indexing, teams deploy lightning-fast semantic search across distributed codebases, ensuring AI outputs align with internal patterns and practices. Whether for developers iterating locally or organizations orchestrating centralized knowledge graphs, Continue unifies personal agility with enterprise-scale context awareness.
{{< /admonition >}}

Continue isn't just another AI toolâ€”it's a developer-defined ecosystem where you shape how AI accelerates your work. Build smarter, ship faster, and focus on what matters: creating exceptional code.

## The Challenge

Developers often work with vast codebases, intricate libraries, and sprawling documentation. Traditional keyword-based search tools struggle to keep pace, failing to surface semantically relevant code snippets, identify nuanced patterns, or retrieve contextually aligned resources.

### Core Requirements

To solve this, Continue required a solution that could:

- **Understand Code Semantics**: Move beyond superficial text matching to analyze the intent and logic behind code, enabling accurate retrieval of functionally similar patterns.
- **Accelerate Developer Workflow**: Deliver instant, context-aware recommendations as developers type, eliminating disruptive latency during critical thinking phases.
- **Scale Seamlessly**: Support massive codebases and diverse programming languages while maintaining consistent performance, even under heavy workloads.

### Technical Constraints

To integrate this capability directly into their open-source VS Code and JetBrains extensions, Continue needed a vector database that prioritized privacy, simplicity, and tight integration with developer environments. The solution had to meet stringent criteria:

{{< admonition warning "Non-Negotiable Requirements" >}}
- **Native TypeScript Support**: Ensure seamless compatibility with Continue's codebase, enabling maintainable, type-safe implementations without external dependencies
- **Local, Offline-First Storage**: Operate entirely within the `~/.continue` directory to guarantee data privacy, eliminate cloud dependencies, and empower developers to work securely offline
- **Zero-Overhead Architecture**: Run embedded within the IDE processâ€”no separate database servers or background processesâ€”to minimize resource consumption and simplify setup
- **Expressive Filtering Capabilities**: Leverage SQL-like query syntax to enable granular search (e.g., filtering by language, project, or custom metadata) while maintaining blazing-fast retrieval speeds
{{< /admonition >}}

Continue's requirements for a vector database were unequivocal: it needed an embedded TypeScript library to ensure seamless integration, lightning-fast lookup times even with on-disk storage, and robust SQL-like filtering capabilities to enable precise, context-aware queries. These features were non-negotiable for delivering a performant, developer-centric experience.

## The Solution

There are a number of available vector databases which are able to performantly handle large codebases. LanceDB stood out as the **only vector database offering an embedded TypeScript library with local disk storage**, enabling Continue to deliver a frictionless, self-contained experience. Its performance-optimized design ensured sub-millisecond lookup times, even with large codebases, while robust SQL-like filtering allowed developers to refine searches with surgical precision.

> "LanceDB is a good choice for this because it can run in-memory with libraries for both Python and Node.js. This means that in the beginning our developers can focus on writing code rather than setting up infrastructure."
> 
> â€” Nate Sesti, Cofounder & CTO @Continue

By storing vectors directly on disk in Lance format, LanceDB also future-proofed Continue's architecture, ensuring effortless scalability from local experimentation to enterprise-grade deployments.

### Implementation Architecture

Here's how Continue leverages LanceDB to power its AI-driven code understanding:

#### Step 1: Code Semantic Embedding

Continue converts code snippets, functions, and documentation into high-dimensional vectors using embedding models (like [Voyage AI's code embedding model](https://blog.voyageai.com/2024/12/04/voyage-code-3/)). This captures the meaning of codeâ€”not just keywordsâ€”enabling the AI to recognize similarities even when syntax differs (e.g., identifying equivalent logic in Python and JavaScript).

#### Step 2: Local Codebase Ingestion

The system crawls your local repository, chunking code into manageable segments (e.g., 10-line blocks). For a 10M-line codebase, this creates ~1M vectors. LanceDB's in-memory architecture ensures this process is fast and resource-efficient, while its disk-based storage keeps data persistent and secure.

#### Step 3: Indexing for Speed & Precision

Continue calls LanceDB APIs to build vector + scalar indexes. This combination allows Continue to retrieve results in milliseconds, even with massive datasets.

#### Step 4: Context-Aware Developer Queries

When a developer searches ("How do we handle API retries?") or requests AI assistance, Continue uses LanceDB to:

- Perform a vector search to find semantically related code
- Apply SQL-like filters (language, project, tags) to refine results
- Return contextually relevant suggestions directly in the IDE

#### Step 5: Seamless Codebase Updates

As developers work across branches or update code, LanceDB's optimizations prevent redundant work:

- **No full reindexing**: Small changes (e.g., two similar branches) only update affected vectors
- **Embedding flexibility**: Swap models (like trying OpenAI vs. custom embeddings) without rebuilding the entire database

## Results & Impact

*Continue's IDE integration showcasing context-aware code suggestions powered by LanceDB's semantic search capabilities.*

![Continue IDE Integration](/assets/blog/the-future-of-ai-native-development-is-local-inside-continues-lancedb-powered-evolution/Screenshot-2025-04-14-at-10.12.14-PM-2.png)

By integrating LanceDB, Continue has successfully transformed its coding assistance capabilities, providing developers with fast, context-aware suggestions that go beyond simple keyword matching.

### Performance Metrics

- **Faster Development**: Auto-completion suggestions improved by 40% in relevance, reducing time spent debugging with context-aware error resolution
- **Scalability**: Handled 1M+ vectors with <10ms latency per query, even on modest hardware - no excessive memory needed
- **User Personalization**: Developers working on ML projects saw tailored suggestions for PyTorch/TensorFlow snippets

> "Thanks for all the work that you do! When I found LanceDB it was exactly what we needed, and has played its role perfectly since then : )"
> 
> â€” Nate Sesti, Cofounder & CTO @Continue

## The Future of AI-Native Development

As Continue reimagines the future of developer tools, it is pioneering a world where AI assistants transcend code to become holistic collaborators. Continue is laser-focused on empowering developers to interact with any resourceâ€”code, images, videos, PDFs, or design specsâ€”as intuitively as they write functions today, all powered by LanceDB's native multimodal support and advanced multivector search.

As enterprises adopt Continue to democratize AI-powered coding across global engineering teams, LanceDB's scalable cloud infrastructure and enterprise-grade security will anchor mission-critical deployments - enforcing compliance, accelerating cross-team collaboration, and future-proofing innovation as organizations grow.

The future belongs to teams that treat AI as a living extension of their collective expertise. With LanceDB as our backbone, Continue will keep turning this vision into realityâ€”one line of code, one breakthrough, and one enterprise at a time.

## Learn More

{{< admonition resources "Additional Resources" >}}
**Continue Resources:**
- [Website](https://www.continue.dev/) - Official Continue homepage
- [Documentation](https://docs.continue.dev/) - Complete setup and usage guides
- [GitHub Repository](https://github.com/continuedev/continue) - Open-source codebase

**LanceDB Resources:**
- [Website](https://lancedb.com/) - LanceDB platform overview
- [Documentation](https://lancedb.github.io/lancedb/) - Technical documentation and API references
- [GitHub Repository](https://github.com/lancedb/lancedb) - Open-source vector database
- [Discord Community](https://discord.gg/G5DcmnZWKB) - Join our developer community
- [Example Recipes](https://github.com/lancedb/vectordb-recipes) - Get started with LanceDB examples
{{< /admonition >}}
```

---

## ðŸ“„ æ–‡ä»¶: the-future-of-open-source-table-formats-iceberg-and-lance.md

---

```md
---
title: "The Future of Open Source Table Formats: Apache Iceberg and Lance"
date: 2025-04-08
draft: false
featured: false
categories: ["Engineering"]
image: /assets/blog/the-future-of-open-source-table-formats-iceberg-and-lance/the-future-of-open-source-table-formats-iceberg-and-lance.png
description: "Explore the future of open source table formats: apache iceberg and lance with practical insights and expert guidance from the LanceDB team."
author: Jack Ye
author_avatar: "/assets/authors/jack-ye.jpg"
author_bio: "Data engineer and table format specialist, working on distributed systems and modern data lake architectures."
author_twitter: "jackye"
author_github: "jackye"
author_linkedin: "jackye"
---

As the scale of data continues to grow, open-source table formats have become essential for efficient data lake management. Apache Iceberg has emerged as a leader in this space, while new formats like [Lance](https://github.com/lancedb/lance) are introducing optimizations for specific workloads. In this post, we'll explore how Iceberg and Lance address different challenges and complement each other in the evolving landscape of data lake table formats.

## **The Rise of Open Source Table Formats**

Table formats like Apache Iceberg, Delta Lake, and Apache Hudi were designed to address the challenges of managing large-scale structured data in cloud storage. These formats brought capabilities like ACID transactions, schema evolution, and time travel, making data lakes more reliable and performant. Among them, Iceberg gained significant traction through widespread adoption in the open-source ecosystem and strong integration into commercial vendor products. Its architecture enables scalable and performant query execution while allowing flexible integration into existing data lake infrastructure.

## **Considerations for Apache Iceberg**

Iceberg is a major advancement over traditional approaches, but it has some challenges to consider:

1. **Metadata Overhead**: Iceberg's metadata management relies on maintaining manifest lists and metadata trees, which can become a bottleneck as datasets grow. Querying data in Iceberg requires multiple hops before accessing the actual data: first, the catalog lookup, then fetching the metadata file (if you are not using REST catalog), followed by retrieving the manifest list, and finally loading the manifest file before reading the data itself. This multi-step process can introduce latency, especially for highly transactional workloads.
2. **Limited Multimodal Support**: Iceberg is built primarily for structured, tabular data and lacks native support for complex data types such as images, videos, audio, and vector embeddings. These types are becoming increasingly important in AI and ML workflows. While it's possible to reference such data externally (e.g., via paths or object URIs) or store it as binary type, Iceberg does not natively handle or optimize storage and access for these data types, limiting its effectiveness in modern, multimodal machine learning pipelines.
3. **Lack of Efficient Random Access**: Iceberg is optimized for large-scale analytical SQL queries, where scan-based access patterns are common. However, it lacks native support for low-latency random access, which is critical for many machine learning scenarios. ML workflows often require retrieving a small number of records or specific features repeatedlyâ€”for example, during training, inference, or feature lookupsâ€”operations that are relatively inefficient with Iceberg's current design. Although features like Parquet bloom filters can help reduce the scan range in some cases, they are not sufficient for true fine-grained, high-performance random access.
4. **Lack of Efficient Column Appends**: Iceberg with its currently supported file formats cannot append new column data to an existing table without rewriting the entire data file. Consider a common ML pipeline where one team produces a source dataset and multiple feature extraction teams append new feature columns to the dataset over time. Iceberg would result in significant inefficiencies when used in such a pipeline because a full table rewrite is required every time some new features are developed and need to be appended to the table as new columns.

## **How Lance Complements Iceberg**

Lance is an emerging open-source table format that focuses on optimizing for modern data applications, particularly AI and ML workloads. Rather than replacing Iceberg, Lance provides capabilities that are well-suited for specific use cases.

1. **Metadata Efficiency**: Instead of maintaining multiple metadata layers, Lance embeds efficient indexing structures within the table to reduce metadata overhead. This design minimizes lookup time and simplifies metadata synchronization, which is particularly beneficial in high-ingest or frequently updated environments. By storing metadata alongside the data in a unified format, Lance streamlines query planning and execution, resulting in lower latency and more predictable performance for ML and AI pipelines.
2. **Multimodal Data Support**: Unlike traditional table formats, Lance is built to efficiently store and query multimodal data, including text, images, audio, video, and vector embeddings. This capability is critical for AI-driven applications, where models often rely on a combination of structured and unstructured data. Lance supports native storage and access patterns for these data types, enabling seamless integration into machine learning workflows without requiring external storage systems or complex workarounds. For example, Lance can directly store image tensors and associate them with structured metadata or text captions in the same table. This simplifies the development of use cases like image classification, video analysis, recommendation systems, and semantic search, where multimodal data is core to the application logic. As a result, Lance empowers AI-driven applications to operate on fully integrated datasetsâ€”structured and unstructured alikeâ€”within a unified, high-performance framework that accelerates development and deployment of complex, multimodal models.
3. **Efficient Random Access**: Lance moves beyond Parquet by using a custom columnar format specifically designed for low-latency random access. On top of that, the Lance also supports a variety of secondary indexing techniques such as B-trees, bitmap indexes, n-gram indices, vector indexes, and full-text search, enabling highly efficient lookups and query acceleration. This makes Lance ideal for machine learning workloads that require fast retrieval of individual rows or columnsâ€”such as during training, inference, or feature lookupsâ€”without scanning large portions of the dataset. The format's ability to support targeted access patterns makes it a strong fit for real-time and interactive AI and ML workflows.
4. **Efficient Column Appends**: Lance allows for appending new column data without rewriting the entire dataset, making schema evolution with backfill more efficient and reducing unnecessary I/O costs. It achieves this by expressing tables as a series of fragments, where each fragment contains multiple files that can contain data for a subset of columns. This architecture enables targeted updates and efficient storage of newly added columns, as backfilling data in a new column for existing fragments is as simple as adding new data files, avoiding the need to rewrite existing data files. This design makes Lance particularly well-suited for collaborative machine learning environments, where teams need to incrementally evolve feature sets without incurring the cost of full table rewrites.

## **The Future of Open Source Table Formats**

The future of data lake table formats will likely be shaped by the evolving needs of AI, real-time analytics, and efficient storage management. Iceberg is well-positioned as a **standard for data exchange**, given its broad adoption and integration into major query engines. We have already seen projects like Apache XTable and Delta Lake Uniform that enable users to use other open table formats while synchronizing to Apache Iceberg. Products like Amazon SageMaker Lakehouse has also proven the possibility of runtime metadata translation to present proprietary table formats like Amazon Redshift in the shape of Iceberg metadata through the Iceberg REST catalog interface. Overall, Apache Iceberg's strong support for analytical workloads and enterprise data lake environments makes it a foundational format for interoperability.

Lance, on the other hand, is emerging as the **format for ML and AI**, where low-latency access, high-performance random access, and efficient column updates are critical. As organizations move toward hybrid analytics and machine learning-driven architectures, the demand for high-performance, scalable, and AI-native table formats will grow. We see a wave of AI-native data comingâ€”multimodal, unstructured, constantly evolvingâ€”and bringing with it a new set of challenges that traditional data infrastructure is not built to handle. Current generation table formats like Iceberg, Delta Lake, and Apache Hudi have served the analytics ecosystem well, but they are not optimized for this new frontier. That's why we believe this space is ripe for innovation. It's an exciting opportunity to reimagine data infrastructure purpose-built for the future of AI, and we expect to see rapid evolution in how data is stored, queried, and shared in AI-centric environments.

Rather than competing, these formats can **coexist and serve complementary roles** in the modern data ecosystem, allowing organizations to optimize their data strategies based on workload-specific needs. We are excited to see collaborative developments such as Iceberg's pluggable DataFile reader and writer API, which opens the door for querying Lance-formatted data through the Iceberg interface. This kind of interoperability reinforces the vision that table formats can work together to serve diverse workloads. We will also continue to evolve the Lance table format for ML and AI use cases, ensuring it meets the unique demands of these rapidly advancing domains.

What are your thoughts on the future of open-source table formats? Let's [discuss](https://discord.gg/RRgWXFTSCs)!
```

---

## ðŸ“„ æ–‡ä»¶: tokens-per-second-is-not-all-you-need.md

---

```md
---
title: "Tokens per Second Is NOT All You Need"
date: 2024-05-01
author: LanceDB
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/tokens-per-second-is-not-all-you-need/preview-image.png
meta_image: /assets/blog/tokens-per-second-is-not-all-you-need/preview-image.png
description: "Explore about tokens per second is not all you need. Get practical steps, examples, and best practices you can use now."
---

> When a measure becomes a target, it ceases to be a good measure.          
> -- Goodhartâ€™s Law

We're excited to have a guest post on our blog today on model inference performance optimization. In this post, **Mingran Wang** and **Tan Li** from [SambaNova](https://sambanova.ai/) talks about the right metrics to use when it comes to LLM inference performance.

In the world of computer systems, innovation is often driven by several key metrics, and building a LLM system is no exception. However, no single metric can fully capture all system requirements. Thus, selecting metrics that balance each other is crucial for the projectâ€™s success. In this blog post, we will examine **Tokens per Second**, a common metric for evaluating LLM systems, and explore its shortcomings and potential for misleading results in real-world applications.

## Throughput vs. Latency

Typically, throughput measures the number of instances processed within a time window, while latency measures the time it takes to process each instance. A common misconception about computer system performance is confusing throughput with latency. Indeed, they complement each other in a powerful balancing loop, i.e. shorter processing times for each instance lead to a higher number of instances processed within a given period, and vice versa.

However, in some scenarios, particularly when optimizing for a smooth end user experience, focusing solely on throughput can be misleading. For example, in chatbot applications, users generally prefer receiving a quick initial response followed by a gradual delivery of the rest, rather than receiving all information at once. Therefore, when optimizing UX for Chatbot systems, **Time to First Token** should be prioritized a bit over **Tokens per Second**, as it better indicates *experienced *system latency, whereas the latter measures throughput.

## Long Input with Short Output

AI agentic workflows, predominantly featuring agent-to-agent interactions as opposed to agent-human interactions, might initially seem to prioritize **Tokens per Second** as the key performance metric. However, upon closer inspection, the complexity of these workflows becomes apparent. In such systems, each model call typically involves a long input for a relatively short output, as the model continually updates the context to predict the next state. Consequently, **Time to First Token** remains crucial for effective system performance, not only in these workflows but also in applications like RAG that process multiple or lengthy documents. Additionally, the current model trend in supporting super long context windows further strengthened this situation and needless to mention those multi-turn conversational chatbot systems.

## A More Balanced Approach

Successful system development is more nuanced than maximizing a single metric. At SambaNova, we strive for a more balanced design that integrates diverse measurements to capture the full spectrum of system capabilities. As a result, SambaNovaâ€™s RDU (Reconfigurable Dataflow Unit) system has an unique 3-tier memory hierarchy as the following:

- **Large SRAM**: enables an reconfigurable dataflow micro-architecture that achieves *430 Tokens per Second*throughput for llama3-8b on a 8-chips (sockets) system via aggressive kernel fusion
- **HBM**: enables efficient graph caching & weight loading operations that lead to *0.2 seconds for Time to First Token* on input size up to 4096 tokens. While other SOTA systems could take over *1.6 seconds* on similar workloads
- **DRAM**: enables large system capacity to accommodate larger model size, longer sentence length and routing techniques such as [Composition of Experts (CoE)](https://sambanova.ai/blog/samba-coe-the-power-of-routing-ml-models-at-scale), significantly enhancing end-to-end statistical performance

![](https://lh7-us.googleusercontent.com/E-NJ8XnhXLpl2QVKL4mVXOcK9KC7r_NVYxelBeNB1fMP3Zp1EVvPES6DdJLNZ03dco8W7Uz7RPCJzBDahZpNgTDCankTrbKWLUSWClfl6p1YwaA-z64m7BBwJHqn0JH-aZ-vAlNowUuxaoKLtEjeLCU)
Table - Input process time and total inference time on llama3-8B (green is better)
![Chart](https://lh7-us.googleusercontent.com/0kSupuOB1GpYoQN2aNuKUpYZ-HtonxZ9bxoPvqMDK8NMYGNtO5iTNPtimi8kXyIFbt0Yj8OjNwQUm_AGM1WMLhPVuHa9YOh_DA0jPfAGsGPCfwr2lWjTzCPCK0C0fOM4ko0qm8NNmnKMuQiRvg1bQuA)
Figure - Total Inference time vs. input tokens (llama 3 8B)

Leveraging these hardware advantages and innovative software techniques, such as tensor parallelism and continuous batching, we are thrilled about the limitless possibilities ahead for LLM systems. More information is available on the [SambaNova blog](https://sambanova.ai/resources/tag/blog).

## Conclusion

In conclusion, the exploration of Tokens per Second reveals its limitations as a sole metric for evaluating LLM systems. This evidence strongly supports the argument for a more comprehensive metric selection in system design. A balanced approach that includes both throughput metrics like Tokens per Second and latency metrics such as Time to First Token ensures a more accurate assessment and optimization of system performance, particularly in user-centric applications.
```

---

## ðŸ“„ æ–‡ä»¶: track-ai-trends-crewai-agents-rag.md

---

```md
---
title: "Track AI Trends: CrewAI Agents & RAG"
date: 2024-03-25
author: LanceDB
categories: ["Engineering"]
draft: false
featured: false
image: /assets/blog/track-ai-trends-crewai-agents-rag/preview-image.png
meta_image: /assets/blog/track-ai-trends-crewai-agents-rag/preview-image.png
description: "This article will teach us how to make an AI Trends Searcher using CrewAI Agents and their Tasks.  But before diving into that, let's first understand what CrewAI is and how we can use it for these applications."
---

This article will teach us how to make an AI Trends Searcher using CrewAI Agents and their Tasks. But before diving into that, let's first understand what CrewAI is and how we can use it for these applications.

#### **What is CrewAI?**

CrewAI is an open-source framework that helps different AI agents work together to do tricky stuff. You can give each Agent its tasks and goals, manage what they do, and help them work together by sharing tasks. These are some unique features of CrewAI:

1. Role-based Agents: Define agents with specific roles, goals, and backgrounds to provide more context for answer generation.
2. Task Management: Use tools to dynamically define tasks and assign them to agents.
3. Inter-agent Delegation: Agents can share tasks to collaborate better.

To better grasp CrewAI, let's create collaborative AI agents to get AI trends search engines using CrewAI.

### **Building Crew to get AI trends**

First, an AI news search agent finds the latest news using the News API and saves it into the LanceDB vector database. Then, a writer agent uses semantic search to gather all the unique news highlights from the vector database and create bullet points summarizing the current trends in AI.

Now we are clear with How we are going to create this crew, let's delve into the coding it

You'll require a News API key and OPENAI_API key for this project. The News API key is freely available[here](https://newsapi.org/); get your API key, and let's get started

#### **Install dependencies**

```bash
pip install crewai langchain-community langchain-openai requests duckduckgo-search lancedb -q
```

#### **Import Required Modules**

```python
from crewai import Agent, Task, Crew, Process
from langchain_openai import ChatOpenAI
from langchain_core.retrievers import BaseRetriever
from langchain_openai import OpenAIEmbeddings
from langchain.tools import tool
from langchain_community.document_loaders import WebBaseLoader
import requests, os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import LanceDB
from langchain_community.tools import DuckDuckGoSearchRun

# Now let's keep our OpenAI API key and News API key as environment variable
os.environ["NEWSAPI_KEY"] = "*********"
os.environ["OPENAI_API_KEY"] = "sk-*********"
```

For this example, we'll use the OpenAI embedding function to embed scraped trending news and insert it in LanceDB. Additionally, we'll employ OpenAI's Language Model (LLM) for creating agents and executing their assigned tasks.

```python
embedding_function = OpenAIEmbeddings()
llm = ChatOpenAI(model="gpt-4-turbo-preview")
```

#### **Setting up LanceDB**

Now, let's set up the LanceDB vector database and table where we'll keep all our news and their respective embeddings.

```python
import lancedb

# creating lancedb table with dummy data
def lanceDBConnection(dataset):
    db = lancedb.connect("/tmp/lancedb")
    table = db.create_table("tb", data=dataset, mode="overwrite")
    return table

embedding = OpenAIEmbeddings()
emb = embedding.embed_query("hello_world")
dataset = [{"vector": emb, "text": "dummy_text"}]

# LanceDB as vector store
table = lanceDBConnection(dataset)
```

Now we have set LanceDB, let's extract AI news and insert it with its embeddings into the vector database.

```python
# Save the news articles in a database
class SearchNewsDB:
    @tool("News DB Tool")
    def news(query: str):
        """Fetch news articles and process their contents."""
        API_KEY = os.getenv('NEWSAPI_KEY')  # Fetch API key from environment variable
        base_url = f"https://newsapi.org/v2/top-headlines?sources=techcrunch"

        params = {
            'sortBy': 'publishedAt',
            'apiKey': API_KEY,
            'language': 'en',
            'pageSize': 15,
        }

        response = requests.get(base_url, params=params)
        if response.status_code != 200:
            return "Failed to retrieve news."

        articles = response.json().get('articles', [])
        all_splits = []
        for article in articles:
            loader = WebBaseLoader(article['url'])
            docs = loader.load()

            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            splits = text_splitter.split_documents(docs)
            all_splits.extend(splits)

        if all_splits:
            vectorstore = LanceDB.from_documents(all_splits, embedding=embedding_function, connection=table)
            retriever = vectorstore.similarity_search(query)
            return retriever
        else:
            return "No content available for processing."
```

#### **Building RAG**

Now we have all the trending news in our database, let's build a RAG around it.

```python
# Get the news articles from the database
class GetNews:
    @tool("Get News Tool")
    def news(query: str) -> str:
        """Search LanceDB for relevant news information based on a query."""
        vectorstore = LanceDB(embedding=embedding_function, connection=table)
        retriever = vectorstore.similarity_search(query)
        return retriever
```

Setting up a search tool for validating news extracted from API on the web, we'll use the DuckDuckGo search engine.

```python
search_tool = DuckDuckGoSearchRun()
```

#### **Setting Up Agents with their roles and goals**

```python
# Defining Search and Writer agents with roles and goals
news_search_agent = Agent(
    role='AI News Searcher',
    goal='Generate key points for each news article from the latest news',
    backstory="""You work at a leading tech think tank.
  Your expertise lies in identifying emerging trends in field of AI.
  You have a knack for dissecting complex data and presenting
  actionable insights.""",
    tools=[SearchNewsDB().news],
    allow_delegation=True,
    verbose=True,
    llm=llm
)

writer_agent = Agent(
    role='Writer',
    goal='Identify all the topics received. Use the Get News Tool to verify the each topic to search. Use the Search tool for detailed exploration of each topic. Summarise the retrieved information in depth for every topic.',
    backstory="""You are a renowned Content Strategist, known for
  your insightful and engaging articles.
  You transform complex concepts into compelling narratives.""",
    tools=[GetNews().news, search_tool],
    allow_delegation=True,
    verbose=True,
    llm=llm
)
```

With the roles and goals for our agents established, let's assign tasks to them so we can officially add them to the crew.

```python
# (Re)defining Search and Writer agents (if running cells independently)
news_search_agent = Agent(
    role='AI News Searcher',
    goal='Generate key points for each news article from the latest news',
    backstory="""You work at a leading tech think tank.
  Your expertise lies in identifying emerging trends in field of AI.
  You have a knack for dissecting complex data and presenting
  actionable insights.""",
    tools=[SearchNewsDB().news],
    allow_delegation=True,
    verbose=True,
    llm=llm
)

writer_agent = Agent(
    role='Writer',
    goal='Identify all the topics received. Use the Get News Tool to verify the each topic to search. Use the Search tool for detailed exploration of each topic. Summarise the retrieved information in depth for every topic.',
    backstory="""You are a renowned Content Strategist, known for
  your insightful and engaging articles.
  You transform complex concepts into compelling narratives.""",
    tools=[GetNews().news, search_tool],
    allow_delegation=True,
    verbose=True,
    llm=llm
)
```

Now let's define tasks associated with these agents.

```python
# Creating search and writer tasks for agents
news_search_task = Task(
    description="""Conduct a comprehensive analysis of the latest advancements in AI in 2024.
  Identify key trends, breakthrough technologies, and potential industry impacts.
  Your final answer MUST be a full analysis report""",
    expected_output='Create key points list for each news',
    agent=news_search_agent,
    tools=[SearchNewsDB().news]
)

writer_task = Task(
    description="""Using the insights provided, summaries each post of them
  highlights the most significant AI advancements.
  Your post should be informative yet accessible, catering to a tech-savvy audience.
  Make it sound cool, avoid complex words so it doesn't sound like AI.
  Your final answer MUST not be the more than 50 words.""",
    expected_output='Write a short summary under 50 words for each news Headline seperately',
    agent=writer_agent,
    context=[news_search_task],
    tools=[GetNews().news, search_tool]
)
```

Now, we are ready with all our bits and pieces of AI trend searching, starting from trending News in LanceDB, RAG, and CrewAI agents' tasks. Let's connect all of it and build a crew that will perform all these tasks with a single kickoff.

#### **Creating Crew**

```python
# Instantiate Crew with Agents and their tasks
news_crew = Crew(
    agents=[news_search_agent, writer_agent],
    tasks=[news_search_task, writer_task],
    process=Process.sequential,
    manager_llm=llm
)
```

### **Kickoff the crew - Let the magic happen**

```python
# Execute the crew to see RAG in action
result = news_crew.kickoff()

print(result)
```

**Colab Walkthrough**

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JG2RR3AH9GfRDkFPiOglYEuKkM0d3kkn#scrollTo=VU3LdZmIWqQH)
#### **Conclusion**

Congratulations! We have created AI agents using CrewAI to search the latest trending AI news search. The CrewAI framework offers flexibility and can be customized to automate various tasks beyond content creation, including data analysis, customer support, and beyond. Try out different tasks, agents, and tools to unlock the complete potential of CrewAI in streamlining your workflows.
```

---

## ðŸ“„ æ–‡ä»¶: training-a-variational-autoencoder-from-scratch-with-the-lance-file-format.md

---

```md
---
title: "Training a Variational AutoEncoder from Scratch with Lance File Format"
date: 2024-09-02
author: LanceDB
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/training-a-variational-autoencoder-from-scratch-with-the-lance-file-format/preview-image.png
meta_image: /assets/blog/training-a-variational-autoencoder-from-scratch-with-the-lance-file-format/preview-image.png
description: "Train a Variational Autoencoder endâ€‘toâ€‘end using Lance for fast, scalable data handling. Youâ€™ll set up the dataset, build the VAE in PyTorch, and run training, sampling, and reconstructions."
---

Autoencoders encode an input into a latent space and decode the latent representation into an output. The output is often referred to as **the reconstruction**, as the primary goal is to reconstruct the input from the latent code.

Autoencoders are a very popular for data *compression* due to their ability to efficiently capture and represent the essential features of the input data in a compact form.

One problem that autoencoders face is the lack of variation. This means that while they can effectively compress and reconstruct data, they struggle to generate new, diverse samples that resemble the original data. The latent space learned by traditional autoencoders can be sparse and disjointed, leading to poor generalization and limited ability to produce meaningful variations of the input data.

## What do we mean by variation?

In the context of generative models, variation refers to the ability to produce a diverse range of outputs from a given latent space. For instance, in image generation, variation means creating different but plausible images that share common characteristics with the training data.

Traditional autoencoders, while good at reconstruction, do not inherently encourage this diversity. They tend to map inputs to specific points in the latent space, which can limit the model's ability to explore and generate new samples.

**Introducing Variational Autoencoders (VAEs) as a Solution**: Variational Autoencoders (VAEs) address this limitation by introducing a probabilistic approach to the encoding process. Instead of mapping inputs to fixed points in the latent space, VAEs map inputs to a distribution over the latent space. This allows for a smoother and more continuous representation of the data, fostering better generalization and enabling the generation of diverse outputs.
![Mapping data to a distribution](/assets/blog/training-a-variational-autoencoder-from-scratch-with-the-lance-file-format/image-1.png)
Mapping data to a distribution (Source: Author)
Here's a high-level overview of how VAEs work:

- **Encoder**: Maps the input data to a mean and variance that define a Gaussian distribution in the latent space.
- **Sampling**: Samples a point from this Gaussian distribution.
- **Decoder**: Uses the sampled point to reconstruct the input data, ensuring that the output is a plausible variation of the input.

In the next sections of this blog post, we talk about the technical details of setting up and training a VAE. We will also explore how the Lance data format can be leveraged to optimize data handling, ensuring efficient and scalable workflows.

By integrating VAEs with Lance, you can unlock new possibilities in data compression, generation, and management, pushing the boundaries of what is achievable with machine learning.

## Setup and Imports

We begin with setting up our imports for the experiment.

```python
# pip install -U -q lance

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils import data
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder

import io

from PIL import Image
from tqdm import tqdm

from matplotlib import pyplot as plt

import requests
import tarfile
import os
import time

import pyarrow as pa
import lance
```

## Configuration

We will now configure our experiment. Please feel free to change the configuration and run the experiment on your own to see what changes! This is a great exercise to indulge in.

```python
vae_config = {
    "BATCH_SIZE": 4096,
    "IN_RESOLUTION": 32,
    "IN_CHANNELS": 3,
    "NUM_EPOCHS": 100,
    "LEARNING_RATE": 1e-4,
    "HIDDEN_DIMS": [64, 128, 256, 512],
    "LATENT_DIM_SIZE": 64,
}
```

## Dataset

This section of the blog post is taken from [Lance Deep Learning Recipes](https://github.com/lancedb/lance-deeplearning-recipes/blob/main/examples/image-classification/image-classification.ipynb).

Here we will download the CINIC-10 dataset and convert it into a Lance dataset. After we have that, we will use our `PyTorch` dataloader to load the dataset into our training pipeline.

```python
# Define the URL for the dataset file
data_url = "https://datashare.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz"

# Create the data directory if it doesn't exist
data_dir = "cinic-10-data"
if not os.path.exists(data_dir):
    os.makedirs(data_dir)

# Download the dataset file
print("Downloading CINIC-10 dataset...")
data_file = os.path.join(data_dir, "CINIC-10.tar.gz")

response = requests.get(data_url, stream=True)
total_size = int(response.headers.get('content-length', 0))
block_size = 1024

start_time = time.time()
progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)

with open(data_file, 'wb') as f:
    for chunk in response.iter_content(chunk_size=block_size):
        if chunk:
            f.write(chunk)
            progress_bar.update(len(chunk))

end_time = time.time()
download_time = end_time - start_time
progress_bar.close()

print(f"\nDownload time: {download_time:.2f} seconds")

# Extract the dataset files
print("Extracting dataset files...")
with tarfile.open(data_file, 'r:gz') as tar:
    tar.extractall(path=data_dir)

print("Dataset downloaded and extracted successfully!")

def process_images(images_folder, split, schema):

    # Iterate over the categories within each data type
    label_folder = os.path.join(images_folder, split)
    for label in os.listdir(label_folder):
        label_folder = os.path.join(images_folder, split, label)

        # Iterate over the images within each label
        for filename in tqdm(os.listdir(label_folder), desc=f"Processing {split} - {label}"):
            # Construct the full path to the image
            image_path = os.path.join(label_folder, filename)

            # Read and convert the image to a binary format
            with open(image_path, 'rb') as f:
                binary_data = f.read()

            image_array = pa.array([binary_data], type=pa.binary())
            filename_array = pa.array([filename], type=pa.string())
            label_array = pa.array([label], type=pa.string())
            split_array = pa.array([split], type=pa.string())

            # Yield RecordBatch for each image
            yield pa.RecordBatch.from_arrays(
                [image_array, filename_array, label_array, split_array],
                schema=schema
            )

# Function to write PyArrow Table to Lance dataset
def write_to_lance(images_folder, dataset_name, schema):
    for split in ['test', 'train', 'valid']:
        lance_file_path = os.path.join(images_folder, f"{dataset_name}_{split}.lance")

        reader = pa.RecordBatchReader.from_batches(schema, process_images(images_folder, split, schema))
        lance.write_dataset(
            reader,
            lance_file_path,
            schema,
        )

dataset_path = "cinic-10-data"
dataset_name = os.path.basename(dataset_path)

start = time.time()
schema = pa.schema([
    pa.field("image", pa.binary()),
    pa.field("filename", pa.string()),
    pa.field("label", pa.string()),
    pa.field("split", pa.string())
])

start = time.time()
write_to_lance(dataset_path, dataset_name, schema)
end = time.time()
print(f"Time(sec): {end - start:.2f}")
```

Build a small utility to visualize our data and the generated images from the model.

```python
def draw_image_grid(samples):
    images = samples[:4]
    images = (images * 0.5) + 0.5  # Unnormalize the images to [0, 1] range
    grid_img = torchvision.utils.make_grid(images, nrow=2)
    plt.figure(figsize=(5, 5))
    plt.imshow(grid_img.permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)
    plt.axis('off')
    plt.show()
```

Let us now build the Data Generator

```python
class CustomImageDataset(data.Dataset):
    def __init__(self, classes, lance_dataset, transform=None):
        self.classes = classes
        self.ds = lance.dataset(lance_dataset)
        self.transform = transform

    def __len__(self):
        return self.ds.count_rows()

    def __getitem__(self, idx):
        raw_data = self.ds.take([idx], columns=['image', 'label']).to_pydict()
        img_data, label = raw_data['image'][0], raw_data['label'][0]

        img = Image.open(io.BytesIO(img_data))

        # Convert grayscale images to RGB
        if img.mode != 'RGB':
            img = img.convert('RGB')

        if self.transform:
            img = self.transform(img)

        label = self.classes.index(label)
        return img, label
```

To instantiate a data loader we use the following code.

```python
train_transform = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # -1 to 1
    ]
)
classes = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

lance_train_dataset = CustomImageDataset(
    classes,
    "cinic-10-data/cinic-10-data_train.lance/",
    transform=train_transform
)

lance_train_loader = torch.utils.data.DataLoader(
    lance_train_dataset,
    batch_size=vae_config["BATCH_SIZE"],
    shuffle=True
)
```

Time to visualize our dataset.

```python
images, _ = next(iter(lance_train_loader))
draw_image_grid(images)
```

![Sample training images](/assets/blog/training-a-variational-autoencoder-from-scratch-with-the-lance-file-format/image-7.png)
## Building the model

A Variational Autoencoder (VAE) is very similar to a standard autoencoder in its structure and basic functionality. Both models consist of an encoder and a decoder. The encoder compresses the input into a latent representation, and the decoder reconstructs the input from this latent code. However, VAEs introduce several critical modifications that address some of the limitations of standard autoencoders, particularly regarding the generation of diverse and meaningful outputs.

### Key Components of the Variational Autoencoder

1. **Encoder**: The encoder in a VAE consists of several convolutional layers (in the case of images) that transform the input data into a lower-dimensional latent space. The encoder outputs the mean and variance parameters of the latent distribution, rather than a fixed latent code.
2. **Reparameterization**: The reparameterization trick allows the model to sample from the latent distribution in a differentiable manner. This step involves sampling a latent code from the Gaussian distribution defined by the mean and variance parameters output by the encoder.
3. **Decoder**: The decoder takes the sampled latent code and reconstructs the input data. It consists of several deconvolutional layers that gradually upsample the latent code back to the original input dimensions.
4. **Loss Function**: The VAE loss function comprises the reconstruction loss and the KL divergence loss. The reconstruction loss ensures that the output is a faithful reconstruction of the input, while the KL divergence loss regularizes the latent space.

Here's a high-level summary of how a VAE operates:

- **Encoding**: The input is passed through the encoder, which outputs the mean and variance of the latent distribution.
- **Sampling**: A latent code is sampled from this distribution using the reparameterization trick.
- **Decoding**: The sampled latent code is passed through the decoder to reconstruct the input.
- **Loss Calculation**: The loss function, combining reconstruction loss and KL divergence loss, is used to optimize the model.

By incorporating these probabilistic elements and regularization techniques, VAEs overcome some of the limitations of traditional autoencoders, particularly in terms of generating diverse and meaningful outputs. In the next sections, we will explore the technical implementation of a VAE, leveraging the Lance data format to optimize data handling and improve the efficiency of our workflows.

```python
# VAE Model
class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(vae_config["IN_CHANNELS"], 32, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Flatten()
        )

        self.fc_mu = nn.Linear(128*4*4, vae_config["LATENT_DIM_SIZE"])
        self.fc_logvar = nn.Linear(128*4*4, vae_config["LATENT_DIM_SIZE"])

        # Decoder
        self.decoder_input = nn.Linear(vae_config["LATENT_DIM_SIZE"], 128*4*4)

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, vae_config["IN_CHANNELS"], kernel_size=4, stride=2, padding=1),
            nn.Tanh()  # Output values in range [-1, 1]
        )

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        x = self.encoder(x)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        z = self.reparameterize(mu, logvar)
        z = self.decoder_input(z)
        z = z.view(-1, 128, 4, 4)
        return self.decoder(z), mu, logvar

    def sample(self, num_samples):
        z = torch.randn(num_samples, vae_config["LATENT_DIM_SIZE"]).to(device)
        return self.decoder(self.decoder_input(z).view(-1, 128, 4, 4))
```

## Time to TRAIN!

```python
device = "cuda"

# Loss Function
def vae_loss(recon_x, x, mu, logvar):
    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + kl_loss

# Initialize model, optimizer
model = VAE().to(device)
optimizer = optim.Adam(model.parameters(), lr=vae_config["LEARNING_RATE"])

for epoch in range(vae_config["NUM_EPOCHS"]):
    model.train()
    train_loss = 0

    # Use tqdm for the progress bar
    pbar = tqdm(
        enumerate(lance_train_loader),
        total=len(lance_train_loader),
        desc=f'Epoch {epoch+1}/{vae_config["NUM_EPOCHS"]}'
    )

    for batch_idx, (data, _) in pbar:
        data = data.to(device)

        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = vae_loss(recon_batch, data, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

        # Update tqdm description with current loss
        pbar.set_postfix({'Loss': loss.item()})

    avg_loss = train_loss / len(lance_train_loader.dataset)
    print(f'Epoch [{epoch + 1}/{vae_config["NUM_EPOCHS"]}] Average Loss: {avg_loss:.4f}')

    # show and display a sample of the reconstructed images
    if epoch % 10 == 0:
        with torch.no_grad():
            sampled_images = model.sample(num_samples=4)
            draw_image_grid(recon_batch.cpu())
```

![Training samples 1](/assets/blog/training-a-variational-autoencoder-from-scratch-with-the-lance-file-format/image-8.png)
![Training samples 2](/assets/blog/training-a-variational-autoencoder-from-scratch-with-the-lance-file-format/image-9.png)
The images generated from the training pipeline are not bad! You have to note that the generations follow the centred theme as that of the dataset. We also see the green grass and blue skies in some images.

Train it longer to see how the model converges!

## Let's see the reconstruction

```python
images, _ = next(iter(lance_train_loader))
draw_image_grid(images)
```

![Original batch](/assets/blog/training-a-variational-autoencoder-from-scratch-with-the-lance-file-format/image-10.png)

```python
with torch.no_grad():
    recon_images, _, _ = model(images.to(device))
draw_image_grid(recon_images.cpu())
```

![Reconstructed batch](/assets/blog/training-a-variational-autoencoder-from-scratch-with-the-lance-file-format/image-11.png)
## Conclusion

In this tutorial, we understood what Variational Autoencoders are, and how to train them. We have also used the powerful `lance` data format for our training pipeline, which makes the training easier and more efficient.
```

---

## ðŸ“„ æ–‡ä»¶: werides-data-platform-transformation-how-lancedb-fuels-model-development-velocity.md

---

```md
---
title: "WeRide's Data Platform Transformation: How LanceDB Fuels Model Development Velocity"
date: 2024-12-10
draft: false
featured: false
categories: ["Case Study", "Autonomous Vehicles"]
image: /assets/blog/werides-data-platform-transformation-how-lancedb-fuels-model-development-velocity/werides-data-platform-transformation-how-lancedb-fuels-model-development-velocity.png
description: "Discover how WeRide, a leading autonomous driving company, leveraged LanceDB to revolutionize their data platform, achieving 90x improvement in ML developer productivity and reducing data mining time from 1 week to 1 hour."
author: Qian Zhu
author_avatar: "/assets/authors/qian-zhu.jpg"
author_bio: "Software Engineer at LanceDB specializing in cloud infrastructure, autonomous vehicle data platforms, and vector database optimization."
author_twitter: "qianzhu56"
author_github: "qianzhu56"
author_linkedin: "qianzhu56"
---

{{< admonition info "Case Study Collaboration" >}}
This is a case study contributed by the following authors:
- [**Fei Chen**](https://www.linkedin.com/in/fei-chen-90364325/), Director of Infrastructure @ WeRide
- [**Qian Zhu**](https://www.linkedin.com/in/qianzhu56/), Software Engineer @ LanceDB
{{< /admonition >}}

![WeRide Fleet](/assets/blog/werides-data-platform-transformation-how-lancedb-fuels-model-development-velocity/weride-quote.png)

## Introduction

Established in 2017, [WeRide](https://www.weride.ai/) (Nasdaq: WRD) is a leading global commercial-stage company that develops autonomous driving technologies from level 2 to level 4. WeRide is the only tech company in the world that holds the driverless permits in China, the UAE, Singapore and the U.S., conducting autonomous driving R&D, tests and operations in over 30 cities of 7 countries around the world. WeRide has operated a self-driving fleet for more than 1,700 days.

As a pioneer in autonomous driving technologies and applications, WeRide excels at fast iteration on model development, particularly on the data loop. In 2023, as the massive amount of complex, multi-modal data continued to grow, *Fei Chen*, director of the data infrastructure team at WeRide, spearheaded the solution of a data platform that can serve lighting fast search (in ms) with high query-per-second (QPS) for data that satisfying specific criteria, e.g. rare data that can be used to train a long tail case, and is cost-effective, and easy to maintain.

## The Challenge

### Revolutionize Data Platform to Support Data Mining

The most challenging problem with autonomous driving is addressing the endless long-tail scenarios. Such scenarios require mining data to identify patterns in sensor data, such as camera images and radar, lidar readings. A data platform to support data mining would meet:

{{< admonition warning "Critical Requirements" >}}
- **Low Latency:** Achieve millisecond P99 latency for top-1000 results retrieval using text-to-image or image-based queries against datasets containing billions of data
- **Hybrid Querying:** Support combined vector and metadata filtering for precise and efficient data retrieval
- **Low I/O:** Minimize bandwidth consumption per second to align with WeRide's high-speed storage system used for feeding model training data
- **Cost Performance:** Deliver a cost-effective solution that scales to accommodate growing business needs while minimizing maintenance overhead
{{< /admonition >}}

As WeRide's data infrastructure team explored solutions to support data mining for long-tail scenarios in autonomous driving, the team quickly realized that traditional SQL-based search was insufficient. Instead, a vector database was necessary to provide rapid search capabilities for the massive amounts of multi-modal data generated after converting text and images into embeddings.

## The Solution

### LanceDB as the Vector Database in the Revolutionized Data Platform

When Fei and his team began searching for a vector database, they had clear requirements: it needed to be cost-effective, support multi-modal data, and provide blazing fast vector search with metadata filtering. After evaluating the available options, WeRide chose to partner with LanceDB to power the search engine within their data platform.

Choosing LanceDB wasn't just about solving WeRide's immediate data mining challenges; it was a strategic decision with long-term implications. With the anticipated growth of WeRide's autonomous driving fleet, Fei and his team foresee LanceDB playing a pivotal role in diverse applications, including driving behavior analysis and user-centric ride customization. They strongly believe that LanceDB will remain an indispensable tool for a wide range of applications.

### Implementation Architecture

To search for data that matches a rare scenario, for example, people lying on the ground, the pipeline would use text-to-image or image-to-image search. The images and videos collected from autonomous vehicles are converted as vectors. All embeddings along with metadata is ingested into LanceDB, backed by the in-house storage cluster built by the data infrastructure team. LanceDB then indexes the data for fast search. As new data gets added, the re-index is triggered on a weekly and monthly basis. During a search, the text or image will first be embedded then do a similar search with all embeddings. Usually, such search will be combined with applying filters on certain metadata fields. Returned search results, e.g. top 1000, will then go through the feature engineering process to be fed into models for training.

*WeRide's data platform architecture showing how LanceDB powers multi-modal search across autonomous vehicle sensor data.*

![WeRide Architecture](/assets/blog/werides-data-platform-transformation-how-lancedb-fuels-model-development-velocity/architecture.png)

> "Lance has helped us streamline our data analysis process, allowing us to quickly and efficiently identify valuable data to enhance our autonomous driving technology"
> 
> â€” Fei Chen, Director of Data Infrastructure @ WeRide

## Results & Impact

By integrating LanceDB as the vector database in their data platform, WeRide is able to streamline the data analysis process, quickly and efficiently identify valuable data to enhance the autonomous driving technology:

### Performance Metrics

- **Productivity improvement:** WeRide has seen **90x** improvement to ML developer productivity for data exploration and debugging with LanceDB serving fast and high quality search
- **Performance improvement:** WeRide has also seen **3x** reduction in ML training time by improving data I/O
- **Time-to-value:** The time on data mining for safety critical edge cases was **reduced from 1 week to 1 hour**

### Strategic Benefits

Beyond its technical advantages, LanceDB's ease of maintenance and seamless scalability have been crucial as WeRide's data volume expanded exponentially.

> "The strategic partnership with LanceDB has provided a robust foundation for future innovation, empowering WeRide's continued business growth."
> 
> â€” Fei Chen, Director of Data Infrastructure @ WeRide
```

---

## ðŸ“„ æ–‡ä»¶: zero-shot-image-classification-with-vector-search.md

---

```md
---
title: "Zero Shot Image Classification with Vector Search"
date: 2024-07-12
author: Vipul Maheshwari
author_avatar: /assets/authors/vipul-maheshwari.jpg
categories: ["Community"]
draft: false
featured: false
image: /assets/blog/zero-shot-image-classification-with-vector-search/preview-image.png
meta_image: /assets/blog/zero-shot-image-classification-with-vector-search/preview-image.png
description: "Get about zero shot image classification with vector search. Get practical steps, examples, and best practices you can use now."
---

## Introduction

This post covers the concept of zero-shot image classification with an example. It is the process where a model can classify images without being trained on a particular use case.

## Fundamentals

To make this work, we need a Multimodal embedding model and a vector database. Let's start with something called CLIP, which stands for **Contrastive Language-Image Pre-Training.**

CLIP acts like a versatile interpreter for various types of data, including images, text, and more. It achieves this through two key components: a Text Encoder and an Image Encoder, both trained to understand these inputs within the same vector space. During training on 400 million internet-collected image-text pairs, CLIP learned to place similar pairs close together in this space while separating dissimilar pairs.

Key features of CLIP include:

- It operates without the need for datasets labeled with specific class categories, relying instead on image-text pairs where the text describes the image.
- Rather than extracting features from images using traditional CNNs, CLIP leverages more detailed text descriptions, enhancing its feature extraction capabilities.
- In performance tests, CLIP demonstrated superior zero-shot classification compared to a fine-tuned ResNet-101 model on datasets derived from ImageNet, showcasing its robust understanding of diverse datasets beyond its specific training data.


![CLIP architecture overview](/assets/blog/zero-shot-image-classification-with-vector-search/Untitled.png)

## How CLIP works

In traditional CNN-based classification, images are labeled with specific classes, and the model learns to recognize features that distinguish between these classes through supervised training. In contrast, zero-shot classification utilizes a Text Encoder and an Image Encoder that produce 512-dimensional vectors for both images and text. These vectors are mapped to the same semantic space, meaning an image vector for "cat" is similar to the vector of a text description like "a photo of a cat".

By leveraging this shared vector space, zero-shot classification allows the model to classify images into categories it hasn't seen during training. Instead of relying solely on predefined class labels, the model compares the vector representation of a new image to vectors representing textual descriptions of various categories, enabling it to generalize across unseen classes.
![Prompt engineering for labels](/assets/blog/zero-shot-image-classification-with-vector-search/recommendation_final.png)

To optimize our zero-shot classification, it's beneficial to transform class labels from basic words like "cat," "dog," and "horse" into descriptive phrases such as "a photo of a cat," "a photo of a dog," or "a photo of a horse." This transformation mirrors the text-image pairs used during the model's pretraining, where prompts like "a photo of a {label}" were paired with each label to create training examples [[1]](https://openai.com/index/clip/). You can play around with the prompts and see how CLIP performs.

## Vector Index

To power our zero-shot classification system, we'll use a vector database to store labels and their embeddings.

## Implementation

You can also follow along with this Colab:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/zero-shot-image-classification/main.ipynb)
Let's take a look at an example. For this demonstration, I'll use the [**uoft-cs/cifar100**](https://huggingface.co/datasets/uoft-cs/cifar100) dataset from Hugging Face Datasets.

```python
from datasets import load_dataset

imagedata = load_dataset(
    'uoft-cs/cifar100',
    split="test"
)

imagedata
```

Letâ€™s see original label names

```python
# labels names
labels = imagedata.info.features['fine_label'].names
print(len(labels))
labels
```

```text
100

['apple',
 'aquarium_fish',
 'baby',
 'bear',
 'beaver',
 'bed',
 'bee',
 'beetle',
 'bicycle',
 'bottle',
 'bowl',
 'boy',
 'bridge',
 'bus',
 'butterfly',
 'camel',
 'can',
 'castle',
 'caterpillar',
 'cattle',
 'chair',
 'chimpanzee',
 'clock',
 'cloud',
 'cockroach',
 ...
 'whale',
 'willow_tree',
 'wolf',
 'woman',
 'worm']
```

Looks good! We have 100 classes to classify images from, which would require a lot of computing power if you go for traditional CNN. However, let's proceed with our zero-shot image classification approach.

Letâ€™s generate the relevant textual descriptions for our labels. (This step is optional recommendation covered in previous section.)

```python
# generate sentences
clip_labels = [f"a photo of a {label}" for label in labels]
clip_labels
```

Now letâ€™s initialize our CLIP embedding model, I will use the CLIP implementation from Hugging face.

```python
import torch
from transformers import CLIPProcessor, CLIPModel

model_id = "openai/clip-vit-large-patch14"
processor = CLIPProcessor.from_pretrained(model_id)
model = CLIPModel.from_pretrained(model_id)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
```

We'll convert our text descriptions into integer representations called input IDs, where each number stands for a word or subword, more formally `tokens`. We'll also need an attention mask to help the transformer focus on relevant parts of the input.

```python
label_tokens = processor(
    text=clip_labels,
    padding=True,
    return_tensors='pt'
).to(device)

# Print the label tokens with the corresponding text
for i in range(5):
    token_ids = label_tokens['input_ids'][i]
    print(f"Token ID : {token_ids}, Text : {processor.decode(token_ids, skip_special_tokens=False)}")
```

```text
Token ID : tensor([49406,   320,  1125,   539,   320,  3055, 49407, 49407, 49407]), Text : <|startoftext|>a photo of a apple <|endoftext|><|endoftext|><|endoftext|>
Token ID : tensor([49406,   320,  1125,   539,   320, 16814,   318,  2759, 49407]), Text : <|startoftext|>a photo of a aquarium _ fish <|endoftext|>
Token ID : tensor([49406,   320,  1125,   539,   320,  1794, 49407, 49407, 49407]), Text : <|startoftext|>a photo of a baby <|endoftext|><|endoftext|><|endoftext|>
Token ID : tensor([49406,   320,  1125,   539,   320,  4298, 49407, 49407, 49407]), Text : <|startoftext|>a photo of a bear <|endoftext|><|endoftext|><|endoftext|>
Token ID : tensor([49406,   320,  1125,   539,   320, 22874, 49407, 49407, 49407]), Text : <|startoftext|>a photo of a beaver <|endoftext|><|endoftext|><|endoftext|>
```

Now letâ€™s get the CLIP embeddings

```python
# encode tokens to sentence embeddings from CLIP

with torch.no_grad():
    # passing the label text as in "a photo of a cat" to get its relevant embedding
    label_emb = model.get_text_features(**label_tokens)

# Move embeddings to CPU and convert to numpy array
label_emb = label_emb.detach().cpu().numpy()
label_emb.shape
```

```text
(100, 768)
```

We now have a 768-dimensional vector for each of our 100 text class sentences. However, to improve our results when calculating similarities, we need to normalize these embeddings.

Normalization helps ensure that all vectors are on the same scale, preventing longer vectors from dominating the similarity calculations simply due to their magnitude. We achieve this by dividing each vector by the square root of the sum of the squares of its elements. This process, known as L2 normalization, adjusts the length of our vectors while preserving their directional information, making our similarity comparisons more accurate and reliable.

```python
import numpy as np

# normalization
label_emb = label_emb / np.linalg.norm(label_emb, axis=0)
label_emb.min(), label_emb.max()
```

Ok, letâ€™s see a random image from our dataset

```python
import random

index = random.randint(0, len(imagedata)-1)
selected_image = imagedata[index]['img']
selected_image
```

![Sample CIFAR image](/assets/blog/zero-shot-image-classification-with-vector-search/Screenshot-2024-07-08-at-2.34.39-PM.png)

First, we'll run the image through our CLIP processor. This step ensures the image is resized first, then the pixels are normalized, then converting it into the tensor and finally adding a batch dimension.

```python
image = processor(
    text=None,
    images=imagedata[index]['img'],
    return_tensors='pt'
)['pixel_values'].to(device)
image.shape
```

```text
torch.Size([1, 3, 224, 224])
```

Now here this shape represents a 4-dimensional tensor:

- **1:** Batch size (1 image in this case)
- **3:** Number of color channels (Red, Green, Blue)
- **224:** Height of the image in pixels
- **224:** Width of the image in pixels

So, we have one image, with 3 color channels, and dimensions of 224x224 pixels. Now we'll use CLIP to generate an embedding.

```python
img_emb = model.get_image_features(image)
img_emb.shape
```

```text
torch.Size([1, 768])
```

We'll use LanceDB to store our labels, with their corresponding embeddings to allow performing vector search across the dataset.

```python
import lancedb
import numpy as np

data = []
for label_name, embedding in zip(labels, label_emb):
    data.append({"label": label_name, "vector": embedding})

db = lancedb.connect("./.lancedb")
table = db.create_table("zero_shot_table", data, mode="Overwrite")

# Prepare the query embedding
query_embedding = img_emb.squeeze().detach().cpu().numpy()
# Perform the search
results = (table.search(query_embedding)
           .limit(10)
           .to_pandas())

print(results.head(n=10))
```

## Results

```text
|   label         | vector | distance |
|-----------------|-----------------------------------------------------------|-------------|
| whale           | [0.05180167, 0.008572296, -0.00027403078, -0.12351207, ...]| 447.551605  |
| dolphin         | [0.09493398, 0.02598409, 0.0057568997, -0.13548125, ...]| 451.570709  |
| aquarium_fish   | [-0.094619915, 0.13643932, 0.030785343, 0.12217164, ...]| 451.694672  |
| skunk           | [0.1975818, -0.04034014, 0.023241673, 0.03933424, ...]| 452.987640  |
| crab            | [0.05123004, 0.0696855, 0.016390173, -0.02554354, ...]| 454.392456  |
| chimpanzee      | [0.04187969, 0.0196794, -0.038968336, 0.10017315, ...]| 454.870697  |
| ray             | [0.10485967, 0.023477506, 0.06709562, -0.08323726, ...]| 454.880524  |
| sea             | [-0.08117988, 0.059666794, 0.09419422, -0.18542227, ...]| 454.975311  |
| shark           | [-0.01027703, -0.06132377, 0.060097754, -0.2388756, ...]| 455.291901  |
| keyboard        | [-0.18453166, 0.05200073, 0.07468183, -0.08227961, ...]| 455.424866  |
```

Here are the results. Our initial accurate prediction is a whale, demonstrating the closest resemblance between the label and the image with minimal distance, just as we had hoped. What's truly remarkable is that we achieved this without running a single epoch for a CNN model. Thatâ€™s zero shot classification for you. Here is the Colab for your reference:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/zero-shot-image-classification/main.ipynb)
Checkout more examples on [VectorDB-recipes](https://github.com/lancedb/vectordb-recipes?tab=readme-ov-file)
```

---

## ðŸ“„ æ–‡ä»¶: _index.md

---

```md
---
title: "LanceDB Blog | Vector Database for RAG, AI Agents & Search"
description: "Tutorials, benchmarks, and case studies on RAG pipelines, agent memory, semantic search, and multimodal vector databases with versioning and object storage."
---
```

---

## ðŸ“„ æ–‡ä»¶: drafts\adaptive-rag.md

---

```md
---
title: "Practical Introduction to Adaptive-RAG"
date: 2024-04-24
author: LanceDB
categories: ["Community"]
draft: true
featured: false
image: /assets/blog/adaptive-rag/preview-image.png
meta_image: /assets/blog/adaptive-rag/preview-image.png
description: "Traditional LLMs provide answers based on a fixed knowledge database on which they are trained.  This limits their ability to respond with current or specific information."
---

![](https://cdn-images-1.medium.com/max/1200/1*dK5Kkum58gauwytt3PkQtg.png)[Adaptive RAG](https://arxiv.org/pdf/2403.14403.pdf)
Traditional LLMs provide answers based on a fixed knowledge database on which they are trained. This limits their ability to respond with current or specific information. While methods like Retrieval-Augmented Generation (RAG) try to solve this by combining LLMs with external data retrieval, they need additional help to handle queries of different complexities effectively.

[Adaptive RAG](https://arxiv.org/pdf/2403.14403.pdf) presents a method that chooses the best strategy for answering questions, from a direct LLM response to single or multiple retrieval steps. This selection is based on the queryâ€™s complexity, as determined by a classifier. The 3 strategies for answering are:

1. **Direct LLM Response**: For simple questions, it uses the LLMâ€™s own stored knowledge to provide quick and direct answers without seeking external information.
2. **Single-Step Retrieval**: For moderately complex questions, it retrieves information from a single external source, ensuring the answer is both swift and well-informed.
3. **Multi-Step Retrieval**: For highly complex questions, it consults multiple sources, piecing together a detailed and comprehensive answer.

### Why is Adaptive-RAG effective?

The flexibility of Adaptive-RAG ensures that each query is handled efficiently, using the most appropriate resources. This saves computational resources and improves user experience by delivering precise and timely answers.

Moreover, the ability of Adaptive-RAG to adapt based on the complexity of the question means it can provide more current, specific, and reliable information. This adaptability significantly reduces the chances of outdated or generic answers.

### Accuracy Enhancement

The accuracy of Adaptive-RAG is unparalleled, thanks to its ability to discern and adapt to the complexity of each query. By dynamically aligning the retrieval strategy with the queryâ€™s needs, Adaptive-RAG ensures that responses are not only relevant but also incorporate the most up-to-date information available. This adaptiveness significantly reduces the likelihood of outdated or overly generic answers, setting a new standard for accuracy automated question-answering.

The image below shows the comparison of different RAG approaches & adaptive rag is in terms of speed & performance
![](https://cdn-images-1.medium.com/max/1200/1*hSctN7sLv90qDybT5Ej9VQ.png)[https://arxiv.org/pdf/2403.14403.pdf](https://arxiv.org/pdf/2403.14403.pdf)
Let's create RAG app based on the discussed strategy.

Our RAG application is structured to route user queries to the most relevant data source, depending on the nature of the query. Hereâ€™s how weâ€™ve set it up:

1. **Web Search**: This route is selected for questions related to recent events, utilizing the Tavily Search API for real-time information retrieval.
2. **Self-Corrective RAG**: For more specialized queries that pertain to our indexed data, we use a self-corrective mechanism that leverages our own curated knowledge base.

![](https://cdn-images-1.medium.com/max/1200/1*okNdKS-83oBKEGGFeSDr4w.png)source from langchain
implementation Details:

- **Indexing Documents:** We begin by indexing documents using LanceDB, a vector database within the LangChain suite. This setup allows us to quickly retrieve relevant documents based on the userâ€™s query.
- **Routing**: The core of our application involves routing each user query to the most appropriate data sourceâ€Šâ€”â€Šeither our web search interface or the internal vector database.
- **Retrieval Grader**: We then employ a retrieval grader that evaluates whether the documents retrieved are relevant to the query, providing a binary â€œyesâ€ or â€œnoâ€ score.
- **Generation**: For generating responses, we pass the prompt, language model (LLM), and retrieval data from the vector database to construct a coherent and contextually appropriate output.
- **Hallucination Grader**: To ensure the authenticity of the information provided, a hallucination grader assesses whether the generated answer is factually grounded, again offering a binary score.
- **Answer Grader**: Additionally, an answer grader evaluates whether the response adequately addresses the userâ€™s question.
- **Question Rewriter**: This component enhances query effectiveness by rewriting the input question into a format optimized for retrieval from the vectorstore.
- **Graph Representation using langgraph** : Lastly, we use LangGraph to visually map out the flow and connections within our application, defining nodes and constructing a graph that represents the entire process.

For those interested in the inner workings of our adaptive response generation system, please refer to our Vector Recipe repository, which contains detailed code examples. Below, I will discuss a few examples illustrating how the Adaptive rag working

### Example 1: General Knowledge Query

Query: â€œWho is the prime minister of India?â€

This example demonstrates how the system processes a general knowledge question related to web-based information.

    # Run
    inputs = {"question": "Who is prime minister of india?"}
    for output in app.stream(inputs):
        for key, value in output.items():
            # Node//
            pprint(f"Node '{key}':")
            # Optional: print full state at each node
            # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
        pprint("\n---\n")

    # Final generation
    pprint(value["generation"])

output:

    â€”ROUTE QUESTION---
    ---ROUTE QUESTION TO WEB SEARCH---
    ---WEB SEARCH---
    "Node 'web_search':"
    '\n---\n'
    ---GENERATE---
    ---CHECK HALLUCINATIONS---
    '---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---'
    "Node 'generate':"
    '\n---\n'
    ---GENERATE---
    ---CHECK HALLUCINATIONS---
    ---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---
    ---GRADE GENERATION vs QUESTION---
    ---DECISION: GENERATION ADDRESSES QUESTION---
    "Node 'generate':"
    '\n---\n'
    ('Narendra Modi is the Prime Minister of India. He has been in office since '
     '2014 and was reelected for a second term in 2019.')

In this scenario, the system successfully finds the answer through a web search node, as shown:

- Routing Question: Direct the query to the appropriate node.
- Web Search: Fetch relevant information.
- Check for Accuracy: Ensure the information is accurate and not based on false data (hallucinations).
- Final Decision: Confirm the information answers the query correctly.
- Result: â€œNarendra Modi is the Prime Minister of India. He has been in office since 2014 and was reelected for a second term in 2019.â€

example 2:

    from pprint import pprint
    # Run
    inputs = {"question": "What is generative agents ?"}
    for output in app.stream(inputs):
        for key, value in output.items():
            # Node//
            pprint(f"Node '{key}':")
            # Optional: print full state at each node
            # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
        pprint("\n---\n")

    # Final generation
    pprint(value["generation"])

Output:

    â€”ROUTE QUESTION---
    ---ROUTE QUESTION TO RAG---
    ---RETRIEVE---
    TAKING DICSNEW BRO...>>>>>>>>
    "Node 'retrieve':"
    '\n---\n'
    ---CHECK DOCUMENT RELEVANCE TO QUESTION---
    ---GRADE: DOCUMENT RELEVANT---
    ---GRADE: DOCUMENT NOT RELEVANT---
    ---GRADE: DOCUMENT NOT RELEVANT---
    ---GRADE: DOCUMENT NOT RELEVANT---
    ---ASSESS GRADED DOCUMENTS---
    ---DECISION: GENERATE---
    "Node 'grade_documents':"
    '\n---\n'
    ---GENERATE---
    ---CHECK HALLUCINATIONS---
    ---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---
    ---GRADE GENERATION vs QUESTION---
    ---DECISION: GENERATION ADDRESSES QUESTION---
    "Node 'generate':"
    '\n---\n'
    ('Generative agents are virtual characters controlled by LLM-powered agents '
     'that simulate human behavior in interactive applications. These agents '
     'combine LLM with memory, planning, and reflection mechanisms to interact '
     'with each other based on past experiences. The design includes a memory '
     'stream, retrieval model, reflection mechanism, and planning & reacting '
     "components to inform the agent's behavior.")

The system routes the question to the RAG (Retrieval-Augmented Generation) system and retrieves relevant documents to generate an accurate response.

- Routing and Retrieval: Identify and fetch relevant data.
- Document Relevance Check: Assess each documentâ€™s relevance to the query.
- Response Generation: Create an answer based on the most relevant documents.
- Accuracy Check: Confirm the generation is grounded in factual documents.
- Result: â€œGenerative agents are virtual characters controlled by LLM-powered agents that simulate human behavior in interactive applications. These agents combine LLM with memory, planning, and reflection mechanisms to interact with each other based on past experiences. The design includes a memory stream, retrieval model, reflection mechanism, and planning & reacting components to inform the agentâ€™s behavior.â€

Due to the complexity of the code, I have attached a link to a Google Colaboratory notebook where you can explore and execute the code yourself.
[

Google Colaboratory

![](https://ssl.gstatic.com/colaboratory-static/common/e3fd1688ddc6244fab65fff8febc6049/img/favicon.ico)

![](https://colab.research.google.com/img/colab_favicon_256px.png)
](https://colab.research.google.com/drive/155U-Zhd70kuldUI4EuBnonX1fukW2pEs?usp=sharing)
### Conclusion

In this blog, we've explored the capabilities of Adaptive-RAG for question-answering tasks, illustrating its remarkable efficiency and precision. Through detailed examples, we demonstrated how Adaptive-RAG dynamically adjusts its approach based on the complexity of the query, ensuring optimal use of computational resources while delivering precise and timely answers.

The flexibility and accuracy of Adaptive-RAG set a new standard in automated response generation, offering more reliable and up-to-date information. This adaptability makes it an invaluable tool for handling a wide range of queries, from simple factual questions to complex informational needs.

Checkout [vector-recipes](https://github.com/lancedb/vectordb-recipes) for other examples and insights into the practical applications of GenAI.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\advanced-rag-context-enrichment-window.md

---

```md
---
title: "Advanced RAG with Context Enrichment Window"
date: 2024-10-17
author: LanceDB
categories: ["Community"]
draft: true
featured: false
image: /assets/blog/advanced-rag-context-enrichment-window/preview-image.png
meta_image: /assets/blog/advanced-rag-context-enrichment-window/preview-image.png
description: ". ."
---

We all know what's Vanilla RAG, how it works, and how to use it but sometimes there are use cases that go beyond the traditional rules. For example, there are some use cases where we need to create very small chunks as larger ones can add noise like conversation history. Using a sentence or couple-level strategy is pretty decent in this case but don't you think the context to the current reply might be hidden somewhere in the previous one or the answer to something can be somewhere in the future ones? This is just one use case and I know you might be thinking why not use a bigger chunk if you're going in that direction? A few reasons such as no. of chunks to send are limited and bigger chunk sizes can add more noise. So? What's the solution?

> ***A man is known by the company he keeps***

![](__GHOST_URL__/content/images/2024/10/image-3.png)
## Context Enrichment

It means that when you get the `Top-K` chunks and the size of the chunks is smaller, you can enrich this context by adding the neighboring chunks too. So when along with the `Top-K` most useful chunks come their Previous and Future `N` neighboring chunks. Simply put, make a sandwich of each context you get from the query.
![](__GHOST_URL__/content/images/2024/10/96k2zr.jpg)[

Google Colab

![](https://ssl.gstatic.com/colaboratory-static/common/858ebdf69a031c79c51b9777a6b9bdb3/img/favicon.ico)

![](https://colab.research.google.com/img/colab_favicon_256px.png)
](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Advanced_RAG_Context_Enrichment_Window/Advanced_RAG_Context_Enrichment_Window.ipynb)
Let's do some coding so that you understand completely. Let's start with simply installing and importing libraries, loading a PDF, and creating a LanceDB table.

    ! pip install -U openai lancedb einops sentence-transformers transformers datasets tantivy rerankers langchain PyMuPDF -qq

    # Get a PDF for example
    !mkdir ./data
    !wget https://ncert.nic.in/ncerts/l/jess301.pdf -O ./data/history_chapter.pdf

    # Import Libraries
    import os, re, random, json
    import pandas as pd
    import torch
    import lancedb
    from lancedb.embeddings import get_registry
    from lancedb.pydantic import LanceModel, Vector
    from tqdm.auto import tqdm
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain.docstore.document import Document
    import fitz
    from typing import List

    pd.set_option('max_colwidth', 750) # For visibility

    model = get_registry().get("sentence-transformers").create(name="BAAI/bge-small-en-v1.5", device=("cuda" if torch.cuda.is_available() else "cpu")) # For embedding

    def read_pdf_to_string(path):
        """
        Read a PDF document from the specified path and return its content as a string.

        Args:
            path (str): The file path to the PDF document.

        Returns:
            str: The concatenated text content of all pages in the PDF document.

        The function uses the 'fitz' library (PyMuPDF) to open the PDF document, iterate over each page,
        extract the text content from each page, and append it to a single string.
        """
        doc = fitz.open(path) # Open the PDF document located at the specified path
        content = ""

        for page_num in range(len(doc)): # Iterate over each page in the document
            page = doc[page_num]  # Get the current page
            content += page.get_text() # Extract the text content from the current page and append it to the content string
        return content

    def split_text_to_chunks_with_indices(text: str, chunk_size: int, chunk_overlap: int) -> List[Document]:
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(Document(page_content=chunk, metadata={"index": len(chunks), "text": text}))
            start += chunk_size - chunk_overlap
        return chunks

    content = read_pdf_to_string("/content/data/history_chapter.pdf")

    CHUNK_SIZE = 512
    CHUNK_OVERLAP = 128

    text_splitter = RecursiveCharacterTextSplitter(
        # Set a really small chunk size, just to show.
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        is_separator_regex=False,
    )

    texts = text_splitter.create_documents([content])

    # Create the table

    class Schema(LanceModel):
        text: str = model.SourceField() # the Columns (field) in DB whose Embedding we'll create
        chunk_index: int
        vector: Vector(model.ndims()) = model.VectorField() # Default field

    chunks = []
    for index, doc in enumerate(texts):
      chunks.append({"text":doc.page_content, "chunk_index": index+1})

    MAX_CHUNK_INDEX = index+1 # we'll need this for our logic to get the final chunk index that exists in DB

    db = lancedb.connect("./db")
    table = db.create_table("documents", schema=Schema)

    table.add(chunks) # ingest docs with auto-vectorization
    table.create_fts_index("text") # Create a fts index before so that we can use BM-25 later if we want to use Hybrid search

So we have created our table where each text chunk has an index associated with it. Let's now do a simple search.

    TOP_K = 3 # How many similar chunks to retrieve
    NEIGHBOUR_WINDOW = 1 # 1 means 1 before and 1 after

    QUERY = "What did the the revolution proclaim and what did the centralised administrative system do?"

    initial_results = table.search(QUERY).limit(TOP_K) # Get all the similar chunks which are sorted by distance by default

    initial_results.to_pandas().drop("vector", axis = 1)

![](__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-13-at-10.53.43-PM.png)
So the most important chunks according to Query are `14,86,16` . Now when we enrich the context with `NEIGHBOUR_WINDOW = 1` , it simply means to get the chunk's IDs `13,14,15` , `85,86,87` and `15,16,17` , in order.

Did you notice something here? Yes, the chunk ID `15`  is repeating with 2 groups. Where would you put it? It makes sense to put it with the higher priority AKA the minimum distance group (which in our case is 14). So we'll write the code the code to get the neighbors.

    similar_chunk_indices = {} # store previous and next neighbour chunk

    for i in initial_results.to_list(): # Get all the similar chunks and their neighbour indices
      index = i["chunk_index"]
      similar_chunk_indices[index] = i["_distance"]

      for near in range(1, NEIGHBOUR_WINDOW+1):
        if (max(0,index-near)) not in similar_chunk_indices: # Previous neighbour
          similar_chunk_indices[(max(0,index-near))] = i["_distance"] # This chunk will also have the same distance

        if min(index + near, MAX_CHUNK_INDEX) not in similar_chunk_indices: # Next neighbour
          similar_chunk_indices[min(index + near, MAX_CHUNK_INDEX)] = i["_distance"]

    similar_chunk_indices # Look at the index 15. It is a part of 14 and 16 both

It gives you a dictionary of chunk IDs to fetch and their distance. We assign the same distance to the neighbor as the parent chunk.

Now there is an interesting phenomenon or you can say an edge case. What if you have overlapping or continuous indices such that `[3,4,5]` has a distance of 0.7, `[20,21,22]` with distance 0.4, `[6,7,8]` with a distance of 0.1 and `[1,2,3]` comes with a distance of 0.9?
If you look at the indices, you see there is a continuous range from 1 to 8 but with different distance and ranges.  In that case, wouldn't it be logical to use the continuous group together at the same position from 1 to 8 and assign it a minimum distance as 0.1 so that it comes before the group `[20,21,22]`?

It makes sense because if we are going to include the chunk group later anyhow, why not to use the continuity to our advantage and save ourselves from a broken context.

    def group_and_rerank_chunks(indices_dict:dict):
      """
      function to take the {"chunk_index":"distance"} dict and return {"priority": indices_group_list} dict
      """

      sorted_indices = sorted(indices_dict.keys()) # Sort the indices

      # Group by distance with continuity consideration
      groups = []
      current_group = []
      current_min_distance = float('inf')

      for i in range(len(sorted_indices)):
          index = sorted_indices[i]
          distance = indices_dict[index]

          if not current_group:  # Start a new group
              current_group.append(index)
              current_min_distance = distance
          else:
              if index == current_group[-1] + 1: # Check continuity
                  current_group.append(index)
                  current_min_distance = min(current_min_distance, distance)
              else: # Save the current group and start a new one
                  groups.append((current_min_distance, current_group))
                  current_group = [index]
                  current_min_distance = distance

      if current_group: # add the last group
          groups.append((current_min_distance, current_group))

      groups.sort(key=lambda x: x[0]) # Sort groups by minimum distance

      return {i: group for i, (dist, group) in enumerate(groups)}

    # group_and_rerank_chunks({
    #         50:75, 51:75, 52:75, 53:75, 54:75, 55:75,
    #         997:1, 998:1, 999:1,
    #         5:50, 6:50, 7:50,
    #         1:100, 2:100, 3:100,
    #         8:100, 9:1000, 10:1000}) # Test this one to understand

    reranked_indices = group_and_rerank_chunks(similar_chunk_indices)
    reranked_indices # Look at the group for 16. Even though it has more disatnce than 86 but since it's part of a continuous group, we put it before

It gives us the two groups results as: `{0: [13, 14, 15, 16, 17], 1: [85, 86, 87]}` and these groups are sorted by how would they occur in the final RAG.

### Now off to some efficient retrieval using LanceDB functionality

Do we need to search the query again? No. Definitely not. Because LanceDB gives functionality to search the SQL-like queries directly. If we know the `chunk_index` already, we can directly write a query to fetch those rows and post-process them. Isn't that beautiful (and efficient)!

    indices_to_search = []
    for priority, indices in reranked_indices.items():
      indices_to_search.extend(indices)

    similar_results = table.search().\
                      where(f"chunk_index IN {tuple(indices_to_search)}").\
                      limit(len(similar_chunk_indices)).\
                      to_pandas().\
                      set_index("chunk_index").loc[indices_to_search, :].reset_index() # Just a trick to sort the DF according to the chunk priority group

    similar_results.drop("vector", axis = 1)

![](__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-13-at-11.12.45-PM.png)
And now we are left with just 1 post-processing step. Which is? Yes, repeated sentences like you teacher. Don't remember? Didn't you put that `CHUNK_OVERLAP = 128` ? So it means that we have to remove those. We simply go group by group and remove the overlapping prefix from the second entry onwards.

    final_rag_text = "## Context - 1:\n"

    group_priority = 0 # Priority of the Chunk group
    grouped_indices = reranked_indices[group_priority]
    remove_overlap = False # from the 2nd element in the group, remove prefix overlap

    for _, row in similar_results.iterrows():
      chunk_index = row["chunk_index"]

      if  remove_overlap: # if the previous chunk is there, remove the overlap
        final_rag_text += row["text"][CHUNK_OVERLAP:]
      else:
         final_rag_text += row["text"]

      remove_overlap = True

      if chunk_index == grouped_indices[-1]: # last element of the group means the new group has started
        group_priority += 1
        remove_overlap = False # new group has started so don't trim the first element

        if group_priority in reranked_indices: # If not the last key in the dict
          final_rag_text += f"\n\n## Context - {group_priority+1}:\n"
          grouped_indices = reranked_indices[group_priority]

    print(final_rag_text)

![](__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-13-at-11.16.21-PM.png)
Both the groups are shown here, in order, with overlapping removed.

If you are interested, you can also check advanced chunking and retrieval techniques like [Parent Document Retriever](__GHOST_URL__/modified-rag-parent-document-bigger-chunk-retriever-62b3d1e79bc6/), [HyDE](__GHOST_URL__/advanced-rag-precise-zero-shot-dense-retrieval-with-hyde-0946c54dfdcb/), [Re-Ranking](https://medium.com/etoai/simplest-method-to-improve-rag-pipeline-re-ranking-cf6eaec6d544) etc
```

---

## ðŸ“„ æ–‡ä»¶: drafts\benchmarking-gpt4-o.md

---

```md
---
title: "Benchmarking GPT4-O"
date: 2024-05-23
author: LanceDB
categories: ["Community"]
draft: true
featured: false
image: /assets/blog/benchmarking-gpt4-o/preview-image.png
meta_image: /assets/blog/benchmarking-gpt4-o/preview-image.png
description: "Last week, OpenAI announced its latest LLM, GPT-4o, which follows GPT-4 Turbo.  In this blog, we benchmark GPT4-o using a new benchmarking toolkit called \"Needle in a Needlestack\"."
---

Last week, OpenAI announced its latest LLM, GPT-4o, which follows GPT-4 Turbo. In this blog, we benchmark GPT4-o using a new benchmarking toolkit called "Needle in a Needlestack".

## What is OpenAIâ€™s GPT-4o?

GPT-4o (â€œoâ€ for â€œomniâ€) is a step towards much more natural human-computer interactionâ€”it accepts as input any combination of text, audio, image, and video any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, similar to human response time(opens in a new window) in a conversation.
![](__GHOST_URL__/content/images/2024/05/download.png)*Comparison between GPT-4 Turbo and GPT-4o*Credits: datacamp
Check out the OpenAI Spring Update for details about GPT-4o.

## Benchmarking

In this blog, we benchmark GPT-4o using the [NIAN](https://github.com/llmonpy/needle-in-a-needlestack) (Needle in a Needlestack) benchmark.
Needle in a Needlestack is a new benchmark to measure how well LLMs pay attention to information within their context window. NAIN creates a prompt with thousands of limericks(short, humorous verses that are usually nonsensical and consist of five lines rhyming AABBA.) and asks a question about a specific limerick at a particular position.

NIAN evaluates the answers to its questions, which is a challenging task for large language models (LLMs). To ensure accuracy, NIAN employs five different LLMs to assess the responses, determining pass or fail by majority vote. The evaluation process is improved using single or few-shot training techniques.

NIAN includes a tool called ***answers*** that identifies every unique answer generated by the LLMs and classifies them with "pass" or "fail" tags based on the majority vote. For variety, I used GPT-3.5, GPT-4, Haiku, and Mistral 7B to generate answers. After examining the responses, I used a few that passed as examples for the evaluators when similar questions previously resulted in failed answers that should have passed.
![](__GHOST_URL__/content/images/2024/05/test_trial_plot_gpt-4o.png)GPT-4o benchmark using NIAN![](__GHOST_URL__/content/images/2024/05/gpt-4o_question_plot_99817.png)QAplot at different token positions
**GPT-4o has made a breakthrough and did quite well on this benchmark.**

Comparing it with the GPT-4 Turbo benchmark reveals a clear difference in performance between the two models.
![](__GHOST_URL__/content/images/2024/05/gpt-4-110-1.png)GPT-4-turbo benchmark using NIAN
Everyone is curious about when OpenAI will explain the improvements that made GPT-4o much better than GPT-4 Turbo.
![Sonnet Image](https://nian.llmonpy.ai/1/mistral-large-32-1.png)Mistral-large-latest benchmark using NIAN
GPT-4o performs better than other LLMs, including Mistral-large, GPT-4-turbo, and Claude-3-sonnet on this benchmark.

NIAN gives very different results depending on the question, so it's important to ask at least five questions. To be sure NIAN isn't just testing how well the LLMs can answer a single question, the vet tool asks various models the same question  `full_questions.json` five times. Any question that doesn't get a perfect score is removed.

## Conclusion

OpenAI made significant advancements that resulted in GPT-4o being notably superior to GPT-4 Turbo. To guarantee the precision of GPT-4o, NIAN uses five different large language models (LLMs) to evaluate the responses, deciding whether they pass or fail through a majority vote. The evaluation method is enhanced by using single or few-shot training techniques.
Check out the blog on  [Benchmarking the OpenAI Embedding Model](https://lance-blog.ghost.io/ghost/#/editor/post/65d5fdcf7ed5d50001d214d7).
```

---

## ðŸ“„ æ–‡ä»¶: drafts\benchmarking-lancedb-92b01032874a.md

---

```md
---
title: "Benchmarking LanceDB"
date: 2023-12-18
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/benchmarking-lancedb-92b01032874a/preview-image.png
meta_image: /assets/blog/benchmarking-lancedb-92b01032874a/preview-image.png
description: "I came upon a yesterday benchmarking LanceDB.  The numbers looked very surprising to me, so I decided to do a quick investigation on my own."
---

I came upon a[ blog post](https://medium.com/@plaggy/lancedb-vs-qdrant-caf01c89965a) yesterday benchmarking LanceDB. The numbers looked very surprising to me, so I decided to do a quick investigation on my own. First, I want to express gratitude to the postâ€™s author for taking the time to do a run down and publishing their results. Running these benchmarks is often a tedious exercise in chasing down details and analyzing tradeoffs. If we feel something was missed in a benchmark of LanceDB, I believe itâ€™s the fault of our documentation and *not* the fault of the author.

> TL;DR: LanceDB is* ***fast**, and achieves <20ms latencies (going as low as 1 ms) across all configurations in the GIST-1M benchmark dataset.

## Benchmark setup

We use the same GIST-1M dataset referred to in the original benchmark. This can be found at the source:[ http://corpus-texmex.irisa.fr/](http://corpus-texmex.irisa.fr/) (`gist.tar.gz`).

The benchmarks shown in this post were performed on a 2022 Macbook Pro (M2 Max). When using LanceDB, hardware can make a difference. Because LanceDB is primarily disk-based for both indexing and storage, it benefits greatly from a fast disk â€” itâ€™s highly recommended to use an SSD instead of older HDDs. While most other vector DBs that use in-memory indexes are sensitive to the amount of RAM, LanceDB is much less memory-intensive.

The original blog post used `v0.3.1`of LanceDB. Because the system is continually improving, I ran these benchmarks using the latest LanceDB `v0.3.6` (and latest lance python `v0.8.21`).

You can find the code to reproduce the benchmarks [here](https://github.com/lancedb/lance/tree/main/benchmarks/sift).

- Download `gist.tar.gz`from [the source](http://corpus-texmex.irisa.fr/) and unzip
- Generate Lance datasets using the datagen.py script:
- Database vectors:

    ./datagen.py ./gist/gist_base.fvecs ./.lancedb/gist1m.lance -g 1024 -m 50000 -d 960

- Query vectors:

    ./datagen.py ./gist/gist_query.fvecs ./.lancedb/gist_query.lance -g 1024 -m 50000 -d 960 -n 1000

- Create index with 256 partitions and 120 subvectors:

    ./index.py ./.lancedb/gist1m.lance -i 256 -p 120

- Run the benchmark for recall@1:

    ./metrics.py ./.lancedb/gist1m.lance results.csv -i 256 -p 120 -q ./.lancedb/gist_query.lance -k 1

The benchmark results will be output to `results.csv` .

## Indexing

The original post set the `num_partitions`as 256 or 512, and `num_sub_vectors`as either 120 or 240. Across all combinations, the blog post reported >1000 sec indexing times using a T4 GPU, which was surprising. In addition, at this scale, a GPU really isnâ€™t necessary for indexing. On my macbook, I timed the indexing job using just my CPU, for a total **wall time** on the order of **1 minute**:
![](https://miro.medium.com/v2/resize:fit:1400/0*WLB1_Zm5tCLkBfZL)The wall time for ivf256 and pq120 is roughly one minute
Even without a GPU, the indexing times should be fast enough on a dataset of size 1M.

## Querying

The original post varied nprobes during querying. The author calculated recall@1 for 20 or 40 nprobes, reporting 0.65 to 0.9 recall at 45 to 177ms latencies. For LanceDB, a key parameter that can make an impact is the `refine_factor`. By setting this parameter, the user can tell LanceDB to compute the full vector distance (instead of just the PQ distance) for a small subset of the results to get a much better recall for a small latency penalty of less than a millisecond on datasets of this size.

## Refine Factor

LanceDB uses *PQ*, or [Product Quantization](https://inria.hal.science/inria-00514462v2/document), to compress vectors and speed up search. However, because PQ is a lossy compression algorithm, it can significantly reduce recall. To combat this problem, we introduce a process called *refinement*. The normal process computes distances using the compressed PQ vectors. We then take the most similar PQ vectors, and *refine *the results by fetching the actual vectors and computing the full vector distance and reordering based on those scores.

If you are retrieving the top 10 results and set `refine_factor`to 5, then LanceDB will fetch the 50 most similar vectors (according to PQ), compute the distances again based on full vectors for those 50, then rerank based on those scores.

## Results

Using the ivf256 and pq120 index I created in my version of the benchmark, I tested recall@1 using the 1000 query vector set provided by GIST across an `nprobes`and a `refine_factor` frontier. As you can see, **3ms** **latency** is sufficient to get above **0.9 recall **(nprobes 25 and rf 30). And **5ms** is sufficient for **0.95 recall** (`nprobes` 50 and rf 30):
![](https://miro.medium.com/v2/resize:fit:1400/0*niFqM_BobpkUzND2)Refine factor greatly improves recall for a very small latency penalty
The full table of results that generated this graph is [published on github](https://raw.githubusercontent.com/lancedb/lance/main/benchmarks/sift/lance_gist1m_stats.csv).

> **3ms*** ***latency*** *is sufficient to get above* ***0.9 recall **(nprobes 25 and rf 30). And* ***5ms*** *is sufficient for* ***0.95 recall*** *(nprobes 50 and rf 30)

We can zoom in a little bit and focus on the high recall range above 0.9 recall, thereâ€™s a fairly wide variety of refine / nprobes that delivers this recall in single digit milliseconds, which means we donâ€™t have to be too sensitive to finding exactly the right configurations here.
![](https://miro.medium.com/v2/resize:fit:1400/0*pyrkqDTByvqwXx9t)Thereâ€™s a wide range of configurations to achieve >0.9 recall at low latency
## Linux

Given that the original benchmarkâ€™s author was using a T4 GPU, Iâ€™m guessing it was done on a Linux machine. So I also ran the same benchmark on Linux (Lenovo P52 with Xeon E-2176M 12-core CPU). This laptop is roughly 5 years old, so weâ€™d expect the indexing performance to not be as good as a modern Macbook. However, we show below that itâ€™s still almost an order of magnitude faster than what was reported in the original blog post.
![](https://miro.medium.com/v2/resize:fit:1400/0*8Hwa_vv3TOgqq-Fq)The `duration` is the wall time for indexing, ~2min 46sec
Itâ€™s clear that the wall time duration for indexing is **2min 46sec.**

As expected, the query benchmarks for the older CPU on Linux are slightly slower than on the Macbook, but still very low, ranging from roughly **7â€“20ms** for recall@1 above 0.9.
![](https://miro.medium.com/v2/resize:fit:1400/0*4Ev37T0q0N_dSySd)The frontiers look pretty similar on Ubuntu, just with higher latencies due to my older generation CPU
You can achieve even more performance if youâ€™re using Linux. Currently, our PyPI build is not as optimized as it could be in order to be compatible with some of the older AWS Lambda CPUs (Haswell). If youâ€™re on a newer generation CPU like Skylake / Icelake, you can get much better performance by building the underlying [Lance](http://github.com/lancedb/lance) artifact from source using `RUSTFLAGS=â€™-C target-cpu=nativeâ€™ maturin develop -r` . In the future weâ€™ll be tackling this so the workaround is not necessary. This is tracked here: [https://github.com/lancedb/lance/issues/1733](https://github.com/lancedb/lance/issues/1733).

## A Different Architecture

The author rightly noted that LanceDB is different because itâ€™s an embedded database that can run inside your application without having to deal with additional infrastructure. This is certainly one reason why a lot of users have chosen LanceDB.

Thereâ€™s another big reason that makes LanceDB different. Because the index is disk-based, it means that LanceDBâ€™s memory requirements are very low. A lot of our users actually store data on S3 and use LanceDB to query their S3 object files directly from AWS Lambda or their own application. And with the recent announcement of [S3 Express](https://aws.amazon.com/s3/storage-classes/express-one-zone/), LanceDB will be able to deliver** even more** performance.

> Many of our users actually store data on S3 and use LanceDB to query their S3 object files directly from AWS Lambda

LanceDB is also among the very few serverless vector databases that actually allows you to separate compute from storage. In OLTP databases, Neon is a great example of the same type of architecture. Nikita Shamgunov, the founder of Neon, posted a great thread explaining the implications of this:

For LanceDB, the implication is that scaling up means getting a bigger disk (or network drive), which is a lot simpler and far less expensive than sharding data across a lot of nodes or buying terabytes of RAM.

Also, LanceDB is *fast*. Iâ€™m able to search through 1 billion vectors (128D) on my macbook in <100ms:
![](https://miro.medium.com/v2/resize:fit:1400/0*0fLf5W7AJVo6puKI)The wall time for searching over 1 billion vectors on a single node is easily under 100ms
> Searching over 1B vectors on a single node in under 100 ms

## Conclusion

In this blog, weâ€™ve seen how appropriately setting the `refine_factor` boosted the recall@1 to 0.95 while still delivering a latency of about 5ms. The indexing time is also quite fast at around 60s for 1M 960D vectors. The key lesson learned for us is that having great technology isnâ€™t enough â€” we also need to make our documentation much better so users arenâ€™t missing such an important parameter to tune their searches if they need high recall.

In the coming weeks, weâ€™ll also be publishing benchmarks *with code* that you can run and replicate on your own datasets, so you can more confidently decide on what tool to proceed with for your RAG and other vector search applications. Stay tuned!
```

---

## ðŸ“„ æ–‡ä»¶: drafts\cambrian-1-vision-centric-exploration.md

---

```md
---
title: "Cambrian-1: Vision-Centric Search"
date: 2024-07-25
author: LanceDB
categories: ["Community"]
draft: true
featured: false
image: /assets/blog/cambrian-1-vision-centric-exploration/preview-image.png
meta_image: /assets/blog/cambrian-1-vision-centric-exploration/preview-image.png
description: "Run visionâ€‘centric exploration with Cambrianâ€‘1 MLLMs over images retrieved via LanceDB. The guide indexes captions, searches similar images, and prompts the model for grounded answers."
---

Cambrian-1 is a family of multimodal LLMs (MLLMs) designed with a **vision-centric** approach. While stronger language models can boost multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research.

Cambrian-1 is built on five key pillars, each providing important insights into the design of multimodal LLMs (MLLMs):

1. **Visual Representations:** They explore various vision encoders and their combinations.
2. **Connector Design:** They design a new dynamic, spatially-aware connector that integrates visual features from several models with LLMs while reducing the number of tokens.
3. **Instruction Tuning Data:** They curate high-quality visual instruction-tuning data from public sources, emphasizing distribution balancing.
4. **Instruction Tuning Recipes:** They discuss strategies and best practices for instruction tuning.
5. **Benchmarking:** They examine existing MLLM benchmarks and introduce a new vision-centric benchmark called "CV-Bench".

We'll learn how Cambrian-1 works with an example of Vision-Centric Exploration on images found through vector search. This will involve two steps.

1. Performing vector search to get related images
2. Use obtained images for vision-centric exploration

![](__GHOST_URL__/content/images/2024/07/Screenshot-from-2024-07-16-10-44-24.png)
## Implementation

This blog contains code snippets, for the whole code with their description use checkout Kaggle Notebook and run it with your own prompt

[https://www.kaggle.com/code/prasantdixit/cambrian-1-vision-centric-exploration-of-images/](https://www.kaggle.com/code/prasantdixit/cambrian-1-vision-centric-exploration-of-images/)

In this example, We will be working with the [Flickr-8k](https://paperswithcode.com/dataset/flickr-8k) dataset. It is a multi-modal dataset comprising images and their corresponding captions.

We'll index these images based on their captions using the `all-mpnet-base-v2` model from the sentence-transformer. The [LanceDB Embedding API ](https://lancedb.github.io/lancedb/embeddings/default_embedding_functions/)will retrieve embeddings from the sentence transformer models.

    embedding_model = (
        get_registry()
        .get("sentence-transformers")
        .create(name="all-mpnet-base-v2", device="cuda:0")
    )

Now we are ready to integrate this embedding function into the schema of the table to simplify the process of data ingestion and querying.

    pa_schema = pa.schema([
        pa.field("vector", pa.list_(pa.float32(), 768)),
        pa.field("image_id", pa.string()),
        pa.field("image", pa.binary()),
        pa.field("captions",pa.string()),
    ])

    class Schema(LanceModel):
        vector: Vector(embedding_model.ndims()) = embedding_model.VectorField()
        image_id: str
        image: bytes
        captions: str = embedding_model.SourceField()

### Ingestion Pipeline

Storing vector embeddings efficiently is crucial for leveraging their full potential in various applications. To **manage, query, and retrieve** embeddings effectively, especially with large-scale and multi-modal data, we need a robust solution.

LanceDB allows you to store, manage, and query embeddings as well as raw data files.

    db = lancedb.connect("dataset")
    tbl = db.create_table("table", schema=Schema, mode="overwrite")
    tbl.add(process_dataset(df))

The above snippet populates a table that you can use to query using captions.

### Vector Search

With LanceDB, performing vector search is pretty straightforward. LanceDB Embedding API implicitly converts the query into embedding and performs vector search to give desired results. Let's take a look at the following example:

    query = "cat sitting on couch"

    hit_lists = tbl.search(query) \
        .metric("cosine") \
        .limit(2) \
        .to_list()

    for hit in hit_lists:
        show_image(hit["image"])

This code sample returns the top two similar images, you can increase the number by changing `limit` parameter.

The results would look like this
![](__GHOST_URL__/content/images/2024/07/Screenshot-from-2024-07-17-11-17-51.png)![](__GHOST_URL__/content/images/2024/07/s.png)
Now we have obtained images from the vector search, let's explore these images with a vision-centric multi-modal LLM(MLLM) Cambrian-1.

### Setup vision-centric Exploration

We'll use Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a **vision-*centric*** approach. For setting up Cambrian-1, we'll use their official GitHub repo [https://github.com/cambrian-mllm/cambrian](https://github.com/cambrian-mllm/cambrian).

Clone the repo and install requirements

    # clone
    !git clone https://github.com/cambrian-mllm/cambrian
    %cd cambrian

    # install gpu related requirements
    !pip install ".[gpu]" -q

Full code can be found here - [https://www.kaggle.com/code/prasantdixit/cambrian-1-vision-centric-exploration-of-images/](https://www.kaggle.com/code/prasantdixit/cambrian-1-vision-centric-exploration-of-images/)

    prompt = "How many cats are in image and Why?"
    images_path = []
    for hit in hit_lists:
        image_path = f"/kaggle/working/Images/{hit['image_id']}"
        images_path.append(image_path)

    infer_cambrian(images_path, prompt)

    import subprocess
    command = "python3 inference.py"
    output = subprocess.check_output(command, shell=True, text=True)

This code snippet will run `inference.py` and return the query results corresponding to each input image.

The result will look like this
![](__GHOST_URL__/content/images/2024/07/Screenshot-from-2024-07-16-12-12-26.png)
Our Prompt for exploring images was `How many cats are in image and Why?` , and Cambrian-1 has clearly given the right and clear output stating the correct count and external scenario.

## Conclusion

In this tutorial, We've explored the field of vision-centric exploration same as `GPT4-o` where you give an image as input and ask questions related to it. We have utilized Cambrian-1 as a multi-modal LLM for vision-centric exploration combined with vector search using text embeddings.

We've seen how LanceDB, with its powerful and elegant design in Embedding API, simplifies the process of storing, managing, and querying these embeddings, making sophisticated tasks like semantic search both accessible and efficient.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\context-aware-chatbot-using-llama-2-lancedb-as-vector-database-4d771d95c755.md

---

```md
---
title: "Context-Aware Chatbot Using Llama 2 & LanceDB Serverless Vector Database"
date: 2023-09-19
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/context-aware-chatbot-using-llama-2-lancedb-as-vector-database-4d771d95c755/preview-image.png
meta_image: /assets/blog/context-aware-chatbot-using-llama-2-lancedb-as-vector-database-4d771d95c755/preview-image.png
description: "Building a real chatbot using RAG method â€“ by Akash A. Get practical steps and examples from 'Context-aware chatbot using Llama 2 & lanceDB serverless vector database'."
---

Building a real chatbot using RAG method â€“ by Akash A. Desai

## **Introduction:**

Many people know about OpenAIâ€™s cool AI models like GPT-3.5 and GPT-4, but theyâ€™re usually not free to use. But hereâ€™s some good news: Meta has introduced a free, super-smart model called [Llama-2](https://ai.meta.com/llama/). In this blog, weâ€™re going to explore Llama-2 and use it to create a chatbot that can work with PDF files from scratch. Letâ€™s break down the important parts step by step.

**The Key Components for Building RAG based applications:**
![](https://miro.medium.com/v2/resize:fit:770/1*3ktrnp7Wh2V-Js-crF2ENw.png)
1. **Large Language Model**: The cornerstone of our project is the Llama-2 Large Language Model. It boasts a multitude of applications and is ideal for our endeavor.
2. **Embedding Model**: To enhance our chatbotâ€™s performance, we employ the Huggingface embedding model, namely â€œ[sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).â€
3. **Vector Store Database**: Efficiently managing data is paramount. To achieve this, we rely on a Vector Store database. We are using [Lance](https://github.com/lancedb/lancedb)DB vector base for this example
4. **Preprocessing Tool**: Streamlining data preprocessing is crucial, and we leverage [**Langchain**](https://python.langchain.com/docs/get_started/introduction) for this purpose.

## **System Compatibility:**

Our system is designed to be accessible to a wide audience. It can be run smoothly on a local machine with a minimum of **15GB VRAM,** even on a CPU setup. For the Llama-2 model, we opt for the quantized GGML version, which can be sourced from the [Bloke repo](https://huggingface.co/TheBloke/Llama-2-7B-GGML.) Additionally, we utilize the Huggingface embedding model [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).

**Unlocking the Power of Large Language Models (LLMs) with RAG**

In todayâ€™s world, making the most of Large Language Models (LLMs) in your business doesnâ€™t have to be expensive or complicated. Letâ€™s explore an approach called Retrieval Augmented Generation (RAG), which can help you get better results without breaking the bank.

## **What is RAG and Why Should You Care?**

RAG, short for Retrieval Augmented Generation, is a clever technique that boosts the abilities of LLMs. It works like this: Imagine you have a smart assistant who can read and understand a huge library of books. When you ask it a question, it doesnâ€™t just guess the answer â€” it actually looks up the answer in those books. This means you get more accurate and reliable information.

Hereâ€™s why RAG is a great choice:

1. **No More Misinformation or Hallucination**: RAG ensures your LLM doesnâ€™t make things up. It retrieves real information, enhancing the reliability of your results. To understand how it works and see examples, weâ€™ll provide references to documents for easy interpretation
2. **Budget-Friendly**: RAG saves you money. Itâ€™s cheaper to use and keeps your LLM up-to-date with the latest information without constant expensive training.
3. **Easy Updates:** When you need to fix or update things, [RAG](__GHOST_URL__/llms-rag-the-missing-storage-layer-for-ai-28ded35fa984) makes it simple. Itâ€™s like adding or removing books from a library, not rewriting a whole new book.

## **The Role of LanceDB: Your Superpowered Database**

In the realm of data management, [LanceDB](https://github.com/lancedb/lancedb) emerges as a pioneering vector database thatâ€™s tailor-made for developers. It offers a unique proposition: It's serverless, It has Python data ecosystem & native javascript support, zero management overhead, developer-friendly features, and open-source accessibility. This lightweight solution seamlessly scales from development environments to production, all while being astoundingly cost-effective, and priced up to 100x cheaper than alternatives

Think of Lancedb as the engine that makes RAG run smoothly. Itâ€™s like having a supercharged search engine for your data. Hereâ€™s why Lancedb is a game-changer:

- **Find What You Need Fast**: Lancedb helps you find the right information quickly and accurately, making your LLM-powered applications more efficient.
- **Grows with You**: As your data grows, Lancedb can handle it without slowing down. It scales up easily.
- **Handles All Kinds of Data**: Whether itâ€™s text, images, or other types of data, Lancedb can handle it all, making it a flexible tool for many tasks.

By using RAG with Lancedb, you can give your LLM-powered systems the ability to provide accurate and up-to-date information, all while keeping your costs in check.

In summary, RAG and Lancedb are like the dynamic duo of cost-effective, reliable LLM-powered applications. They help your systems give you better, more trustworthy information without the complexity or high costs. This means you can stay competitive and provide top-notch service in todayâ€™s data-driven world.

By storing embeddings in LanceDB, we open up exciting possibilities, such as building chatbots that can engage with PDF documents.

## **Enough theory! Now itâ€™s time to code**

Follow along with Google Colab
[

Google Colab

![](https://ssl.gstatic.com/colaboratory-static/common/005460c8a91a7de335dec68f82b6f6e5/img/favicon.ico)

![](https://colab.research.google.com/img/colab_favicon_256px.png)
](https://colab.research.google.com/drive/1-KLXbVZjxOtA_B5EjWWlcHOTM477Fka2?usp=sharing)
letâ€™s shift our focus to the coding aspect. First, we need to install the required packages:

Install some important packages

    !pip install --quiet langchain
    !pip install --quiet -U lancedb
    !pip install pypdf
    !pip install sentence-transformers
    !pip install unstructured
    !pip install ctransformers

let's import the packages

    import lancedb
    import re
    import pickle
    import requests
    import zipfile
    from pathlib import Path
    from langchain.document_loaders import UnstructuredHTMLLoader
    from langchain.embeddings import OpenAIEmbeddings
    from langchain.embeddings import HuggingFaceEmbeddings
    from langchain.document_loaders import PyPDFLoader, DirectoryLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    import lancedb
    from langchain.llms import CTransformers
    from langchain.chains import RetrievalQA
    from langchain.vectorstores import LanceDB
    from langchain.document_loaders import TextLoader
    from langchain.text_splitter import CharacterTextSplitter

The next step is to attach the Google Drive to the collab notebook, where your PDFs are present as well as the LLM model weight.

In this example, two PDF files are placed in a folder named â€œ**data**â€ within Google Drive.

    #mount the drive
    from google.colab import drive
    drive.mount('/content/drive')

Letâ€™s load the embedding model

The **all-mpnet-base-v2** model provides the best quality, while **all-MiniLM-L6-v2** is 5 times faster and still offers good quality. you can try different models from [here](https://huggingface.co/models?library=sentence-transformers)

we are using all -MiniLM-L6-V2 models for this demo, & we are using CPU for inference

    #load embedding model

    embeddings_mini = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',
                                       model_kwargs={'device': 'cpu'})

effortlessly you can manage multiple PDFs.
download the sample PDFs from [here](https://github.com/akashAD98/llam2_with_lancedb_/tree/main/multiple_pdf) & give the path of the PDF folder

    #Load multiple pdfs

    pdf_folder_path = '/content/drive/llm/pdf_paths/'

    from langchain.document_loaders import PyPDFDirectoryLoader
    loader = PyPDFDirectoryLoader(pdf_folder_path)
    docs = loader.load()

for a single PDF, you can use the below format

    # load single pdf
    from langchain.document_loaders import PyPDFLoader
    loader = PyPDFLoader("/content/drive/llm/pdfs/layoutparser.pdf")
    docs = loader.load_and_split()

we are using it is [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) its configured to split text into smaller chunks for efficient processing, with a chunk size of 200 characters and a chunk overlap of 50 characters. These parameters can be adjusted to fine-tune the splitting process for optimal results.

    # Now that we have our raw documents loaded, we need to pre-process them to generate embeddings:

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=200, # 500
        chunk_overlap=50, #100 play with this numbers for better results
    )
    documents = text_splitter.split_documents(docs)
    embeddings = embeddings_mini

let's use our Vector store LanceDB

we are connecting to LanceDB, creating a table, and setting up a document search.

    db = lancedb.connect('/tmp/lancedb')
    table = db.create_table("pdf_search", data=[
        {"vector": embeddings.embed_query("Hello World"), "text": "Hello World", "id": "1"}
    ], mode="overwrite")
    docsearch = LanceDB.from_documents(documents, embeddings, connection=table)

Now, as we are using the Llama 2 quantized model, you need to download and upload it to your Google Drive. You can choose different versions based on your system specifications. For demonstration purposes, we are using a small model, but feel free to explore other options by downloading the models from [Thebloke](https://huggingface.co/TheBloke/Llama-2-7B-GGML) repo.

    #Loading the model
    def load_llm():
        # Load the locally downloaded model here
        llm = CTransformers(
            model = "llama-2-7b-chat.ggmlv3.q2_K.bin",
            model_type="llama",
            max_new_tokens = 512,
            temperature = 0.6
        )
        return llm

    qa = RetrievalQA.from_chain_type(llm=load_llm(), chain_type="stuff", retriever=docsearch.as_retriever())

Finally, you can ask the questions to your llm & it will give you a response

    query = "What is DIA?"
    qa.run(query)

    query = "What is LayoutParser ?"
    qa.run(query)

Feel free to explore the power of LLMs on your own data with thisGoogle Colab
[

Google Colab

![](https://ssl.gstatic.com/colaboratory-static/common/005460c8a91a7de335dec68f82b6f6e5/img/favicon.ico)

![](https://colab.research.google.com/img/colab_favicon_256px.png)
](https://colab.research.google.com/drive/1-KLXbVZjxOtA_B5EjWWlcHOTM477Fka2?usp=sharing)
**Summary :**
Throughout our journey, weâ€™ve achieved two significant milestones: first, crafting a chatbot capable of working with single documents, and second, advancing to the realm of multi-document chatbots with the ability to remember previous interactions. Along the way, weâ€™ve delved into the intricacies of embeddings, vector storage, and fine-tuning parameters for our chatbotâ€™s efficiency.

Stay tuned for upcoming blogs where weâ€™ll take a deeper dive into the captivating realm of Large Language Models (LLMs). If youâ€™ve found our exploration enlightening, weâ€™d greatly appreciate your support. Be sure to leave a like!

But thatâ€™s not all. For even more exciting applications of vector databases and Large Language Models (LLMs), be sure to explore the** **[**LanceDB**](https://github.com/lancedb/lancedb) repository. LanceDB offers a powerful and versatile vector database that can revolutionize the way you work with data.

Explore the full potential of this cutting-edge technology by visiting the [vector-recipes](https://github.com/lancedb/vectordb-recipes). Itâ€™s filled with real-world examples, use cases, and recipes to inspire your next project. We hope you found this journey both informative and inspiring. Cheers!
```

---

## ðŸ“„ æ–‡ä»¶: drafts\create-llm-apps-using-rag.md

---

```md
---
title: "Create LLM Apps Using RAG"
date: 2024-03-21
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/create-llm-apps-using-rag/preview-image.png
meta_image: /assets/blog/create-llm-apps-using-rag/preview-image.png
description: "Get about create llm apps using rag. Get practical steps, examples, and best practices you can use now."
---

## **Understanding the Limitations of ChatGPT and LLMs**

ChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.

Imagine requesting the model to enhance your company policies. ChatGPTs and other large language models might need more training on your company's data to provide factual responses in such scenarios. Instead, they may generate nonsensical or irrelevant, unhelpful responses. How can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.
![](https://lh7-us.googleusercontent.com/PadHKVHGh32-TBWZLbezkZ16yIE_F9HQDZPBdJphbFNo-WBGWvu44HmRGGj5hd8rGbUn81CHW-3v0sKt-QW0oRxETR7Bkz3FFWBjmI4-9C3gkwltmNlkFYlkIykNzH3UPlOZj4UIq8bGDNHZFCOzwUc)
## **What is RAG?**

RAG, or Retrieval Augmented Generation, uses three main workflows to generate and give a better response

- Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that machines can understand.
- LLM: The retrieved data is then presented to the LLM or Large Language Model, along with the user's query. The LLM uses this new knowledge and training data to generate the response.
- Response: Finally, the LLM generates a more accurate and relevant response since it has been augmented with the retrieved information. We gave the LLM some additional information from our Knowledge library, which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answers.

Let's take the example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now, if someone asks anything specific to the policies, the bot can pull the most recent policy documents from the knowledge library, pass the relevant context to a well-crafted prompt, and then pass the prompt further to the LLM for generating the response.

To make it easier, Imagine an LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science, from Politics to Philosophy. Now, picture yourself asking this friend a few questions:

- "Who handles my laundry on weekends?"
- "Who lives next door to me?"
- "What brand of peanut butter do I prefer?"

Chances are, your friend wouldn't be able to answer these questions. Most of the time, no. But let's say this distant friend becomes closer to you over time; he comes to your place regularly, knows your parents very well, you both hang out pretty often, you go on outings, blah blah blah.. You got the point.

I mean, he is gaining access to personal and insider information about you. Now, when you pose the same questions, he can somehow answer those questions more relevantly now because he is better suited to your personal insights.

Similarly, when provided with additional information or access to your data, an LLM won't guess or hallucinate. Instead, it can leverage that access data to provide more relevant and accurate answers.

## **Here are the steps to create any RAG application...**

1. Extract the relevant information from your data sources.

2. Break the information into small chunks.

3. Store the chunks as their embeddings into a vector database.

4. Create a prompt template which will be fed to the LLM with the query and the context.

5. Convert the query to its relevant embedding using the same embedding model.

6. Fetch k number of relevant documents related to the query from the vector database.

7. Pass the relevant documents to the LLM and get the response.

## **FAQs**

1. We will be using [Langchain](https://python.langchain.com/docs/get_started/introduction) for this task. Basically, it's like a wrapper that lets you talk and manage your LLM operations better. Note that Langchain is updating very fast, and some functions and other classes might move to different modules. So, if something doesn't work, just check if you are importing the libraries from the right sources!

2. We will also use [Hugging Face](https://huggingface.co/), an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use HuggingFace, we need an access token, which you get [here](https://huggingface.co/docs/hub/security-tokens).

3. we'll need two critical components for our models: an LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we'll utilize open-source models to ensure accessibility for everyone.

4. Now, we need a Vector Database to store our embeddings. We've got [LanceDB](https://lancedb.com/) for that task â€“it's like a super-smart data lake for handling lots of information. It's a top-notch vector database, making it the go-to choice for dealing with complex data like vector embeddings... And the best part? It won't burn a dent in your pocket because it's open-source and free to use!!

5. Our data ingestion process will use a URL and some PDFs to keep things simple. While you can incorporate additional data sources if needed, we'll concentrate solely on these two.

With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we're all set to go! This way, we will save some bucks while still having everything we need. Let's move to the next steps.

## **Environment Setup**

I am using a MacBook Air M1, and it's important to note that specific dependencies and configurations may vary depending on your system type. Now open your favorite editor, create a Python environment, and install the relevant dependencies.

    # Create a virtual environment
    python3 -m venv env

    # Activate the virtual environment
    source env/bin/activate

    # Upgrade pip in the virtual environment
    pip install --upgrade pip

    # Install required dependencies
    pip3 install lancedb langchain langchain_community prettytable sentence-transformers huggingface-hub bs4 pypdf pandas

    # This is optional, I did it for removing a warning
    pip3 uninstall urllib3
    pip3 install 'urllib3<2.0'

Create a .env file in the same directory to place your Hugging Face API credentials like this.

    HUGGINGFACEHUB_API_TOKEN = hf_........

Ensure the name ***HUGGINGFACEHUB_API_TOKEN***remains unchanged, which is crucial for authentication purposes.

If you prefer a straightforward approach without relying on external packages or file loading, you can directly configure the environment variable within your code like this.

    HF_TOKEN = "hf_........."
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = HF_TOKEN

Finally, a data folder will be created in the project's root directory, designated as the central repository for storing PDF documents. You can add some sample PDFs for testing purposes; for instance, I am using the [Yolo V7](https://arxiv.org/pdf/2207.02696.pdf) and [Transformers](https://arxiv.org/abs/1706.03762) paper for demonstration. It's important to note that this designated folder will function as our primary source for data ingestion.

Everything is in order, and we're all set!

## **Step 1: Extracting the relevant information**

To get your RAG application running, the first thing we need to do is extract the relevant information from the various data sources. Whether it's a website page, a PDF file, a notion link, or a Google Doc, whatever it is, it needs to be extracted from its original source first.

    import os
    from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoader

    # Put the token values inside the double quotes
    HF_TOKEN = "hf_......"
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = HF_TOKEN

    # Loading the web URL and data
    url_loader = WebBaseLoader("https://gameofthrones.fandom.com/wiki/Jon_Snow")
    documents_loader = DirectoryLoader('data', glob="./*.pdf", loader_cls=PyPDFLoader)

    # Creating the instances
    url_docs = url_loader.load()
    data_docs = documents_loader.load()

    # Combining all the data that we ingested
    docs = url_docs + data_docs

This will ingest all the data from the URL link and the PDFs.

## **Step 2: Breaking the information into smaller chunks**

We have all the necessary data for developing our RAG application. Now, it's time to break down this information into smaller chunks. Later, we'll utilize an embedding model to convert these chunks into their respective embeddings. But why is this important?

Think of it like this: If you're tasked with digesting a 100-page book all at once and then asked a specific question about it, it would be challenging to retrieve the necessary information from the entire book to provide an answer. However, if you're permitted to break the book into smaller, manageable chunksâ€”say ten pages eachâ€”and each chunk is labeled with an index from 0 to 9, the process becomes much more straightforward. When the same question is posed after this breakdown, you can easily locate the relevant chunk based on its index and then extract the information needed to answer the question accurately.

Picture the book as your extracted information, with each 10-page segment representing a small chunk of data and the index pages as the embedding. We'll apply an embedding model to these chunks to transform the information into their respective embeddings. While, as humans, we may not directly comprehend or relate to these embeddings, they serve as numeric representations of the chunks of our application.  This is how you can do this in Python

    from langchain.text_splitter import RecursiveCharacterTextSplitter

    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
    chunks = text_splitter.split_documents(docs)

Now, the chunk_size parameter specifies the maximum number of characters a chunk can contain, while the chunk_overlap parameter specifies the number of characters that should overlap between two adjacent chunks. With the chunk_overlap set to 50, the last 50 characters of the adjacent chunks will be shared with each other.

This approach helps to prevent important information from being split across two chunks, ensuring that each chunk contains sufficient contextual information of their neighbor chunks for the subsequent processing or analysis.

Shared information at the boundary of neighboring chunks enables a more seamless transition and understanding of the text's content. The best strategy for choosing the *chunk_size *and *chunk_overlap* parameters largely depends on the documents' nature and the application's purpose.

## **Step 3: Create the embeddings and store in a vector database**

There are two primary methods to generate embeddings for our text chunks. The first involves downloading a model, managing preprocessing, and conducting computations independently. Alternatively, we can leverage Hugging Face's model hub, which offers a variety of pre-trained models for various NLP tasks, including embedding generation.

Opting for the latter approach allows us to utilize one of Hugging Face's embedding models. With this method, we simply provide our text chunks to the chosen model, saving us from the resource-intensive computations on our local machines. ðŸ’€

Hugging Face's model hub provides numerous options for embedding models, and you can explore the [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) to select the most suitable one for your requirements. For now, we'll proceed with "sentence-transformers/all-MiniLM-L6-v2." This model is fast and highly efficient in our task!!

    from langchain_community.embeddings import HuggingFaceEmbeddings

    embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'
    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})

Here's a way to see the number of embeddings for each chunk

    query = "Hello I want to see the length of the embeddings for this document."
    print(len(embeddings.embed_documents([query])[0]))

    384

We have the embeddings for our chunks; now, we need a vector database to store them.

When it comes to vector databases, there are plenty of options out there to suit various needs. Databases like Pinecone offer adequate performance and advanced features but come with a hefty price tag. On the other hand, open-source alternatives like FAISS or Chroma may lack some extras but are more than sufficient for those who don't require extensive scalability.

But wait, I am dropping a bomb here. I've recently come across LanceDB, an open-source vector database similar to FAISS and Chroma. What makes LanceDB stand out is not just its open-source nature but also its unparalleled scalability. In fact, after a closer look, I realized that I hadn't done justice to highlighting LanceDB's true value propositions earlier!!

Surprisingly, LanceDB is the most scalable vector database available, outperforming even the likes of Pinecone, Chroma, Qdrant, and others. Scaling up to a billion vectors locally on your laptop is only achievable with LanceDB. This capability is a game-changer, especially when you compare it to other vector databases struggling even with a hundred million vectors. LanceDB manages to offer this unprecedented scalability at a fraction of the cost; they are offering the utilities and database tools at much cheaper rates than its closest counterparts.

So now, We'll create an instance of the LanceDB vector database by calling

    lancedb.connect ("lance_database")

 This line essentially sets up a connection to the LanceDB database named "*lance_database*" Next, we create a table within the database named "*rag_sample*" using the create_table function. Now we initialized this table with a single data entry which includes a numeric vector generated by the embed_query function.

So, the text "Hello World" is first converted to its numeric representation (fancy name of embeddings), and then mapped to `id` number 1. Like a key-value pair. Lastly, the mode="overwrite" parameter ensures that if the table "rag_sample" already exists, it will be overwritten with the new data.

This happens with all the text chunks, and it's pretty straightforward.

    import lancedb
    from langchain_community.vectorstores import LanceDB

    db = lancedb.connect("lance_database")
    table = db.create_table(
    	"rag_sample",
    	data=[
        	{
            	"vector": embeddings.embed_query("Hello World"),
            	"text": "Hello World",
            	"id": "1",
        	}
    	],
    	mode="overwrite",
    )

    docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)

**NO ROCKET SCIENCE, HA!**

## **Step 4: Create a prompt template, which will be fed to the LLM**

Okay, now comes the prompt template. When you write a question to the ChatGPT, and it answers that question, you are providing a prompt to the model so that it can understand what the question is. When companies train the models, they decide what kind of prompt they will use to invoke the model and ask the question.

For example, if you are working with "Mistral 7B instruct" and you want the optimal results, it's recommended to use the following chat template:

    <s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]

Note that \<s> and \</s> are special tokens to represent the beginning of the string (BOS) and end of the string (EOS), while [INST] and [/INST] are regular strings. It's just that the Mistral 7B instruct is made so that the model looks for those unique tokens to understand the question better. Different types of LLMs have various kinds of instructed prompts.

Now, for our case, we will use [huggingfaceh4/zephyr-7b-alpha](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha), a text generation model. Just to make it clear, Zephyr-7B-Î± has not been aligned or formatted to human preferences with techniques like RLHF (Reinforcement Learning with Human Feedback) or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).

Instead of writing a Prompt of our own, I will use the *ChatPromptTemplate* class, which creates a prompt template for the chat models. In layman's terms, instead of writing a specified prompt, I am letting *ChatPromptTemplate* do it for me. Here is an example prompt template generated from the manual messages.

    from langchain_core.prompts import ChatPromptTemplate

    chat_template = ChatPromptTemplate.from_messages(
    	[
        	("system", "You are a helpful AI bot. Your name is {name}."),
        	("human", "Hello, how are you doing?"),
        	("ai", "I'm doing well, thanks!"),
        	("human", "{user_input}"),
    	]
    )

    messages = chat_template.format_messages(name="Bob", user_input="What is your name?")

If you don't want to write the manual instructions, you can use the *from_template* function to generate a more generic prompt template I used for this project. Here it is.

    from langchain_core.prompts import ChatPromptTemplate

    template = """
    {query}
    """

    prompt = ChatPromptTemplate.from_template(template)

Our prompt is set! We've crafted a single message, assuming it's from a human xD. If you're not using the *from_messages* function, the ChatPromptTemplate will ensure your prompt works seamlessly with the language model by reserving some additional system messages. There's always room for improvement with more generic prompts to achieve better results. For now, this setup should work.

## **Step 5: Convert the query to its relevant embedding using the same embedding model.**

Now, let's discuss the query or question we want to ask our RAG application. We can't just pass the query to our model and expect information in return. Instead, we need to pass the query through the same embedding model used for the chunks earlier. Why is this important? By embedding queries, we allow models to compare them efficiently with previously processed chunks of text. This enables tasks like finding similar documents or generating relevant responses.

To understand it better, Imagine you and your friend speak different languages, like English and Hindi, and you need to understand each other's writings. If your friend hands you a page in Hindi, you won't understand it directly. So, your friend will translate it first, turning Hindi into English for you. So now, if your friend asks you a question in Hindi, you can easily translate that question into English first and look for the relevant answers in that translated English Text.

Similarly, we initially transformed textual information into its corresponding embeddings. Now, when you pose a query, it undergoes a similar conversion into numeric form using the same embedding model applied previously to process our textual chunks. This consistent approach allows for efficient retrieval of relevant responses.

## **Step 6: Fetch K number of documents.**

Now, let's talk about the retriever. Its job is to dive into the vector database and search for relevant documents. It returns a set number, let's call it "k", of these documents, which are ranked based on their contextual relevance to the query or question you asked. You can set "k" as a parameter, indicating how many relevant documents you want - whether it's 2, 5, or 10. Generally, if you have a smaller amount of data, it's best to stick with a lower "k", around 2. For longer documents or larger datasets, a "k" between 10 and 20 is recommended.

Different [search techniques](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore) can be employed to fetch relevant documents more effectively and quickly from a vector database. The choice depends on various factors, such as your specific use case, the amount of data you have, what kind of vector database you use, and the context of your problem.

    retriever = docsearch.as_retriever(search_kwargs={"k": 3})
    docs = retriever.get_relevant_documents("what did you know about Yolo V7?")
    print(docs)

When you run this code, the retriever will fetch the three most relevant documents from the vector database. These documents will be the contexts for our LLM model to generate the response for our query.

## **Step 7: Pass the relevant documents to the LLM and get the response.**

So far, we've asked our retriever to fetch a set number of relevant documents from the database. We need a language model (LLM) to generate an appropriate response based on that context. To ensure robustness, let's remember that at the beginning of this blog, I mentioned that LLMs like ChatGPT can sometimes generate irrelevant responses, especially when asked about specific use cases or contexts. However, this time, we're providing the context from our data to the LLM as a reference. So, this reference and its general capabilities will be considered when answering the question. That's the whole idea behind using RAG!

Now, let's dive into implementing the language model (LLM) aspect of our RAG setup. We'll be using the Zephyr model architecture from the Hugging Face Hub. Here's how we do it in Python:

    from langchain_community.llms import HuggingFaceHub

    # Model architecture
    llm_repo_id = "huggingfaceh4/zephyr-7b-alpha"
    model_kwargs = {"temperature": 0.5, "max_length": 4096, "max_new_tokens": 2048}
    model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)

In this code excerpt, we initialize our language model using the Hugging Face Hub. Specifically, we select the Zephyr 7 billion model, which is placed in this repository ID: [huggingfaceh4/zephyr-7b-alpha](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha). Choosing this model isn't arbitrary; as I said before, it's based on its suitability for our specific task and requirements. We have already implemented only open-source components, so Zephyr 7 billion works well enough to generate a useful response with minimal overhead and low latency.

This model comes with some additional parameters to fine-tune its behavior. We've set the temperature to 0.5, which controls the randomness of the generated text. As a lower temperature tends to result in more conservative and predictable outputs, and when the temperature is set to max, which is 1, the model tries to be as creative as it can, so based on what type of output you want for your use case, you can tweak this parameter. For simplicity and demonstration purposes, I set it to 0.5 to ensure we get decent results. Next is the max_length parameter, which defines the maximum length of the generated text and includes the size of your prompt and the response.

max_new_tokens sets the threshold on the maximum number of new tokens generated. As a general rule of thumb, the max_new_tokens should always be less than or equal to the max_length parameter. Why? Think about it.

## **Step 8: Create a chain for invoking the LLM.**

We have everything we want for our RAG application. The last thing we need to do is create a chain for invoking the LLM on our query to generate the response. There are different types of chains for different types of use cases. If you like your LLM to remember the context of the chat over time like the ChatGPT, you would need a memory instance that can be shared among multiple conversation pieces. For such cases, there are conversational chains available.

For now, we just need a chain that can combine our retrieved contexts and pass it with the query to the LLM to generate the response.

    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.runnables import RunnablePassthrough

    rag_chain = (
    	{"context": retriever,  "query": RunnablePassthrough()}
    	| prompt
    	| model
    	| StrOutputParser()
    )
    response = rag_chain.invoke("Who killed Jon Snow?")

We have our Prompt, model, context, and the query! All of them are combined into a single chain. It's what all the chains do! Now, before running the final code, I want to give a quick check on these two helper functions:

1. RunnablePassthrough()
2. StrOutputParser()

The *RunnablePassthrough* class in *LangChain* serves to pass inputs unchanged or with additional keys. In our chain, a prompt expects input in the form of a map with keys "context" and "question." However, user input only includes the "question." or the "query".  Here, *RunnablePassthrough* is utilized to pass the user's question under the "question" key while retrieving the context using a retriever. It just ensures that the input to the prompt conforms to the expected format.

Second, *StrOutputParser* is typically employed in RAG chains to parse the model's output into a human-readable string. In layman's terms, it is responsible for transforming the model's output into a more coherent and grammatically correct sentence, which is generally better readable by Humans! That's it!

## **D-Day**

![](https://lh7-us.googleusercontent.com/qF59U_HImv4SZ3d6tdqMJNtfYSf12aEaF63p94MAkOpBzB1hGlSZ4JZw6ZfEEkqGd5i3--eRi_mR7K1aDFrVWkmzx6VkL2z0kawnSbCK2HcxHzG3lkeExbGAXhbr15bFD1ahVIztXrvYraI_Swn7-BU)
To ensure we get the entire idea even if the response gets cut off, I've implemented a function called *get_complete_sentence()*. This function helps extract the last complete sentence from the text. So, even if the response hits the maximum token limit that we set and gets truncated midway, we will still get a coherent understanding of the message.

For testing, I suggest storing some low-sized PDFs in your project's data folder. You can choose PDFs related to various topics or domains you want the chatbot to interact with. Additionally, providing a URL as a reference for the chatbot can be helpful for testing. For example, you could use a Wikipedia page, a research paper, or any other online document relevant to your testing goals. During my testing, I used a URL containing information about Jon Snow from Game of Thrones, PDFs of Transformers paper, and the YOLO V7 paper to evaluate the bot's performance. Let's see how our bot performs in varied content.

    import os
    import time
    import lancedb
    from langchain_community.vectorstores import LanceDB

    from langchain_community.llms import HuggingFaceHub
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import LanceDB
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoader
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables import RunnablePassthrough
    from pretty table import PrettyTable

    HF_TOKEN = "hf*********"
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = HF_TOKEN

    # Loading the web URL and breaking down the information into chunks
    start_time = time.time()

    loader = WebBaseLoader("https://gameofthrones.fandom.com/wiki/Jon_Snow")
    documents_loader = DirectoryLoader('data', glob="./*.pdf", loader_cls=PyPDFLoader)

    # URL loader
    url_docs = loader.load()

    # Document loader
    data_docs = documents_loader.load()

    # Combining all the information into a single variable
    docs = url_docs + data_docs

    # Specify chunk size and overlap
    chunk_size = 256
    chunk_overlap = 20
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = text_splitter.split_documents(docs)

    # Specify Embedding Model
    embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'
    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})

    # Specify Vector Database
    vectorstore_start_time = time.time()
    database_name = "LanceDB"
    db = lancedb.connect("src/lance_database")
    table = db.create_table(
    	"rag_sample",
    	data=[
        	{
            	"vector": embeddings.embed_query("Hello World"),
            	"text": "Hello World",
            	"id": "1",
        	}
    	],
    	mode="overwrite",
    )
    docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)
    vectorstore_end_time = time.time()

    # Specify Retrieval Information
    search_kwargs = {"k": 3}
    retriever = docsearch.as_retriever(search_kwargs = {"k": 3})

    # Specify Model Architecture
    llm_repo_id = "huggingfaceh4/zephyr-7b-alpha"
    model_kwargs = {"temperature": 0.5, "max_length": 4096, "max_new_tokens": 2048}
    model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)

    template = """
    {query}
    """

    prompt = ChatPromptTemplate.from_template(template)

    rag_chain_start_time = time.time()
    rag_chain = (
    	{"context": retriever, "query": RunnablePassthrough()}
    	| prompt
    	| model
    	| StrOutputParser()
    )
    rag_chain_end_time = time.time()

    def get_complete_sentence(response):
    	last_period_index = response.rfind('.')
    	if last_period_index != -1:
        	return response[:last_period_index + 1]
    	else:
        	return response

    # Invoke the RAG chain and retrieve the response
    rag_invoke_start_time = time.time()
    response = rag_chain.invoke("Who killed Jon Snow?")
    rag_invoke_end_time = time.time()

    # Get the complete sentence
    complete_sentence_start_time = time.time()
    complete_sentence = get_complete_sentence(response)
    complete_sentence_end_time = time.time()

    # Create a table
    table = PrettyTable()
    table.field_names = ["Task", "Time Taken (Seconds)"]

    # Add rows to the table
    table.add_row(["Vectorstore Creation", round(vectorstore_end_time - vectorstore_start_time, 2)])
    table.add_row(["RAG Chain Setup", round(rag_chain_end_time - rag_chain_start_time, 2)])
    table.add_row(["RAG Chain Invocation", round(rag_invoke_end_time - rag_invoke_start_time, 2)])
    table.add_row(["Complete Sentence Extraction", round(complete_sentence_end_time - complete_sentence_start_time, 2)])

    # Additional information in the table
    table.add_row(["Embedding Model", embedding_model_name])
    table.add_row(["LLM (Language Model) Repo ID", llm_repo_id])
    table.add_row(["Vector Database", database_name])
    table.add_row(["Temperature", model_kwargs["temperature"]])
    table.add_row(["Max Length Tokens", model_kwargs["max_length"]])
    table.add_row(["Max New Tokens", model_kwargs["max_new_tokens"]])
    table.add_row(["Chunk Size", chunk_size])
    table.add_row(["Chunk Overlap", chunk_overlap])
    table.add_row(["Number of Documents", len(docs)])

    print("\nComplete Sentence:")
    print(complete_sentence)

    # Print the table
    print("\nExecution Timings:")
    print(table)

    +------------------------------+----------------------------------------+
    |         	Task         	|      	Time Taken (Seconds)      	|
    +------------------------------+----------------------------------------+
    | 	Vectorstore Creation 	|             	16.21              	|
    |   	RAG Chain Setup    	|              	0.03              	|
    | 	RAG Chain Invocation 	|              	2.06              	|
    | Complete Sentence Extraction |              	0.0               	|
    |   	Embedding Model    	| sentence-transformers/all-MiniLM-L6-v2 |
    | LLM (Language Model) Repo ID | 	huggingfaceh4/zephyr-7b-alpha  	|
    |   	Vector Database    	|            	LanceDB             	|
    |     	Temperature      	|              	0.5               	|
    |  	Max Length Tokens   	|              	4096              	|
    |    	Max New Tokens    	|              	2048              	|
    |      	Chunk Size      	|              	256               	|
    |    	Chunk Overlap     	|               	20               	|
    | 	Number of Documents  	|               	39               	|
    +------------------------------+----------------------------------------+

To enhance readability and present the execution information in a structured tabular format, I have used the *PrettyTable* library. You can add it to your virtual environment using the command *`pip3 install prettytable`*.

So this is the response I received in less than < 1 minute, which is quite considerable for the starters. The time it takes can vary depending on your system's configuration, but you'll get decent results in just a few minutes. So, please be patient if it's taking a bit longer.

    Human:
    Question: Who killed Jon Snow?

    Answer:
    In the TV series Game of Thrones, Jon Snow was stabbed by his
    fellow Night's Watch members in season 5, episode 9,
    "The Dance of Dragons." However, he was later resurrected by Melisandre
    in season 6, episode 3, "Oathbreaker." So, technically,
    no one killed Jon Snow in the show.

![](https://lh7-us.googleusercontent.com/993Lv_uQr6ZxbiI--2cQTLJZMSw8jC-RKZzy1AU-k6Jmh9Xau6hXVCs1D8SbNF2SSV22OAUknd_2kbvkd0ZjUXdNTjhkPmPBM2m6RgDeV4Hscil4D1QO5YF_Fgf-iiMxJOnng2GG-G0Vf_bzhPJP7bU)
Have fun experimenting with various data sources! You can try changing the website addresses, adding new PDF files, or changing the template a bit. LLMs are fun; you never know what you'll get!

Here is the Google Colab link for reference.
[

Google Colab

![](https://ssl.gstatic.com/colaboratory-static/common/005460c8a91a7de335dec68f82b6f6e5/img/favicon.ico)

![](https://colab.research.google.com/img/colab_favicon_256px.png)
](https://colab.research.google.com/drive/1YsOfovVdNPBwCDMWHvLfOaNtqXn4qXTs?usp=sharing&amp;ref=blog.lancedb.com)
## **What's next?**

There are plenty of things we can adjust here. We could switch to a more effective embedding model for better indexing, try different searching techniques for the retriever, add a reranker to improve document ranking, or use a more advanced LLM with a larger context window and faster response times. Based on these factors, every RAG application is just an enhanced version. However, the fundamental concept of how an RAG application works remains the same.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\customer-support-bot-rasa-x-lancedb.md

---

```md
---
title: "Customer Support Bot : RASA X LanceDB"
date: 2024-12-31
author: Rithik Kumar
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/customer-support-bot-rasa-x-lancedb/preview-image.png
meta_image: /assets/blog/customer-support-bot-rasa-x-lancedb/preview-image.png
description: "Unlock about customer support bot : rasa x lancedb. Get practical steps, examples, and best practices you can use now."
---

Have you ever wondered how businesses manage to provide instant, accurate, and personalized customer support around the clock? This article will teach us how to make an **Advanced Customer Support Chatbot** using **Rasa**, **LanceDB**, and **OpenAIâ€™s Large Language Models (LLMs)**.

## What is RASA?

**Rasa** is an open-source framework designed to build intelligent, contextual, and scalable chatbots and virtual assistants. Unlike some one-size-fits-all solutions, Rasa offers the flexibility to customize your botâ€™s behavior, making it perfectly tailored to your business needs.

1. **Open-Source Goodness:** Free to use and highly customizable.
2. **Natural Language Understanding (NLU):** Rasa interprets user inputs to identify intents and extract relevant entities, enabling the chatbot to understand the purpose behind each query.
3. **Dialogue Management:** Rasa manages the flow of conversation, maintaining context across multiple interactions and ensuring coherent and context-aware responses.
4. **Custom Actions:** Through its `actions.py` file, Rasa executes custom actions that perform specific tasks, such as connecting with databases, APIs, and other services like LanceDB and OpenAIâ€™s LLMs.

## Quick Overview

This guide explains the process of building an **Advanced Customer Support Chatbot** by integrating **Rasa**, a robust conversational framework; **LanceDB**, a high-performance vector database; and **OpenAIâ€™s LLM**, a state-of-the-art language model.
![](__GHOST_URL__/content/images/2024/12/NLU--2--1.jpg)How RASA performs custom actions to do the query on Lance Table and then perform API call to LLM to generate refined response
So, youâ€™ve got Rasa, LanceDB, and OpenAI LLMs ready to join forces. How do these components work together to create a seamless customer support chatbot? Letâ€™s break it down:

1. **User Interaction:**
- A customer sends a query to the chatbot, such as *â€œHow do I reset my password?â€*

2. **Rasa NLU and Core:**
- Rasa NLU interprets the intent and extracts relevant entities (if any) and then Rasa core decides if it's time to trigger custom action or give direct response.

3. **Fetching Knowledge from LanceDB:**
- Using the intent and entities, Rasa triggers a custom action that queries LanceDB for specific support information related to user query (password resets).

4. **Generating a Smart Response with OpenAI:**
- The retrieved information from LanceDB is then fed into OpenAIâ€™s LLM, which crafts a comprehensive and personalized response for the customer.

5. **Delivering the Response:**
- The chatbot sends the generated response back to the customer, ensuring they receive clear and helpful guidance.

This integration ensures that your chatbot is not only responsive but also intelligent, capable of handling a wide array of customer inquiries with ease.

## Let's begin

Now we are clear about the flow of this project, let's delve into coding it. Make sure you have OPENAI_API key for this project.

#### Install dependencies

    pip install rasa lancedb openai==0.28 python-dotenv -q

If installing on colab you will need to restart the session after installing the packages - ***ctrl + M.***

#### Initialize a new Rasa project which sets up the necessary directory structure and files.

    rasa init --no-prompt

This command sets up the basic directory structure with sample data.  To verify the setup, you can train the initial model and interact with the sample bot:

    rasa train
    rasa shell

#### Create a .env file and store necessary environment variables in it

    OPENAI_API_KEY = "sk-*********"

#### Step 1 - Embed all the customer support data in LanceDB which can be queried by RASA custom actions later

**LanceDB** serves as the knowledge base for the chatbot, storing FAQs and support information. The following steps outline the setup process:

    # Import necessary libraries
    import os
    import subprocess
    import time
    import threading
    import lancedb
    import requests
    import json
    from lancedb.pydantic import LanceModel, Vector
    from lancedb.embeddings import get_registry

    # Initialize LanceDB
    db = lancedb.connect("./content/lancedb")  # Local storage within Colab

    # Initialize the language model for generating embeddings
    model = get_registry().get("sentence-transformers").create(name="BAAI/bge-small-en-v1.5", device="cpu")

    # Create table from schema
    class Documents(LanceModel):
        vector: Vector(model.ndims()) = model.VectorField()
        content: str = model.SourceField() # Field to store the actual content/response

    company_support_data = [
        { "content": "To reset your password, navigate to the login page and click on 'Forgot Password'. You'll receive an email with instructions to create a new password." },
        { "content": "To update your account information, log in to your profile and click on 'Edit Profile'. From there, you can change your email, phone number, and other personal details." }
        #,.... rest of the data
    ]

    # Knowledge data from the data above
    knowledge_data = company_support_data

    # Define table name
    table_name = "knowledge_base"

    # Retrieve existing table names
    existing_tables = db.table_names()

    if table_name not in existing_tables:
        # Create a new table with the schema and insert data
        tbl = db.create_table(table_name, schema=Documents)
        tbl.add(knowledge_data)
        print(f"Created new table '{table_name}' and inserted data.")
    else:
        # Append data to the existing table
        table = db.open_table(table_name)
        table.add(knowledge_data, mode="overwrite")
        print(f"Overwrited data to the existing table '{table_name}'.")

We will be using pre-trained **Sentence Transformer model** (BAAI/bge-small-en-v1.5) to convert textual support data into vector embeddings. You can change it accordingly.

The code above will create a table - "knowledge_base" in the DB and insert all the customer support information in this table.

#### Step 2 - Configure RASA files according to our use case

- **domain.yml** - The domain.yml file serves as the core configuration for your Rasa chatbot. It defines the chatbotâ€™s intents, entities, slots, responses, actions, forms, and policies.

    version: "3.0"
    language: "en"
    intents:
      - greet
      - ask_knowledge
      - goodbye

    entities:
      - project
      - service
    responses:
      utter_greet:
        - text: "Hello! How can I assist you today?"
      utter_goodbye:
        - text: "Goodbye! Have a great day!"
        - text: "Bye! Let me know if you need anything else."
        - text: "See you later! Feel free to reach out anytime."
    actions:
      - action_search_knowledge

- **endpoints.yml** - It specifies the URLs and connection details for services like the custom action server (actions.py), enabling Rasa to communicate with it.

    action_endpoint:
      url: "http://localhost:5055/webhook"

- **data/stories.yml** - The stories.yml file contains training stories that define example conversational paths your chatbot can take.

    version: "3.0"
    stories:
      - story: Greet and ask question
        steps:
          - intent: greet
          - action: utter_greet
          - intent: ask_knowledge
          - action: action_search_knowledge

      - story: ask question
        steps:
          - intent: ask_knowledge
          - action: action_search_knowledge

      - story: Goodbye
        steps:
          - intent: goodbye
          - action: utter_goodbye

      - story: greet and goodbye
        steps:
          - intent: greet
          - action: utter_greet
          - intent: goodbye
          - action: utter_goodbye

- **data/rules.yml **- The rules.yml file defines rule-based conversations that specify exact steps the chatbot should follow in certain situations. Unlike stories, rules are strict paths that Rasa should follow without deviation.

    version: "3.0"
    rules:
      - rule: Greet
        steps:
          - intent: greet
          - action: utter_greet

      - rule: Goodbye
        steps:
          - intent: goodbye
          - action: utter_goodbye

      - rule: Answer Knowledge Questions
        steps:
          - intent: ask_knowledge
          - action: action_search_knowledge

- **data/nlu.yml** - The nlu.yml file contains Natural Language Understanding (NLU) training data. It includes examples of user inputs categorized by intents and annotated with entities to train Rasaâ€™s NLU component.

    version: "3.0"
    nlu:
      - intent: greet
        examples: |
          - hello
          - hi
          - hey
          - good morning
          - good evening
          - greetings

      - intent: goodbye
        examples: |
          - bye
          - goodbye
          - see you later
          - catch you later
          - see ya
          - take care

      - intent: ask_knowledge
        examples: |
          - I need help with my account
          - Can you assist me with billing?
          - How do I reset my password?
          - I'm facing issues with my order
          - Tell me about your support services
          - How can I contact customer service?
          - What are your support hours?
          - I have a question about Project Alpha
          - Help me understand Project Beta
          - How can I track my purchase?

- **config.yml** - The config.yml file defines the pipeline and policies used by Rasa for processing natural language inputs and managing dialogue workflows.

    version: "3.0"
    language: "en"
    pipeline:
    - name: WhitespaceTokenizer
    - name: RegexFeaturizer
    - name: LexicalSyntacticFeaturizer
    - name: CountVectorsFeaturizer
    - name: CountVectorsFeaturizer
      analyzer: char_wb
      min_ngram: 1
      max_ngram: 4
    - name: DIETClassifier
      epochs: 100
    - name: EntitySynonymMapper
    - name: ResponseSelector
      epochs: 100

    policies:
    - name: RulePolicy
    - name: UnexpecTEDIntentPolicy
      max_history: 5
      epochs: 100
    - name: TEDPolicy
      max_history: 5
      epochs: 100
    assistant_id: 20241227-151505-young-attachment

#### Step 3 - Implement Custom Actions (actions.py) file

- **actions/actions.py file** - The actions.py file is where you define custom actions for your Rasa chatbot. Custom actions are Python functions that can execute arbitrary logic, here we will query LanceDB database and then call OpenAI api for refined/personalized response.

Let's breakdown and see how to implement actions.py

- **Import necessary libraries and functions**
- **Loading Environment Variables:** Utilizes `python-dotenv` to securely load sensitive information such as the OpenAI API key from a `.env` file.

    from typing import Any, Text, Dict, List
    from rasa_sdk import Action, Tracker
    from rasa_sdk.executor import CollectingDispatcher
    import lancedb
    import logging
    from google.colab import userdata
    import openai
    import os
    from dotenv import load_dotenv

    # Load environment variables from .env
    load_dotenv()

    # Configure logging
    logger = logging.getLogger(__name__)
    logging.basicConfig(level=logging.INFO)

    class ActionSearchKnowledge(Action):
        def name(self) -> Text:
            return "action_search_knowledge"

We will implement 3 functions in this class

- **Initialization (`__init__`):**
- Sets up the OpenAI API key.
- Establishes a connection to LanceDB and accesses the `knowledge_base` table.

        def __init__(self):

            # Initialize OpenAI API key from environment variables
            self.openai_api_key = os.getenv("OPENAI_API_KEY")
            if not self.openai_api_key:
                logger.error("OpenAI API key not found. Please set OPENAI_API_KEY in your environment.")
            openai.api_key = self.openai_api_key

            # Initialize LanceDB connection once
            try:
                self.db = lancedb.connect("./content/lancedb")
                self.table_name = "knowledge_base"
                if self.table_name not in self.db.table_names():
                    logger.error(f"Table '{self.table_name}' does not exist in LanceDB.")
                    self.table = None
                else:
                    self.table = self.db.open_table(self.table_name)
                    logger.info(f"Connected to table '{self.table_name}' in LanceDB.")
            except Exception as e:
                logger.error(f"Error connecting to LanceDB: {e}")
                self.table = None

- **Run Method (`run`):**
- **Get User Message:** Retrieve user message from the tracker.
- **Knowledge Retrieval:** Performs a semantic search in LanceDB to find the most relevant piece of information based on the userâ€™s query.
- **Response Generation:** Sends the retrieved information to generate_response(...) or gives direct response decided by relevance of the user's query on LanceDB table.
- **Error Handling:** Gracefully manages any errors by informing the user and logging the issue for further investigation.

      def run(self, dispatcher: CollectingDispatcher,
            tracker: Tracker,
            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:

        # Get the latest user message
        user_message = tracker.latest_message.get('text')
        logger.info(f"User message: {user_message}")

        if not user_message:
            dispatcher.utter_message(text="Sorry, I didn't catch that. Could you please repeat?")
            return []

        try:
            # Perform similarity search in LanceDB
            query_result = self.table.search(user_message).limit(1).to_pandas()

            # Filter results based on the _distance parameter (smaller _distance means more similar)
            relevant_content = [query_result.loc[0, "content"] if query_result.loc[0, "_distance"] < 0.65 else None][0]
            response_text = "Null"

            # If we find relevant content , sent it to LLM or Else send automated reply
            if not relevant_content == None:
                logger.info(f"Retrieved answer from knowledge base.")
                # Use OpenAI to generate a more refined response
                response_text = self.generate_response(user_message, relevant_content)
            else:
                # If user has ask not a relevant question, reply with the following
                response_text = "I'm sorry, I don't have an answer to that question."
                logger.info(f"No matching content found in knowledge base.")

            # Send the answer back to the user
            dispatcher.utter_message(text=response_text)

        except Exception as e:
            logger.error(f"Error during search operation: {e}")
            dispatcher.utter_message(text="Sorry, something went wrong while processing your request.")

        return []

- **`generate_response` Method:**
- Constructs a prompt combining the userâ€™s question and the relevant knowledge base content.
- Calls OpenAIâ€™s API to generate a refined response.
- Implements a fallback mechanism in case of API failures.

      def generate_response(self, user_message: Text, relevant_content: Text) -> Text:
        """
        Use OpenAI's API to generate a refined response based on user message and relevant content.
        """
        try:
            system_prompt = "You are an company support assistant that provides helpful and accurate answers based on the provided information. You talk professionally and like a customer support executive."

            prompt = (
                f"User Question: {user_message}\n"
                f"Relevant Information: {relevant_content}\n\n"
                f"Provide a detailed and helpful response to the user's question based on the relevant information above."
            )

            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=450,
                temperature=0.7,
            )

            generated_text = response.choices[0].message['content'].strip()
            logger.info("Generated response using OpenAI API.")
            return generated_text

        except Exception as e:
            logger.error(f"Error generating response with OpenAI API: {e}")
            return relevant_content  # Fallback to relevant content if OpenAI fails

#### Step 4 - Training RASA Model

After setting up the configuration and integrating necessary components, the next step is to train the Rasa model.

    rasa train

- The training process compiles the `nlu.yml`, `stories.yml`, and other configuration files to create a model that can interpret user intents, extract entities, and manage dialogue flows.
- A confirmation message indicates the successful creation of the model, typically stored in the `models` directory.

#### Step 5 - Run Rasa Server and Action Server

To operationalize the chatbot, both the **Rasa Server** and the **Action Server** must be running concurrently. In environments like **Google Colab**, where running multiple persistent processes is challenging, leveraging Pythonâ€™s threading capabilities facilitates simultaneous server execution.

    rasa run
    rasa run actions

Action server runs on port `5055` while the Rasa server run on port `5005`. Make sure they are free otherwise error might come.

### Step 6 - Interact with the bot and ask it questions

    # Function to send messages to the Rasa server
    def send_message(message):
        url = "http://localhost:5005/webhooks/rest/webhook"
        payload = {
            "sender": "test_user",
            "message": message
        }
        headers = {
            "Content-Type": "application/json"
        }
        try:
            response = requests.post(url, data=json.dumps(payload), headers=headers)
            return response.json()
        except requests.exceptions.ConnectionError:
            return {"error": "Could not connect to Rasa server."}

    # Example interactions
    print("User: Hi")
    assistant_response = send_message("Hi")
    if assistant_response:
        for resp in assistant_response:
            if isinstance(resp, dict) and "text" in resp:
                print("Assistant:", resp["text"])
            else:
                print("Assistant:", resp)

    print("\nUser: How do I reset my password? Explain in french")
    assistant_response = send_message("How do I delete my account? Explain in french")
    if assistant_response:
        print("Assistant:", end = " ")
        for resp in assistant_response:
            if isinstance(resp, dict) and "text" in resp:
                print(resp["text"])
            else:
                print(resp)

## Colab Walkthrough

[

Google Colab

![](__GHOST_URL__/content/images/icon/favicon-18.ico)

![](__GHOST_URL__/content/images/thumbnail/colab_favicon_256px-18.png)
](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/RASA_Customer-support-bot/main.ipynb)
## Conclusion

Congratulations! ðŸŽ‰ Youâ€™ve just navigated through the exciting process of building an **Advanced Customer Support Chatbot** using **Rasa**, **LanceDB**, and **OpenAIâ€™s LLM**. By integrating these powerful tools, youâ€™ve created a chatbot that delivers accurate, timely, and personalized support to the customer.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\customer-support-bot-with-openais-swarm-agent.md

---

```md
---
title: "Customer Support Bot with OpenAI's Swarm Agent"
date: 2024-11-04
author: LanceDB
categories: ["Community"]
draft: true
featured: false
image: /assets/blog/customer-support-bot-with-openais-swarm-agent/preview-image.png
meta_image: /assets/blog/customer-support-bot-with-openais-swarm-agent/preview-image.png
description: "Master about customer support bot with openai's swarm agent. Get practical steps, examples, and best practices you can use now."
---

ðŸ’¡

This is a community blog by Prashant Kumar

Difference between an assistant and a Swarm Agent- an assistant is a simple  sequential program without function calls. Agent Swarm resembles a modern modular program where specific tasks are managed by dedicated functions.

### What is OpenAI Swarm?

OpenAI Swarm is a framework for efficiently managing multiple AI agents. Its most crucial aspects are simplicity and control. It is designed to ***Create, Coordinate, and Control*** a group of agents that can share tasks and decisions in the simplest form.

Swarm focuses on two key ideas: ***Agents***, which perform specific functions, and ***Handoffs***, which allow one agent to pass a task to another based on the situation. This setup makes building flexible and complex AI systems with straightforward logic easier.

OpenAI Swarm is built for developers and researchers interested in exploring multi-agent systems. It is particularly well-suited for those working on AI assistants and task automation. While still, "*OpenAIâ€™s Swarm is not intended for production use, this experimental framework for multi-agent systems could potentially revolutionize how tasks are distributed and executed in various industries".*

Swarm is designed for specific real-world use cases:

1. **Customer Support Bots:** Different agents handle inquiries like billing and technical issues, passing complex questions to the right specialist.
2. **Personal Shopper: **Agents that can help with making sales and refunding orders.

and many more ...

In this article, we'll explore the implementation of a Customer Support bot. We'll utilize OpenAI's data and build a support bot around it to answer user queries using Swarm agents.

### Implementation

[

Google Colab

![](https://ssl.gstatic.com/colaboratory-static/common/8f9c73a58195bb6a1915ebabd5eedb19/img/favicon.ico)

![](https://colab.research.google.com/img/colab_favicon_256px.png)
](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/assistance-bot-with-swarm/assitant_bot_with_swarm.ipynb)
To implement a customer support bot for OpenAI using data in JSON format and ingesting it into a LanceDB table, here's a simple data ingestion pipeline:

    import pandas as pd
    import lancedb
    from lancedb.pydantic import LanceModel, Vector
    from lancedb.embeddings import get_registry

    from datasets import load_dataset

    EMBEDDING_MODEL = "text-embedding-ada-002"

    # load dataset
    dataset_name = "Prasant/openai-dataset"
    dataset = load_dataset(dataset_name)
    article_df = pd.DataFrame(dataset["train"])

    # ingest data inside table
    db = lancedb.connect("/tmp/db")
    func = get_registry().get("openai").create(name=EMBEDDING_MODEL)

    class Article(LanceModel):
        text: str = func.SourceField()
        vector: Vector(func.ndims()) = func.VectorField()
        title: str
        url: str

    table = db.create_table("help-center", schema=Article, mode="overwrite")
    table.add(article_df)

Now we have data ingested in the table. To create a customer service bot, we need two agents: a user Interface agent and a help center agent, both equipped with different tools. Weâ€™ll use the helper function `run_demo_loop` to set up an interactive Swarm session.

1. **User Interface Agent:** Manages initial user interactions and directs users to the help center agent based on their needs.
2. **Help Center Agent:** Offers detailed help and support, using various tools and integrating with a LanceDB VectorDB for retrieving documentation.

Here's a sample code for it

    import re

    import lancedb
    from swarm import Agent
    from swarm.repl import run_demo_loop

    db = lancedb.connect("/tmp/db")
    table = db.open_table("help-center")

    def query_lancedb(query, top_k=5):
        # Creates embedding vector from user query
        query_results = table.search(query).limit(top_k).to_pandas()

        return query_results

    def query_docs(query):
        """Query the knowledge base for relevant articles."""
        print(f"Searching knowledge base with query: {query}")
        query_results = query_lancedb(query)
        output = []

        print(query_results)
        for index, article in query_results.iterrows():
            output.append((article['title'], article['text'], article['url']))

        if output:
            title, content, _ = output[0]
            response = f"Title: {title}\nContent: {content}"
            truncated_content = re.sub(
                r"\s+", " ", content[:50] + "..." if len(content) > 50 else content
            )
            print("Most relevant article title:", truncated_content)
            return {"response": response}
        else:
            print("No results")
            return {"response": "No results found."}

    def send_email(email_address, message):
        """Send an email to the user."""
        response = f"Email sent to: {email_address} with message: {message}"
        return {"response": response}

    def submit_ticket(description):
        """Submit a ticket for the user."""
        return {"response": f"Ticket created for {description}"}

    def transfer_to_help_center():
        """Transfer the user to the help center agent."""
        return help_center_agent

    user_interface_agent = Agent(
        name="User Interface Agent",
        instructions="You are a user interface agent that handles all interactions with the user. Call this agent for general questions and when no other agent is correct for the user query.",
        functions=[transfer_to_help_center],
    )

    help_center_agent = Agent(
        name="Help Center Agent",
        instructions="You are an OpenAI help center agent who deals with questions about OpenAI products, such as GPT models, DALL-E, Whisper, etc.",
        functions=[query_docs, submit_ticket, send_email],
    )

    if __name__ == "__main__":
        run_demo_loop(user_interface_agent)

Here we are ready to start asking questions from Swarm agents, and this is how its results will look like
![](__GHOST_URL__/content/images/2024/10/Screenshot-from-2024-10-24-11-47-07.png)
However Swarm is still experimental, its lightweight design allows it to manage complex workflows with multiple agents effectively.

## Comparison with Other Multi-Agent Frameworks

Swarm is unique in its simplicity and modular design. Compared to other frameworks like AutoGPT or LangChain, Swarm focuses more on real-time agent coordination and handoffs, rather than complex pre-defined agent behavior. While LangChain is great for RAG tasks and large-scale data handling, Swarm excels in specific task orchestration, where agents interact and respond dynamically based on real-time conditions.

Checkout for example code and Drop us a ðŸŒŸ
[

GitHub - lancedb/vectordb-recipes: High quality resources & applications for LLMs, multi-modal models and VectorDBs

High quality resources &amp; applications for LLMs, multi-modal models and VectorDBs - GitHub - lancedb/vectordb-recipes: High quality resources &amp; applications for LLMs, multi-modal models andâ€¦

![](https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg)GitHublancedb

![](https://opengraph.githubassets.com/5fd0da5be4838f8b45a2dbf566a9dcdd1117f99dad9fdfc09ee089650415e6a3/lancedb/vectordb-recipes)
](https://github.com/lancedb/vectordb-recipes)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\demystifying-llm-apps-with-lance-c6d90b066698.md

---

```md
---
title: "Demystifying LLM Apps with Lance"
date: 2023-05-22
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/demystifying-llm-apps-with-lance-c6d90b066698/preview-image.png
meta_image: /assets/blog/demystifying-llm-apps-with-lance-c6d90b066698/preview-image.png
description: "Go from transcripts to a working LLM Q&A app using Lance for storage and search. The walkthrough covers data prep, embeddings, indexing, prompt wiring, and a simple evaluation loop."
---

Thereâ€™s a lot of justifiable excitement around using LLMs as the basis for building a new generation of smart apps. While tools like LangChain or GPT-Index makes it easy to create prototypes, itâ€™s useful to look behind the curtain to see what the wizard looks like before taking these apps into production. In this post, weâ€™ll show you how to build a Q&A chatbot using just base-level OSS python tooling, OpenAI, and Lance.

**tl;dr** â€” if you just want to play with the worked example, you can find a [notebook on the lance github](https://github.com/eto-ai/lance/blob/main/notebooks/youtube_transcript_search.ipynb).

## Goal

Weâ€™ll build a question-answer bot with answers drawn from youtube transcripts. The result is going to be a Jupyter Notebook where you can get the answer to a question you ask AND it shows the top matching YouTube video starting at the relevant point.

## Data prep

To make things simple, weâ€™ll just use a ready made dataset of YouTube transcripts on HuggingFace:

    from datasets import load_dataset
    data = load_dataset('jamescalam/youtube-transcriptions', split='train')

This dataset has 208619 transcript sentences (across 700 videos)

    Dataset({
        features: ['title', 'published', 'url', 'video_id', 'channel_id', 'id', 'text', 'start', 'end'],
        num_rows: 208619
    })

Weâ€™ll use pandas to create the context windows of size 20 with stride 4. That means every 4 sentences, it creates a â€œcontextâ€ by concatenating the next 20 sentences together. Eventually, the answers will come from finding the right context and summarizing it.

    import numpy as np
    import pandas as pd

    window = 20
    stride = 4

    def contextualize(raw_df, window, stride):
        def process_video(vid):
            # For each video, create the text rolling window
            text = vid.text.values
            time_end = vid["end"].values
            contexts = vid.iloc[:-window:stride, :].copy()
            contexts["text"] = [' '.join(text[start_i:start_i+window])
                                for start_i in range(0, len(vid)-window, stride)]
            contexts["end"] = [time_end[start_i+window-1]
                                for start_i in range(0, len(vid)-window, stride)]
            return contexts
        # concat result from all videos
        return pd.concat([process_video(vid) for _, vid in raw_df.groupby("title")])

    df = contextualize(data.to_pandas(), 20, 4)

A brief aside: itâ€™s annoying that pandas does not provide rolling window functions on non-numeric columns because `process_video` should just be simple call to `pd.DataFrame.rolling` .
![](https://miro.medium.com/v2/resize:fit:548/0*0pVh2VKPjMRt1Gg9.gif)Me finding out pandas rolling windows calls text columns â€œnuisanceâ€
With this we go from 200K+ sentences to <50K contexts

    >>> len(df)
    48935

## Embeddings

Here weâ€™ll use the OpenAI embeddings API to turn text into embeddings. There are other services like Cohere providing similar functionality, or you can run your own.

The OpenAI python API requires an API key for authentication. For details, you can refer to their [documentation](https://platform.openai.com/docs/api-reference/authentication) to see how to set it up.

The documentation *looks* super simple:

    import openai
    openai.Embedding.create(input="text", engine="text-embedding-ada-002")

But of course, the OpenAI API is almost always out of capacity, or youâ€™re being throttled, or thereâ€™s some cryptic JSON decode error. To make our lives easier, we can use the `ratelimiter` and `retry` packages in python:

    import functools
    import openai
    import ratelimiter
    from retry import retry

    embed_model = "text-embedding-ada-002"

    # API limit at 60/min == 1/sec
    limiter = ratelimiter.RateLimiter(max_calls=0.9, period=1.0)

    # Get the embedding with retry
    @retry(tries=10, delay=1, max_delay=30, backoff=3, jitter=1)
    def embed_func(c):
        rs = openai.Embedding.create(input=c, engine=embed_model)
        return [record["embedding"] for record in rs["data"]]

    rate_limited = limiter(embed_func)

And we can also request batches of embeddings instead of just one at a time

    from tqdm.auto import tqdm
    import math

    # We request in batches rather than 1 embedding at a time
    def to_batches(arr, batch_size):
        length = len(arr)
        def _chunker(arr):
            for start_i in range(0, len(df), batch_size):
                yield arr[start_i:start_i+batch_size]
        # add progress meter
        yield from tqdm(_chunker(arr), total=math.ceil(length / batch_size))

    batch_size = 1000
    batches = to_batches(df.text.values.tolist(), batch_size)
    embeds = [emb for c in batches for emb in rate_limited(c)]

## Search

Once the embeddings are created, we have to make it searchable. At this point, most existing toolchains require you to spin up a separate service or store your vectors separately from the data separately from the vector index. This is where Lance comes in. We can merge the embeddings (vectors) with the original data and write it to disk. So the vectors you need for similarity search lives with the index to make it fast lives with the data you need to filter / return to the user.

    import lance
    import pyarrow as pa
    from lance.vector import vec_to_table

    table = vec_to_table(np.array(embeds))
    combined = pa.Table.from_pandas(df).append_column("vector", table["vector"])
    ds = lance.write_dataset(combined, "chatbot.lance")

This is sufficient to do vector search in Lance

    ds.to_table(nearest={"column": "vector",
                         "q": [<query vector>],
                         "k": <how many results>}).to_pandas()

The above query retrieves the `k` rows whose `vector` is most similar to the query vector `q` .

But a brute force approach is a bit slow (~150ms). Instead, if youâ€™re going to be making a bunch of requests, you can create an ANN index on the vector column:

    ds = ds.create_index("vector", index_type="IVF_PQ",
                         num_partitions=64, num_sub_vectors=96)

If you run the same ANN search query, you should get a much faster answer now.

Because Lance is columnar, you can easily retrieve additional columns. By default, the `to_table` returns all columns plus an extra â€œscoreâ€ column. You can choose to add a `columns` argument to `to_table` to select a subset of available columns. You can also filter the ANN results by passing a SQL where clause string to the `filter` parameter in `to_table` .
![](https://miro.medium.com/v2/resize:fit:733/0*Dv6u1VYGYRgXsOVZ.jpg)Me talking about Lance
## Prompting

Unless youâ€™re already familiar with how LLM apps work, you might be asking â€œwhat do embeddings have to do with Q&A bots?â€ This is where prompt engineering comes in. There are lots of great material dedicated to prompt engineering so Iâ€™ll spare the details in this post. Here weâ€™ll just go through a practical example.

The magic happens in 3 steps:

## 1. Embed the query text

Letâ€™s say you want to ask â€œWhich training method should I use for sentence transformers when I only have pairs of related sentences?â€ Since weâ€™ve converted all the transcripts into embeddings, weâ€™ll also do the same with the query text itself

    query = ("Which training method should I use for sentence transformers "
             "when I only have pairs of related sentences?")
    openai.Embedding.create(input=query, engine="text-embedding-ada-002")

## 2. Search for most similar context

We then use the query vector to find the most similar context

    context = ds.to_table(
            nearest={
                "column": "vector",
                "k": 3,
                "q": query_vector
            }).to_pandas()

## 3. Create a prompt for the OpenAI completion API

LangChain and similar tools have lots of Prompt templates for your use case. For the task at hand, we create our own

            "Answer the question based on the context below.\n\n"+
            "Context:\n"

And we plug in the text from step 2 above into the context. We then use OpenAI again to get the answer

    def complete(prompt):
        # query text-davinci-003
        res = openai.Completion.create(
            engine='text-davinci-003',
            prompt=prompt,
            temperature=0,
            max_tokens=400,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
            stop=None
        )
        return res['choices'][0]['text'].strip()

An example usage would look like:

    >> query = "who was the 12th person on the moon and when did they land?"
    >> complete(query)
    'The 12th person on the moon was Harrison Schmitt, and he landed on December 11, 1972.'

## Putting it all together

Remember our question was, â€œWhich training method should I use for sentence transformers when I only have pairs of related sentences?â€ Using the steps outlined above, I get back:

â€œNLI with multiple negative ranking lossâ€

And since Lance allows you to retrieve additional columns easily, I can not only show the most relevant YouTube video, but start it at the right place in the video:
![](https://miro.medium.com/v2/resize:fit:770/1*vy3vdkL1aYOtH_a0OF1s_g.png)
## Conclusions

In this post, we saw how to use Lance as a critical component to power an LLM-based app. In addition, we went through an end-to-end workflow peeling back the covers. For these types of search workflows, Lance is a great fit because 1) itâ€™s super easy to use for ANN, 2) itâ€™s columnar so you can add a ton of additional features, and 3) it has [lightning fast random access](https://medium.com/etoai/benchmarking-random-access-in-lance-ed690757a826) speed so the index can be disk-based and extremely scalable.

If youâ€™d like to give it a shot, you can find the [Lance repo on github](https://github.com/eto-ai/lance). If you like us, weâ€™d really appreciate a â­ï¸ and your feedback!

Special thanks to Rob Meng for inspiring this post
```

---

## ðŸ“„ æ–‡ä»¶: drafts\evaluate-rag-app-on-2.md

---

```md
---
title: "End to End Evaluation Template for RAG Apps"
date: 2025-02-22
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/evaluate-rag-app-on-2/preview-image.png
meta_image: /assets/blog/evaluate-rag-app-on-2/preview-image.png
description: "Explore about end to end evaluation template for rag apps. Get practical steps, examples, and best practices you can use now."
---

ðŸ’¡

This is a community post by Mahesh Deshwal

## Comprehensive Evaluation Metrics for RAG Applications Using LanceDB

In the world of Retrieval-Augmented Generation (RAG) applications, evaluating the performance and reliability of models is critical. Evaluation metrics play a crucial role in assessing and enhancing the performance of models and their it is a continuous, iterative process.

This involves two main types of evaluations: Offline and Online. Each type uses a variety of metrics to assess the system's effectiveness and quality. In this blog post, we'll delve deeply into these evaluation methods and metrics, with a particular focus on integrating LanceDB, a specialized vector database, to enhance performance.

## Understanding RAG Evaluation Metrics

**Offline Evaluation** is perfect for the early days of model development when you're testing features against pre-set data. It's like training wheels for your modelâ€Š-â€Šnecessary, but not great for showing off. It is conducted in a controlled environment where models are tested against benchmark datasets. This method leverages a variety of metrics to assess the effectiveness and quality of the system components, including the Embedding Model, Vector Database, Rerankers, and Chunking Strategy.

### Key Metrics for Offline Evaluation

**Effectiveness Metrics**

1. **Hit Rate**: Measures the frequency with which relevant documents are retrieved.
2. **NDCG (Normalized Discounted Cumulative Gain)**: Evaluates the usefulness of ranked retrieval results.
3. **MRR (Mean Reciprocal Rank)**: Assesses the rank position of the first relevant document.
4. **Precision@K and Recall@K**: Quantify the system's ability to retrieve relevant documents among the top K results.

**Quality Metrics**

1. **Fluency and Complexity**: Ensure the generated text is linguistically fluent and appropriately complex for the target audience.
2. **Perplexity**: Indicates how well the model predicts a sample, with lower values suggesting better performance.
3. **BERTScore, BLEU, ROUGE, METEOR**: These metrics compare the generated responses with reference answers, evaluating their similarity and overlap in terms of word choice and sequence.
4. **Groundedness and Hallucination Rate**: Groundedness ensures responses are based on factual information, while the hallucination rate checks for unfounded or incorrect information in the response.
5. **Toxicity, Context Adherence, and Faithfulness**: Assess whether the responses are non-toxic, adhere closely to the context provided, and are factually accurate.

**Online Evaluation** is executed in real time, focusing on the system's operational performance and user interaction issues. This evaluation helps identify and address live problems such as slow response times, API failures, and system reliability under load.

### Key Metrics for Online Evaluation

1. **API Failure Rate**: Monitors the frequency of API errors or failures, which is crucial for maintaining system reliability.
2. **Latency**: Measures the time taken for the system to respond to requests, a key performance indicator in user experience.
3. **Throughput and Load Resistance**: Evaluate the system's capacity to handle a large number of requests simultaneously without degrading performance.
4. **Context Retrieval Efficiency**: Assesses the speed and accuracy with which relevant context is retrieved, impacting the overall response quality.

**Detailed Overview of Evaluation Classes and Metrics**

The evaluation process for RAG applications involves several classes, each focusing on different evaluation aspects:

**IOGuards**: Essential for safeguarding against prompt injections and ensuring that the model does not produce harmful or sensitive outputs. This class uses various models to detect issues like toxicity, bias, polarity, and sentiment in queries, contexts, and responses.

- Harmful Content Detection: Uses sentence embeddings to detect harmful queries or responses.
- Pattern Detection: Employs regex to reveal sensitive information patterns, such as credit card numbers or email addresses.
- Toxicity and Sentiment Analysis: Utilizes pre-trained models to assess the sentiment and potential toxicity of inputs and outputs.
- Polarity and Bias Detection: Evaluates the polarity of responses and detects any inherent biases.
- Prompt Injection Classification: Identifies and mitigates risks of prompt injection attacks using classification models.

Follow along with the below code
[

vectordb-recipes/applications/evaluate_RAG at main Â· lancedb/vectordb-recipes

High quality resources & applications for LLMs, multi-modal models and VectorDBs - lancedb/vectordb-recipes

![](__GHOST_URL__/content/images/icon/pinned-octocat-093da3e6fa40-5.svg)GitHublancedb

![](__GHOST_URL__/content/images/thumbnail/vectordb-recipes)
](https://github.com/lancedb/vectordb-recipes/tree/main/applications/evaluate_RAG)
    class IOGuards:
        def __init__(self):
            """
            A huge data for injection, harmful prompts is given at: https://github.com/verazuo/jailbreak_llms/tree/main/data
            """
            with st.spinner("Loading `IO Guard` model..."):
                self.sim_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

                self.harmful_in = self.sim_model.encode(["How to commit suicide",
                                                    "How to make bomb",
                                                    "how to do harm to someone something",
                                                    "how to kill someone", "how to hack a website router etc"])

                self.harmful_out = self.sim_model.encode(["This is how you can commit suicide",
                                                "There are multiple Ways to make bombs such as",
                                                "You can kill someone by using",
                                                "Ways to hack a website are given "])

                self.refusal = self.sim_model.encode(["I'm very sorry, but I cannot provide information or assistance on . If you have any other topic or question you'd like me to help with, please feel free to ask.",
                                            "Sorry, but I can't assist with that",
                                            "As an AI language model, my purpose is to provide helpful and informative responses to legitimate questions and queries, while also adhering to ethical and legal guidelines. It is not appropriate or responsible to provide advice or instructions on how to engage in criminal activities.",
                                            "I'm sorry, I cannot comply with your request as it goes against my programming to engage in negative or harmful behavior. My purpose is to assist and provide helpful responses."
                                            "I don't have the answer", "I can't answer that", "This can't be answered",
                                            "I don't the answer is present in the context"
                                            ])

            with st.spinner("Compiling`Guard Regexes` ..."):
                self.regexes = {
                    "credit": re.compile("\b((4\d{3}|5[1-5]\d{2}|2\d{3}|3[47]\d{1,2})[\s\-]?\d{4,6}[\s\-]?\d{4,6}?([\s\-]\d{3,4})?(\d{3})?)\b"),
                    "email" : re.compile("\b[a-z0-9._%\+\-â€”|]+@[a-z0-9.\-â€”|]+\.[a-z|]{2,6}\b"),
                    "ipv4": re.compile("\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b"),
                    "ipv6" : re.compile("\b([\d\w]{4}|0)(\:([\d\w]{4}|0)){7}\b")
                    }

            with st.spinner("Loading `Toxic Guard` model ..."):
                self.toxic_tokenizer = AutoTokenizer.from_pretrained("martin-ha/toxic-comment-model")
                self.toxic_model = AutoModelForSequenceClassification.from_pretrained("martin-ha/toxic-comment-model")
                self.toxic_pipeline =  TextClassificationPipeline(model=self.toxic_model, tokenizer=self.toxic_tokenizer)

            with st.spinner("Loading `Sentiment` model ..."):
                nltk.download('vader_lexicon')
                self.sentiment_analyzer = SentimentIntensityAnalyzer()

            with st.spinner("Loading `Polarity Guard` model ..."):
                self.polarity_regard = evaluate.load("regard")

            with st.spinner("Loading `Bias Guard` model ..."):
                self.bias_tokenizer = AutoTokenizer.from_pretrained("d4data/bias-detection-model")
                self.bias_model = TFAutoModelForSequenceClassification.from_pretrained("d4data/bias-detection-model")
                self.bias_pipeline = pipeline('text-classification', model = self.bias_model, tokenizer = self.bias_tokenizer)

            with st.spinner("Loading `Prompt Injection Guard` model ..."):
                self.inj_tokenizer = AutoTokenizer.from_pretrained("ProtectAI/deberta-v3-base-prompt-injection-v2")
                self.inj_model = AutoModelForSequenceClassification.from_pretrained("ProtectAI/deberta-v3-base-prompt-injection-v2")
                self.inj_classif = pipeline("text-classification", model=self.inj_model, tokenizer = self.inj_tokenizer,
                                            truncation=True, max_length=512, device = DEVICE)

        def harmful_refusal_guards(self, input, context, response, thresh = 0.8):
            resp = self.sim_model.encode(response)
            return {"harmful_query": np.any((self.sim_model.encode(input) @ self.harmful_in.T) > thresh),
            "harmful_context": np.any((self.sim_model.encode(context) @ self.harmful_out.T) > thresh),
            "harmful_response": np.any((resp @ self.harmful_out.T) > thresh),
            "refusal_response": np.any((resp @ self.refusal.T) > thresh)}

        def detect_pattern(self,output):
            """
            Help locate Phone, Email, Card, , IP, Address etc. Most useful for Output but can be used to mask info in Context and Query if using Third Part LLM
            https://help.relativity.com/RelativityOne/Content/Relativity/Relativity_Redact/Regular_expression_examples.htm
            """
            RES = {}
            for (key, reg) in self.regexes.items():
                pat = re.findall(reg, output)
                if pat: RES[key] = pat
            return RES

        def toxicity(self, input):
            """
            Can be used for Both Query and Response
            Models:
                1.  "alexandrainst/da-hatespeech-detection-small"
                2:  "martin-ha/toxic-comment-model"
            """
            return self.toxic_pipeline(input)

        def sentiment(self, text):
            """
            Can be used for Input or Output
            NOTE: This is different from the polarity below named as "regard"
            """
            return  self.sentiment_analyzer.polarity_scores(text)

        def polarity(self, input):
            if isinstance(input, str): input = [input]
            results = []
            for d in self.polarity_regard.compute(data = input)['regard']:
                results.append({l['label']: round(l['score'],2) for l in d})
            return results

        def bias(self, query):
            """
            Most needed for Response but can be used for Context and Input which might influence the Response
            """
            return self.bias_pipeline(query)

        def prompt_injection_classif(self, query):
            """
            Classification using: ProtectAI/deberta-v3-base-prompt-injection-v2
            """
            return self.inj_classif(query)

**TextStat**: Evaluates the readability and complexity of generated text. It uses traditional readability metrics to assess the fluency and understandability of outputs.

- Readability Scores: Metrics like Flesch Reading Ease, SMOG Index, and Automated Readability Index provide insights into how easily a text can be read and understood.
- Text Complexity: Measures such as average word length, sentence length, and word diversity offer a deeper understanding of text complexity.

    class TextStat():
        def __init__(self):
            """
            This metric calculates mostly the Quality of output based on traditional metrics to detect fluency, readability, simplicity etc
            """

        def calculate_text_stat(self, test_data):
            """
           To add:
                1. N-Gram (Lexical or Vocab) Diversity: Unique vs repeated %
                2. Grammatical Error %
                3. Text Avg Word Length, No of Words, No of unique words, average sentence length etc etc
            """
            return {"flesch_reading_ease":textstat.flesch_reading_ease(test_data),
            "flesch_kincaid_grade": textstat.flesch_kincaid_grade(test_data),
            "smog_index": textstat.smog_index(test_data),
            "coleman_liau_index" : textstat.coleman_liau_index(test_data),
            "automated_readability_index" : textstat.automated_readability_index(test_data),
            "dale_chall_readability_score" : textstat.dale_chall_readability_score(test_data),
            "difficult_words" : textstat.difficult_words(test_data),
            "linsear_write_formula" : textstat.linsear_write_formula(test_data),
            "gunning_fog" : textstat.gunning_fog(test_data),
            "text_standard" : textstat.text_standard(test_data),
            "fernandez_huerta" : textstat.fernandez_huerta(test_data),
            "szigriszt_pazos" : textstat.szigriszt_pazos(test_data),
            "gutierrez_polini" : textstat.gutierrez_polini(test_data),
            "crawford" : textstat.crawford(test_data),
            "gulpease_index" : textstat.gulpease_index(test_data),
            "osman" : textstat.osman(test_data)}

**ComparisonMetrics**: Invaluable for tasks like summarization and paraphrasing, providing insights into the similarity between different segments of text.

- Reference-Based Metrics: BERTScore, ROUGE, BLEU, METEOR, and BLEURT are used to compare the generated responses with reference texts, evaluating similarity and quality.
- Hallucination and Contradiction Detection: Models are employed to detect inconsistencies and unfounded claims in responses.
- String Similarity Metrics: Metrics like BM25, Levenshtein Distance, and Fuzzy Score assess syntactic similarity, useful for evaluating paraphrased or summarized content.

    class ComparisonMetrics:
        def __init__(self,):
            """
            1. Metrics which are Generation dependent and require Input and Generated
                Can be used for:
                    Query - Context, Query - Response, Response - Context

                Metrics Included:
                    BLUE, ROUGE, METEOR, BLEURT, BERTScore, Contradiction

            2. There are some metrics used for Contradiction and Hallucination detection
                Can be used with:
                    Query - Context, Query - Response, Response - Context

            3. String Comparison Metrics which are purely syntactic like BM-25 score, Levenstien Distance, Fuzzy score, Shingles
                Can be used in Paraphrasing, Summarisation etc
            """
            with st.spinner("Loading `Hallucination Detection` model ..."):
                self.hallucination_model =  CrossEncoder('vectara/hallucination_evaluation_model')

            with st.spinner("Loading `Contradiction Detection` model ..."):
                self.contra_tokenizer = AutoTokenizer.from_pretrained("MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7")
                self.contra_model = AutoModelForSequenceClassification.from_pretrained("MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7")

            with st.spinner("Loading `ROUGE` ..."): self.rouge = evaluate.load('rouge')
            with st.spinner("Loading `BLEU` ..."): self.bleu = evaluate.load("bleu")
            with st.spinner("Loading `BLEURT` ..."): self.bleurt = evaluate.load("bleurt", module_type="metric")
            with st.spinner("Loading `METEOR` ..."): self.meteor = evaluate.load('meteor')
            with st.spinner("Loading `BERTScore` ..."): self.bertscore = evaluate.load("bertscore")

        def hallucinations(self, input, response):
            return self.hallucination_model.predict([[input,response]])

        def contradiction(self, query, response):
            """
            Given a Query and Response, find it the response contradicts the query or not
                Can be used for Query - Response majorly but it useful for Context - Response and Query - Context too
            """
            input = self.contra_tokenizer(query, response, truncation=True, return_tensors="pt")
            output = self.contra_model(input["input_ids"])
            prediction = torch.softmax(output["logits"][0], -1).tolist()
            label_names = ["entailment", "neutral", "contradiction"]
            return {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}

        def ref_focussed_metrics(self, reference, response):
            if isinstance(reference, str): reference = [reference]
            if isinstance(response, str): response = [response]

            return {"bertscore": self.bertscore.compute(predictions = response, references=reference, lang="en"),
                    "rouge": self.rouge.compute(predictions = response, references=reference, use_aggregator=False),
                    "bleu": self.bleu.compute(predictions = response, references = reference, max_order=4),
                    "bleurt": self.bleurt.compute(predictions = response, references = reference),
                    "meteor": self.meteor.compute(predictions = response, references = reference)
                    }

        def string_similarity(self, reference, response):
            """
            """
            tokenized_corpus = [doc.split(" ") for doc in [reference]] # Only 1 reference is there so the whole corpus
            bm25 = BM25Okapi(tokenized_corpus) # build index

            return {"fuzz_q_ratio":fuzz.QRatio(reference, response),
            "fuzz_partial_ratio":fuzz.partial_ratio(reference, response),
            'fuzz_partial_token_set_ratio':fuzz.partial_token_set_ratio(reference, response),
            'fuzz_partial_token_sort_ratio':fuzz.partial_token_sort_ratio(reference, response),
            'fuzz_token_set_ratio':fuzz.token_set_ratio(reference, response),
            'fuzz_token_sort_ratio':fuzz.token_sort_ratio(reference, response),
            "levenshtein_distance": lev_distance(reference, response),
            "bm_25_scores" : bm25.get_scores(response.split(" ")) # not a very good indicator for QUERY but for CONTEXT versus response it can work well
            }

**AppMetrics**: Focuses on operational performance of the application, capturing metrics related to the speed and efficiency of various processes.

- Execution Times: Measures the time taken for each stage of processing, from input to output, helping identify bottlenecks.
- System Load and Failure Rates: Monitors system performance under load and tracks the frequency of failures or refusals to respond.
- Resource Usage: Evaluates the CPU, GPU, and memory usage during the application run to optimize resource allocation

    class AppMetrics:
        def __init__(self):
            """
            App specific metrics can be calculated like:
                1. Time to generate First token
                2. App failure rate
                3. How many times model denied to answer
                4. Time to taken from input to output
                5. No of requests sucessfully served
                6. No of times less than, exactly 'k' contexts recieved with >x% similarity
                7. No of times No context was found
                8. Time taken to fetc the contexts
                10. Time taken to generate the response
                11. Time taken to evaluate the metrics
                12. CPU, GPU, Memory usage
            """
            self.exec_times = {}

        @staticmethod
        def measure_execution_time(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                result = func(*args, **kwargs)
                end_time = time.time()
                execution_time = end_time - start_time
                return result, round(execution_time, 5)
            return wrapper

 **LLMasJudge**: Uses language models as evaluators, allowing for comprehensive assessments of response quality and accuracy.

- Reasoning and Correctness: Ensures responses are logically consistent and based on robust reasoning.
- Fact-Checking and Hallucination Detection: Evaluates the factual accuracy of responses, ensuring they are based on the context provided.

    class LLMasEvaluator():
        """
        Using LLM as evaluators. It can be a custom fune tuned model on your task, rubrics etc
        or it can be a bigger LLM with specified prompts. It can be used to:
            1. Find if the response is related to query, context and answers fully
            2. Find if the reasoning is correct in response AKS Hallucination detection
            3. Find if all the facts are correct and are from context only

            etc etc
        """
        def __init__(self, llm):
            assert NotImplemented("This is more like a prompting thing to LLM. Easy to do, Skipping for now")
            self.llm = llm

        def run(self,prompt):
            """
            Prompt should have your QUERY, Context, Response
            Make a custom task specific prompt to pass to get the responses depending on task to task
            """
            return self.llm(prompt)

**TraditionalPipelines**:  While not fully implemented in the current evaluation pipeline, traditional NLP tasks like Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and Topic Classification are crucial for understanding the content and structure of queries and responses.

- Topic Classification: Identifies common topics across queries, contexts, and responses, ensuring thematic consistency.
- NER and POS Tagging: Provides insights into the entities and grammatical structure of texts, useful for detailed offline analysis.

    class TraditionalPipelines():
        def __init__(self, model):
            """
            Models like NER, Topics, POS etc that are helpful in offline and online evaluations too
            """
            raise NotImplementedError("Pipeline Becomes Too heavy. Skipping for Now")
            self.topic_classif = pipeline("zero-shot-classification", model="MoritzLaurer/mDeBERTa-v3-base-mnli-xnli")

        def topics(self, input):
            """
            Apply Multi Label Topic Classification
            Helps in finding common Topics between:
                Query - Context, Query - Response, Response - Context, [Response Chunks]

            It can be done by calculating the similarity between Chunks of respponse to see whether they are talking of same thing or not
            """
            candidate_labels = ["politics", "economy", "entertainment", "environment"]
            return self.topic_classif(input, candidate_labels, multi_label = True)

        def NER(self, input):
            """
            Apply NER for inputs. Helps in finding common entities and their distribution among:
                Query - Context, Query - Response, Response - Context
            """
            pass

        def POS(self, input):
            """
            Add POS tagging. Not very useful but can be used to do analysis offline
            """
            pass

**LanceDB**: Enhancing Vector Database Operations, is integral to the RAG evaluation pipeline, serving as an efficient vector database for storing and retrieving embeddings. Its capabilities significantly enhance the performance of context retrieval processes.

**Advantages of Using LanceDB Fast Retrieval**

- LanceDB's approximate nearest neighbour (ANN) search algorithms enable rapid retrieval of relevant embeddings, crucial for real-time applications.
- Scalable Architecture: Designed to handle extensive datasets, LanceDB ensures scalability for applications with increasing data volumes.
- Seamless Integration: LanceDB easily integrates into the evaluation pipeline, facilitating efficient embedding storage and retrieval without disrupting other processes.

### Putting all the evaluations in a small app using `LanceDB` and `Streamlit`

    import streamlit as st
    from PyPDF2 import PdfReader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import LanceDB
    from langchain.chains.question_answering import load_qa_chain
    from langchain.prompts import PromptTemplate
    from langchain_openai import ChatOpenAI
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from eval_metrics import *

    if 'api_key' not in st.session_state: st.session_state['api_key'] = None
    if 'user_turn' not in st.session_state: st.session_state['user_turn'] = False
    if 'pdf' not in st.session_state: st.session_state['pdf'] = None
    if "embed_model" not in st.session_state: st.session_state['embed_model'] = None
    if "vector_store" not in st.session_state: st.session_state['vector_store'] = None
    if "eval_models" not in st.session_state: st.session_state["eval_models"] = {"app_metrics": AppMetrics()}

    st.set_page_config(page_title="Document Genie", layout="wide")

    def get_pdf_text(pdf_docs):
        text = ""
        for pdf in pdf_docs:
            pdf_reader = PdfReader(pdf)
            for page in pdf_reader.pages:
                text += page.extract_text()
        return text

    @AppMetrics.measure_execution_time
    def build_vector_store(text):
        text_splitter = RecursiveCharacterTextSplitter(chunk_size = st.session_state['chunk_size'] , chunk_overlap= st.session_state['chunk_overlap'])
        text_chunks = text_splitter.split_text(text)
        st.session_state['vector_store']= LanceDB.from_texts(text_chunks, st.session_state["embed_model"])

    @AppMetrics.measure_execution_time
    def fetch_context(query):
        return st.session_state['vector_store'].similarity_search(query, k = st.session_state['top_k'])

    def get_conversational_chain():
        prompt_template = """
        Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in
        provided context just say, "I don't think the answer is available in the context", don't provide the wrong answer\n\n
        Context:\n {context}?\n
        Question: \n{question}\n

        Answer:
        """
        model = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0, openai_api_key=st.session_state['api_key'])
        prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
        chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
        return chain

    @AppMetrics.measure_execution_time
    def llm_output(chain, docs, user_question):
        return chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)

    def user_input(user_question):
        contexts_with_scores, exec_time = fetch_context(user_question)
        st.session_state["eval_models"]["app_metrics"].exec_times["chunk_fetch_time"] = exec_time

        chain = get_conversational_chain()
        response, exec_time = llm_output(chain, contexts_with_scores, user_question)
        st.session_state["eval_models"]["app_metrics"].exec_times["llm_resp_time"] = exec_time

        st.write("Reply: ", response["output_text"])

        ctx = ""
        for item in contexts_with_scores:
            if len(item.page_content.strip()):
                ctx += f"<li>Similarity Score: {round(float(item.metadata['_distance']), 2)}<br>Context: {item.page_content}<br>&nbsp</li>"

        with st.expander("Click to see the context passed"):
            st.markdown(f"""<ol>{ctx}</ol>""", unsafe_allow_html=True)

        return contexts_with_scores, response["output_text"]

    def evaluate_all(query, context_lis, response):
        guard =  st.session_state["eval_models"]["guards"]
        stat =  st.session_state["eval_models"]["textstat"]
        comp =  st.session_state["eval_models"]["comparison"]
        context = "\n\n".join(context_lis) if len(context_lis) else "no context"

        RESULT = {}

        RESULT["guards"] = {
            "query_injection": guard.prompt_injection_classif(query),
            "context_injection": guard.prompt_injection_classif(context),
            "query_bias": guard.bias(query),
            "context_bias": guard.bias(context),
            "response_bias": guard.bias(response),
            "query_regex": guard.detect_pattern(query),
            "context_regex": guard.detect_pattern(context),
            "response_regex": guard.detect_pattern(response),
            "query_toxicity": guard.toxicity(query),
            "context_toxicity": guard.toxicity(context),
            "response_toxicity":  guard.toxicity(response),
            "query_sentiment": guard.sentiment(query),
            "query_polarity": guard.polarity(query),
            "context_polarity":guard.polarity(context),
            "response_polarity":guard.polarity(response),
            "query_response_hallucination" : comp.hallucinations(query, response),
            "context_response_hallucination" : comp.hallucinations(context, response),
            "query_response_hallucination" : comp.contradiction(query, response),
            "context_response_hallucination" : comp.contradiction(context, response),
        }

        RESULT["guards"].update(guard.harmful_refusal_guards(query, context, response))

        tmp = {}
        for key, val in comp.ref_focussed_metrics(query, response).items():
            tmp[f"query_response_{key}"] = val

        for key, val in comp.ref_focussed_metrics(context, response).items():
            tmp[f"context_response_{key}"] = val

        RESULT["reference_based_metrics"] = tmp

        tmp = {}
        for key, val in comp.string_similarity(query, response).items():
            tmp[f"query_response_{key}"] = val

        for key, val in comp.string_similarity(context, response).items():
            tmp[f"context_response_{key}"] = val

        RESULT["string_similarities"] = tmp

        tmp = {}
        for key, val in stat.calculate_text_stat(response).items():
            tmp[f"result_{key}"] = val
        RESULT["response_text_stats"] = tmp

        RESULT["execution_times"] = (st.session_state["eval_models"]["app_metrics"].exec_times)

        return RESULT

    def main():
        st.markdown("""## RAG Pipeline Example""")

        st.info("Note: This is a minimal demo focussing on ***EVALUATION*** so you can do simple Document QA which uses GPT-3.5 without any persistant memory hence no multi-turn chat is available there. If the question is out of context from the document, this will not work so ask the questions related to the document only. You can optimise the workflow by using Re-Rankers, Chunking Strategy, Better models etc but this app runs on CPU right now easily and is about, again, ***EVALUATION***", icon = "â„¹ï¸")

        st.error("WARNING: If you reload the page, everything (model, PDF, key) will have to be loaded again. That's how `streamlit` works", icon = "ðŸš¨")

        with st.sidebar:
            st.title("Menu:")
            st.session_state['api_key'] = st.text_input("Enter your OpenAI API Key:", type="password", key="api_key_input")

            _ =  st.number_input("Top-K Contxets to fetch", min_value = 1, max_value = 50, value = 3, step = 1, key="top_k")
            _ = st.number_input("Chunk Length", min_value = 8, max_value = 4096, value = 512, step = 8, key="chunk_size")
            _ = st.number_input("Chunk Overlap Length", min_value = 4, max_value = 2048, value = 64, step = 1, key="chunk_overlap")

            st.session_state["pdf"] = st.file_uploader("Upload your PDF Files and Click on the Submit & Process Button", accept_multiple_files=True, key="pdf_uploader")

            if st.session_state["pdf"]:
                if st.session_state["embed_model"] is None:
                    with st.spinner("Setting up `all-MiniLM-L6-v2` for the first time"):
                        st.session_state["embed_model"] = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

                with st.spinner("Processing PDF files..."): raw_text = get_pdf_text(st.session_state["pdf"])
                with st.spinner("Creating `LanceDB` Vector strores from texts..."):
                    _, exec_time = build_vector_store(raw_text)
                    st.session_state["eval_models"]["app_metrics"].exec_times["chunk_creation_time"] = exec_time
                    st.success("Done")

        if not st.session_state['api_key']: st.warning("Enter OpenAI API Key to proceed")
        elif not st.session_state["pdf"]: st.warning("Upload a PDf file")
        else:
            st.markdown("""#### Ask a Question from the PDF file""")
            user_question = st.text_input("", key="user_question")

            if user_question and st.session_state['api_key']:  # Ensure API key and user question are provided
                with st.spinner("Getting Response from LLM..."):
                    contexts_with_scores, response = user_input(user_question)

                st.warning("There are 5 major types metrics computed below having multiple sub metrics. Also, 2 abstract classes are defined `LLMasEvaluator` (to use any LLM as a judge) and `TraditionalPipelines` (for Topics, NER, POS etc)", icon="ðŸ¤–")
                metric_calc = st.button("Load Models & Compute Evaluation Metrics")
                if metric_calc:
                    if len(st.session_state["eval_models"]) <= 1:
                        st.session_state["eval_models"].update({
                            "guards": IOGuards(),
                            "textstat": TextStat(),
                            "comparison": ComparisonMetrics(),
                            # "llm_eval": LLMasEvaluator(),
                            # "traditional_pipeline": TraditionalPipelines(),
                            })

                    with st.spinner("Calculating all the matrices. Please wait ...."):
                        eval_result = evaluate_all(user_question, [item.page_content for item in contexts_with_scores], response)
                        st.balloons()

                    # with st.expander("Click to see all the evaluation metrics"):
                        st.json(eval_result)

    if __name__ == "__main__":
        main()

![](__GHOST_URL__/content/images/2025/02/eval.png)
Star ðŸŒŸ LanceDB recipes to keep yourself updated -
[

GitHub - lancedb/vectordb-recipes: High quality resources & applications for LLMs, multi-modal models and VectorDBs

High quality resources &amp; applications for LLMs, multi-modal models and VectorDBs - GitHub - lancedb/vectordb-recipes: High quality resources &amp; applications for LLMs, multi-modal models andâ€¦

![](__GHOST_URL__/content/images/icon/pinned-octocat-093da3e6fa40-7.svg)GitHublancedb

![](__GHOST_URL__/content/images/thumbnail/vectordb-recipes-2)
](https://github.com/lancedb/vectordb-recipes/)
## Conclusion

Evaluating RAG applications involves a multifaceted approach, employing a blend of offline and online metrics to ensure effectiveness and quality. The integration of LanceDB as the vector database enhances the system's performance, providing efficient and scalable solutions for managing embeddings. By understanding and utilizing these evaluation metrics, developers can significantly improve the performance and reliability of their RAG applications.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\geospatial-restaurant-recommendation-system.md

---

```md
---
title: "Geospatial Restaurant Recommendation System"
date: 2024-12-31
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/geospatial-restaurant-recommendation-system/preview-image.png
meta_image: /assets/blog/geospatial-restaurant-recommendation-system/preview-image.png
description: "Get about geospatial restaurant recommendation system. Get practical steps, examples, and best practices you can use now."
---

ðŸ’¡

This is a community blog by Vipul Maheshwari

![](__GHOST_URL__/content/images/2024/12/image-10.png)
In 2024, I did spend significant amount of time playing with RAG and Vector Database. Typically, LanceDB was at the core and it was one amazing hell of a ride. As the year is ending, I tried to make a simple restaurant recommendation system but it's going to be slightly different. The idea is to use this [Restaurant Dataset](https://www.kaggle.com/datasets/abhijitdahatonde/swiggy-restuarant-dataset?resource=download) to answer queries like "Is there any cafe nearby that serves Punjabi food?" or "What are the top 5 restaurants in the city that serve Chinese food?".

The twist is, that we need some form of geospatial data to mold the data points into a form that can be used for queries that are related to the distance.
![](__GHOST_URL__/content/images/2024/12/Geospatial-Recommendation-System.png)
### How to start?

Well, the first thing is to sort out some relevant features that are required for the recommendation system. So generally, folks who are searching for any recommendation for a restaurant, are looking out for things like the restaurant's name, location, and what kind of food they serve, or other details like ratings or maybe how much time it takes for a restaurant to deliver the food or what's the average order prices.  To follow along, here is the colab
[

Google Colab

![](__GHOST_URL__/content/images/icon/favicon-19.ico)

![](__GHOST_URL__/content/images/thumbnail/colab_favicon_256px-19.png)
](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Geospatial-Recommendation-System/geospatial-recommendation.ipynb)
So, we need to extract these features from the dataset.

    import pandas as pd

    restaurant_data = pd.read_csv("data.csv")
    restaurant_data = restaurant_data[restaurant_data.columns[1:]]
    restaurant_data.dropna(inplace=True)
    restaurant_data.drop_duplicates(inplace=True)
    restaurant_data.head()

![](__GHOST_URL__/content/images/2024/12/image-7.png)
To keep things simple, weâ€™re just going to focus on a few key details for now: the type of food a restaurant serves, how customers rate it on average, and where it's located. By sticking to these basics, we can quickly give people great recommendations without complicating things.

    data_points_vectors = []

    for _, row in restaurant_data.iterrows():
        filter_cols = ['Food type', 'Avg ratings', 'Address']
        data_point = "#".join(f"{col}/{row[col]}" for col in filter_cols)
        data_points_vectors.append(data_point)

    # Add the new column to the DataFrame
    restaurant_data["query_string"] = data_points_vectors

You can see that I've used `#` to separate different sections and `/` for splitting up the key-value pairs. Just a heads up, you can pick different separators and delimiters if you like, but since I'm using FTS (full-text search) from LanceDB, a few are reserved for internal representations. If you need to, you can use a backslash as a prefix to support the reserved ones and still use them.

This is how a single query string looks :

    'Food type/Biryani,Chinese,North Indian,South Indian#Avg ratings/4.4#Address/5Th Block'

Ok, this looks good! Next, we need to turn our query string into a vector. You can choose any embedding model that fits your needs, but I'll be using the `paraphrase-MiniLM-L6-v2` model for now. Basically, all we have to do is encode our query strings into vectors and then load up the payload with the relevant information.

    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

    list_of_payloads = []

    for index, row in restaurant_data.iterrows():
        encoded_vector = model.encode(row['query_string'])
        payload = {
            'Area': row['Area'],
            'City': row['City'],
            'Restaurant': row['Restaurant'],
            'Price': row['Price'],
            'Avg_ratings': row['Avg ratings'],
            'Total_ratings': row['Total ratings'],
            'Food_type': row['Food type'],
            'Address': row['Address'],
            'Delivery_time': row['Delivery time'],
            'query_string': row['query_string'],
            'vector': encoded_vector
        }

        list_of_payloads.append(payload)

### Setting up Vector Database

So, we've got our `list_of_payloads` that includes all the relevant data we're going to store in our vector database. Letâ€™s get LanceDB set up here:

    import lancedb

    # Connect to the LanceDB instance
    uri = "data"
    db = lancedb.connect(uri)

    lancedb_table = db.create_table("restaurant-geocoding-app", data=list_of_payloads)
    lancedb_df = lancedb_table.to_pandas()
    lancedb_df.head()

![](__GHOST_URL__/content/images/2024/12/image-8.png)
### Setting up the Geospatial Reference.

Now that our vector database is ready, the next step is to convert user queries into our specific query format. Essentially, what we will do here is to carefully extract key details from each user query to form a structured dictionary. This structured data will then be reformatted to match the pattern of our query strings. To achieve this, Iâ€™ll just use an LLM to decipher the user queries and identify the crucial entities we need.

    import os
    from openai import OpenAI
    from dotenv import load_dotenv

    load_dotenv()

    api_key = os.getenv("OPENAI_API_KEY")

    # Initialize the OpenAI client
    client = OpenAI(api_key=api_key)

    query_string = "Hi, I am looking for a casual dining restaurant where Indian or Italian food is served near the HSR Bangalore"

    # Helper prompt to extract structured data from ip_prompt
    total_prompt = f"""Query String: {query_string}\n\n\
    Now from the query string above extract these following entities pinpoints:
    1. Food type : Extract the food type
    2. Avg ratings : Extract the average ratings
    3. Address : Extract the current exact location, don't consider the fillers like "near" or "nearby".

    NOTE : For the Current location, try to understand the pin point location in the query string. Do not give any extra information. If you make the mistakes, bad things
    will happen.

    Finally return a python dictionary using those points as keys and don't write the markdown of python. If value of a key is not mentioned, then set it as None.
    """

    # Make a request to OpenAI's API
    completion = client.chat.completions.create(
        model="gpt-4o",  # Use the appropriate model
        store=True,
        messages=[
            {"role": "user", "content": total_prompt}
        ]
    )

    # Extract the generated text
    content = completion.choices[0].message.content
    print(content)

    {
      "Food type": "Indian or Italian",
      "Avg ratings": None,
      "Address": "HSR Bangalore"
    }

Now, all we need to do is process this output.

    import ast

    # Convert the string content to a dictionary
    try:
        response_dict = ast.literal_eval(content)
    except (ValueError, SyntaxError) as e:
        print("Error parsing the response:", e)
        response_dict = {}

    filter_cols = ['Food type', 'Avg ratings', 'Address']
    query_string_parts = [f"{col}/{response_dict.get(col)}" for col in filter_cols if response_dict.get(col)]

    query_string = "#".join(query_string_parts)
    print((query_string))

Well, now the user query looks like this:

    Food type/Indian or Italian#Address/HSR Bangalore

Well, this user query is now formatted exactly like our `query_strings`. We can go ahead and search through the vector database to find the top restaurants that best match this query.

### Searching like a pro

I'll be using the Full Text Search (FTS) feature from LanceDB to run the search. It's basically a semantic search. You can read more about whatâ€™s happening behind the scenes [here](https://lancedb.github.io/lancedb/fts/#example).

    # Create the FTS index and search
    lancedb_table.create_fts_index("query_string", replace=True)
    results = lancedb_table.search(query_string).to_pandas()
    results.head()

![](__GHOST_URL__/content/images/2024/12/image-9.png)
### Using the Geospatial data

So basically when someone searches for nearby restaurants, maybe they're craving a specific type of cuisine or looking for highly-rated places, we first search up the places that fills there requirements. Now after identifying potential options, we use the [Geospatial API](https://developers.google.com/maps/documentation/geocoding/overview) to pinpoint their exact locations. The Google Maps API is perfect for thisâ€”it grabs the latitude and longitude so we know precisely where each restaurant is. With these coordinates, we can then easily figure out which places are closest to the userâ€™s location, by just calculating the distance.

If you didn't get that, Bear with me, this is going to be super cool. So first thing we need to do is to set our Geospatial function which takes a place and returns the coordinates:

    import requests
    import math

    def get_google_geocoding(address, api_key):
        base_url = "https://maps.googleapis.com/maps/api/geocode/json"
        params = {"address": address, "key": api_key}
        response = requests.get(base_url, params=params)

        if response.status_code == 200:
            result = response.json()
            if result["status"] == "OK":
                latitude = result["results"][0]["geometry"]["location"]["lat"]
                longitude = result["results"][0]["geometry"]["location"]["lng"]
                return (latitude, longitude)
            else:
                print(f"Google API: No results found for address: {address}")
                return None
        else:
            print(f"Google API: Request failed for address: {address}")
            return None

For the distance calculation, there's this thing called the `Haversine formula`. It uses the coordinates of two points and basically draws an imaginary straight line between them across the earth to measure how far they are from each other. There's a bit of math involved in how this formula works, but we can skip that part for now. Hereâ€™s what the formula looks like:

    def haversine(coord1, coord2):
        R = 6371.0  # Radius of the Earth in kilometers
        lat1, lon1 = map(math.radians, coord1)
        lat2, lon2 = map(math.radians, coord2)
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
        distance = R * c
        return distance

Well, everything seems solid now, the only thing left is the current location. Hereâ€™s what we can do: if a user asks about restaurants near a specific area like `nearby HSR layout` we can easily pull the current location from the postprocessing we did earlier.  If not, for now, we can just input the current location manually. Then, weâ€™ll check how far our top-n restaurants are from our user's place.

Well let's see what we get:

    def process_top_restaurants(data, current_location, api_key, top_n=5):
        current_coords = get_google_geocoding(current_location, api_key)
        if not current_coords:
            return

        for index, row in data.head(top_n).iterrows():
            complete_address = f"{row['Restaurant']}, {row['City']}"
            restaurant_coords = get_google_geocoding(complete_address, api_key)
            if restaurant_coords:
                distance = haversine(current_coords, restaurant_coords)
                print(f"Restaurant Name: {row['Restaurant']}")
                print(f"Distance: {distance:.2f} km")
                print(f"Area: {row['Area']}")
                print(f"Price: {row['Price']}")
                print(f"Coordinates: {restaurant_coords}")
                print(f"Cuisines Type: {row['Food_type']}")
                print("-" * 40)

    # Example usage
    api_key = os.getenv('GOOGLE_MAPS_API')
    current_location = 'HSR Layout, Bangalore'
    process_top_restaurants(results, current_location, api_key, top_n=3)

    Restaurant Name: Brooks And Bonds Brewery
    Distance: 3.36 km
    Area: Koramangala
    Price: 200.0
    Coordinates: (12.9341801, 77.62334249999999)
    Cuisines Type: Indian
    ----------------------------------------
    Restaurant Name: Cafe Azzure
    Distance: 8.06 km
    Area: Ashok Nagar
    Price: 1000.0
    Coordinates: (12.975012, 77.6076558)
    Cuisines Type: American,Italian
    ----------------------------------------
    Restaurant Name: Tottos Pizza
    Distance: 7.92 km
    Area: Central Bangalore
    Price: 500.0
    Coordinates: (12.9731935, 77.607012)
    Cuisines Type: Continental,Italian
    ----------------------------------------

Here we go... This is cool :)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\how-to-build-a-serverless-multi-modal-search-engine-application-8c8224d3e2e3.md

---

```md
---
title: "Serverless Multi-Modal Search Engine Application"
date: 2023-09-25
author: Ayush Chaurasia
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/how-to-build-a-serverless-multi-modal-search-engine-application-8c8224d3e2e3/preview-image.png
meta_image: /assets/blog/how-to-build-a-serverless-multi-modal-search-engine-application-8c8224d3e2e3/preview-image.png
description: "Master about serverless multi-modal search engine application. Get practical steps, examples, and best practices you can use now."
---

In this writeup, youâ€™ll learn the process of building a multi-modal search engine using roboflowâ€™s CLIP inference API and LanceDB, serverless vector with native javascript support.

By the end of this, you should be able to build something like this, a search engine that can search images using text or other images

Full implementation can be found [here](https://github.com/lancedb/vectordb-recipes/tree/main/applications/multimodal-search)
![](https://miro.medium.com/v2/resize:fit:651/1*CkRyPrIi5OXiqxh82Jl3Wg.gif)
Letâ€™s get started!

## CLIP

CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3
![](https://miro.medium.com/v2/resize:fit:770/1*JZey6K72V64VOxoVw7drbQ.png)[source](https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2)
## LanceDB: Serverless VectorDB in browser

![](https://miro.medium.com/v2/resize:fit:770/1*QWz_uromVPB5LKAgfrfKIg.png)
Let us now set up the vector database. Weâ€™ll use nextjs serverless functions.

LanceDB is an open-source database for vector-search built with persistent storage, which greatly simplifies retrieval, filtering and management of embeddings. LanceDB has native support for both python and javascript/Typescript. As this is a web application, weâ€™ll use the node package.

Let us now look at some of the main parts/snippets that do all the heavy lifting.

## Get Image embeddings

Roboflow CLIP inference API accepts images as base64 strings. The following snippet is a function that takes the image file as base64 string, runs it through CLIP inference API and returns the embeddings. The API uses axios to process requests

    async function embedImage(file: string) {
      const response = await axios({
        method: "POST",
        url: `https://infer.roboflow.com/clip/embed_image`,
        params: {
          api_key: process.env.RF_API_KEY || "",
        },
        data: {
          clip_version_id: "ViT-B-16",
          image: [
            {
              type: "base64",
              value: file,
            },
          ],
        },
        headers: {
          "Content-Type": "application/json",
        },
      });

      return response.data.embeddings[0];
    }

## Get Text embeddings

Similarly, you can also get the text embeddings.

    async function embedText(text: string) {
      const response = await axios({
        method: "POST",
        url: "https://infer.roboflow.com/clip/embed_text",
        params: {
          api_key: process.env.RF_API_KEY || "",
        },
        data: {
          clip_version_id: "ViT-B-16",
          text: text,
        },
        headers: {
          "Content-Type": "application/json",
        },
      });

      return response.data.embeddings[0];
    }

## Create LanceDB table

Now we can simply call the above functions as nextjs apis to create LanceDB embeddings table.

    async function getImgEmbeddings(img_files: Array<string>, db: any){

    for (var i = 0; i < img_files.length; i++) {
    const response = await fetch(`${baseUrl}/api/embed`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ data: img_files[i], type: "image" }),
    });
    const json = await response.json();
    embeddings.push(json.embedding);
    }

    var data = [];
    for (var i = 0; i < img_files.length; i++) {
    data.push({
    img: imgs[i],
    vector: embeddings[i],
    });
    }
    await db.createTable("table", data);

## Searching for similar images

Now that weâ€™ve created the embedding table, letâ€™s see how we can use it to search for similar Images using text or another image. The thing to keep in mind is that the CLIP model is capable of projecting both images and texts in the same embedding space, which is what weâ€™ll utilize here.

export async function retrieveContext(query: Array<number>, table: string) {
const db = await connect(process.env.LANCEDB_URI);
const tbl = await db.openTable(table);

// Search for similar image and get top 25 results
const result = await tbl.search(query).select(["img"]).limit(25).execute();

const imgs = result.map((r) => r.img);

return imgs;
}

Thatâ€™s pretty much all that is needed to build a multi-modal semantic search engine. Weâ€™ve covered the building blocks â€” Embedding images and text, populating the LanceDB table, and retrieving relevant results.

The full implementation of the Application using nextjs and tailwind can be found here on [GitHub](https://github.com/lancedb/vectordb-recipes/tree/rf/applications/multimodal-search).

Learn more about LanceDB or learn more about applied GenAI applications on our [vectordb-recipes](https://github.com/lancedb/vectordb-recipes) . Donâ€™t forget to drop us a ðŸŒŸ!
```

---

## ðŸ“„ æ–‡ä»¶: drafts\hybrid-search-and-reranking-report.md

---

```md
---
title: "Improve (almost) Every Retriever with LanceDB Hybrid Search and Reranking"
date: 2024-08-26
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/hybrid-search-and-reranking-report/preview-image.png
meta_image: /assets/blog/hybrid-search-and-reranking-report/preview-image.png
description: "Retrieval is a key component of any type of recommender system, including RAG.  The quality of responses generated by chatbot can only be as good as the retrieved context."
---

Retrieval is a key component of any type of recommender system, including RAG. The quality of responses generated by chatbot can only be as good as the retrieved context. In this blog, we'll see how you can go about improving retrieval performance of limit our discussion to vectorDB-based retrievers.

There are various ways which you can go about improving the retrieval performance of vectorDB-based retriever but they can often end up disrupting the existing setup in place. For example, if the first thing you want to try to improve your retriever is change the chunking strategy, you'll need to re-ingest the entire dataset. Same goes if you want to change the embedding model. So, its always better to try simpler & faster to implement solutions before jumping to modelling or chunking. LanceDB comes with built-in APIs to streamline these experiments. Let's get started.

### Experiment setup

We'll start off by selecting 2 dataset and set a baseline for the retrieval performance - SQuAD and a synthetic dataset generated from LLama2 review paper. The metric used here will be **hit-rate @ top-5**

**Setting up the embedding model**

Throughout this experiment the embedding model used will be `BAAI/bge-small-en-v1.5` . With LanceDB embedding API, you can setup the tables to automatically embed your dataset and queries in the background, abstracting away the modelling details.

    import lancedb
    from lancedb.embeddings import get_registry
    from lancedb.pydantic import Vector, LanceModel

    model = get_registry().get("sentence-transformers").create(name="BAAI/bge-small-en-v1.5")

    class Schema(LanceModel):
        text: str = model.SourceField()
        vector: Vector(model.ndims()) = model.VectorField()
        # vector field is automatically generated

    db = lancedb.connect("~/db")
    table = db.create_table("table", schema=Schema)

    data = [{"text": "data"}, {"text": "dddadata}, ...]
    table.add(data)
    table.search("query")

### The baseline

DatasetQuery TypeRerankerHit RateSQuADvector81.322FTS83.509LLama2-reviewvector58.63FTS59.54
### Reranking results

In the context of search, reranking means reordering the search results returned by a search algorithm based on some criteria. This can be useful when the initial ranking of the search results is not satisfactory or when the user has provided additional information that can be used to improve the ranking of the search results.

**LanceDB Reranking API**

LanceDB comes with some built-in rerankers.To use a reranker, you need to create an instance of the reranker and pass it to the `rerank` method of the query builder.

    from lancedb.rerankers import ColbertReranker

    colbert = ColbertReranker()

    table.search("query").rerank(reranker=colbert) # reranker vector search
    table.search("query", query_type="fts").rerank(reranker=colbert)

### Reranking models used

The rerankers used in this experiment are all available out of the box with LanceDB. These are a mix of open-source and proprietary API-based models.

- **Cross Encoder** - This uses open source cross encoder architecture for generating similarity scores of query-context pairs for reranking. The model used is `cross-encoder/ms-marco-TinyBERT-L-6` . This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.
- **Cohere - **This uses API-based  proprietary reranking model by Cohere. The model used here is `rerank-english-v2.0`
- **ColBERT** - ColBERT relies on fine-grained **contextual late interaction**: it encodes each passage into a **matrix** of token-level embeddings. Then at search time, it embeds every query into another matrix and efficiently finds passages that contextually match the query using scalable vector-similarity operators. This experiment uses `colbert-v2` models with approx ~110M params
- **AnswerDotAi Colbertv1** -  This model builds upon the JaColBERTv2.5 recipe and has just **33 million parameters**, meaning itâ€™s able to search through hundreds of thousands of documents in milliseconds, **on CPU**. With its very small parameter count, it demonstrates that thereâ€™s a lot of retrieval performance to be squeezed out of creative approaches, such as multi-vector models, with low parameter counts, which are better suited to a lot of uses than gigantic 7-billion parameter embedders. Learn more about this in the AnswerDotAI announcement [blog](https://www.answer.ai/posts/2024-08-13-small-but-mighty-colbert.html).

**Reranked vector and FTS results**
DatasetQuery TypeRerankerHit RateSQuADvectorCrossEncoder85.22vectorColBERT85.49vectorAnswerdotai ColBERT85.95vectorCohere**86.09**FTS83.509FTSCrossEncoder86.73FTSColBERT86.74FTSAnswerdotai ColBERT87.24FTSCohere**87.72**LLama2-reviewvectorCrossEncoder62.27vectorColBERT62.72vectorAnswerdotai ColBERT65.45vectorCohere**66.80**FTS59.54FTSCrossEncoder60.90FTSColBERT64.54FTSAnswerdotai ColBERT65.00FTSCohere**67.27**
Note: In case of vector and FTS only search, the results were over-fetched by a factor of 2 for reranking and then the top-k( top-5 ) results were taken. This is needed as there is only one result set, unlike hybrid search where there are result sets from both vector and FTS. Without over-fetching, reranking would not have any effect.

### Hybrid Search

Hybrid search is broad term that refers to  a search algorithm that combines multiple search techniques. You can perform hybrid search in LanceDB by combining the results of semantic and full-text search via a reranking algorithm of your choice.

**LanceDB hybrid search with custom Reranking**

    table.search("query", query_type="hybrid") # uses Liner combination reranker

    table.search("query", query_type="hybrid").rerank(reranker=colbert)

**Hybrid search with reranking results**
DatasetQuery TypeRerankerHit RateSQuADHybridLinearCombination88.42HybridReciprocal Rank Fusion89.81HybridCrossEncoder91.51HybridColBERT91.53HybridAnswerdotai ColBERT92.26HybridCohere**92.35**LLama2-reviewHybridLinearCombination62.27HybridReciprocal Rank Fusion66.36HybridCrossEncoder65.45HybridColBERT70.90HybridAnswerdotai ColBERT70.90HybridCohere**75.00**
## Full benchmark

DatasetQuery TypeRerankerHit RateSQuADvector81.322vectorCrossEncoder85.22vectorColBERT85.49vectorAnswerdotai ColBERT85.95vectorCohere**86.09**FTS83.509FTSCrossEncoder86.73FTSColBERT86.74FTSAnswerdotai ColBERT87.24FTSCohere**87.72**HybridLinearCombination88.42HybridReciprocal Rank Fusion89.81HybridCrossEncoder91.51HybridColBERT91.53HybridAnswerdotai ColBERT92.26HybridCohere**92.35**
---
DatasetQuery TypeRerankerHit RateLLama2-reviewvector58.63vectorCrossEncoder62.27vectorColBERT62.72vectorAnswerdotai ColBERT65.45vectorCohere**66.80**FTS59.54FTSCrossEncoder60.90FTSColBERT64.54FTSAnswerdotai ColBERT65.00FTSCohere**67.27**HybridLinearCombination62.27HybridReciprocal Rank Fusion66.36HybridCrossEncoder65.45HybridColBERT70.90HybridAnswerdotai ColBERT70.90HybridCohere**75.00**
### **SQuAD**

**Baseline: Vector Search - 81.322**

**Best result: Hybrid search with Cohere reranker - 92.35**

### **Llama-2-review **

**Baseline: Vector Search - 58.63**

**Best result: Hybrid search with Cohere reranker - 75.00**

Most rerankers ended up improving the result with Cohere leading the pack. The  highlight here would be **AnswerDotAi ColBERT-small-v1** as it performs almost as good as Cohere reranker, which has been by far the best reranker across all our tests. Also, is more than 3x smaller than ColBERT-v2 so it can be run locally on CPU.

## Conclusion

![](__GHOST_URL__/content/images/2024/08/Screenshot-2024-08-30-at-11.42.23-AM.png)
Our experiments on the SQuAD and Llama2-review datasets demonstrated significant improvements in model performance using these techniques. Specifically, we observed an **11%** increase in accuracy on SQuAD (from **81.22%** to **92.35%**) and a **16%** increase in hit-rate on Llama2-review (from **58.63%** to **75.00%**). These methods are particularly effective in scenarios where a slight increase in query latency is acceptable, as they offer substantial gains in accuracy without sacrificing too much speed.

This study exclusively explored techniques to optimize the retriever's performance without requiring a complete dataset re-ingestion. These methods represent the most straightforward approaches to enhance accuracy. Reranker models, often overlooked due to their substantial latency during queries, are becoming more viable with the emergence of smaller, on-device models that can significantly reduce query time.

## future work

A comprehensive evaluation of reranker performance would require a careful consideration of the trade-off between accuracy and latency. We intend to investigate this aspect in future studies. It's important to note that direct comparisons can be challenging, as some of the most effective rerankers are API-based, and their latency can fluctuate due to network factors.

Another important direction is evaluating other methods of improving retrieval performance like - fine-tuning embedding models, choosing the best embedding model architectures depending on the dataset types, the effect choosing various chunking techniques & other best practices involved. These methods are generally more disruptive compared to reranking & hybrid search, as they require complete data re-ingestion, but are useful in cases where low latency at query time is critical.

## References

The benchmarking code was based on [Ragged repo](https://github.com/lancedb/ragged), that contains a bunch of utility functions for ingesting and evaluating retrievers.
[

GitHub - lancedb/ragged

Contribute to lancedb/ragged development by creating an account on GitHub.

![](https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg)GitHublancedb

![](https://opengraph.githubassets.com/0b2b0f371912a96f32e5ee1c1b17e746f19d9261e813c657a983b27923e30821/lancedb/ragged)
](https://github.com/lancedb/ragged)
It allows you to simply run evaluations across multiple query-types, rerankers and embedding models powered by LanceDB. For example this is the evaluation code for SQuAD data with linear combination reranker.

    from ragged.dataset import CSVDataset, SquadDataset
    from ragged.rag import llamaIndexRAG
    from ragged.metrics.retriever.hit_rate import HitRate
    from lancedb.rerankers import LinearCombinationReranker
    from ragged.search_utils import QueryType
    import wandb

    dataset = SquadDataset()
    reranker = LinearCombinationReranker()
    hit_rate = HitRate(dataset, embedding_registry_id="sentence-transformers", embed_model_kwarg={"name": "BAAI/bge-small-en-v1.5", "device": "cuda"})

    query_types = [QueryType.HYBRID, QueryType.RERANK_VECTOR, QueryType.RERANK_FTS]

    run = wandb.init(project="ragged_bench", name=f"linearcombination")

    for query_type in query_types:
        hr = hit_rate.evaluate(5, query_type=query_type, use_existing_table=use_existing_table)
        run.log({f"{query_type}": hr.model_dump()[f"{query_type}"]})

    wandb.finish()

Note: The hit-rate eval in ragged, discards failed queries. So evaluation results with other implementations might be different. The purpose of this evaluation is to just show performance gains on a standard metrics that remains constant for all evaluations.

The llama2-review synthetic dataset was generated from llama-2-paper dataset on [LLamahub](https://llamahub.ai/l/llama_datasets/Llama%202%20Paper%20Dataset?from=llama_datasets) using the datagen utils in ragged:

    from ragged.dataset.gen.gen_retrieval_data import gen_query_context_dataset
    fragged.dataset.gen.llm_calls import OpenAIInferenceClient

    clinet = OpenAIInferenceClient()
    df = gen_query_context_dataset(directory="data/source_files", inference_client=clinet)

    print(df.head())
    # save the dataframe
    df.to_csv("data.csv")

The public W&B dashboards for both these experiments can be found here.

[SQuAD](https://wandb.ai/cayush/ragged_bench?nw=nwusercayush)
![](__GHOST_URL__/content/images/2024/08/Screenshot-2024-08-22-at-3.00.09-PM.png)
[LLama2-review](https://wandb.ai/cayush/ragged_bench_1_llama?nw=nwusercayush)
![](__GHOST_URL__/content/images/2024/08/Screenshot-2024-08-22-at-3.20.26-PM.png)
### Useful Links

- AnswerDotAI Colbert-small-v1 [announcement blog](https://www.answer.ai/posts/2024-08-13-small-but-mighty-colbert.html), [HF hub](https://huggingface.co/answerdotai/answerai-colbert-small-v1)
- LanceDB [Hybrid search](https://lancedb.github.io/lancedb/hybrid_search/hybrid_search/) & [Reranking](https://lancedb.github.io/lancedb/reranking/)
- LanceDB [embedding API](https://lancedb.github.io/lancedb/embeddings/)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\improving-llm-based-web-applications-with-easy-to-use-and-free-serverless-vector-database-lancedb-254e1442a9b0.md

---

```md
---
title: "Improving LLM-Based Web Applications with Easy-to-Use and Free Serverless Vector Database LanceDB"
date: 2023-08-16
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/improving-llm-based-web-applications-with-easy-to-use-and-free-serverless-vector-database-lancedb-254e1442a9b0/preview-image.png
meta_image: /assets/blog/improving-llm-based-web-applications-with-easy-to-use-and-free-serverless-vector-database-lancedb-254e1442a9b0/preview-image.png
description: "Unlock about improving llm-based web applications with easy-to-use and free serverless vector database lancedb. Get practical steps, examples, and best practices you can use now."
---

by Tevin Wang

I believe that there are currently two major challenges using LLMs. (1) outdated information (ChatGPT has limited knowledge of the world and events after 2021) and (2) hallucinations (when LLMs produce incorrect or biased responses that seem confident).

**Retrieval augmented generation (RAG)** is the key to helping LLMs provide the best and up-to-date results for specific use cases and contexts. It gives LLMs context from a knowledge base by augmenting its prompts, and thus, allows it to retrieve knowledge-grounded information. Such generation technique have been [shown to help reduce hallucinations.](https://arxiv.org/pdf/2104.07567.pdf)

With the help of a serverless vector database like LanceDB that has native JS support, we are able to integrate retrieval augmented generation directly in serverless functions on-prem. No api keys for the database needed. This is great for web-based AI applications, like a chatbot agent that works with website sitemaps to retrieve context and provide information about that website.
![](https://miro.medium.com/v2/resize:fit:770/1*6J1xo4qiwuAaP4gYoTqypA.png)LanceDB Website Chatbot Template
Letâ€™s take a look at how this works in Next.js. The full source code for the website chatbot template can be found in this [Github repo](https://github.com/lancedb/lancedb-vercel-chatbot). You can also deploy this template to Vercel in one [click](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Flancedb%2Flancedb-vercel-chatbot&amp;env=OPENAI_API_KEY&amp;envDescription=OpenAI+API+Key+for+chat+completion.&amp;project-name=lancedb-vercel-chatbot&amp;repository-name=lancedb-vercel-chatbot&amp;demo-title=LanceDB+Chatbot+Demo&amp;demo-description=Demo+website+chatbot+with+LanceDB.&amp;demo-url=https%3A%2F%2Flancedb.vercel.app&amp;demo-image=https%3A%2F%2Fi.imgur.com%2FazVJtvr.png).

There are two main functions needed for the chatbot:

- **Insertion:** Weâ€™ll scrape a website sitemap provided through a form input and insert website information into LanceDB.
- **Retrieval:** With a query from a chat interface, weâ€™ll query LanceDB to augment the LLM prompt with website context, and return the result from the LLM.

## Insertion

We start by using a simple form to gather the url for the website sitemap:
![](https://miro.medium.com/v2/resize:fit:730/1*apsoc5GUuTAfr8yNO9USPQ.png)You can find code for this element [here](https://github.com/lancedb/lancedb-vercel-chatbot/blob/main/src/app/components/Form.tsx).
We then send a POST request to a serverless function with this information.

        const response = await fetch("/api/context", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ url: state.website, pages: state.pages, })
        })

In the `/api/context` route, weâ€™ll use Cheerio to get website links from the sitemap, and retrieve the actual text content from the website.

    async function getEntriesFromLinks(links: string[]): Promise<Entry[]> {
      let allEntries: Entry[] = [];

      for (const link of links) {
        console.log('Scraping ', link);
        try {
          const response = await fetch(link);
          const $ = load(await response.text());

          const contentArray: string[] = [];
          $('p').each((index: number, element: Element) => {
            contentArray.push($(element).text().trim());
          });

          const content = contentArray
            .join('\n')
            .split('\n')
            .filter(line => line.length > 0)
            .map(line => ({ link: link, text: line }));

          allEntries = allEntries.concat(content);
        } catch (error) {
          console.error(`Error processing ${link}:`, error);
        }
      }

      return allEntries;
    }

    export async function getDomObjects(url: string, pages: number): Promise<Entry[]> {
      const sitemapUrls = await getWebsiteSitemap(url, pages);
      const allEntries = await getEntriesFromLinks(sitemapUrls);
      return allEntries;
    }

We save these in the `allEntries` array to get ready for inserting into LanceDB. Next, we connect to a temporary directory, `/tmp/website-lancedb`, to actually store our data.

    const db = await connect('/tmp/website-lancedb')

Before we insert, we should provide expanded context for each text entry. This will help us retrieve more information for each query. For this demo, weâ€™ll merge 5 entries of data together, grouped by each website page.

    // Each article line has a small text column, we include previous lines in order to
    // have more context information when creating embeddings
    function contextualize(rows: Entry[], contextSize: number, groupColumn: string): EntryWithContext[] {
      const grouped: { [key: string]: any } = []
      rows.forEach(row => {
        if (!grouped[row[groupColumn]]) {
          grouped[row[groupColumn]] = []
        }
        grouped[row[groupColumn]].push(row)
      })

      const data: EntryWithContext[] = []
      Object.keys(grouped).forEach(key => {
        for (let i = 0; i < grouped[key].length; i++) {
          const start = i - contextSize > 0 ? i - contextSize : 0
          grouped[key][i].context = grouped[key].slice(start, i + 1).map((r: Entry) => r.text).join(' ')
        }
        data.push(...grouped[key])
      })
      return data
    }

To actually create vector embeddings from the website text, weâ€™ll use OpenAIâ€™s `text-embedding-ada-002` embedding model. An `OPENAI_API_KEY` must be added as an environment variable.

      // The embedding function will create embeddings for the 'context' column
      const embedFunction = new OpenAIEmbeddingFunction('context', apiKey)

Finally, weâ€™ll batch insert our text entries into a Table, which is the primary abstraction youâ€™ll use to work with your data in LanceDB. A Table is designed to store large numbers of columns and huge quantities of data. For those interested, a LanceDB is columnar-based and uses Lance, an open data format to store data.

      const tbl = await db.createTable(`website-${hash}`, data.slice(0, Math.min(batchSize, data.length)), embedFunction)
      for (var i = batchSize; i < data.length; i += batchSize) {
        await tbl.add(data.slice(i, Math.min(i + batchSize, data.length)))
      }

## Retrieval

We start with a chat interface provided by the Vercel AI SDK, which allows for a streaming UI experience for chat models. You can test out the Vercel AI SDK at [https://sdk.vercel.ai](https://sdk.vercel.ai/).
![](https://miro.medium.com/v2/resize:fit:770/1*AOq8wdNz6bVymYTIAsr6Hw.png)[https://sdk.vercel.ai](https://sdk.vercel.ai/)
Here is LanceDBâ€™s chat page that appears after insertion is finished.
![](https://miro.medium.com/v2/resize:fit:770/1*1iIM-h5HttfjtaxDdwYCWA.png)You can find code for the chat page [here](https://github.com/lancedb/lancedb-vercel-chatbot/blob/main/src/app/components/Chat.tsx).
Once a text query is recieved, we send a request to `/api/chat` with the following router, which uses the Edge runtime.

    import { OpenAIStream, StreamingTextResponse } from 'ai'
    import { Configuration, OpenAIApi } from 'openai-edge'
    import { createPrompt } from './prompt'

    // Create an OpenAI API client (that's edge friendly!)
    const config = new Configuration({
      apiKey: process.env.OPENAI_API_KEY
    })
    const openai = new OpenAIApi(config)

    // IMPORTANT! Set the runtime to edge
    export const runtime = 'edge'

    export async function POST(req: Request) {
      // Extract the `messages` from the body of the request
      const { messages, table } = await req.json()

      const baseUrl = process.env.VERCEL_URL ? 'https://' + process.env.VERCEL_URL : 'http://localhost:3000'
      const context = await fetch(`${baseUrl}/api/retrieve`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ query: messages[messages.length - 1].content, table })
      })
      messages[messages.length - 1].content = createPrompt(messages[messages.length - 1].content, (await context.json()) as EntryWithContext[])
      // Ask OpenAI for a streaming chat completion given the prompt
      const response = await openai.createChatCompletion({
        model: 'gpt-3.5-turbo',
        stream: true,
        messages
      })
      // Convert the response into a friendly text-stream
      const stream = OpenAIStream(response)
      // Respond with the stream
      return new StreamingTextResponse(stream)
    }

This first calls `/api/retrieve` to get context from LanceDB. We make a search query on the table generated from insertion, utilizing the same OpenAI embedding model, returning the results.

      const embedFunction = new OpenAIEmbeddingFunction('context', apiKey)
      const tbl = await db.openTable(table, embedFunction)
      console.log('Query: ', query)
      return await tbl
        .search(query)
        .select(['link', 'text', 'context'])
        .limit(3)
        .execute()
    }

Then, we utilize these results to create and augment our prompt.

    messages[messages.length - 1].content = createPrompt(messages[messages.length - 1].content, (await context.json()) as EntryWithContext[])
    export function createPrompt(query: string, context: EntryWithContext[]) {
      let prompt =
        'The context that follows is pulled from a website. Respond based on the website information below, acting as an agent guiding someone through the website.\n\n' +
        'Context:\n'

      // need to make sure our prompt is not larger than max size
      prompt = prompt + context.map(c => c.context).join('\n\n---\n\n').substring(0, 3750)
      prompt = prompt + `\n\nQuestion: ${query}\nAnswer:`
      return prompt
    }

Finally, we ask for chat completion from OpenAI.

    // Ask OpenAI for a streaming chat completion given the prompt
    const response = await openai.createChatCompletion({
      model: 'gpt-3.5-turbo',
      stream: true,
      messages
    })
    // Convert the response into a friendly text-stream
    const stream = OpenAIStream(response)
    // Respond with the stream
    return new StreamingTextResponse(stream)

And here are the results of the query `Tell me about Vercel`.
![](https://miro.medium.com/v2/resize:fit:770/1*WvLW81xuU4m9tBQ6U9J9cQ.png)
## Summary

- **Retrieval augmented generation (RAG)** is a great way to provide LLMs relevant context from a knowledge base.
- We need a** good vector database solution** that provides RAG on the web, as many AI and LLM-related applications are hosted on the web.
- **LanceDB** is a **serverless vector database** **that is up for the task**. It is easy-to-use and free, as it can be embedded directly into a serverless function, with data stored on-prem.
- With** Vercel AI SDK** **and LanceDB**, we can create some cool Next.js web apps that utilize RAG, such as a LLM-based website chatbot.

Thanks for reading! If you have questions, feedback, or want help using LanceDB in your app, donâ€™t hesitate to drop us a line at [contact@lancedb.com](mailto:contact@lancedb.com). And weâ€™d really appreciate your support in the form of a Github Star on our [LanceDB repo](https://github.com/lancedb/lancedb) â­.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\interning-at-lancedb.md

---

```md
---
title: "Interning at LanceDB"
date: 2024-12-20
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/interning-at-lancedb/preview-image.png
meta_image: /assets/blog/interning-at-lancedb/preview-image.png
description: "My name is Jun Wang, I am a master's student at Clark University, this fall semester I was very fortunate to have the opportunity working at LanceDB as a software engineer intern."
---

My name is Jun Wang, I am a master's student at Clark University, this fall semester I was very fortunate to have the opportunity working at LanceDB as a software engineer intern.

I worked on Lance, a modern columnar data format for ML and LLMs. Lance is set to be the file format for AI workloads, this is a ambitious goal and it has been very successful, with more than 3.5k projects use it on Github.

**A few things I worked on**

Encodings: Encodings are ways to compress data. The workload Lance targets requires good compression ratio and fast random access read.

    Strings:
        Traditionally strings are compressed using dictionary encoding and block compression algorithms like snappy, zstd, lz4. However, dictionary encoding is only applicable when input data has low cardinality and block compression algorithms have huge decoding speed penalty during random access â€“ we have to decompress the whole block to read any single row.

[FSST](https://www.vldb.org/pvldb/vol13/p2649-boncz.pdf) is a string compression algorithm invented by [Peter Boncz](https://scholar.google.com/citations?user=DCIZE1kAAAAJ&amp;hl=en) et al at CWI, it builds a symbol table from the input data and substitutes the input substrings that match the symbol table with a one-byte symbol table index, thus providing both good compression ratio and fast random access.

        I wrote the first Rust implementation of FSST algorithm and integrated it with Lance's MiniBlock page layout, experiments using the first column of [MS MACRO](https://microsoft.github.io/msmarco/) dataset show that Lance has less disk size and more than 60x faster random access speed.

![](__GHOST_URL__/content/images/2024/12/string_random_access_read_comparison-4.png)

    Integers:
        Integers are often compressed with encodings like bit-packing, frame-of-reference, and delta encoding. Bit-packing encoding packs the input data to the maximum bits they need, for example, when the input data is of type int32 and the data ranges between 0 ~ 1023, then we can use 10 bits to store each int32, squeezing them all together continuously on disk.
        The challenge here is that to perform this "squeeze" and "unsqueeze", we do lots of bit by bit left/right shift, OR/AND and branching, which hinders the potential of modern CPU.

[Fastlanes](https://www.vldb.org/pvldb/vol16/p2132-afroozeh.pdf?ref=blog.lancedb.com) algorithm is invented by [Azim Afroozeh](https://scholar.google.com/citations?user=h-vgI8UAAAAJ&amp;hl=en) and Peter Boncz at CWI, it groups every 1024 integers together and brilliantly interleaves them in a transposed order and thus enables compression and decompression of these 1024 values be done with SIMD operations. The decoding speed in their paper is astonishing, more than 100 billion integers per second.

        I integrated SpiralDB's [Fastlanes](https://github.com/spiraldb/fastlanes) implementation with the Lance MiniBlock page layout, experiments using 1 billion random generated integers between 0 and 2^20 shows that Lance file size is 2x smaller than parquet and around 170x faster for random access reads.

![](__GHOST_URL__/content/images/2024/12/integer_comparison-5.png)

Data statistics gathering
    To decide which encoding to use to compress the data and which page layout to use for storing the data, we need some understanding of the input data.  I designed and implemented a datatype agnostic way in Lance to gather input statistics that enables easy encoding selection and encoding cascade. For example, in Lance, the indices of dictionary encoding are compressed implicitly without any code to specify it, with each group of 1024 values having its own statistics for encoding algorithms to use.

**Working at LanceDB**
    The working experience at LanceDB is awesome and [We're hiring](https://lancedb.notion.site/Backend-Software-Engineer-Cloud-4c2d55c484374ffea2a4b91cf64ac934)!

    Rarely a software engineer gets the opportunity developing a popular file format and I feel very privileged working on Lance. [Weston](https://github.com/westonpace), with his expertise in this field, put unique thoughts designing and meticulously implementing Lance format 2.0 and 2.1.  He and other teammates give me tremendous help whenever I needed discussion and advice.

    The vector database industry is just taking off and lots of engineering challenges remain unsettled, for aspiring computer scientists and software engineers craving for big puzzles to unleash their creativity, it's a great time now to get into this field.

**Computer systems programming**
    Programming in low level systems like Lance is quite a unique experience. A complete understanding of the codebase is needed while debugging only with "printf", writing encoding algorithms feels like a delicate dance with the CPU, and modern CPUs, powerful as a wild beast, also has a temper with what you feed them, how you orchestrate them.

    Software performance engineering is crucial in Lance not only because the speed boost and resource saving directly means saving money, but also because it enables use cases that are otherwise impossible.  For example, in autonomous vehicle navigation, real time vector search is used and it won't be feasible if the vector search latency is high.

    However, as Donald Knuth used to say: "premature (software) optimization is the root of all evil", software performance engineering is not a easy endeavor. Measuring and profiling needs to be rigorously done, or you misguide yourself going south for weeks. Writing code hacks like [bit hacks](https://graphics.stanford.edu/~seander/bithacks.html) is common, you feel you coded these tricks many times you start feeling confident at it, yet mishaps come and you shoot yourself in the foot, getting wrong results.

    Like when a con artist that lost his suits and props and has to start over on the street with only his words, system programming is "a cappella" with the computer hardware.

**Going Forward**
    Well, when you start your career at LanceDB, it's hard not to set yourself to be an exceptional software engineer. As I am wrapping up this internship, questions pop up like "what software systems I want to build?", "what software engineering challenges I want to tackle?". And my quest shall begin soon.

It's been a great honor working at LanceDB and I had great fun here. Special thanks goes to [Weston Pace](https://github.com/westonpace)!

*To reproduce the performance results in this article, benchmark code can be found *[*here*](https://github.com/broccoliSpicy/lance/tree/internship_summary_benchmark_code)*.*
```

---

## ðŸ“„ æ–‡ä»¶: drafts\lance-v0-15-0.md

---

```md
---
title: "Lance V0.15.0"
date: 2024-07-30
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/lance-v0-15-0/preview-image.png
meta_image: /assets/blog/lance-v0-15-0/preview-image.png
description: "Lance 0. 15."
---

Lance 0.15.0 introduces several experimental features, marking major milestones on important projects in our library. It exposes the first public APIs for full-text search indices. It also provides opt-in support for two new encodings: better string compression using FSST and a packed struct encoding optimized for random access. These advancements move us another step closer to being the best format for AI data lakes.

This release wouldn't be possible without the hard work of our amazing contributors. While many of our major features are driven by full-time employees at LanceDB, we are excited to see some major features being pushed forward by community members. This release contains one such feature: FSST string compression. We extend our gratitude to everyone who has contributed to Lance thus far, and look forward to continuing to innovate together.

## New inverted indices for full-text search

Initial full-text search support is now exposed in Lanceâ€™s API. These indices are called "inverted indices" in Lance.

You can pass the `full_text_query` parameter to the query APIs, such as `dataset.to_table()`. This only works for columns where an index is present. Trying to search a string column with no index will simply give an error right now. To create an inverted index, pass `index_type="INVERTED"` to `Dataset.create_scalar_index()`. For example, you can search a set of headlines like so:

    import lance
    import pyarrow as pa

    headlines = [
        "Scientists may have discovered 'dark oxygen' being created without photosynthesis",
        "A hydrothermal explosion sends Yellowstone visitors running",
        "Not a B movie: Sharks are ingesting cocaine in the ocean, scientists find",
        "A study finds that dogs can smell your stress â€” and make decisions accordingly",
        "Bats have a lot of secrets. These bat-loving scientists are investigating",
        "Crows can count out loud like human toddlers â€” when they aren't cheating the test",
    ]
    data = pa.table({"headline": headlines})
    dataset = lance.write_dataset(data, "test_path")

    # If you use without an index, will give error:
    # "LanceError(IO): Column headline has no inverted index"
    dataset.create_scalar_index("headline", index_type="INVERTED")

    dataset.to_table(full_text_query="human")

    pyarrow.Table
    headline: string
    ----
    headline: [["Crows can count out loud like human toddlers â€” when they aren't cheating the test"]]

Full text search already exists downstream in LanceDB Python. However, itâ€™s implemented using Tantivy Python, so it doesnâ€™t work for the Node or Rust bindings. Additionally, Tantivyâ€™s indices are designed for local filesystems, so those indices donâ€™t work on object storage like S3. By rewriting full-text search in our Lance Rust library, weâ€™ll be able to remove both these limitations.

Thanks to LanceDB engineer Yang Cen for his work on this feature. You can follow progress on this feature in the [Full-Text Search epic](https://github.com/lancedb/lance/issues/1195).

## FSST string compression

Experimental support for string compression has been added. This uses the "FSST" encoding, developed by Peter Boncz, Thomas Neumann, and Viktor Leis ([article](https://www.vldb.org/pvldb/vol13/p2649-boncz.pdf)), and used in modern systems like DuckDB ([link](https://duckdb.org/2022/10/28/lightweight-compression.html#fsst)). This makes string columns much smaller on disk but, unlike generic compression such as Snappy, it still allows random access. That is, you can access a single string value without having to decode an entire page of column data.

In many cases, FSST can achieve similar compression ratios to Snappy. As an example, we compressed 200,000 Wikipedia articles. FSST provided 39.4% smaller files in Lance, while Parquet with Snappy compression provided a 42.8% smaller file than in-memory uncompressed data.
![](__GHOST_URL__/content/images/2024/07/data-src-image-855d126c-e47f-44a7-b692-75aaba859bd2.png)
FSST in Lance is available for the Lance V2 format behind an environment variable. To try it out, set the `LANCE_USE_FSST` to `true` and pass the flag `use_legacy_format=False` when writing data. FSST compression will become available by default in a future version of Lance.

Our Rust implementation of FSST was developed by community member Jun Wang (`broccoliSpicy` on GitHub) and LanceDB engineer Weston Pace ([PR](https://github.com/lancedb/lance/pull/2470)). Big thanks to both of them. You can follow the progress of this feature in the [FSST epic](https://github.com/lancedb/lance/issues/2602).

## Packed struct encoding

In addition to better string compression, we also have released an experimental encoding for struct columns that makes them better for random access. Currently, if you read a row of a struct column, we must issue a separate IO call for each sub column. The packed struct encoding puts all the data for a single row together, so a full row can be retrieved in a single IO call. This makes random access much faster, including when retrieving rows for a vector search query.

    import pyarrow.compute as pc

    # 10 columns of random float data
    inner_data = pa.table({
        f'x{i}': pc.random(1000)
        for i in range(10)
    }).to_batches()[0]
    struct_column = pa.StructArray.from_arrays(inner_data.columns, inner_data.schema.names)

    schema = pa.schema([
        pa.field('before', struct_column.type),
        pa.field('packed', struct_column.type, metadata={b"packed": b"true"})
    ])
    table = pa.table({
        'before': struct_column,
        'packed': struct_column
    }, schema=schema)

    dataset = lance.write_dataset(table, "test_packed_struct", use_legacy_format=False)

    import random
    indices = [random.randint(0, 1000) for _ in range(25)]

    %timeit dataset.take([42, 35, 12, 11], columns=['before'])
    %timeit dataset.take([42, 35, 12, 11], columns=['packed'])

    392 Î¼s Â± 17.5 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)
    220 Î¼s Â± 1.51 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)

That's 40% faster access. This is testing on a local SSD with little throughput. When there is much higher load on the disk or using a higher latency system such as S3, the difference may be more dramatic.

Like the FSST encoding, this is also not turned on by default. To turn this on for a struct column, add the metadata `{b"packed": b"true"}` to the field in the schema.

This performance improvement has a tradeoff. Reading a single sub column of a struct will be slower with the packed encoding than with the standard "shredded" encoding. Therefore, consider how you will be querying your data before deciding to use packed structs. They aren't the best choice in all cases. For that reason, it will likely never be turned on by default.

Thanks to LanceDB engineer Raunak Shah for implementing this new encoding ([PR](https://github.com/lancedb/lance/pull/2593)).

## Conclusion

These are the major changes in v0.15.0, but not the only ones. For a full list of changes, see our change log: [https://github.com/lancedb/lance/releases/tag/v0.15.0](https://github.com/lancedb/lance/releases/tag/v0.15.0)

We continue to innovate on the Lance format, implementing additional index types and encodings. The combination of full-text search and FSST string compression will make Lance a great choice for search. Packed struct layouts will improve retrieval performance for many use cases as well. Together, we are building a more powerful and efficient data lake for the AI era. Your contributions and feedback are crucial as we move forward. We look forward to what we can build together.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\lance-v0-16-1-feature-roundup.md

---

```md
---
title: "Lance V0.16.1 Feature Roundup"
date: 2024-08-26
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/lance-v0-16-1-feature-roundup/preview-image.png
meta_image: /assets/blog/lance-v0-16-1-feature-roundup/preview-image.png
description: "In Lance v0. 16."
---

In Lance v0.16.1 we introduced several new features, implemented by a combination of LanceDB engineers and community contributors. Lance is an OSS project that is open-to-contribution, so we are very pleased to have major features brought by community members.

In this blog post, weâ€™ll highlight some of the most important new features, which include:

- Version tagging
- Update subschemas in `merge_insert`
- New file format versioning API
- Distributed ANN index creation API

## Version tags

Lance uses multi-version concurrency control, which means each change you make to the dataset creates a new version. Many of these versions are temporary, and later cleaned up. But some you might want to keep around for a while and give a name. With the new Tags feature, you can now do this.
![](__GHOST_URL__/content/images/2024/08/versions_feature.png)
Any version can be given one or more tags. Once tagged, a version of the dataset canâ€™t be deleted, unless you delete the tag. When you load a dataset, you can pass the tag name to load that version.

For example, we can create three versions of a dataset by applying three write operations:

    import pyarrow as pa
    import lance

    assert lance.__version__ == "0.16.1", "wrong version: %s" % lance.__version__

    data = pa.table({"x": [1, 2, 3]})
    ds = lance.write_dataset(data, "dataset")
    ds = lance.write_dataset(data, "dataset", mode="append")
    ds = lance.write_dataset(data, "dataset", mode="overwrite")

    ds.version

    3

We can label the first two versions with a tag. The first one we label as `original` and the second we label as `duplicated`. Now we can pass the `duplicated` tag when opening a dataset to see the version where we duplicated some of the data:

    ds.tags.create("original", 1)
    ds.tags.create("duplicated", 2)
    lance.dataset("dataset", version="duplicated").to_table()

    pyarrow.Table
    x: int64
    ----
    x: [[1,2,3],[1,2,3]]

We can also pass the `original` tag to the `checkout_version` method:

    ds.checkout_version("original").to_table()

    pyarrow.Table
    x: int64
    ----
    x: [[1,2,3]]

We hope these tags open up new workflows for working with datasets and managing versions. We currently support creating and deleting tags. But we also have [an open `good-first-issue` to add support for updating tags](https://github.com/lancedb/lance/issues/2742) too, which could unlock other use cases. For example, imagine maintaining a `production` tag that is live to users, allowing you to perform operations on your dataset and validate the correctness and performance *first* before updating the tag to point to a new version.

Community contributor [dsgibbons](https://github.com/lancedb/lance/pulls?q=is%3Apr+author%3Adsgibbons+is%3Aclosed+) (GitHub username) implemented this new tags feature. Big thanks for their very high quality work. ([PR](https://github.com/lancedb/lance/pull/2482))

## Update subcolumns with merge_insert

Merge insert lets you merge new data into a table, inserting new rows and updating existing row. However, right now, you need to supply all the columns in your new data. Sometimes, you want to update a subset of columns.

One use case is where you have a table of documents, and you have a metrics column like â€œpopularityâ€ or â€œviewâ€ count. You would like to update those metrics, but donâ€™t want to have to re-insert the documents or vectors. In Lance v0.16.1, you can do just this! Pass only the match on column and those you wish to update, and it will update only the subset of columns.

    documents = pa.Table.from_pylist([
        dict(id=1, text="hello", popularity=100),
        dict(id=2, text="beautiful", popularity=5_000),
        dict(id=3, text="world", popularity=1_000),
    ])
    ds = lance.write_dataset(documents, "documents", mode="overwrite")
    ds.to_table()

    pyarrow.Table
    id: int64
    text: string
    popularity: int64
    ----
    id: [[1,2,3]]
    text: [["hello","beautiful","world"]]
    popularity: [[100,5000,1000]]

Updating the `popularity` column:

    new_metrics = pa.Table.from_pylist([
        dict(id=1, popularity=200),
        dict(id=2, popularity=10_000),
        dict(id=3, popularity=2_000),
    ])
    ds.merge_insert(on="id").when_matched_update_all().execute(new_metrics)
    ds.to_table()

    pyarrow.Table
    id: int64
    text: string
    popularity: int64
    ----
    id: [[1,2,3]]
    text: [["hello","world","!"]]
    popularity: [[200,10000,2000]]

This is an optimized code path that will only rewrite the relevant columns, even if it affects all rows. This can be a huge savings in AI data, where unstructured data like text or vector embeddings would be expensive to rewrite. For example, we can show a 5,000-fold speedup in rewriting a metric column in a vector dataset with 1 million 1536-dimensional vectors:

    # 1 million 1536 dimensional 32-bit vectors
    import pyarrow.compute as pc
    dim = 1536

    def next_batch(batch_size, offset):
        global i
        values = pc.random(dim * batch_size).cast('float32')
        return pa.table({
            'id': pa.array([offset + j for j in range(batch_size)]),
            'vector': pa.FixedSizeListArray.from_arrays(values, dim),
            'metric': pc.random(batch_size),
        }).to_batches()[0]

    def batch_iter(num_rows):
        i = 0
        while i < num_rows:
            batch_size = min(10_000, num_rows - i)
            yield next_batch(batch_size, i)
            i += batch_size

    schema = next_batch(1, 0).schema

    ds = lance.write_dataset(batch_iter(1_000_000), "vectors", schema=schema, mode="overwrite", data_storage_version="2.0")

Previously, you would have to rewrite all columns, which takes over 50s:

    %timeit ds.merge_insert(on="id").when_matched_update_all().execute(pa.RecordBatchReader.from_batches(schema, batch_iter(1_000_000)))

    52.9 s Â± 4.26 s per loop (mean Â± std. dev. of 7 runs, 1 loop each)

Now you can just update the metric column in 10ms, which is about 5,000x times faster:

    def narrow_iter(num_rows):
        for batch in batch_iter(num_rows):
            yield batch.select(["id", "metric"])
    update_schema = schema.remove(schema.get_field_index("vector"))
    reader = pa.RecordBatchReader.from_batches(update_schema, narrow_iter(1_000_000))
    %timeit ds.merge_insert(on="id").when_matched_update_all().execute(reader)

    9.73 ms Â± 776 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)

We hope this feature unlocks new use cases for storing metrics along side vectors in Lance and LanceDB.

This was implemented by Will Jones (me!). ([PR](https://github.com/lancedb/lance/pull/2639))

## V2 format versioning API

As we stabilize the current V2 file format, we are establishing a versioning scheme for our data files. Previously, we provided a `use_legacy_format` flag to select between V1 and V2 files. But in order to allow continuously rolling out new features in the file format, we'll be providing finer grain version selection. We are close to stabilizing a 2.0 format, and have moved current unstable features into the 2.1 format. The current format versions are:

- 0.1 (legacy): The original Lance file format
- 2.0 (stable): The currently stable V2 format
- 2.1 (next): V2 format with FSST and bitpacking

As we roll out new features in the format, weâ€™ll be adding new versions. An up-to-date version of this version list will be maintained in our docs at: [https://lancedb.github.io/lance/format.html#file-version](https://lancedb.github.io/lance/format.html#file-version)

When creating new dataset, you can select any of these versions. The legacy / 0.1 version remains the default for now, until we announce the full stability of the 2.0 format. You can either request the file format version with a label ("stable", "legacy", or "next"):

    ds_legacy = lance.write_dataset(data, "new_dataset",
                                    data_storage_version="stable")
    ds_legacy.data_storage_version

    '2.0'

Or you can pass an explicit version:

    ds_legacy = lance.write_dataset(data, "experimental_dataset",
                                    data_storage_version="2.1")
    ds_legacy.data_storage_version

    '2.1'

Explicit versions will be a good choice if you know the exact version you want to use. Whereas `stable` is a good choice if you want to pick up the latest features, and `next` if you want to try out some unstable features. (We do not recommend `next` for any production use case. It's there for the purposes of collecting feedback on in-progress features.)

As we continue to develop the format and stabilize features, the `stable` and `next` version labels will move up versions. This will mean new tables created with these same tags will get new features as they are rolled out. However, existing tables will be kept at a fixed number version, resolved at the time you create the table. You will be able to migrate existing tables to new versions.

This new API was developed by LanceDB Engineer Weston Pace, who has been heading up our V2 file format development. ([PR](https://github.com/lancedb/lance/pull/2673))

## Distributed indexing

On the indexing front, we are exposing new APIs to scale up indexing performance. Our new distributed APIs allow breaking down the steps of creating an IVF_PQ ANN index and parallelizing some of them. By breaking down into steps, you can re-use certain intermediate results. This can be useful if you need to retry aborted jobs or restart execution with more resources.
![](__GHOST_URL__/content/images/2024/08/distributed-indexing-feature.png)
The most intensive steps are now able to be run in parallel. One of these is transforming the vectors from their lossless representation to their PQ-compressed representation. The other is assigning vectors to IVF partitions. These can be performed as a single transformation, and done completely in parallel. The other step that can be done in parallel is local shuffling vectors into partitions.

    import pyarrow.compute as pc

    dims = 128
    nrows = 100_000
    inner_vector = pc.cast(pc.random(nrows * dims), pa.float32())
    vector = pa.FixedSizeListArray.from_arrays(inner_vector, dims)
    table = pa.table({
        "vector": vector,
        "id": range(nrows),
    })
    # Using a small number of rows per file, to simulate a large dataset with
    # many files.
    ds = lance.write_dataset(table, 'test', mode='overwrite', max_rows_per_file=2_000)

Previously, creating an index was one step. This is still a good way to do indexing on a single machine, as it still uses multiple threads to take advantage of multi-core CPUs. Only when you want to do distributed indexing does the new API make sense.

    # Indexing on single machine looks like:
    ds.create_index(
        column='vector',
        index_type='IVF_PQ',
        metric="cosine",
        num_partitions=256,
        num_sub_vectors=32,
    )

The first two steps are creating the IVF and PQ models:

    from lance.indices import IvfModel, PqModel, IndicesBuilder

    builder = IndicesBuilder(ds, "vector")

    ivf = builder.train_ivf(
        num_partitions=256,
        sample_rate=256,
        distance_type="l2",
    )

    pq = builder.train_pq(ivf, num_subvectors=32)

Both of these can be saved to file. This is so they can be re-used, but also because in the distributed steps later they will need to be read by other processes.

    import tempfile
    import os

    tmp_dir = tempfile.TemporaryDirectory()

    ivf.save(os.path.join(tmp_dir.name, "ivf.index"))
    pq.save(os.path.join(tmp_dir.name, "pq.index"))

Now we are at the step we can parallelize. As a simple example, we use a `ThreadPoolExecutor`, but this can be any sort of distributed process. (In fact, using a thread pool is a bad idea for real work. The `transform_vectors` step already has internal threading, so there's no point in trying to distribute multiple invocations across threads.) We use the fragments as the unit of parallelism, creating one task per fragment. Each of these becomes one output file.

    from concurrent.futures import ThreadPoolExecutor

    def transform_step(
        ds_uri: str,
        working_dir: str,
        fragment_id: int
    ) -> str:
        ds = lance.dataset(ds_uri)
        fragment = ds.get_fragment(fragment_id)

        ivf = IvfModel.load(os.path.join(working_dir, "ivf.index"))
        pq = PqModel.load(os.path.join(working_dir, "pq.index"))

        filename = "partition_" + str(fragment_id)
        uri = os.path.join(working_dir, filename)
        builder.transform_vectors(ivf, pq, uri, fragments=[fragment])
        return filename

    pool = ThreadPoolExecutor(max_workers=4)

    fragment_ids = [frag.fragment_id for frag in ds.get_fragments()]
    transformed_files = list(pool.map(
        lambda fragment_id: transform_step(ds.uri, tmp_dir.name, fragment_id),
        fragment_ids
    ))

Next, we can do the shuffle steps, which can also be done in parallel:

    from typing import List

    def shuffle_step(
        working_dir: str,
        transformed_files: List[str],
        job_id: int,
    ):
        ivf = IvfModel.load(os.path.join(working_dir, "ivf.index"))

        return builder.shuffle_transformed_vectors(
            transformed_files,
            dir_path=tmp_dir.name,
            ivf=ivf,
            shuffle_output_root_filename=f"shuffled_{job_id}",
        )

    # Do one shuffle job per file
    jobs = pool.map(
        lambda x: shuffle_step(tmp_dir.name, [x[1]], x[0]),
        zip(range(4), transformed_files)
    )
    shuffled_files = [file for job in jobs for file in job]

The final step is to combine these shuffled files into an index file and commit that to the dataset:

    builder.load_shuffled_vectors(
        shuffled_files,
        dir_path=tmp_dir.name,
        ivf=ivf,
        pq=pq
    )

    ds = ds.checkout_version(ds.latest_version)
    ds.list_indices()

    [{'name': 'vector_idx',
      'type': 'Vector',
      'uuid': 'a657b964-d19e-41cc-ae8b-389cd2a95f9e',
      'fields': ['vector'],
      'version': 40,
      'fragment_ids': {0,
       1,
       2,
       3,
       ...
       }}]

Given the complexity of the steps, this is a low-level API we donâ€™t expect any but the most motivated power users to call. Instead, these power users may wrap them in distributed frameworks familiar to users, such as Ray and Spark. This will allow users to leverage their existing cluster infrastructure for their large-scale indexing jobs.

These features were developed by LanceDB engineer Weston Pace and our recent intern Raunak Shah.

## Conclusion

These are a few of the major changes in `v0.16.0` and `v0.16.1`. For a full list of changes, see our change logs:

- [https://github.com/lancedb/lance/releases/tag/v0.16.0](https://github.com/lancedb/lance/releases/tag/v0.16.0)
- [https://github.com/lancedb/lance/releases/tag/v0.16.1](https://github.com/lancedb/lance/releases/tag/v0.16.1)

In summary, the updates in Lance v0.16.1 focus on enhancing data management, improving performance, and facilitating distributed processing. The introduction of version tagging allows users to manage dataset versions more effectively. The ability to update subcolumns with `merge_insert` optimizes data updates, especially beneficial for large datasets. The stabilization of the V2 format ensures consistency and reliability in data storage. Lastly, the new distributed indexing APIs enable more efficient processing of large-scale datasets, paving the way for advanced and scalable data operations. These improvements not only streamline workflows but also expand the possibilities for users working with large and complex datasets.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\lance-v2-is-now-in-beta.md

---

```md
---
title: "Lance V2 Is Now in Beta"
date: 2024-06-05
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/lance-v2-is-now-in-beta/preview-image.png
meta_image: /assets/blog/lance-v2-is-now-in-beta/preview-image.png
description: "Lance v2 beta brings faster scans, true null support, and easier large multimodal ingestion. This post shows how to enable v2, whatâ€™s improved over v1, and the roadmap ahead."
---

We've been talking for a while about a [new iteration of our file format](__GHOST_URL__/lance-v2/).  We're pleased to announce that the new v2 format is now in available as an opt-in feature for Lance (in release [0.12.1](https://github.com/lancedb/lance/releases/tag/v0.12.1)) and LanceDB (release [0.8.2](https://github.com/lancedb/lancedb/releases/tag/python-v0.8.2)) for users that want to try it out ahead of time and give us some early feedback.

ðŸ’¡

This beta feature should only be used for evaluation & experimentation. There are not many known bugs but we have not yet extensively tested all workflows. There is a chance that we may need to make a format change during this beta period and data created with the beta will not be migrated.

ðŸ’¡

(UPDATE) The Lance v2 format is now the default in pylance and lancedb. We will leave this blog up for historical purposes but some of the information may be outdated.

## How can you help?

We're eager for current Lance and LanceDB users to run experiments and benchmarks with the new format.  We especially want to know:

- Do you run into any bugs using the new file format in your workflow?
- Are there any situations where the performance of the new file format is slower than the old format?

If you run into any issues please make sure to [file an issue](https://github.com/lancedb/lance/issues/new) or let us know on [Discord](https://discord.gg/G5DcmnZWKB).

## How to enable Lance v2

Turning on support for the new format is easy, but limited to creating new tables.  Migration of old data will arrive as we are more confident in the stability of the format.  To enable the new format simply set the `use_legacy_format` option to `False` when creating the table.

    # By default, tables are still created in v1 mode
    v1_tbl = await db.create_table("v1", data)

    # The use_legacy_format flag will let you opt-in to v2 files
    v2_tbl = await db.create_table("v2", data, use_legacy_format=False)

ðŸ’¡

These instructions are now outdated. The v2 format is the default in pylance and lancedb. Nothing should be needed. Old datasets, which are still using the v1 format, can be migrated to v2 by creating a copy of the dataset.

## What's available in the beta?

The beta supports all *existing* dataset operations today (except dictionary encoding).  In addition, most of the *new* functionality is already available.  This means you can already start taking advantage of the following features.

### Support for nulls (not placeholders) in all data types (except struct)

In Lance v1 we automatically converted nulls to a placeholder value (0 for numeric types and empty lists/strings for string/list types).  This is a problem since it is impossible to distinguish between a null and a placeholder after the data is written.  We now support nulls for all data types except struct (which we will be working on shortly).

    data = pa.table(
        {
            "ints": [1, None],
            "floats": [1.1, None],
            "strings": ["", None],
            "lists": [[1, 2, 3], None],
            "vectors": pa.array([[1.0, 2.0], None], pa.list_(pa.float64(), 2)),
        }
    )

    def print_table(tbl):
        for name in tbl.column_names:
            print(name, tbl.column(name).to_pylist())

    v1_tbl = await db.create_table("v1", data)
    print("With legacy format")
    print_table(await v1_tbl.query().to_arrow())

    # With legacy format
    # ints [1, 0]
    # floats [1.1, 0.0]
    # strings [None, None]
    # lists [[1, 2, 3], []]
    # vectors [[1.0, 2.0], [0.0, 0.0]]

    v2_tbl = await db.create_table("v2", data, use_legacy_format=False)
    print("With new v2 format")
    print_table(await v2_tbl.query().to_arrow())

    # With new v2 format
    # ints [1, None]
    # floats [1.1, None]
    # strings ['', None]
    # lists [[1, 2, 3], None]
    # vectors [[1.0, 2.0], None]

### Easily create files without using excessive RAM

In Lance v1 both the RAM used by the file readers/writers and the performance of the resulting file were controlled by the row group size.  In Lance v2 there is no row group size.  It's now much easier to create large multi-modal datasets without using an excessive amount of memory.

    db = await lancedb.connect_async("/tmp/my_db")

    image_paths = glob.glob("/home/pace/dev/data/Dogs/**/*.jpg")

    def load_images():
        for idx, image_path in enumerate(image_paths):
            img = Image.open(image_path)
            img_bytes = io.BytesIO()
            img.save(img_bytes, format="PNG")
            yield pa.table({"img": [img_bytes.getvalue()], "ids": [idx]})

    # With the v1 format this command could potentially run out of memory
    # with large images or videos as it had to buffer an entire row group.
    #
    # This is especially true when you have many parallel writers which is
    # common when your ingestion path needs to encode or decode images.
    #
    # With the v2 format there is no concern about RAM.
    tbl = await db.create_table(
        "images",
        load_images(batch_size),
        schema=pa.schema([pa.field("img", pa.binary()), pa.field("ids", pa.int64())]),
        use_legacy_format=False,
    )

    # Since we don't need to clump data into small row groups any more the
    # performance of this query is 6x faster than the v1 counterpart.
    await tbl.query().select(["ids"]).to_arrow()

### Faster scans

LanceDB has always supported lightning fast vector searches.  However, with the Lance v1 format, full scans of the dataset could sometimes be slow, especially when smaller row groups were needed.  In Lance v2 a full dataset scan is much more efficient, giving stellar performance for scalar columns and vector columns alike.

    db = await lancedb.connect_async("/tmp/my_db")

    data = pq.read_table("/home/pace/dev/data/lineitem_10.parquet")

    tbl = await db.create_table("lineitem", data, use_legacy_format=False)

    # Querying TPC-H data is over 3x faster with Lance v2
    async for batch in await tbl.query().to_batches(max_batch_length=8192):
        pass

### Whats still in progress?

We have a rough roadmap describing the major features we will be working on over the summer.
![](__GHOST_URL__/content/images/2024/06/Lance-v2-Roadmap-2.png)
### Statistics / Pushdown Filtering

Pushdown filtering is nearly complete.  Lance v2 will have the same great pushdown filtering experience as Lance v1, enabling both statistics-based zone skipping as well as late materialization.  Lance v2 also allows new statistics to be added as custom encodings paving the way for interesting experiments and specialized use cases.

### Nulls in Structs

Null support for structs is planned and will be arriving soon.  This will support "null struct", "struct of nulls", and everything in between.

### Compressive Encodings

One of the most asked for features in LanceDB is the ability to use compression on scalar columns.  We are in the middle of developing a number of special encodings which provide compression without slowing down point lookups or introducing excess read amplificationThis will not only lead to less disk utilization but also should lead to significantly faster queries.

### Migration Support

Once the v2 format is stable we will offer a command to migrate an existing dataset to the new format.

### Custom Encoding SDK

Users with specialized use cases will be able to implement their own encodings.  Some examples of experiments we would like to do with this capability are "packed structs" (virtually combine several columns into one to speed up lookups that always load the same set of columns), "variants" (columns that can have more than one data type, essential for storing semi-structured or unstructured data), and multi-modal specific encodings (e.g. pushdown filtering for audio/images)

### Performance Optimization

Finally, we will continue to optimize v2 at high scale in the coming weeks to further improve performance for a wide variety of use cases. If you're interested in kicking the tires on your data, we'd love to get your feedback.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\lancedb-polars-2d5eb32a8aa3.md

---

```md
---
title: "LanceDB + Polars"
date: 2024-01-19
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/lancedb-polars-2d5eb32a8aa3/preview-image.png
meta_image: /assets/blog/lancedb-polars-2d5eb32a8aa3/preview-image.png
description: "Get about lancedb + polars. Get practical steps, examples, and best practices you can use now."
---

### A (near) perfect match

A spiritual successor to [pandas](https://github.com/pandas-dev/pandas), [Polars](https://github.com/pola-rs/polars) is a new blazing fast DataFrame library for Python written in Rust. At LanceDB, we hear from many of our users that they process data using Polars before inserting it into LanceDB and converting search results into Polars data frames for downstream processing. To make things easier for them, we decided to add a direct integration with Polars. Now LanceDB can ingest Polars dataframes, return results as polars dataframes, and export the entire table as a polars lazyframe. And a quick acknowledgment, this integration was made possible by the great work of Ritchie and Kyle [supporting FixedSizeList](https://github.com/pola-rs/polars/pull/8943), which makes storing and working with embeddings way easier.

> LanceDB, as of v0.4.5, can ingest Polars DataFrames, return results as Polars DataFrames,, and export the entire table as a Polars LazyFrame.

## A Simple Walkthrough

Letâ€™s walk through a simple example of how the LanceDB-polars integration works. If you just want to see all of the code together, please see this [colab notebook for details](https://colab.research.google.com/drive/15EKQmXqfMDouQINpA1SjA-zbSXbukpaZ?usp=sharing).

## â€ï¸ Read raw data into Polars

For this walkthrough, weâ€™ll be retrieving Rick and Morty quotes to answer user questions. Letâ€™s start with reading the raw data in CSV format using polars:
![](https://miro.medium.com/v2/resize:fit:770/1*DCpIahADQbISYT0NzzLt0w.png)Raw input data is a csv file with the quote id, the quote, and the quote author
## Setup embedding model

Next, weâ€™re going to set up a LanceDB table for retrieval. For this table, weâ€™ll use the `sentence-transformers` model from [HuggingFace](https://huggingface.co/sentence-transformers). It is integrated as one of the default embedding models in LanceDB. To keep the installation footprint small, HuggingFace is an *optional* dependency for LanceDB, so if youâ€™re starting from a fresh environment, youâ€™ll want to first run `pip install -U sentence-transformers`.
![](https://miro.medium.com/v2/resize:fit:770/1*5WCj3ewbNe-N5HADxjAbeg.png)Creating an embedding function
You can refer to other APIâ€™s or pretrained embedding models by name like â€œopenaiâ€, â€œcohereâ€, â€œopenclipâ€, and more. You can see all of the [available functions here](https://lancedb.github.io/lancedb/embeddings/default_embedding_functions/), or you can extend it with your own.

## Ingest Polars into LanceDB

Next, weâ€™re ready to create a LanceDB table for semantic search. We can use pydantic to make creating the table schema easier by creating a `LanceModel` (subclass of `pydantic.BaseModel`). As you can see below, the table schema matches the polars dataframe schema, with the exception of an additional `vector` column. The embedding function we created can be configured to automatically create the vectors at ingestion time. Weâ€™re using pydantic annotations to indicate that `quote` is the source column for embeddings and `vector` is to be generated from the source column using the sentence-transformers model we set up.
![](https://miro.medium.com/v2/resize:fit:770/1*2VbkYnesnJUC1ivWeFhL0g.png)Table schema matches the input data, with the exception of a vector column
Now we can put it together. Weâ€™ll create the table under the `~/.lancedb` directory. We then use the `Quotes` pydantic model to create the table by using the `schema` parameter. And finally, we add the polars dataframe `df` to the LanceDB table. Note that the embedding generation is handled automatically by LanceDB.
![](https://miro.medium.com/v2/resize:fit:770/1*42oY_hBjRATPkITTzWOyVQ.png)
## Querying the table

Now we can ask Rick and Morty some really deep philosophical questions, like â€œWhat is the purpose of existence?â€, and we can export the results as a polars dataframe. As you would expect, the top answer seems to be that â€œNobody exists on purposeâ€.
![](https://miro.medium.com/v2/resize:fit:770/1*sLid0_9Uhz1vw0MmwBN_Fg.png)This show is pretty dark
## Reading the whole table into Polars

As you iterate on your application, youâ€™ll likely need to work with the whole tableâ€™s data pretty frequently. LanceDB tables can also be converted directly into a polars `LazyFrame` . The reason why we donâ€™t convert it to a DataFrame is because LanceDB tables can potentially be very large, like way bigger than memory, and LazyFrames allows us to work with large datasets easily.
![](https://miro.medium.com/v2/resize:fit:770/1*Wrrj0-K7vDgQ35JwibpTpA.png)letâ€™s be lazy
[UsageWith the lazy API, Polars doesn't run each query line-by-line but instead processes the full query end-to-end.docs.pola.rs](https://docs.pola.rs/user-guide/lazy/using/?source=post_page-----2d5eb32a8aa3--------------------------------)

Of course, if you know your table will fit into memory, itâ€™s pretty easy to do `lancedb_table.to_polars().collect()` to turn the LazyFrame into a DataFrame.

## Room for Improvement

LanceDB and polars work really well with each other, but there are many places where the user experience can be improved even more.

## Batch ingestion of LazyFrame

Currently, you can ingest a DataFrame but not a LazyFrame. And if you try to pass in a LazyFrame anyway, youâ€™ll get a TypeError:
![](https://miro.medium.com/v2/resize:fit:770/1*VTV1hT5-rZbdG2izwcxl2w.png)
This means that if you have a large dataset, youâ€™ll have to manually read data in batches. The reason for this is that polars does not have an API to iterate over batches of a LazyFrame.

> polars does not have an API to iterate over batches of a LazyFrame

## Filter pushdowns

We use the `scan_pyarrow_dataset` to convert to LazyFrame. Polars is able to push-down some filters to a pyarrow dataset, but the pyarrow dataset expects pyarrow compute expressions while Lance expects SQL strings. This means that weâ€™ve had to disable the filter push-downs, meaning that Polars wonâ€™t be able to take advantage of the fast filtering and indexing features of Lance.

[Cloud storagePolars can read and write to AWS S3, Azure Blob Storage and Google Cloud Storage. The API is the same for all threeâ€¦docs.pola.rs](https://docs.pola.rs/user-guide/io/cloud-storage/?source=post_page-----2d5eb32a8aa3--------------------------------#scanning-with-pyarrow)

> It would be great if Polars can push-down filters to LanceDB as SQL string instead of compute expression

And just musing on the long-term future of integrations between libraries that can pass off computation subtasks to each other, it would be great to coalesce around a standard like [Substrait](https://substrait.io/). Like how Arrow has become the standard for the exchange of in-memory data, Substrait can become the standard for the exchange of computational IR.

## Native Lance reader + UDFs

Currently, the interface between Lance and polars is the `scan_pyarrow_dataset` function in polars, which means it needs to be compatible with the generic pyarrow dataset interface. If there was a direct integration for Lance, we could build a udf and allow polars to push-down vector search directly. A sample API could look like:

ldf = pl.scan_lance("~/.lancedb/really_large_table.lance")
df = ldf.search("A natural language query").limit(10).collect()

## Integration at the Rust level

So far, the integration between LanceDB and Polars happens at the Python level, but I think thereâ€™s an opportunity to push the integration down to Rust. This can then enable a cross-language integration, for example, polars-nodejs + lancedb nodejs, which is an ecosystem thatâ€™s becoming a lot more important for AI.

## Conclusion

I think 2024 will be the year of Rust ðŸ¦€ for data infrastructure. Thereâ€™s a plethora of opportunities for Rust-based projects to integrate and create a rich ecosystem of data tooling for data eng, ML, and AI. If you use polars today, setting up a RAG application just became a little easier. And if youâ€™re interested in working on Rust and open-source, weâ€™re hiring (contact@lancedb.com)!
```

---

## ðŸ“„ æ–‡ä»¶: drafts\langoid-multi-agent-programming-framework-for-llms-e609377de227.md

---

```md
---
title: "Langroid: Multi-Agent Programming Framework for LLMs"
date: 2024-01-08
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/langoid-multi-agent-programming-framework-for-llms-e609377de227/preview-image.png
meta_image: /assets/blog/langoid-multi-agent-programming-framework-for-llms-e609377de227/preview-image.png
description: "In this era of Large Language Models (LLMs), there is unprecedented demand to create intelligent applications powered by this transformative technology."
---

In this era of Large Language Models (LLMs), there is unprecedented demand to create intelligent applications powered by this transformative technology. What is the best way for developers to harness the potential of LLMs in complex application scenarios?

For a variety of technical and practical reasons (context length limitations, LLM brittleness, latency, token costs), this is not as simple as throwing a task at an LLM system and expecting it to get done. What is needed is a principled programming framework, offering the right set of abstractions and primitives to make developers productive when building LLM applications.

## Langroidâ€™s Elegant Multi-Agent Paradigm

The [Langroid](https://github.com/langroid/langroid) team (ex-CMU/UW-Madison researchers) has a unique take on this â€” they have built an open-source Python framework to simplify LLM application development using a Multi-Agent Programming paradigm. Langroidâ€™s architecture is founded on Agents as first-class citizens: they are message-transformers and accomplish tasks collaboratively via messages.

Langroid is emerging as a popular LLM framework; developers appreciate its clean design and intuitive, extensible architecture. Programming with Langroid is natural and even fun: you configure Agents and equip them with capabilities ( such as LLMs, vector databases, and Function-calling/tools), connect them, and have them collaborate via messages. This is a â€œConversational Programmingâ€ paradigm and works with local/open and remote/proprietary LLMs. (Importantly, it does not use LangChain or any other existing LLM framework).
![](https://miro.medium.com/v2/resize:fit:700/0*ZyNDw0VdCOT0n3eM.png)An Agent serves as a convenient abstraction, encapsulating the state of LLM conversations, access to vector stores, and various tools (functions or plugins). A Multi-Agent Programming framework seamlessly aligns with the demands of complex LLM-based applications.
An Agent serves as a convenient abstraction, encapsulating the state of LLM conversations, access to vector stores, and various tools (functions or plugins). A Multi-Agent Programming framework naturally aligns with the demands of complex LLM-based applications.

## Connecting Agents via Tasks

In Langroid, a ChatAgent has a set of â€œresponderâ€ methods, one for each â€œentityâ€: an LLM, a human, and a tool handler. However, it does not have any way to iterate through these responders. This is where the Task class comes in: A Task wraps an Agent and gives it the ability to loop through its responders via the `Task.run()` method.

A Task loop is organized around simple rules that govern when a responder is eligible to respond, what is considered a valid response, and when the task is complete. The simplest example of a Task loop is an interactive chat with the human user. A Task also enables an Agent to interact with other agents: other tasks can be added to a task as sub-tasks in a recursive, hierarchical (or DAG) structure. From a Taskâ€™s perspective, sub-tasks are just additional responders and present the same string-to-string message-transformation interface (function signature) as the Agentâ€™s â€œnativeâ€ responders. This is the key to the composability of tasks in Langroid since a sub-task can act the same way as an Agentâ€™s â€œnativeâ€ responders and is subject to the same rules of task orchestration. The result is that the same task orchestration mechanism seamlessly enables tool handling, retries when LLM deviates, and delegation to sub-tasks. More details are in the Langroid [quick-start guide](https://langroid.github.io/langroid/quick-start/).

## A Taste of Coding with Langroid

To get started with Langroid, simply install it from pypi into your virtual environment:

    pip install langroid

To directly chat with an OpenAI LLM, define the LLM configuration, instantiate a language model object, and interact with it: (Langroid works with non-OpenAI local/proprietary LLMs as well, see their [tutorial](https://langroid.github.io/langroid/tutorials/non-openai-llms/)) For the examples below, ensure you have a file `.env` containing your OpenAI API key with this line: `OPENAI_API_KEY=sk-...`.

The MDL does not maintain any conversation state; for that, you need a `ChatAgent`:

    import langroid as lr
    import langroid.language_models as lm
    llm_cfg = lm.OpenAIGPTConfig() # default GPT4-Turbo
    mdl = lm.OpenAIGPT(llm_cfg)
    mdl.chat("What is 3+4?", max_tokens=10)

    agent_cfg = lr.ChatAgentConfig(llm=llm_cfg)
    agent = lr.ChatAgent(agent_cfg)
    agent.llm_response("What is the capital of China?")
    agent.llm_response("What about France?") # interprets based on previous msg

    task = lr.Task(agent, name="Bot")
    task.run("Hello")

Have a Teacher Agent talk to a Student Agent:

    teacher = lr.ChatAgent(agent_cfg)
    teacher_task = lr.Task(
    teacher, name="Teacher",
    system_message="""
    Ask your student simple number-based questions, and give feedback.
    Start with a question.
    """,
    )
    student = lr.ChatAgent(agent_cfg)
    student_task = lr.Task(
    student, name="Student",
    system_message="Concisely answer your teacher's questions."
    )
    teacher_task.add_sub_task(student_task)
    teacher_task.run()

Retrieval Augmented Generation (RAG) and Vector Databases

One of the most popular LLM applications is question-answering on documents via Retrieval-Augmented Generation (RAG), powered by a vector database. Langroid has a built-in DocChatAgent that incorporates several advanced RAG techniques, clearly laid out so they can be easily understood and extended.

## Built-in Support for LanceDB

![](https://miro.medium.com/v2/resize:fit:500/1*1aHRaS2KR9jqGJlLiTo6CQ.jpeg)
Langroid uses LanceDB as the default vector store for its DocChatAgent.

Langroidâ€™s DocChatAgent uses the LanceDB serverless vector database by default. Since LanceDB uses file storage, it is easy to set up and use (no need for docker or cloud services), and due to its use of the Lance columnar format, it is highly performant and scalable. In addition, Langroid has a specialized `LanceDocChatAgent` that leverages LanceDB's unique features such as Full-text search, SQL-like filtering, and pandas dataframe interop. Setting up a basic RAG chatbot is as simple as (assume the previous imports):

    from langroid.agent.special.lance_doc_chat_agent import import (
        LanceDocChatAgent, DocChatAgentConfig
    )
    llm_config = lm.OpenAIGPTConfig()

    rag_agent_config = DocChatAgentConfig(
        llm=llm_config,
        doc_paths=["/path/to/my/docs"], # files, folders, or URLs.
    )
    rag_agent = LanceDocChatAgent(rag_agent_config)
    rag_task = lr.Task(rag_agent, name="RAG")
    rag_task.run()ðŸ“„Chat with IMDB movies dataset

For an example showcasing Tools/Function-calling + RAG in a multi-agent setup, see their quick-start [Colab notebook](https://colab.research.google.com/github/langroid/langroid/blob/main/examples/Langroid_quick_start.ipynb) which shows a 2-agent system where one agent is tasked with extracting structured information from a document and generates questions for the other agent to answer using RAG. In the Langroid-examples repo there is a [script](https://github.com/langroid/langroid-examples/blob/main/examples/docqa/chat_multi_extract.py) with the same functionality, and here is what it looks like in action:
![](https://miro.medium.com/v2/resize:fit:700/0*o47mJpw_AbdhgJi5.gif)
## Retrieval Augmented Analytics

One of the unique features of LanceDB is its SQL-like filtering and Pandas dataframe interoperability. LLMs are great at generating SQL queries, and also Pandas computation code such as `df.groupby("col").mean()`. This opens up a very interesting possibility, which we call **Retrieval Augmented Analytics:** Suppose a user has a large dataset of movie descriptions with metadata such as rating, year, and genre, and wants to ask:

> **What is the highest-rated Comedy movie about college students made after 2010?**

It is not hard to imagine that an LLM should be able to generate a **Query Plan** to answer this, consisting of:

- A SQL-like filter: `genre = "Comedy" and year > 2010`
- A Pandas computation: `df.loc[df["rating"].idxmax()]`
- A rephrased query given the filter: â€œMovie about college studentsâ€ (used for semantic/lexical search)

Langroidâ€™s Multi-Agent framework enables exactly this type of application. The [`LanceRAGTaskCreator`](https://github.com/langroid/langroid/blob/main/langroid/agent/special/lance_rag/lance_rag_task.py) takes a `LanceDocChatAgent` and adds two additional agents:

- QueryPlannerAgent: Generates the Query Plan
- QueryPlanCriticAgent: Critiques the Query Plan and Answer received from the RAG Agent so that the QueryPlanner can generate a better plan if needed.

Checkout the [`lance-rag-movies.py`](https://github.com/langroid/langroid-examples/blob/main/examples/docqa/lance-rag-movies.py) script in the langroid-examples repo to try this out.

## Try it out and get involved!

This was just a glimpse of what you can do with Langroid and how your code would look. Give it a shot and learn more about the features and roadmap of Langroid on their [GitHub repo](https://github.com/langroid/langroid). Langroid welcomes contributions, and they have a friendly [Discord](https://discord.gg/ZU36McDgDs) community.

If you like it, donâ€™t forget to drop a ðŸŒŸ.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\late-chunking-aka-chunked-pooling-2.md

---

```md
---
title: "Practical Introduction to Late Chunking or Chunked Pooling"
date: 2024-10-29
author: LanceDB
categories: ["Community"]
draft: true
featured: false
image: /assets/blog/late-chunking-aka-chunked-pooling-2/preview-image.png
meta_image: /assets/blog/late-chunking-aka-chunked-pooling-2/preview-image.png
description: "Master about practical introduction to late chunking or chunked pooling. Get practical steps, examples, and best practices you can use now."
---

ðŸ’¡

This is a community blog by Mahesh Deshwal

*Improved Contextual Retrieval over large Documents for RAG applications*

The pronoun is the King (or Queen), and so is the Context. I hope the previous line gave you the gist that, given the right context, you can use your brain or LLM more effectively to produce even better outcomes. In terms of RAG pipelines that we know and use, there are some inherent issues for long documents, especially ***Context loss***.

Let's take the example of a long document where the first paragraph talks about Messi, Football, Barca, and a lot of things. The second paragraph talks in pronouns about Messi's unmatched numbers and the third paragraph, it's all about the club's stats using "it", "them" etc. We make the chunks out of documents, embed each of those as usual, and search.

**Issue? Given a query:**

> How many years did Messi play for Barca?

How do you think the chunks would come out? There could be a sentence somewhere "He was creating history there for 18 Years". Since there is no direct info and relation given within consecutive sentences, trust me, there's going to be a lot of problems getting the right context as these things are spanning over multiple paragraphs and our traditional RAG won't be able to make a relation for "he" "Messi" "them". Solution?

### ColBERT: Late Interaction

Don't get confused by the Late Chunking topic in this blog. In this idea, what we do is:

1. ***Pass the Whole Document*** to the Model
2. Get ***`Token Embedding`*** for each token
3. ***Similarly***, get Token Embedding for each token in the query
4. Take the ***maximum similarity between each token of query*** to the document
5. Add those and ***retrieve***

Nice and easy but with a twist here. You see, in the Vanilla model if there are 10 chunks for a document and the model embedding size is 512, we'll get `num_chunks x 512` what you search for `1x512` query vector. If we see the same thing here in Colbert, it becomes `num_document_tokens x 512` for document and then you do the maximum similarity with `num_query_token x 512` and then do the final retrieval.

**Two problems arise**:

1. Huge space requirement for saving the vectors for each token in the document
2. Computational Overhead for getting the similarity

So what now?

### **Late Chunking**

![](__GHOST_URL__/content/images/2024/10/7421f971-d1f5-43e8-9552-38fdad4e36a0_text.gif)
Researchers from [Jina AI published a paper](https://arxiv.org/abs/2409.04701) that seems to be the middle ground for both issues. The idea is simple:

1. Take ***ANY*** model which is capable of doing ***Mean Pooling*** and (preferable) having a longer context window
2. Make the ***chunks out of the text*** and record their starting and ending boundaries in the original document (Like in Vanilla Chunking)
3. Pass the WHOLE document to the model and ***get the Token Embeddings*** (Like in Colbert)
4. Now ***split and group those tokens*** according to the chunk they belong to. Easy to do as you already know the start and end boundaries of a chunk.
5. ***Pool the embedding from each group*** to make 1 vector per Chunk (Like Vanilla Chunking)

Now think within a larger frame. The model knows the whole document and creates the embedding thinking of each word and line in context. So if a word  `Chunk-i` is being referenced indirectly  `Chunk-j` then the semantic meaning is already preserved in both so it gets the Colbert properties and the size is similar to the vanilla Chunking.

Let's see that in action with a live example
[

Google Colab

![](https://ssl.gstatic.com/colaboratory-static/common/619f9a0953a37862014e2aa1a229e12b/img/favicon.ico)

![](https://colab.research.google.com/img/colab_favicon_256px.png)
](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Advanced_RAG_Late_Chunking/Late_Chunking_(Chunked_Pooling).ipynb)
    !pip install transformers datasets einops lancedb sentence-transformers -qq

    from transformers import AutoModel
    from transformers import AutoTokenizer
    import pandas as pd
    import lancedb
    pd.set_option('max_colwidth', 200)

    # Any model which supports mean pooling can be used here. However, models with a large maximum context-length are preferred
    tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)
    model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)

    def chunk_by_sentences(input_text: str, tokenizer: callable):
        """
        Split the input text into sentences using the tokenizer
        args:
          input_text: The text snippet to split into sentences
          tokenizer: The tokenizer to use
        return: A tuple containing the list of text chunks and their corresponding token spans
        """
        inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)
        punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')
        sep_id = tokenizer.convert_tokens_to_ids('[SEP]')
        token_offsets = inputs['offset_mapping'][0]
        token_ids = inputs['input_ids'][0]
        chunk_positions = [
            (i, int(start + 1))
            for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))
            if token_id == punctuation_mark_id
            and (
                token_offsets[i + 1][0] - token_offsets[i][1] > 0
                or token_ids[i + 1] == sep_id
            )
        ]
        chunks = [
            input_text[x[1] : y[1]]
            for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)
        ]
        span_annotations = [
            (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)
        ]
        return chunks, span_annotations

    # -------------------------------

    def late_chunking(model_output, span_annotation: list, max_length=None):
        token_embeddings = model_output[0]
        outputs = []
        for embeddings, annotations in zip(token_embeddings, span_annotation):
            if (
                max_length is not None
            ):  # remove annotations which go bejond the max-length of the model
                annotations = [
                    (start, min(end, max_length - 1))
                    for (start, end) in annotations
                    if start < (max_length - 1)
                ]
            pooled_embeddings = [
                embeddings[start:end].sum(dim=0) / (end - start)
                for start, end in annotations
                if (end - start) >= 1
            ]
            pooled_embeddings = [
                embedding.detach().cpu().numpy() for embedding in pooled_embeddings
            ]
            outputs.append(pooled_embeddings)

        return outputs

Let's create a dummy example by using indirect references

> Germany is known for it's automative industry, jevlin throwers, football teams and a lot more things from the history. It's Capital is Berlin and is pronounced as 'ber-liin' in German. The capital is the largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.

This one has 5 Chunks:

> - "Germany is known for it's automative industry, jevlin throwers, football teams and a lot more things from the history."
> - " It's Capital is Berlin and is pronounced as 'ber-liin' in German."
> - " The capital is the largest city of Germany, both by area and by population."
> - " Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits."
> - " The city is also one of the states of Germany, and is the third smallest state in the country in terms of area."

Let's create both Vanilla and Late Chunking Table Embeddings

    db = lancedb.connect("./db")

    vanilla_chunk_embeddings = model.encode(chunks)

    vanilla_data = []
    for index, chunk in enumerate(chunks):
        vanilla_data.append(
            {   "text": chunk,
                "vector": vanilla_chunk_embeddings[index],
            }
        )

    vanilla_table = db.create_table("vanilla_table", data=vanilla_data)

    # -------------------------------

    inputs = tokenizer(dummy_long_document, return_tensors='pt')
    model_output = model(**inputs)
    late_chunk_embeddings = late_chunking(model_output, [span_annotations])[0]

    late_chunk_data = []
    for index, chunk in enumerate(chunks):
        late_chunk_data.append(
            {   "text": chunk,
                "vector": late_chunk_embeddings[index],

            }
        )

    late_chunk_table = db.create_table("late_chunk_table", data=late_chunk_data)

Now let's take 3 Queries that test the idea to the core:

    QUERY_EMBED_1 = model.encode('What are some of the attributes about the capital of a country whose Oktoberfest is famous?')
    QUERY_EMBED_2 = model.encode('What are some of the attributes about capital of Germany?')
    QUERY_EMBED_3 = model.encode('What are some of the attributes about Berlin?')

    METRIC = "cosine" # "cosine" "l2" "dot"

Want to get surprised?
![](__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-16-at-2.22.19-PM.png)
**Did you notice something?**

You see, in the Vanilla Chunking, in the 3rd query where `Berlin` is explicitly mentioned, the naive (Vanilla) chunking gave Top-3 results where there are no real specifications mentioned about Berlin BUT only the word is used.

But when you look at the Late chunking, Top-3 results are specifically about the specifications even though the word is out of scope.

Also, the main thing to look at is the cosine similarity where in the Naive chunking, the chunk:

> Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.

is having last place with a distance `0.28` is very relevant to the query and for a turn of surprise, it has more distance than the chunk:

> Germany is known for it's automative industry, jevlin throwers, football teams and a lot more things from the history

which has a distance of `0.23`

On the other hand, Late Chunking gave reasonable distances and rankings. Also, the distance for the best result is far off in the distance (`0.2`) while in the Late chunking, the minimum distance is (`0.15`).

### **Conclusion**

From the results of the above queries*, *It is understood that Late chunking inherently tends to work as a semantic reranker.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\llms-powering-computer-vision-tasks.md

---

```md
---
title: "LLMs Powering Computer Vision Tasks"
date: 2024-12-01
author: Prashant Kumar
categories: ["Community"]
draft: true
featured: false
image: /assets/blog/llms-powering-computer-vision-tasks/preview-image.png
meta_image: /assets/blog/llms-powering-computer-vision-tasks/preview-image.png
description: "Understand about llms powering computer vision tasks. Get practical steps, examples, and best practices you can use now."
---

Computer vision turns visual data into valuable insights, making it exciting. Now, with Large Language Models (LLMs), combining vision and language opens up even more possibilities. Before we explore how LLMs are powering computer vision, letâ€™s first take a quick look at how they differ.

### What are LLMs and Computer Vision models?

Large Language Models (LLMs) are AI systems trained on a lot of text, which helps them understand and generate human language. They have many parameters that allow them to do tasks like chatting, translating languages, or helping with code writing.

Computer Vision is a branch of AI that helps machines understand visual data, like images or videos. It involves tasks such as identifying objects (like recognizing a dog or cat), breaking images into specific parts (like pinpointing the exact pixels of a dog), and using this information to make decisions, such as guiding self-driving cars or diagnosing medical conditions.

![](__GHOST_URL__/content/images/2024/11/image.png)
### What is Multimodal RAG?

Multimodal RAG (Retrieval Augmented Generation) is about using different types of data, like text and images, together to help models give better answers. Instead of just using text, the model can also look at relevant images to understand things better.

For example, if you give the model a picture of a car and also show it examples of captions for cars, it can come up with more detailed and useful captions for that image. This method helps the model create more accurate and richer responses by combining different kinds of information.

In LanceDB's [vectordb-recipes](https://github.com/lancedb/vectordb-recipes), there's a section focused on multimodal examples, highlighting various use cases that leverage the multimodal capabilities of LLMs by combining different types of data, such as text and images, to address a range of problems.

### How are LLMs transforming the field of Computer Vision?

LLMs are powering the Computer Vision field in two major ways:
First, they ***manage different modelsâ€”such as vision and audioâ€”so they can work together smoothly***. This helps streamline tasks and makes the whole process more efficient.

Second, ***LLMs enhance the flexibility of vision tasks***. For example, models like GPT-4 with vision can turn sketches into HTML code without extra fine-tuning, and they can also analyze high-resolution images with much greater detail.

These advancements are improving tasks like answering questions about images, finding objects in images, and classifying images, where LLMs break down complex problems into smaller steps and combine the results. As these models continue to evolve, they will make computer vision even more powerful and adaptable across a wide range of industries.

In LanceDB's [vectordb-recipes](https://github.com/lancedb/vectordb-recipes), a few examples demonstrate some of these tasks. Let's explore them one by one, Let's start

#### Zero-Shot Object Localization and Detection with OpenAI's CLIP

[

Google Colab

![](__GHOST_URL__/content/images/icon/favicon-12.ico)

![](__GHOST_URL__/content/images/thumbnail/colab_favicon_256px-12.png)
](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/zero-shot-object-detection-CLIP/zero_shot_object_detection_clip.ipynb)
This example is about how to perform object detection on images using CLIP and vector search. The process will be broken down into two simple steps:

1. First, the user will enter the name of the object they want to detect.
2. Then, a vector search will be done to find images that match the query.
3. Finally, the most similar image will be used to detect the object from the query.

Object detection with CLIP follows a process similar to YOLO. Here's a simple breakdown:
**1. Split the Image into Patches**: The image is divided into smaller sections for easier analysis.
**2. Analyze Patches with CLIP**: CLIP processes each patch using a sliding window approach to understand the features.
**3. Calculate Coordinates**: CLIP determines the coordinates (Xmin, Ymin, Xmax, Ymax) for the objectâ€™s bounding box.
**4. Draw the Bounding Box**: Finally, the bounding box is drawn on the image to highlight the detected object.

This process helps CLIP accurately detect and locate objects within an image.
![](__GHOST_URL__/content/images/2024/11/image-1.png)
#### Cambrian-1: Vision-centric exploration of images

[https://www.kaggle.com/code/prasantdixit/cambrian-1-vision-centric-exploration-of-images/](https://www.kaggle.com/code/prasantdixit/cambrian-1-vision-centric-exploration-of-images/)

This example explores images through a Vision-Centric approach using vector search. The process involves two simple steps:

1. Performing Vector Search: Weâ€™ll first search for images that match the query.
2. Vision-Centric Exploration: Then, weâ€™ll use the retrieved images for further exploration and analysis.

Cambrian-1 is a family of multimodal LLMs (MLLMs) designed with a **vision-centric** approach. While stronger language models can boost multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research.
![](__GHOST_URL__/content/images/2024/11/image-2.png)
Read more about this example - [https://blog.lancedb.com/cambrian-1-vision-centric-exploration/](__GHOST_URL__/cambrian-1-vision-centric-exploration/)

#### Social Media Caption Generation using Llama3.2 11B Vision

[

Google Colab

![](__GHOST_URL__/content/images/icon/favicon-13.ico)

![](__GHOST_URL__/content/images/thumbnail/colab_favicon_256px-13.png)
](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/social-media-caption-generation-with-llama3.2/social_media_caption_generation_llama3_2_11B.ipynb)
This example uses the ***Conceptual Captions*** dataset by Google Research. The image descriptions in the dataset are primarily used to search for relevant images. Once we find the matching image, we'll use it as a social media post and generate engaging, creative captions for it.

---

These are just a few examples of how Computer Vision and LLMs work together. LanceDB's [**vectordb-recipes**](https://github.com/lancedb/vectordb-recipes) that explore different ways of using LLMs with images to unlock new possibilities.

### Conclusion

Thereâ€™s a lot of research on models that combine LLMs and computer vision, but the big hype around computer vision with LLMs hasnâ€™t happened yet. While thereâ€™s progress, weâ€™re still in the early stages, kind of like where LLMs were before ChatGPT took off.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\llms-rag-the-missing-storage-layer-for-ai-28ded35fa984.md

---

```md
---
title: "LLMs, RAG, & the Missing Storage Layer for AI"
date: 2023-09-05
author: Ayush Chaurasia
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/llms-rag-the-missing-storage-layer-for-ai-28ded35fa984/preview-image.png
meta_image: /assets/blog/llms-rag-the-missing-storage-layer-for-ai-28ded35fa984/preview-image.png
description: "Build about llms, rag, & the missing storage layer for ai. Get practical steps, examples, and best practices you can use now."
---

In the rapidly evolving landscape of artificial intelligence, Generative AI, especially Language Model Machines (LLMs) have emerged as the veritable backbone of numerous applications, from natural language processing and machine translation to virtual assistants and content generation. The advent of GPT-3 and its successors marked a significant milestone in AI development, ushering in an era where machines could not only understand but also generate human-like text with astonishing proficiency. However, beneath the surface of this AI revolution lies a crucial missing element, one that has the potential to unlock even greater AI capabilities: the storage layer.

## The objective truths about LLMs

Letâ€™s set the context with an LLM refresher. Here are some of the highlights with what has happened in the last couple years.

## LLMs are most advanced AI systems ever built

LLMs and the derivative works have the potential to revolutionise many industries and fields of research. For example, they can be used to create more natural and engaging user interfaces, develop new educational tools, and improve the accuracy of machine translation. They can also be used to generate new ideas and insights, and to create new forms of art and literature which opens up a new direction in the field of language and linguistics.

These models have crushed human levels on benchmarks faster than ever seen before:
![](https://miro.medium.com/v2/resize:fit:770/1*zLnFwvljMZRAKtSe4_9jVA.png)[Kiela et all. 2021](https://aclanthology.org/2021.naacl-main.324/)
## These systems lie confidently

Although LLMs are highly advanced and in most cases it is almost impossible to distinguish a generated text from human response, these systems suffer from various levels of hallucination. Another way to put this would be that these systems lie confidently, and because of human level proficiency in generation, these lies are pretty convincing.

Consider this interaction with ChatGPT
![](https://miro.medium.com/v2/resize:fit:770/1*jgdr6vS5oYMwqXDEnISL0Q.png)Originally posted [hackernews](https://news.ycombinator.com/item?id=33841672)
At a glance, these results seem impressive but once you follow the links, they all lead to 404s. This is dangerous on various levels:

- First, because these links look convincing, one might just take them for citations without checking
- The best case scenario would be that you check the first link and on realising it lead to 404, youâ€™ll check the other ones as well.
- But the worst case would be if only the first link actually exists and you only check that. It would lead you to believe all links are valid

This just one example of hallucination, there are many others that are subtler, like simply making things up.

## The most powerful LLMs continue to be a black box

Personally I donâ€™t like the term â€œblack boxâ€ thrown at all of deep learning because most of these models can be easily dissected and modified. In fact, in most cases the modified versions of originally published models have been more popular and useful. Interpretability has historically been challenging but that is not enough to call these models black boxes. But this is different for LLMs. The most powerful LLMs are closed source and are only accessible via API requests. Additionally, due to the high cost of training and proprietary datasets, there arenâ€™t enough resources or engineering expertise to reproduce the result. These do fit the definition of a black box.
![](https://miro.medium.com/v2/resize:fit:770/1*roOanTKp6futZWH3_VIFpQ.png)Originally published on [cohere blog](https://docs.cohere.com/docs/prompt-engineering)
## Response Vs Representation based systems

In prompt based approach, you rely on LLMs to generate responses directly from your (or your usersâ€™) queries. Using LLMs to generate responses if pretty powerful, and simple to get started with. But it gets scary soon enough when you realise that you donâ€™t(canâ€™t) control any aspect of the lifecycle of these systems. Combined with objective truths discussed above, this can soon become a recipe for disaster.

## Using LLMs for representation

How about instead of using LLMs end-to-end, we simply use it to represent our knowledge base? The obvious way to do that would be to embed our database using these powerful models. You can have a numerical representation of your unstructured data that captures semantic meaning.
![](https://miro.medium.com/v2/resize:fit:770/1*CPAo9M4pILESmlNvuJ6XLQ.png)
These vectors capture the relationship between entities in a high dimensional space. For example, hereâ€™s an example word embeddings where similar words in meaning are close by.
![](https://miro.medium.com/v2/resize:fit:770/1*WxJ3c67UTfTf5_W6tqKI_A.png)
## Retrieval Augmented Generation(RAG)

We have all the pieces needed to build a RAG system. In a RAG setting, instead of using LLMs to generate responses from the prompts, we retrieve relevant representations using a retriever and sew them together by prompting the LLM to form the response.
![](https://miro.medium.com/v2/resize:fit:770/1*_ahHXf5Hf-TgFIf3KwHuDg.png)
Now, you can provide exact citations from the knowledge base documents that were used to generate the response. It becomes possible to trace back response to its source

## A change in domain

What weâ€™ve accomplished so far with RAG is that weâ€™ve reduced reliance on LLM to answer on our behalf. Instead, we now have a modular system that has different parts, each of which operates independently:

- Knowledge base
- Embedding model
- Retriever
- Response generator (LLM)

This causes a change in domain, where we go from relying blackbox AI to modular components backed by decades of research

> *â€œItâ€™s AI as long as it doesnâ€™t work well, itâ€™s computer science when it starts workingâ€*

This quote has stuck with me ever since I head it almost a decade ago. I think itâ€™s by Eric Schmidt, but I wasnâ€™t able to find that exact lecture.

The general idea is that now weâ€™ve changed the domain of the problem in way that the retriever becomes the centre piece of the system and we can now utilise the research work in CS sub-domains such as information retrieval, ranking, etc.

## Sounds like I need to spend $$$ ?

Costs depend on a lot of factors. Let us now shift our attention to common problems a deployed ML system faces and how this approach to generative AI aims to solve them.

- **Interpretability** â€” There isnâ€™t a way to confidently interpret deep neural nets, but this is an active area of research. It is even more difficult to interpret LLMs. But in a RAG setting, you build response in stages allowing insights to the cause of a decision.
- **Modularity â€” **There are a lot of advantages to a system thatâ€™s modular when compared to an end-to-end API accessible model. In our case, we have a granular control on what goes into our knowledge base and how it is updated over time, the configuration of our retriever and ranking algorithms, and what models we feed this information to generate final responses.
- **(Re) Training Cost **â€” The biggest problem with LLMs (including local models) is the massive data and infra requirements. This makes almost unfeasible to re-train them as new data comes in.

For example, let us consider an example of a recent event that that occurs after the training finishes (or more specifically dataset creation date cutoff). Here are some questions related to events that occurred a few days ago to some that occurred a few months ago, asked to ChatGPT.
![](https://miro.medium.com/v2/resize:fit:770/1*mCuFr1TW1PL50h1zfa9Pkw.png)Missing 23rd August soft landing of chandrayaan-3![](https://miro.medium.com/v2/resize:fit:770/1*MYbBSk1Wq3eAR_g5c0oGbw.png)Missing 2022 world cup results
The rate of change of information in a sub-domain of business for which an LLM is fine tuned could be much higher, making these models out of date very quickly.

**If the same system relied on RAG system, it would simply need to update the knowledge base**, i.e, run the new events/information through the embedding model and the rest would be handled by the retriever. On the other hand, the LLM would require retraining on new data in a direct response based system.

So it turns out this methods not only provides you finer control and modularity, but is also much cheaper to update as new information comes in.

**Finetuning Vs RAG**

The debate on whatâ€™s better between fine-tuning a model on domain specific data and using a general model with for RAG is pointless. Of course ideally, you want both. A fine-tuned model with better context of the domain will provide better â€œgeneralâ€ response with contexual vocabulary. But you need a RAG model for better control & interpretability of the responses.

## The need for the AI native Database

At this point it is pretty clear that there is a need for well maintained knowledge base. Although there are many traditional solutions, we need to rethink those for AI solutions. Here are the main requirements:

- AI needs a LOT of Data
- Models are becoming multi-modal. Data needs to follow
- Scaling shouldnâ€™t break the bank.

Multi-modality is the next frontier and most LLM providers have either planned to support multi-modal features or theyâ€™re already testing them.

of these while keeping in mind that we donâ€™t need invent yet another sub-domain in ML. So leveraging existing tools and interface would be ideal.

### LanceDB: the AI Native, multi-modal, & embedded vector Database

LanceDB is an open-source database for vector-search built with persistent storage, which greatly simplifies retrieval, filtering and management of embeddings.

### np.array â€” The OG vectorDB

There is an ongoing debate about the need for vectorDB and if np.array is all you need for all vector search operations.

Letâ€™s take a look at an example of using np.array for storing vectors and finding similar vectors

    import numpy as np
    import pandas as pd

    embeddings = []
    ids = []
    for i, item in enumerate(data):
        embeddings.append(embedding_model(item))
        ids.append(i)

    df = pd.DataFrame({"id": ids, "embedding": embeddings})
    df.to_pickle("./dummy.pkl")

    ...
    df = pd.read_pickle("./dummy.pkl")
    embeddings, ids = df["embedding"].to_numpy(), df["id"]
    sim = cosine_sim(embeddings[0], embeddings[1])
    ...

This is great for rapid-prototyping **but how does this scale to millions of entries? What about billions? **Is loading all the embeddings in memory efficient at scale? What about multimodal data?

The ideal solution for AI-native vectorDB would be something that would would be easy to set up and should integrate with existing APIs for rapid prototyping but should be able to scale without additional changes.

LanceDB is designed with this approach.** Being server-less, it requires no setup** â€” just import and start using. Persisted in HDD, allowing **compute-storage separation** so you can** run operations without loading the entire dataset in memory**. Native integration with Python and Javascript ecosystems , allowing to scale from prototype to production applications from the same codebase.

**Compute Storage Separation**

Compute-storage separation is a design pattern that decouples the compute and storage resources in a system. This means that the compute resources are not located on the same physical hardware as the storage resources. There are several benefits to compute-storage separation, including scalability, performance, and cost-effectiveness.

### LanceDB â€” The embedded VectorDB

Hereâ€™s how you can integrate the above example with LanceDB

    db = lancedb.connect("db")
    embeddings = []
    ids = []

    for i, item in enumerate(data):
        embeddings.append(embedding_model(item))
        ids.append(i)

    df = pd.DataFrame({"id": ids, "embedding": embeddings})
    tbl = db.create_table("tbl", data=df)
    ...

    tbl = db.open_table("tbl")
    sim = tbl.search(embedding_model(data)).metric("cosine").to_pandas()

Thatâ€™s all you need to do to supercharge your vector embeddings workflow for any scale. LanceDB is built on top of Lance file format which is modern columnar data format for ML and LLMs implemented in Rust

Weâ€™ve created on some benchmarks that provide approximate measure of performance of lance format â€” achieving up to [2000x better performance with Lance over Parquet](https://blog.eto.ai/benchmarking-random-access-in-lance-ed690757a826)

In the upcoming blog, weâ€™ll discuss how does LanceDB works with more technical details about lance file format, lanceDB & our vision to build the storage layer for AI.

Visit about [LanceDB ](https://github.com/lancedb/lancedb)or [vectordb-recipes](https://github.com/lancedb/vectordb-recipes) and Drop us a ðŸŒŸ
```

---

## ðŸ“„ æ–‡ä»¶: drafts\maximizing-developer-workflows-using-lance-and-claude-2-2c6e18703555.md

---

```md
---
title: "Maximizing Developer Workflows Using Lance and Claude 2"
date: 2023-08-06
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/maximizing-developer-workflows-using-lance-and-claude-2-2c6e18703555/preview-image.png
meta_image: /assets/blog/maximizing-developer-workflows-using-lance-and-claude-2-2c6e18703555/preview-image.png
description: "Discover about maximizing developer workflows using lance and claude 2. Get practical steps, examples, and best practices you can use now."
---

by Leon Yee

**Introduction**

Just a few days ago, I went to Anthropicâ€™s Hackathon, where hackers utilized their Claude 2 APIs to build some amazing programs and projects. My team and I got together to build an automatic GitHub bot, **drcode**, that scours a repository and suggests code optimization and readability changes â€” effectively doubling your productivity and cleaning up your codebase.

However, Iâ€™m not here to showcase our hackathon project; Iâ€™m excited to delve deeper into the fascinating ways to utilize these technologies.

Currently, large language models (LLMs) have a limited context window, requiring a sacrifice of important material in order to make room for context. In these kinds of situations, dynamic context retrieval can be difficult, because of how little you can input. But in the war between retrieval augmented generation and context windows, why donâ€™t we choose both? Claude and LanceDB allow us to do just this, retrieving relevant information from embedded vector stores, and providing this context into Claudeâ€™s 100k token context window. In our case, we fetched parts of the documentation for each line in the codebase to fill up this large context window.
![A slide on what our program will do](https://miro.medium.com/v2/resize:fit:770/1*g5aRvV-C5zLKMfR3zZ4iCQ.png)
**A breakdown**
![A workflow of our hackathon project.](https://miro.medium.com/v2/resize:fit:770/1*fU1jpXgKYO5VIzpOWM1FUg.png)Our project workflow
As an example, Iâ€™ll take the program that we worked on during Anthropicâ€™s Hackathon. This is the workflow of how it operates:

1. **drcode **first downloads all the documentation of a certain programming language, and passes it through **Claude **to summarize each of the files. For our project, we only downloaded the Python documentation.
2. We then pass all the data that Claude outputs into **LangChain**, a framework for developing LLM applications, utilizing one of LangChainâ€™s functions to recursively split the data by character. This basically just means that it tries to split chunks of text until itâ€™s a small enough chunk size.
3. Then, we use a sentence transformer called **Transformers.js**, from HuggingFace. This function just embeds all the text into a 384-dimensional vector space. **LanceDB** accepts an embedding function along with the data, automatically embedding all the data we input into the vector table.
4. Next, we scrape all the code from your GitHub repository and query the **LanceDB** table for context. What we did was for every 3rd line, we query the table with those lines to pull the most relevant Python documentation.
5. Finally, we craft a large prompt asking Claude to output a unified diff XML file, given all the information we gathered. This file is then passed into GitHub to generate a pull request.

**The example**

Letâ€™s take a look at an example of how it works. Weâ€™ll first go through gathering the documentation and embedding the text. Keep in mind that the code is just a general outline and that our team worked in both Python and JavaScript.

First, we download the documentation and load the Anthropic Claude API.

    import Anthropic from '@anthropic-ai/sdk'
    const anthropic = new Anthropic({
        apiKey: process.env.CLAUDE_API,
    });

    import { download } from '@guoyunhe/downloader';
    await download("https://docs.python.org/3/archives/python-3.11.4-docs-text.zip", "data/", { extract: true })

Now, for every file in the folders and subfolders, we want to input it into Claude for summarization. This can take quite a while and can be intensive on API usage.

    // Iterate docPath through every folder.
    var output = fs.readFileSync(docPath).toString();
    const prompt = `\n\n${Anthropic.HUMAN_PROMPT}: You are to act as a summarizer bot whose task is to read the following code documentation about a python function and summarize it in a few paragraphs without bullet points.
                                                    You should include what the function does and potentially a few examples on how to use it, in multiple paragraphs, but leave out any unrelated information that is not about the functionality of it in python, including a preface to the response,
                                                    such as 'Here is a summary...'. \n\nREMEMBER: \nDo NOT begin your response with an introduction. \nMake sure your entire response can fit in a research paper. \nDo not use bullet points. \nKeep responses in paragraph form. \nDo not respond with extra context or your introduction to the reponse.
                                                    \n\nNow act like the summarizer bot, and follow all instructions. Do not add any additional context or introduction in your response. Here is the documentation file:
                                                    \n${output}\n\n${Anthropic.AI_PROMPT}:`
    let completion = await anthropic.completions.create({
        model: 'claude-2',
        max_tokens_to_sample: 100000,
        prompt: prompt,
    });
    completion = completion.completion.split(":\n\n").slice(1);
    completion = completion.join(":\n\n");
    docs = docs.concat(completion); //docs will contain all information.

With all of the documentation now read and summarized, we can go ahead and use LangChain to split the texts, and then format it to be inserted into LanceDB.

    import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

    const splitter = new RecursiveCharacterTextSplitter({
        chunkSize: 250,
        chunkOverlap: 50,
    });
    docs = await splitter.createDocuments(docs);

    let data = [];
    for (let doc of docs) {
        data.push({text: doc['pageContent'], metadata: JSON.stringify(doc['metadata'])});
    }

Weâ€™re almost ready to insert the data! All we have to do now is to create an embedding function using TransformersJS.

    const { pipeline } = await import('@xenova/transformers')
    const pipe = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');

    const embed_fun = {}
    embed_fun.sourceColumn = 'text'
    embed_fun.embed = async function (batch) {
        let result = []
        for (let text of batch) {
            const res = await pipe(text, { pooling: 'mean', normalize: true })
            result.push(Array.from(res['data']))
        }
        return (result)
    };

Then, we can create our table including the embedding function. The benefit of using LanceDB is that it stores everything locally, so inserting and retrieving takes a really short amount of time.

    const lancedb = await import("vectordb");

    const db = await lancedb.connect("data/sample-lancedb")
    const table = await db.createTable("python_docs", data, embed_fun, { writeMode: WriteMode.Overwrite });

Great! Moving on, weâ€™ll have to extract all the code from a GitHub repository. We can do this by using GitHubâ€™s API and a Personal Access Token to gather the content, but I wonâ€™t show it here. If you want to take a look at how we did it, [here is the GitHub file](https://github.com/TevinWang/code-improvement-bot/blob/main/github_scraper.py).

We now need to embed the query, AKA the lines of the files. By iterating through the repository files we just scraped, we can pass it back into LanceDB, like this:

    const db = await lancedb.connect("data/sample-lancedb")
    const table = await db.openTable("python_docs", embed_fun);

    let input = "FILE LINES INPUT HERE";

    let result = await table.search(input).select(['text']).limit(1).execute();

    console.log(result);

The piece of code above is going to call the `table.search` function on the input, select the specific `text` column, and output a single text response: the context.

Joining everything together, (the file contents, split lines, and line context), into a single file would look something like this (Python):

    # python_files is now [(FILE_PATH, [FILE LINES], [(LINE NUMBER, LINE, CONTEXT)])]
    with open("python_files.txt", "w", encoding="utf-8") as f:
        f.write('<files>\n')
        for file_path, file_content, file_context in python_files:
           f.write(f"<file>\n<file_path>{file_path}</file_path>\n<file_content>\n{file_content}\n</file_content>\n<file_context>\n")
           for context in file_context:
               f.write(f"<line>\n<line_number>{context[0]}</line_number>\n<line_content>{context[1]}</line_content>\n<context>\n{context[2]}</context>\n</line>\n")
           f.write("</file_context>\n</file>\n")
        f.write('</files>')

    # LOOKS LIKE

    # <files>
    #     <file>
    #         <file_path>path/to/file.py</file_path>
    #         <file_content>
    #             import os, subprocess
    #             etc
    #             etc
    #         </file_content>
    #         <file_context>
    #             <line>
    #                 <line_number>1</line_number>
    #                 <line_content>import os, subprocess</line_content>
    #                 <context>
    #                     import os, subprocess
    #                     context here
    #                 </context>
    #             </line>
    #             <line>
    #                 <line_number>2</line_number>
    #                 etc
    #                 etc
    #             </line>
    #         </file_context>
    #     </file>
    #     <file>
    #         etc
    #         etc
    #     </file>
    # </files>

Finally, our last step is to input our `python_files.txt` back into Claude, with all of its instructions. Because of its 100k context window, we can stack a lot of information into the prompt, including exactly what we want to be outputted.

    # {python_files} is read from the file `python_files.txt`

    prompt = f"""{HUMAN_PROMPT}
    Description:
    In this prompt, you are given a open source codebase that requires thorough cleanup, additional comments, and the implementation of documentation tests (doc tests). Your task is to enhance the readability, maintainability, and understanding of the codebase through comments and clear documentation. Additionally, you will implement doc tests to ensure the accuracy of the documentation while also verifying the code's functionality.
    Tasks:
    Codebase Cleanup:
    Identify and remove any redundant or unused code.
    Refactor any convoluted or confusing sections to improve clarity.
    Comments and Documentation:
    Add inline comments to explain complex algorithms, logic, or code blocks.
    Document the purpose, input, output, and usage of functions and methods.
    Describe the role of key variables and data structures used in the code.
    Doc Tests Implementation:
    Identify critical functions or methods that require doc tests.
    Write doc tests that demonstrate the expected behavior and output of the functions.
    Ensure the doc tests cover various scenarios and edge cases.
    Function and Variable Naming:
    Review function and variable names for clarity and consistency.
    Rename functions and variables if needed to improve readability.
    Readme File Update (Optional):
    Update the README file with a summary of the codebase and its purpose.
    Provide clear instructions for running the code and any dependencies required.
    Note:
    The codebase provided may lack sufficient comments and documentation.
    Focus on making the code easier to understand for others who read it in the future.
    Prioritize clarity and conciseness when writing comments and documentation.
    Implement doc tests using appropriate testing frameworks or methods.
    Ensure that the doc tests cover various scenarios to validate the code's correctness.
    This prompt allows the LLM to work on improving codebase quality through comments and documentation while also implementing doc tests for verification. Cleaning up and enhancing codebases in this way is a crucial skill for any developer, as it facilitates teamwork, code maintenance, and future development efforts.Claude, I'm seeking your expertise in adding comments and doc tests to Python code files.:
    Provide the updated code in a xml structure where your entire response is parseable by xml:
    <root>
    <diff>
    <!--Ensure the diff follows the unified diff format that would be returned by python difflib, providing clear context and line-by-line changes for ALL files.
    Give line numbers with the first line of the file content being line 1,
    ONLY CHANGE LINES OF FILE CONTENT (NOT ANY OF THE XML TAGS). Do this for all files.
    Add the entire thing as a cdata section '<![CDATA['
    This is what it is supposed to look like per file:
    --- a/path/to/file.txt (make sure to include the 'a/' in the path, and exactly 3 +s)
    +++ b/path/to/file.txt (make sure to include the 'b/' in the path, and exactly 3 -s)
    @@ -1,4 +1,4 @@ (ANYTHING after the @@ MUST BE ON A NEW LINE)
    This is the original content.s
    -Some lines have been removed.
    +Some lines have been added.
    More content here.
    Remove this comment and add the diff patch contents in the diff tag directly. DO NOT ADD THIS IN THE COMMENT
    -->
    </diff>
    [NO MORE DIFF SYNTAX AFTER THE DIFF TAG]
    <title>
    <!-- Relevant emoji + Include a github pull request title for your changes -->
    </title>
    <changes>
    <!-- Include details of the changes made in github BULLET POINTS, not xml, with some relevant emojis -->
    </changes>
    </root>
    Your focus should be on pythonic principles, clean coding practices, grammar, efficiency, and optimization. Do not change the file if you don't know what to do.
    Before you make a change, evaluate the following:
    - The code must work and stay valid
    - The code doesn't add any duplicate code that isn't necessary
    - The code has the right indentation for python
    - The code works
    If one of these is not valid, do not add the change.
    Reminder to add the entire diff as a cdata section '<![CDATA[' (not individually)
    Make sure to add ANYTHING after the @@ ON A NEW LINE
    Be sure to add ANYTHING after the @@ ON A NEW LINE
    Be sure to add changes to all files provided.
    Reminder that the title should contain a relevant emoji and be github style. The changes section should include changes in bullet points.
    Please find the files for review and modification below. They also contain the relevant context and documentation from python to help guide you.
    {python_files}
    Remember the output is in the form: <root>
    <diff>
    </diff>
    <title>
    </title>
    <changes>
    </changes>
    </root>
    DO NOT STOP IN THE MIDDLE.
    Now act as a XML code outputter. Generate based off of the entire instructions, do not cut out in the middle (remember to populate the patch in the diff section). Do not add any additional context or introduction in your response, make sure your entire response is parseable by xml.
    {AI_PROMPT}"""

    <root>
    <diff>
    diff --git a/app/01_â“_Ask.py b/app/01_â“_Ask.py
    index 86ce99c..f265376 100644
    --- a/app/01_â“_Ask.py
    +++ b/app/01_â“_Ask.py
    @@ -2,6 +2,7 @@
     import os

     import streamlit as st
    +from components.utils import query_gpt, show_pdf
     from components.sidebar import sidebar
     from s3 import S3
    @@ -35,11 +36,9 @@

     if "chosen_pdf" not in st.session_state:
         st.session_state.chosen_pdf = "--"

    -if "memory" not in st.session_state:
    -    st.session_state.memory = ""

With this XML unified diff file, we can now open a pull request on GitHub, displaying all the code improvements that Claude has generated.
![Github page of the output of our code](https://miro.medium.com/v2/resize:fit:770/1*bGsQowl23H0dN-eLwXJ0PQ.png)The open PR
**Conclusion**

I hope that this implementation of these technologies has been insightful for you! I believe that combining retrieval augmented generation and large context windows can lead to substantial leaps in the field of natural language processing. Of course, LLMs still have a long way to go before they can fully emulate human-like comprehension, so not everything will be fully accurate. But using vector databases to fetch context, such as **LanceDB**, and a large context window LLM, such as **Claude**, can definitely unlock new possibilities in all sorts of domains.

Thank you for reading! If you would like to check out our full code, [here it is on GitHub](https://github.com/TevinWang/code-improvement-bot/tree/main). Weâ€™d really appreciate you leaving a â­ on our [LanceDB repo](https://github.com/lancedb/lancedb).
```

---

## ðŸ“„ æ–‡ä»¶: drafts\movie-recommendation-system-using-lancedb-and-doc2vec.md

---

```md
---
title: "Movie Recommendation System Using Doc2Vec and Genre Classification"
date: 2024-05-27
author: Vipul Maheshwari
categories: ["Community"]
draft: true
featured: false
image: /assets/blog/movie-recommendation-system-using-lancedb-and-doc2vec/preview-image.png
meta_image: /assets/blog/movie-recommendation-system-using-lancedb-and-doc2vec/preview-image.png
description: "This is a LanceDB community blog post written by \"Vipul Maheshwari\". Get practical steps and examples from 'Movie recommendation system using Doc2Vec and genre classification'."
---

This article provides a comprehensive guide on creating a movie recommendation system by using vector similarity search and multi-label genre classification.
![](__GHOST_URL__/content/images/2024/05/1_AatBvnpVpEPoQvZAMeqU-A.webp)
Here's what we'll cover:

1. Data ingestion and preprocessing techniques for movie metadata
2. Training a Doc2Vec model for Embeddings
3. Training a Neural network for genre classification task
4. Using Doc2Vec, LanceDB and the trained classifier get the relevant recommendations

Let's get started!

### Why use embeddings for recommendation systems?

Scrolling through streaming platforms can be frustrating when the movie suggestions don't match our interests. Building recommendation systems is a complex task, as there isn't one metric that can measure the quality of recommendations. To improve this, we can combine embeddings and VectorDB for better recommendations.

These embeddings serve dual purposes: they can either be directly used as input to a classification model for genre classification or stored in a VectorDB for retrieval purposes. By storing embeddings in a VectorDB, efficient retrieval and query search for recommendations become possible at a later stage.

This architecture offers a holistic understanding of the underlying processes involved.
![](__GHOST_URL__/content/images/2024/05/recommendation_final-1-.png)
### Data Ingestion and preprocessing techniques for movie metadata

Our initial task involves gathering and organizing information about movies. This includes gathering extensive details such as the movie's type, plot summary, genres, audience ratings, and more.

Fortunately, we have access to a robust dataset on [Kaggle](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset) containing information from various sources for approximately 45,000 movies. To follow along, please download the data from Kaggle and place it inside your working directory.

If you require additional data, you can supplement the dataset by extracting information from platforms like Rotten Tomatoes, IMDb, or even box-office records.

Our next step is to extract the core details from this dataset and generate a universal summary for each movie. Initially, I'll combine the movie's title, genres, and overviews into a single textual string. Then, this text will be tagged to create `TaggedDocument` instances, which will be utilized to train the `Doc2Vec` model later on.

Before moving forward, let's install the relevant libraries to make our life easier.

    !pip install torch scikit-learn lancedb nltk gensim lancedb scipy==1.12 kaggle pandas numpy

Next, we'll proceed with the ingestion and preprocessing of the data. To simplify the process, we'll work with chunks of 1000 movies at a time. For clarity, we'll only include movie indices with non-null values for genres, accurate titles, and complete overviews. This approach ensures that we're working with high-quality, relevant data for our analysis.

    import torch
    import pandas as pd
    import numpy as np
    from gensim.models.doc2vec import Doc2Vec, TaggedDocument
    from nltk.tokenize import word_tokenize
    from tqdm import tqdm

    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    from gensim.models.doc2vec import Doc2Vec, TaggedDocument
    from nltk.tokenize import word_tokenize
    from sklearn.preprocessing import MultiLabelBinarizer
    from sklearn.model_selection import train_test_split
    from tqdm import tqdm

    import nltk
    nltk.download('punkt')

    # Read data from CSV file
    movie_data = pd.read_csv('movies_metadata.csv', low_memory=False)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def preprocess_data(movie_data_chunk):
        tagged_docs = []
        valid_indices = []
        movie_info = []

        # Wrap your loop with tqdm
        for i, row in tqdm(movie_data_chunk.iterrows(), total=len(movie_data_chunk)):
            try:
                # Constructing movie text
                movies_text = ''
                genres = ', '.join([genre['name'] for genre in eval(row['genres'])])
                movies_text += "Overview: " + row['overview'] + '\n'
                movies_text += "Genres: " + genres + '\n'
                movies_text += "Title: " + row['title'] + '\n'
                tagged_docs.append(TaggedDocument(words=word_tokenize(movies_text.lower()), tags=[str(i)]))
                valid_indices.append(i)
                movie_info.append((row['title'], genres))
            except Exception as e:
                continue

        return tagged_docs, valid_indices, movie_info

### Generating embeddings using Doc2Vec

Next, we'll utilize the `Doc2Vec` model to generate embeddings for each movie based on the preprocessed text. We'll allow the Doc2Vec model to train for several epochs to capture the essence of the various movies and their metadata in the multidimensional latent space. This process will help us represent each movie in a way that captures its unique characteristics and context.

    def train_doc2vec_model(tagged_data, num_epochs=20):
        # Initialize Doc2Vec model
        doc2vec_model = Doc2Vec(vector_size=100, min_count=2, epochs=num_epochs)
        doc2vec_model.build_vocab(tqdm(tagged_data, desc="Building Vocabulary"))
        for epoch in range(num_epochs):
            doc2vec_model.train(tqdm(tagged_data, desc=f"Epoch {epoch+1}"), total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)

        return doc2vec_model

    # Preprocess data and extract genres for the first 1000 movies
    chunk_size = 1000
    tagged_data = []
    valid_indices = []
    movie_info = []
    for chunk_start in range(0, len(movie_data), chunk_size):
        movie_data_chunk = movie_data.iloc[chunk_start:chunk_start+chunk_size]
        chunk_tagged_data, chunk_valid_indices, chunk_movie_info = preprocess_data(movie_data_chunk)
        tagged_data.extend(chunk_tagged_data)
        valid_indices.extend(chunk_valid_indices)
        movie_info.extend(chunk_movie_info)

    doc2vec_model = train_doc2vec_model(tagged_data)

The `train_doc2vec_model` function trains a `Doc2Vec` model on the tagged movie data, producing 100-dimensional embeddings for each movie. These embeddings act as input features for the neural network.

With our current training setup, we are sure that movies with identical genres and similar kinds of overviews will be positioned closer to each other in the latent space, reflecting their thematic and content similarities.

### Extracting the unique genre labels

Next, our focus shifts to compiling the names of relevant movies along with their genres. Now won

To illustrate this, Let's consider a movie with three genres: 'Drama', 'Comedy', and 'Horror'. Using the `MultiLabelBinarizer`, we'll represent these genres with lists of 0s and 1s. If a movie belongs to a particular genre, it will be assigned a 1; if it doesn't, it will receive a 0. Now each row in our dataset will indicate which genres are associated with a specific movie. This approach simplifies the genre representation for easier analysis.

Let's take the movie "Top Gun Maverick" as a reference. We'll associate its genres using binary encoding. Suppose this movie is categorized only under 'drama', not 'comedy' or 'horror'. When we apply the `MultiLabelBinarizer`, the representation would be: Drama: 1, Comedy: 0, Horror: 0. This signifies that "Top Gun Maverick" is classified as a drama but not as a comedy or horror. We'll replicate this process for all the movies in our dataset to identify the unique genre labels present in our data.

### Training a Neural Network for genre classification task

We'll define a neural network consisting of four linear layers with ReLU activations. The final layer utilizes softmax activation to generate probability scores for various genres. If your objective is primarily classification within the genre spectrum, where you input a movie description to determine its relevant genres, you can establish a threshold value for the multi-label softmax output. This allows you to select the top 'n' genres with the highest probabilities.

Here's the neural network class, hyperparameter settings, and the corresponding training loop for training our model.

    # Extract genre labels for the valid indices
    genres_list = []
    for i in valid_indices:
        row = movie_data.loc[i]
        genres = [genre['name'] for genre in eval(row['genres'])]
        genres_list.append(genres)

    mlb = MultiLabelBinarizer()
    genre_labels = mlb.fit_transform(genres_list)

    embeddings = []
    for i in valid_indices:
        embeddings.append(doc2vec_model.dv[str(i)])

    X_train, X_test, y_train, y_test = train_test_split(embeddings, genre_labels, test_size=0.2, random_state=42)

    X_train_np = np.array(X_train, dtype=np.float32)
    y_train_np = np.array(y_train, dtype=np.float32)
    X_test_np = np.array(X_test, dtype=np.float32)
    y_test_np = np.array(y_test, dtype=np.float32)

    X_train_tensor = torch.tensor(X_train_np)
    y_train_tensor = torch.tensor(y_train_np)
    X_test_tensor = torch.tensor(X_test_np)
    y_test_tensor = torch.tensor(y_test_np)

    class GenreClassifier(nn.Module):
        def __init__(self, input_size, output_size):
            super(GenreClassifier, self).__init__()
            self.fc1 = nn.Linear(input_size, 512)
            self.bn1 = nn.BatchNorm1d(512)
            self.fc2 = nn.Linear(512, 256)
            self.bn2 = nn.BatchNorm1d(256)
            self.fc3 = nn.Linear(256, 128)
            self.bn3 = nn.BatchNorm1d(128)
            self.fc4 = nn.Linear(128, output_size)
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(p=0.2)  # Adjust the dropout rate as needed

        def forward(self, x):
            x = self.fc1(x)
            x = self.bn1(x)
            x = self.relu(x)
            x = self.dropout(x)
            x = self.fc2(x)
            x = self.bn2(x)
            x = self.relu(x)
            x = self.dropout(x)
            x = self.fc3(x)
            x = self.bn3(x)
            x = self.relu(x)
            x = self.dropout(x)
            x = self.fc4(x)
            return x

    # Move model to the selected device
    model = GenreClassifier(input_size=100, output_size=len(mlb.classes_)).to(device)

    # Define loss function and optimizer
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Training loop
    epochs = 50
    batch_size = 64

    train_dataset = TensorDataset(X_train_tensor.to(device), y_train_tensor.to(device))
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)  # Move data to device
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
        epoch_loss = running_loss / len(train_loader.dataset)
        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}')

That's it! We've successfully trained a neural network for our genre classification task. Let's test how our model is performing on the genre classification task.

    from sklearn.metrics import f1_score

    model.eval()
    with torch.no_grad():
        X_test_tensor, y_test_tensor = X_test_tensor.to(device), y_test_tensor.to(device)  # Move test data to device
        outputs = model(X_test_tensor)
        test_loss = criterion(outputs, y_test_tensor)
        print(f'Test Loss: {test_loss.item():.4f}')

    thresholds = [0.1] * len(mlb.classes_)
    thresholds_tensor = torch.tensor(thresholds, device=device).unsqueeze(0)

    # Convert the outputs to binary predictions using varying thresholds
    predicted_labels = (outputs > thresholds_tensor).cpu().numpy()

    # Convert binary predictions and actual labels to multi-label format
    predicted_multilabels = mlb.inverse_transform(predicted_labels)
    actual_multilabels = mlb.inverse_transform(y_test_np)

    # Print the Predicted and Actual Labels for each movie
    for i, (predicted, actual) in enumerate(zip(predicted_multilabels, actual_multilabels)):
        print(f'Movie {i+1}:')
        print(f'    Predicted Labels: {predicted}')
        print(f'    Actual Labels: {actual}')

    # Compute F1-score
    f1 = f1_score(y_test_np, predicted_labels, average='micro')
    print(f'F1-score: {f1:.4f}')

    # Saving the trained model
    torch.save(model.state_dict(), 'trained_model.pth')

### A movie recommendation system

To build a movie recommendation system, we'll allow users to input a movie name, and based on that, we'll return relevant recommendations. We'll save our Doc2Vec embeddings in a vector database to achieve this. When a user inputs a query in the form of a new movie, we'll first locate its embeddings in our vector database. Once we have this, we'll find the 'n' number of movies whose embeddings are similar to our query movie. We can assess the similarity using various search algorithms like cosine similarity or finding the least Euclidean distance.

We'll organize the data and the embeddings into a CSV file.

    import lancedb
    import numpy as np
    import pandas as pd

    data = []

    for i in valid_indices:
        embedding = doc2vec_model.dv[str(i)]
        title, genres = movie_info[valid_indices.index(i)]
        data.append({"title": title, "genres": genres, "vector": embedding.tolist()})

    db = lancedb.connect(".db")
    tbl = db.create_table("doc2vec_embeddings", data, mode="Overwrite")
    db["doc2vec_embeddings"].head()

Essentially, we establish a connection to LanceDB, set up our table, and add our movie data to it.

Each row in the table represents a single movie, with columns containing data like the title, genres, overview, and embeddings. For each movie title, we check if it exists in our dataset. If it does, we perform a cosine similarity search on all other movies and return the top 10 most relevant titles. This columnar format makes it easy to store and retrieve information for various tasks involving embeddings.

### Using Doc2Vec Embeddings to get the relevant recommendations.

Our recommendation engine combines a neural network-based genre prediction model with a vector similarity search to provide relevant movie recommendations.

For a given query movie, first, we use our trained neural network to predict its genres. Based on these predicted genres, we filter our movie database to include only those movies that share at least one genre with the query movie, achieved by constructing an appropriate SQL filter.

We then perform a vector similarity search on this filtered subset to retrieve the most similar movies based on their vector representations. This approach ensures that the recommended movies are not only similar in terms of their vector characteristics but also share genre preferences with the query movie, resulting in more relevant and personalized recommendations.

    # Function to get genres for a single movie query
    def get_genres_for_query(model, query_embedding, mlb, thresholds, device):
        model.eval()
        with torch.no_grad():
            query_tensor = torch.tensor(query_embedding, dtype=torch.float32).unsqueeze(0).to(device)
            outputs = model(query_tensor)
            thresholds = [0.001] * len(mlb.classes_)
            thresold_tensor = torch.tensor(thresholds, device=device).unsqueeze(0)
            predicted_labels = (outputs >= thresold_tensor).cpu().numpy()
            predicted_multilabels = mlb.inverse_transform(predicted_labels)
            return predicted_multilabels

    def movie_genre_prediction(movie_title):
        movie_index = movie_data.index[movie_data['title'] == movie_title].tolist()[0]
        query_embedding = doc2vec_model.dv[str(movie_index)]
        predicted_genres = get_genres_for_query(model, query_embedding, mlb, [0.1] * len(mlb.classes_), device=device)
        return predicted_genres

And now, after all the groundwork, we've arrived at the final piece of the puzzle. Let's generate some relevant recommendations using embeddings and LanceDB.

    def get_recommendations(title):
        pd_data = pd.DataFrame(data)
        title_vector = pd_data[pd_data["title"] == title]["vector"].values[0]
        predicted_genres = movie_genre_prediction(title)
        genres_movie = predicted_genres[0]  # Assuming predicted_genres is available

        genre_conditions = [f"genres LIKE '%{genre}%'" for genre in genres_movie]
        where_clause = " OR ".join(genre_conditions)

        result = (
            tbl.search(title_vector)
            .metric("cosine")
            .limit(10)
            .where(where_clause)
            .to_pandas()
        )

        return result[["title"]]

    get_recommendations("Toy Story")

![](__GHOST_URL__/content/images/2024/05/image-7-1.png)
Some of the recommended movies are remarkably close matches. For example, when looking at "Toy Story," which falls under "animation" and "family" movies, our recommendation system can find other movies in these genres.

### **Conclusion**

In this blog, we've crafted a movie recommendation system that delivers highly relevant and personalized movie suggestions by combining neural network-based genre prediction with vector similarity search.

Follow through Google Colab for complete code
[

Google Colab

![](https://ssl.gstatic.com/colaboratory-static/common/6ce86a0709aa302fe2604e7aaf7351ab/img/favicon.ico)

![](https://colab.research.google.com/img/colab_favicon_256px.png)
](https://colab.research.google.com/drive/1ouQdHw26mqiMS8L6dFAsMSxycgpUj0R8?usp=sharing)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\multi-lingual-search-with-cohere-and-lancedb-efaca566e30c.md

---

```md
---
title: "Multi-Lingual Search with Cohere and LanceDB"
date: 2023-12-03
author: Kaushal Choudhary
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/multi-lingual-search-with-cohere-and-lancedb-efaca566e30c/preview-image.png
meta_image: /assets/blog/multi-lingual-search-with-cohere-and-lancedb-efaca566e30c/preview-image.png
description: "Master about multi-lingual search with cohere and lancedb. Get practical steps, examples, and best practices you can use now."
---

## Overview

[**Cohere**](https://txt.cohere.com/multilingual/)** **provides a Multi-Lingual Embedding model which promises to cross the language barriers in language models which were predominantly based on English. Their models support over 100 languages and can deliver [3X better performance](https://txt.cohere.com/multilingual/#benchmarks) than existing open-source models. We are going to use the cohere API to build, test and create a python app with this model on multilingual data, embedding and retrieving through LanceDB.

This model possesses the capability to comprehend and extract valuable insights from various languages, akin to their proficiency in English.

Cohereâ€™s multilingual model excels in three key tasks:

1. [**Multilingual Semantic Search**](https://docs.cohere.com/docs/multilingual-semantic-search)**:** Elevating the standard of search outcomes, Cohereâ€™s multilingual model swiftly generates precise results irrespective of the language used in the search query or the source content.
2. [**Aggregating Customer Feedback**](https://docs.cohere.com/docs/customer-feedback-aggregation)**:** Deploying Cohereâ€™s multilingual model streamlines the organization of customer feedback across a multitude of languages, presenting a simplified solution for a significant challenge in international operations.
3. [**Cross-Lingual Zero-Shot Content Moderation**](https://docs.cohere.com/docs/cross-lingual-content-moderation)**:** Tackling the identification of harmful content in global online communities is a formidable task.

## Notebook Walk-through

Letâ€™s jump straight to Code snippets.

> Follow along with this* *[**Colab**](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/multi-lingual-wiki-qa/main.ipynb).

We will be using the Wikipedia dataset in English and French.

    from datasets import load_dataset

    en = dataset = load_dataset("wikipedia", "20220301.en", streaming=True,)
    fr = load_dataset("wikipedia", "20220301.fr", streaming=True)

    datasets = {"english": iter(en['train']), "french": iter(fr['train'])}

Looking at the dataset format

    next(iter(en['train']))

    {'id': '12',
     'url': 'https://en.wikipedia.org/wiki/Anarchism',
     'title': 'Anarchism',
     'text': 'Anarchism is a political philosophy and movement that is sceptical
    of authority and rejects all involuntary, coercive forms of hierarchy.
    Anarchism calls for the abolition of the state, which it holds to be
    unnecessary, undesirable, and harmful. As a historically left-wing movement,
    placed on the farthest left of the political spectrum, it is usually described
    alongside communalism and libertarian Marxism as the libertarian wing (
    libertarian socialism)...'}

    next(iter(fr['train']))

    {'id': '3',
     'url': 'https://fr.wikipedia.org/wiki/Antoine%20Meillet',
     'title': 'Antoine Meillet',
     'text': "Paul Jules Antoine Meillet, nÃ© le  Ã  Moulins (Allier) et mort le  Ã 
    ChÃ¢teaumeillant (Cher), est le principal linguiste franÃ§ais des premiÃ¨res
    dÃ©cennies du . Il est aussi philologue.\n\nBiographie \nD'origine bourbonnaise
    , fils d'un notaire de ChÃ¢teaumeillant (Cher), Antoine Meillet fait ses Ã©tudes
    secondaires au lycÃ©e de Moulins.\n\nÃ‰tudiant Ã  la facultÃ© des lettres de Paris
     Ã  partir de 1885 oÃ¹ il suit notamment les cours de Louis Havet, il assiste
    Ã©galement Ã  ceux de Michel BrÃ©al ..."}

## LanceDB Embeddings API

We use LanceDBâ€™s embedding API for seamless data ingestion and vectorization, enabling efficient querying. This API automates the process of converting data into numerical vectors, crucial for tasks like similarity search.

LanceDB embedding API leverages Pydantic schema and ingests vectorizerâ€™s information in the table. This basically allows you to perform implicit vectorisation of input types. This allows you to simply forget about the embedding function once set, make it disappear in the background.

[**documentation**](https://lancedb.github.io/lancedb/embeddings/)

***Cohere Example:***

Now, letâ€™s create a embedding using the Cohereâ€™s multilingual model as well.

> the LanceDB embedding API automatically vectorize queries, after ingesting embedding functions

    #call the cohere multilingual model from registry
    registry = EmbeddingFunctionRegistry().get_instance()
    cohere = registry.get("cohere").create() # uses multi-lingual model by default (768 dim)

    #create the schema for lancedb table
    class Schema(LanceModel):
        vector: Vector(cohere.ndims()) = cohere.VectorField()
        text: str = cohere.SourceField()
        url: str
        title: str
        id: str
        lang: str

    #connect to the db and create the table based on the defined schema
    db = lancedb.connect("~/lancedb")
    tbl_cohere = db.create_table("wikipedia-cohere", schema=Schema, mode="overwrite")

Letâ€™s Ingest the Data.

    from tqdm.auto import tqdm
    import time
    # let's use cohere embeddings. Use can also set it to openai version of the table
    tbl = tbl_cohere
    batch_size = 1000
    num_records = 10000
    data = []

    for i in tqdm(range(0, num_records, batch_size)):

        for lang, dataset in datasets.items():

            batch = [next(dataset) for _ in range(batch_size)]

            texts = [x['text'] for x in batch]
            ids = [f"{x['id']}-{lang}" for x in batch]
            data.extend({
               'text': x['text'], 'title': x['title'], 'url': x['url'], 'lang': lang, 'id': f"{lang}-{x['id']}"
            } for x in batch)

        # add in batches to avoid token limit
        # 'text' will be automatically vectorized by LanceDB embedding API
        tbl.add(data)
        data = []
        time.sleep(20) # wait for 20 seconds to avoid rate limit

Searching the multi-lingual space

    it = iter(fr['train'])
    for i in range(5):
        next(it)
    query = next(it)
    query

Letâ€™s take the first line from the embedded text

    L'ArmÃ©e rÃ©publicaine irlandaise (, IRA ; ) est le nom portÃ©, depuis le dÃ©but
    du , par plusieurs organisations paramilitaires luttant par les armes contre
    la prÃ©sence britannique en Irlande du Nord.

this translates in English

    The Irish Republican Army (, IRA; ) is the name worn, since the beginning of
    the 19th century, by several paramilitary organizations fighting with arms
    against the British presence in Northern Ireland.

Letâ€™s see if the results that are semantically closer to this in our dataset.

    db = lancedb.connect("~/lancedb")
    tbl = db.open_table("wikipedia-cohere") # We just open the existing
    rs = tbl.search(query["text"]).limit(3).to_list()

We can load the table even in a different session because LanceDBâ€™s built in embedding function API has implementation/support for cohere embedding models out of the box. Using embedding API allows you to ingest the function itself so the table knows how to vectorize the inputs- making the embedding function disappear in the background.

    for r in rs:
        print(f" **TEXT id-{r['id']}** \n {r['text']} \n")

    **TEXT id-french-12**
     L'ArmÃ©e rÃ©publicaine irlandaise (, IRA ; ) est le nom portÃ©, depuis le dÃ©but du , par plusieurs organisations paramilitaires luttant par les armes contre la prÃ©sence britannique en Irlande du Nord. Les diffÃ©rents groupes se rÃ©fÃ©rent Ã  eux comme Ã“glaigh na hÃ‰ireann (Â« volontaires d'Irlande Â»).

     L' appelÃ©e aussi Old IRA, issue de l'union en 1916 entre l' (proche du Parti travailliste irlandais) et les Irish Volunteers (alors gÃ©nÃ©ralement proches de l'IRB), est active entre  et , pendant la guerre d'indÃ©pendance irlandaise. Si ceux qui ont acceptÃ© le traitÃ© anglo-irlandais forment les Forces de DÃ©fense irlandaises, une partie de l'organisation, refusant cet accord, se constitue en une nouvelle Irish Republican Army, illÃ©gale.
     L'Irish Republican Army anti-traitÃ© apparaÃ®t entre avril et  du fait du refus du traitÃ© anglo-irlandais par une partie de l'Old IRA. Elle participe ainsi Ã  la guerre civile irlandaise de  Ã  . Elle maintient son activitÃ© dans les deux Irlandes (Ã‰tat libre d'Irlande, indÃ©pendant, et Irlande du Nord, britannique), mais concentre son action sur les intÃ©rÃªts britanniques, surtout en Irlande du Nord. En 1969 l'organisation se divise, donnant naissance Ã  lOfficial Irish Republican Army et Ã  la Provisional Irish Republican Army, minoritaire, moins socialiste et plus activiste.
     LOfficial Irish Republican Army, proche de l'''Official Sinn FÃ©in, plus socialiste et moins nationaliste que la Provisional Irish Republican Army, mÃ¨ne des campagnes d'attentats principalement entre 1969 et 1972 durant le conflit nord-irlandais, avant de dÃ©crÃ©ter un cessez-le-feu.
     La Provisional Irish Republican Army, minoritaire aprÃ¨s la scission de 1969 (d'oÃ¹ son nom de provisional, Â« provisoire Â») devient rapidement grÃ¢ce Ã  son militantisme la principale organisation armÃ©e rÃ©publicaine du conflit nord-irlandais. Le terme de provisional est d'ailleurs abandonnÃ© vers la fin des annÃ©es 1970. Elle fut active de 1969 Ã  1997 (date du cessez-le-feu dÃ©finitif), puis dÃ©posa dÃ©finitivement les armes en 2005. Refusant le processus de paix, deux organisations scissionnÃ¨rent d'avec la PIRA : la Real Irish Republican Army et la Continuity Irish Republican Army.
     ...

     **TEXT id-english-14732**
     The Irish Republican Army (IRA; ) was an Irish republican revolutionary paramilitary organisation. The ancestor of many groups also known as the Irish Republican Army, and distinguished from them as the "Old IRA", it was descended from the Irish Volunteers, an organisation established on 25 November 1913 that staged the Easter Rising in April 1916. In 1919, the Irish Republic that had been proclaimed during the Easter Rising was formally established by an elected assembly (DÃ¡il Ã‰ireann), and the Irish Volunteers were recognised by DÃ¡il Ã‰ireann as its legitimate army. Thereafter, the IRA waged a guerrilla campaign against the British occupation of Ireland in the 1919â€“1921 Irish War of Independence.

    Following the signing in 1921 of the Anglo-Irish Treaty, which ended the War of Independence, a split occurred within the IRA. Members who supported the treaty formed the nucleus of the Irish National Army. However, the majority of the IRA was opposed to the treaty. The anti-treaty IRA fought a civil war against the Free State Army in 1922â€“23, with the intention of creating a fully independent all-Ireland republic. Having lost the civil war, this group remained in existence, with the intention of overthrowing the governments of both the Irish Free State and Northern Ireland and achieving the Irish Republic proclaimed in 1916.

    Origins

    The Irish Volunteers, founded in 1913, staged the Easter Rising, which aimed at ending British rule in Ireland, in 1916. Following the suppression of the Rising, thousands of Volunteers were imprisoned or interned, leading to the break-up of the organisation. It was reorganised in 1917 following the release of first the internees and then the prisoners. At the army convention held in Dublin in October 1917, Ã‰amon de Valera was elected president, Michael Collins Director for Organisation and Cathal Brugha Chairman of the Resident Executive, which in effect made him Chief of Staff.

    Following the success of Sinn FÃ©in in the general election of 1918 and the setting up of the First DÃ¡il (the legislature of the Irish Republic), Volunteers commenced military action against the Royal Irish Constabulary (RIC), the paramilitary police force in Ireland, and subsequently against the British Army. It began with the Soloheadbeg Ambush, when members of the Third Tipperary Brigade led by SÃ©umas Robinson, SeÃ¡n Treacy, Dan Breen and SeÃ¡n Hogan, seized a quantity of gelignite, killing two RIC constables in the process.

    The DÃ¡il leadership worried that the Volunteers would not accept its authority, given that, under their own constitution, they were bound to obey their own executive and no other body. In August 1919, Brugha proposed to the DÃ¡il that the Volunteers be asked to swear allegiance to the DÃ¡il, but one commentator states that another year passed before the movement took an oath of allegiance to the Irish Republic and its government in "August 1920". In sharp contrast, a contemporary in the struggle for Irish independence notes that by late 1919, the term "Irish Republican Army (IRA)" was replacing "Volunteers" in everyday usage. This change is attributed to the Volunteers, having accepted the authority of the DÃ¡il, being referred to as the "army of the Irish Republic", popularly known as the "Irish Republican Army".

    A power struggle continued between Brugha and Collins, both cabinet ministers, over who had the greater influence. Brugha was nominally the superior as Minister for Defence, but Collins's power base came from his position as Director of Organisation of the IRA and from his membership on the Supreme Council of the Irish Republican Brotherhood (IRB). De Valera resented Collins's clear power and influence, which he saw as coming more from the secretive IRB than from his position as a Teachta DÃ¡la (TD) and minister in the Aireacht. Brugha and de Valera both urged the IRA to undertake larger, more conventional military actions for the propaganda effect but were ignored by Collins and Mulcahy. Brugha at one stage proposed the assassination of the entire British cabinet. This was also discounted due to its presumed negative effect on British public opinion. Moreover, many members of the DÃ¡il, notably Arthur Griffith, did not approve of IRA violence and would have preferred a campaign of passive resistance to the British rule. The DÃ¡il belatedly accepted responsibility for IRA actions in April 1921, just three months before the end of the Irish War of Independence.

    ...

     **TEXT id-english-5859**
     The Continuity Irish Republican Army (Continuity IRA or CIRA), styling itself as the Irish Republican Army (), is an Irish republican paramilitary group that aims to bring about a united Ireland. It claims to be a direct continuation of the original Irish Republican Army and the national army of the Irish Republic that was proclaimed in 1916. It emerged from a split in the Provisional IRA in 1986 but did not become active until the Provisional IRA ceasefire of 1994. It is an illegal organization in the Republic of Ireland and is designated a terrorist organization in the United Kingdom, New Zealand and the United States. It has links with the political party Republican Sinn FÃ©in (RSF).

    Since 1994, the CIRA has waged a campaign in Northern Ireland against the British Army and the Police Service of Northern Ireland (PSNI), formerly the Royal Ulster Constabulary. This is part of a wider campaign against the British security forces by dissident republican paramilitaries. It has targeted the security forces in gun attacks and bombings, as well as with grenades, mortars and rockets. The CIRA has also carried out bombings with the goal of causing economic harm and/or disruption, as well as many punishment attacks on alleged criminals.

    To date, it has been responsible for the death of one PSNI officer. The CIRA is smaller and less active than the Real IRA, and there have been a number of splits within the organisation since the mid-2000s.

    Origins
    The Continuity IRA has its origins in a split in the Provisional IRA. In September 1986, the Provisional IRA held a General Army Convention (GAC), the organisation's supreme decision-making body. It was the first GAC in 16 years. The meeting, which like all such meetings was secret, was convened to discuss among other resolutions, the articles of the Provisional IRA constitution which dealt with abstentionism, specifically its opposition to the taking of seats in DÃ¡il Ã‰ireann (the parliament of the Republic of Ireland). The GAC passed motions (by the necessary two-thirds majority) allowing members of the Provisional IRA to discuss and debate the taking of parliamentary seats, and the removal of the ban on members of the organisation from supporting any successful republican candidate who took their seat in DÃ¡il Ã‰ireann.

    The Provisional IRA convention delegates opposed to the change in the constitution claimed that the convention was gerrymandered "by the creation of new IRA organisational structures for the convention, including the combinations of Sligo-Roscommon-Longford and Wicklow-Wexford-Waterford." The only IRA body that supported this viewpoint was the outgoing IRA Executive. Those members of the outgoing Executive who opposed the change comprised a quorum. They met, dismissed those in favour of the change, and set up a new Executive. They contacted Tom Maguire, who was a commander in the old IRA and had supported the Provisionals against the Official IRA (see Irish republican legitimatism), and asked him for support. Maguire had also been contacted by supporters of Gerry Adams, then president of Sinn FÃ©in, and a supporter of the change in the Provisional IRA constitution.

    Maguire rejected Adams' supporters, supported the IRA Executive members opposed to the change, and named the new organisers the Continuity Army Council. In a 1986 statement, he rejected "the legitimacy of an Army Council styling itself the Council of the Irish Republican Army which lends support to any person or organisation styling itself as Sinn FÃ©in and prepared to enter the partition parliament of Leinster House." In 1987, Maguire described the "Continuity Executive" as the "lawful Executive of the Irish Republican Army."

    1986 establishments in Ireland ...

The closest match to the text itself is the text we used to search. The second closest match in an English text with a similar meaning referring to IRA.

You can get the [**Cohere** ](https://dashboard.cohere.com/welcome/login?redirect_uri=%2F)API key and get started with these models.

Visit our [**GitHub** ](https://github.com/lancedb)and if you wish to learn more about LanceDB python and Typescript library.
For more such applied GenAI and VectorDB applications, examples and tutorials visit [**VectorDB-Recipes.**](https://github.com/lancedb/vectordb-recipes/tree/main)** **Donâ€™t forget to leave a star at the repo.

Lastly, for more information and updates, follow our** **[**LinkedIn**](https://www.linkedin.com/company/lancedb/) and [**Twitter.**](https://twitter.com/lancedb)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\multi-modal-ai-made-easy-with-lancedb-clip-5aaf8801c939.md

---

```md
---
title: "Multi-Modal AI Made Easy with LanceDB & CLIP"
date: 2023-11-27
author: Kaushal Choudhary
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/multi-modal-ai-made-easy-with-lancedb-clip-5aaf8801c939/preview-image.png
meta_image: /assets/blog/multi-modal-ai-made-easy-with-lancedb-clip-5aaf8801c939/preview-image.png
description: "Understand about multi-modal ai made easy with lancedb & clip. Get practical steps, examples, and best practices you can use now."
---

by 

One of the most exciting areas of research in deep learning currently is multi-modality and its applications. Kick-started by open sourcing of the CLIP model by OpenAI, multi-modal capabilities have evolved rapidly over the span of last couple of years. This primer covers 3 multi-modal applications built using CLIP powered by LanceDB as vector store.

- Multi-Modal Search using CLIP and LanceDB
- Turning that into Gradio application
- Multi-Modal Video Search

![](https://miro.medium.com/v2/resize:fit:770/1*goBzf9Hb8abalCmIiheffQ.jpeg)CLIP model from OpenAI
## Overview

In the above picture you can see the CLIP model(**Contrastive Language-Image Pre-Training**), which is trained on humongous corpus of image-text pairs. This is model on which we are going to focus on this blog.

So, letâ€™s jump right to Code.

First, we will discuss about the Multi-Modal Search using CLIP.

We will be using keywords, **SQL** commands and Embeddings to search the most relevant image.

## Notebook Walk through

## Example I : Multi-Modal Search

> Follow this* *[**Colab** ](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/multimodal_search/main.ipynb)along.

Letâ€™s dive right into the code.

This will help you understand CLIP model even better.

We will be using the** **[**Animals** ](https://huggingface.co/datasets/CVdatasets/ImageNet15_animals_unbalanced_aug1)dataset from Huggingface.

Here we are loading the CLIP model from GitHub, unlike using Huggingface transformers from before.

    %pip install pillow datasets lancedb
    %pip install git+https://github.com/openai/CLIP.git

Loading the dataset

    dataset = load_dataset("CVdatasets/ImageNet15_animals_unbalanced_aug1", split="train")

The dataset only labels the images with numbers, which is not very easy to understand for us. So, we will create an `enum` to map the numbers with class names.

    #creating a class to map all the classes
    class Animal(Enum):
        italian_greyhound = 0
        coyote = 1
        beagle = 2
        rottweiler = 3
        hyena = 4
        greater_swiss_mountain_dog = 5
        Triceratops = 6
        french_bulldog = 7
        red_wolf = 8
        egyption_cat = 9
        chihuahua = 10
        irish_terrier = 11
        tiger_cat = 12
        white_wolf = 13
        timber_wolf = 14

    print(dataset[0])
    print(Animal(dataset[0]['labels']).name)

We will be using 32 bit precision pretrained ViT (vision transformer) from CLIP.

    import clip
    import torch

    #use GPU if available
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, preprocess = clip.load("ViT-B/32", device=device)

We are going to create a image embedding function here, so it can be fed into LanceDB. Also, we want the embeddings to be a standard list, so we are converting the Tensor array into Numpy array and then list.

We are using the `encode_image` function here to embed the image.

    # embed the image
    def embed(img):
        image = preprocess(img).unsqueeze(0).to(device)
        embs = model.encode_image(image)
        return embs.detach().cpu().numpy()[0].tolist()

We are going to create a PyArrow schema and enter the data into LanceDB.

    # define a schema for the lancedb table
    schema = pa.schema(
      [
          pa.field("vector", pa.list_(pa.float32(), 512)),
          pa.field("id", pa.int32()),
          pa.field("label", pa.int32()),
      ])
    tbl = db.create_table("animal_images", schema=schema)

Letâ€™s append the data to the table.

    import pyarrow as pa

    #create the db with defined schema
    db = lancedb.connect('./data/tables')
    schema = pa.schema(
      [
          pa.field("vector", pa.list_(pa.float32(), 512)),
          pa.field("id", pa.int32()),
          pa.field("label", pa.int32()),
      ])
    tbl = db.create_table("animal_images", schema=schema, mode="overwrite")

    #append the data into the table
    data = []
    for i in tqdm(range(1, len(dataset))):
        data.append({'vector': embed(dataset[i]['img']), 'id': i, 'label': dataset[i]['labels']})

    tbl.add(data)
    #converting to pandas for better visibility
    tbl.to_pandas()

![the dataset after loading it into vector database](https://miro.medium.com/v2/resize:fit:578/1*zpG2IW7dtbu948jAdJirWg.png)Table after loading the Data
Now to test the image search, a good practice would be to check with the validation set first.

    #load the dataset
    test = load_dataset("CVdatasets/ImageNet15_animals_unbalanced_aug1", split="validation")#display the data along with length

    print(len(test))
    print(test[100])
    print(Animal(test[100]['labels']).name)
    test[100]['img']

the results should be something like this:
![](https://miro.medium.com/v2/resize:fit:770/1*Ks1p9_UURkVI41Qw4RCelA.png)Image search using CLIP model with LanceDB
To search the table, we can use this way

- Embed the image(s) we want
- Calling the search function
- Returning a Pandas DataFrame.

    embs = embed(test[100]['img'])

    #search the db after embedding the question(image)
    res = tbl.search(embs).limit(1).to_df()
    res

![](https://miro.medium.com/v2/resize:fit:635/1*wEBX_KgeQT2zG2thXVqHLw.png)
We can also put everything into a function for easier inference.

    #creating an image search function
    def image_search(id):
        print(Animal(test[id]['labels']).name)
        display(test[id]['img'])

        res = tbl.search(embed(test[id]['img'])).limit(5).to_df()
        print(res)
        for i in range(5):
            print(Animal(res['label'][i]).name)
            data_id = int(res['id'][i])
            display(dataset[data_id]['img'])

Great Job! If you have come this far, I would like to congratulate you on your patience and dedication. Now, it gets just better.

We can begin the multi-modal text search.

We will use `encode_text` function here instead of encode_image.

    #text embedding function
    def embed_txt(txt):
        text = clip.tokenize([txt]).to(device)
        embs = model.encode_text(text)
        return embs.detach().cpu().numpy()[0].tolist()

    #check the length of the embedded text
    len(embed_txt("Black and white dog"))

Searching the table

    #search through the database
    res = tbl.search(embed_txt("a french_bulldog ")).limit(1).to_df()
    res

    print(Animal(res['label'][0]).name)
    data_id = int(res['id'][0])
    display(dataset[data_id]['img'])

![](https://miro.medium.com/v2/resize:fit:710/1*VbO8D0QwqqNhbOg7zjqviw.png)Irish Terrier(French Bulldog)
Again, letâ€™s combine everything into a function.

    #making a text_search function to streamline the process
    def text_search(text):
        res = tbl.search(embed_txt(text)).limit(5).to_df()
        print(res)
        for i in range(len(res)):
            print(Animal(res['label'][i]).name)
            data_id = int(res['id'][i])
            display(dataset[data_id]['img'])

Great, we have used CLIP model for SQL, keyword, image and text search.

## Example II : Multi-Modal Search using CLIP

> Follow along with this* *[**Colab**](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/multimodal_clip/main.ipynb#scrollTo=b6f40300).

Loading the data. We will use diffusiondb data already stored in a S3 bucket.
![](https://miro.medium.com/v2/resize:fit:1100/1*ertaYy_ta5Y2iKMpZ8MEyA.png)DiffusionDB Dataset Preview
    !wget https://eto-public.s3.us-west-2.amazonaws.com/datasets/diffusiondb_lance.tar.gz
    !tar -xvf diffusiondb_lance.tar.gz
    !mv diffusiondb_test rawdata.lance

Create and open the LanceDB table.

    import pyarrow.compute as pc
    import lance

    db = lancedb.connect("~/datasets/demo")
    if "diffusiondb" in db.table_names():
        tbl= db.open_table("diffusiondb")
    else:
        # First data processing and full-text-search index
        data = lance.dataset("rawdata.lance/diffusiondb_test").to_table()
        # remove null prompts
        tbl = db.create_table("diffusiondb", data.filter(~pc.field("prompt").is_null()), mode="overwrite")
        tbl.create_fts_index(["prompt"])

Creating the CLIP Embeddings, for the text. If you want to know more about embeddings, you can read more about them [**here**](__GHOST_URL__/multitask-embedding-with-lancedb-be18ec397543).

    from transformers import CLIPModel, CLIPProcessor, CLIPTokenizerFast

    MODEL_ID = "openai/clip-vit-base-patch32"

    tokenizer = CLIPTokenizerFast.from_pretrained(MODEL_ID)
    model = CLIPModel.from_pretrained(MODEL_ID)
    processor = CLIPProcessor.from_pretrained(MODEL_ID)
    Crea
    def embed_func(query):
        inputs = tokenizer([query], padding=True, return_tensors="pt")
        text_features = model.get_text_features(**inputs)
        return text_features.detach().numpy()[0]

Letâ€™s see the schema, and the data inside the LanceDB table.

    tbl.schema
    tbl.to_pandas().head()

![](https://miro.medium.com/v2/resize:fit:1100/1*iIv9UPWBo8ixfgAhDJqRgg.png)
Now, to properly visualize our embeddings and data, we will create a Gradio Interface. Letâ€™s build some utility search functions beforehand.

    #find the image vectors from the database
    def find_image_vectors(query):
        emb = embed_func(query)
        code = (
            "import lancedb\n"
            "db = lancedb.connect('~/datasets/demo')\n"
            "tbl = db.open_table('diffusiondb')\n\n"
            f"embedding = embed_func('{query}')\n"
            "tbl.search(embedding).limit(9).to_df()"
        )
        return (_extract(tbl.search(emb).limit(9).to_df()), code)

    #find the image keywords
    def find_image_keywords(query):
        code = (
            "import lancedb\n"
            "db = lancedb.connect('~/datasets/demo')\n"
            "tbl = db.open_table('diffusiondb')\n\n"
            f"tbl.search('{query}').limit(9).to_df()"
        )
        return (_extract(tbl.search(query).limit(9).to_df()), code)

    #using SQL style commands to find the image
    def find_image_sql(query):
        code = (
            "import lancedb\n"
            "import duckdb\n"
            "db = lancedb.connect('~/datasets/demo')\n"
            "tbl = db.open_table('diffusiondb')\n\n"
            "diffusiondb = tbl.to_lance()\n"
            f"duckdb.sql('{query}').to_df()"
        )
        diffusiondb = tbl.to_lance()
        return (_extract(duckdb.sql(query).to_df()), code)

    #extract the image
    def _extract(df):
        image_col = "image"
        return [(PIL.Image.open(io.BytesIO(row[image_col])), row["prompt"]) for _, row in df.iterrows()]

Letâ€™s set up the Gradio Interface.

    import gradio as gr

    #gradio block
    with gr.Blocks() as demo:
        with gr.Row():
            with gr.Tab("Embeddings"):
                vector_query = gr.Textbox(value="portraits of a person", show_label=False)
                b1 = gr.Button("Submit")
            with gr.Tab("Keywords"):
                keyword_query = gr.Textbox(value="ninja turtle", show_label=False)
                b2 = gr.Button("Submit")
            with gr.Tab("SQL"):
                sql_query = gr.Textbox(value="SELECT * from diffusiondb WHERE image_nsfw >= 2 LIMIT 9", show_label=False)
                b3 = gr.Button("Submit")
        with gr.Row():
            code = gr.Code(label="Code", language="python")
        with gr.Row():
            gallery = gr.Gallery(
                    label="Found images", show_label=False, elem_id="gallery"
                ).style(columns=[3], rows=[3], object_fit="contain", height="auto")

        b1.click(find_image_vectors, inputs=vector_query, outputs=[gallery, code])
        b2.click(find_image_keywords, inputs=keyword_query, outputs=[gallery, code])
        b3.click(find_image_sql, inputs=sql_query, outputs=[gallery, code])

    demo.launch()

![](https://miro.medium.com/v2/resize:fit:1100/1*fjcthhO2vBIPHSi0aG6OLg.gif)Gradio Interface for MultiModal Search using CLIP
We can also search through **Image** and **Text.**

## Example III : Multi-Modal Video Search

We will now use it to **search videos**.

For brevity, Iâ€™m going to focus on the essential part here.

> So, kindly follow along this** **[**Colab** ](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/multimodal_video_search/main.ipynb)for full exposure.

We have already made a tar file consisting the data in** **[**Lance **](https://github.com/lancedb/lance)Format.

    #getting the data
    !wget https://vectordb-recipes.s3.us-west-2.amazonaws.com/multimodal_video_lance.tar.gz
    !tar -xvf multimodal_video_lance.tar.gz
    !mkdir -p data/video-lancedb
    !mv multimodal_video.lance data/video-lancedb/

Create the Table

    #intialize the db and open a table
    db = lancedb.connect("data/video-lancedb")
    tbl = db.open_table("multimodal_video")

CLIP model with tokenizer, processor, and the embedding function

    from transformers import CLIPModel, CLIPProcessor, CLIPTokenizerFast

    MODEL_ID = "openai/clip-vit-base-patch32"

    #load the tokenizer and processor for CLIP model
    tokenizer = CLIPTokenizerFast.from_pretrained(MODEL_ID)
    model = CLIPModel.from_pretrained(MODEL_ID)
    processor = CLIPProcessor.from_pretrained(MODEL_ID)

    #embedding function for the query
    def embed_func(query):
        inputs = tokenizer([query], padding=True, return_tensors="pt")
        text_features = model.get_text_features(**inputs)
        return text_features.detach().numpy()[0]

We will be using Gradio, so letâ€™s define some search utility functions beforehand.

    #function to find the vectors most relevant to a video
    def find_video_vectors(query):
        emb = embed_func(query)
        code = (
            "import lancedb\n"
            "db = lancedb.connect('data/video-lancedb')\n"
            "tbl = db.open_table('multimodal_video')\n\n"
            f"embedding = embed_func('{query}')\n"
            "tbl.search(embedding).limit(9).to_df()"
        )
        return (_extract(tbl.search(emb).limit(9).to_df()), code)

    #function to find the search for the video keywords from lancedb
    def find_video_keywords(query):
        code = (
            "import lancedb\n"
            "db = lancedb.connect('data/video-lancedb')\n"
            "tbl = db.open_table('multimodal_video')\n\n"
            f"tbl.search('{query}').limit(9).to_df()"
        )
        return (_extract(tbl.search(query).limit(9).to_df()), code)

    #create a SQL command to retrieve the video from the db
    def find_video_sql(query):
        code = (
            "import lancedb\n"
            "import duckdb\n"
            "db = lancedb.connect('data/video-lancedb')\n"
            "tbl = db.open_table('multimodal_video')\n\n"
            "videos = tbl.to_lance()\n"
            f"duckdb.sql('{query}').to_df()"
        )
        videos = tbl.to_lance()
        return (_extract(duckdb.sql(query).to_df()), code)

    #extract the video from the df
    def _extract(df):
        video_id_col = "video_id"
        start_time_col = "start_time"
        grid_html = '<div style="display: grid; grid-template-columns: repeat(3, 1fr); grid-gap: 20px;">'

        for _, row in df.iterrows():
            iframe_code = f'<iframe width="100%" height="315" src="https://www.youtube.com/embed/{row[video_id_col]}?start={str(row[start_time_col])}" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>'
            grid_html += f'<div style="width: 100%;">{iframe_code}</div>'

        grid_html += '</div>'
        return grid_html

Setting up the Gradio Interface

    import gradio as gr

    #gradio block
    with gr.Blocks() as demo:
        gr.Markdown('''
                # Multimodal video search using CLIP and LanceDB
                We used LanceDB to store frames every thirty seconds and the title of 13000+ videos, 5 random from each top category from the Youtube 8M dataset.
                Then, we used the CLIP model to embed frames and titles together. With LanceDB, we can perform embedding, keyword, and SQL search on these videos.
                ''')
        with gr.Row():
            with gr.Tab("Embeddings"):
                vector_query = gr.Textbox(value="retro gaming", show_label=False)
                b1 = gr.Button("Submit")
            with gr.Tab("Keywords"):
                keyword_query = gr.Textbox(value="ninja turtles", show_label=False)
                b2 = gr.Button("Submit")
            with gr.Tab("SQL"):
                sql_query = gr.Textbox(value="SELECT DISTINCT video_id, * from videos WHERE start_time > 0 LIMIT 9", show_label=False)
                b3 = gr.Button("Submit")
        with gr.Row():
            code = gr.Code(label="Code", language="python")
        with gr.Row():
            gallery = gr.HTML()

        b1.click(find_video_vectors, inputs=vector_query, outputs=[gallery, code])
        b2.click(find_video_keywords, inputs=keyword_query, outputs=[gallery, code])
        b3.click(find_video_sql, inputs=sql_query, outputs=[gallery, code])

    demo.launch()

![](https://miro.medium.com/v2/resize:fit:1100/1*Dkb4OdnXSOVncfgzhRbqAw.gif)MultiModal Video search using CLIP and LanceDB
Phew, that was a long learning session. Hope you guys were as excited as I was while writing this.

Visit our [GitHub ](https://github.com/lancedb)and if you wish to learn more about LanceDB python and Typescript library.
For more such applied GenAI and VectorDB applications, examples and tutorials visit [VectorDB-Recipes.](https://github.com/lancedb/vectordb-recipes/tree/main)

Lastly, for more information and updates, follow our [LinkedIn](https://www.linkedin.com/company/lancedb/) and [Twitter.](https://twitter.com/lancedb)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\multimodal-rag-applications.md

---

```md
---
title: "GTA5 Multimodal RAG Application"
date: 2024-03-07
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/multimodal-rag-applications/preview-image.png
meta_image: /assets/blog/multimodal-rag-applications/preview-image.png
description: "Today, we will work on Multimodality, a concept that empowers AI models with the capacity to perceive, listen to, and comprehend data in diverse formats together with textâ€”pretty much like how we do! Get practical steps."
---

Today, we will work on Multimodality, a concept that empowers AI models with the capacity to perceive, listen to, and comprehend data in diverse formats together with textâ€”pretty much like how we do!

In an ideal situation, we should be able to mix different types of data and show them to a generative AI model simultaneously and iterate on it. It could be as simple as telling the AI model, "Hey, a few days ago, I sent you a picture of a brown, short dog. Can you find that picture for me?" the model should then give us the details. We want the AI to understand things more like we humans do, and we want it to become really good at handling and responding to all kinds of information.

But the challenge here is to make a computer understand one data format with its related reference, which could be a mix of text, audio, thermal imagery, and videos. Now, to make this happen, we use something called Embeddings. It's a numeric vector containing numbers written together that might not mean much to us but are understood by machines very well.

## **Cat is equal to Cat**

Let's think of the text components for now. We are currently aiming for our model to learn that words like "Dog" and "Cat" are closely linked to the word "Pet." This understanding is easily achievable using an embedding model that converts these text words into their respective embeddings. Then, the model is trained to follow a straightforward logic: if words are related, they are close together in the vector space; if not, the adequate distance would separate them.
![](https://lh7-us.googleusercontent.com/vnCg5gc3NQ0xDYCLTGUgiMmEt9WeBHDBH6Ao8BRiBjJlRos_E_pDC0ZEempNQG-XGcXfz3fBKFJDVPCoe918zqNRr6Do53Cp0aEqmCMbnprwKhKmkho3MjYbLnRKp-wmHWPSTjOUUUgOctyX1dwlkC0)
But we rely on Multimodal embedding to help a model recognize that an image of a "Cat" and the word "Cat" are similar. To simplify things a bit, imagine there is a magic box that is capable of handling various inputs â€“ images, audio, text, and more.

Now, when we feed the box with an image of a "Cat" with the text "Cat," it performs its magic and produces two numeric vectors. When these two vectors were given to a machine, it made machines think, "Hmm, based on these numeric values, it seems like both are connected to "Cat." So that's exactly what we were aiming for! Our goal was to help machines recognize the close connection between an image of a "Cat" and the text "Cat."

However, to validate this concept, we plot those two numeric vectors in a vector space, which are very close. This outcome exactly mirrors what we observed earlier with the proximity of the two text words "Cat" and "Dog" in the vector space.

## Ladies and gentlemen, that's the essence of Multimodality.

So, we made our model comprehend the association between "Cat" images and the word "Cat." Well, this is it; if you can do this, you will have ingested the audio, images, videos, and the word "Cat," and the model will understand how the cat is being portrayed across all kinds of file formats.

## RAG is here..

If you need to learn what RAG means, I recommend [this article](https://vipul-maheshwari.github.io/2024/02/14/rag-application-with-langchain) I recently wrote, which was popular and an excellent place to get started.

So, there are impressive models like DALLE-2 that provide text-to-image functionality. You input text, and the model generates relevant images for you. But can we create a system similar to Multimodal RAG, where the model produces output images based on our data? Alright, so the goal for today is to build an AI model that, when asked something like, "How many girls were there in my party?" ðŸ’€ not only provides textual information but also includes a relevant related image. Think of it as an extension of a simple RAG system, but now incorporating images.

Before we dive in, remember that Multimodality isn't limited to text-to-image or image-to-text; it encompasses the freedom to input and output any type of data. However, let's concentrate exclusively on the interaction from image to text.

## Contrastive learning

Now, the question is, what exactly was that box doing? The magic it performs is known as Contrastive Learning. While the term might sound complex, it's not that tricky. To simplify, consider a dataset with images and a caption describing each image.
![](https://lh7-us.googleusercontent.com/JpGpRcOegYrDiK58K2O1NISQcGWKfMR8XRcv-P0k-d6hubQKJx87e8NbCJBYjKoRtpIbRUszsuqaIMD5i_uKh7xP7uR_25shScJp1yUA07s2jA9c3um7bVSlMy5nJHg1RUgpRNBuQFPXQS-CZfnYKMY)
Now, we give our text-image model with these Positive and Negative samples, each consisting of an image and descriptive text. Positive samples are those where the image and text are correctly alignedâ€”for instance, a picture of a cat matched with the text "this is an image of a cat." Conversely, negative samples involve a mismatch, like presenting an image of a dog alongside the text "This is an image of a cat."

Now, we train our text-image model to recognize that positive samples offer accurate interpretations, while negative samples are misleading and should be disregarded during training. In formal terms, this technique is called [CLIP](https://openai.com/research/clip) (Contrastive Language-Image Pre-training), introduced by OpenAI where authors trained an image-text model on something around 400 million image caption pairs taken from the internet, and every time model makes a mistake, the contrastive loss function increases and penalizes it to make sure the model trains well. The same principles are applied to the other modality combinations as well. Hence, the cat's voice with the word cat is a positive sample for the speech-text model, and a video of a cat with the descriptive text "this is a cat" is a positive sample for the video-text model.
![](https://lh7-us.googleusercontent.com/B9WTX1JNqQx-1zKkh0FxPtAihonMyMN0VsYBrJDX_7mcjwNKuCSld9ZlIizj0PB0rYiCSl5HPr0g_Qkd82mLWylt_JPjh6j3dB6_wAZ8fsyCZX3EcLJi3eBh1h5jkOOgiRcMkbXs6sFCdpYSgxS365g)
## Show time

You don't have to build that box from scratch because folks have already done it for us. There's a Multimodal embedding model, like the "ViT-L/14" from OpenAI. This model can handle various data types, including text, images, videos, audio, and thermal and gyroscope data. Now, let's move on to the following question: How do we store those embeddings?

We'll need a vector database that can efficiently fetch, query, and retrieve relevant embeddings for us,  ideally one that supports multimodal data and doesn't burn a hole in our wallets. That's where LanceDB comes into play.

## Vector database

When we talk about the vector database, many options are available in the current market, but there is something about the LanceDB that makes it stand out as an optimal choice for a vector database. As far as I have used it, it addresses traditional embedded databases' limitations in handling AI/ML workloads. When I say traditional, I mean those database management tools that arenâ€™t optimized for the heavy computation usage of the ML infrastructure.

TLDR: LanceDB is a serverless architecture, meaning storage and compute are separated into two distinct units. This design makes it exceptionally fast for RAG use cases, ensuring fast fetching and retrieval. Additionally, it has some notable advantagesâ€”being open source, utilizing its Lance columnar data format built on top of Apache Arrow for high efficiency and persistent storage capabilities, and incorporating its own Disk Approximate Nearest Neighbor search. All these factors collectively make LanceDB an ideal solution for accessing and working with multimodal data. *I love you LanceDB â¤ ï¸*

## Data time

To add excitement, I've crafted a GTA-V Image Captioning dataset featuring thousands of images, each paired with descriptive text illustrating the image's content. When we train our magic box, the expectation is clearâ€”if I ask that box to provide me an image of a "road with a stop sign," it should deliver a GTA-V image of a road with a stop sign. Otherwise, what's the point, right?

## FAQ

1. We will use "ViT-L/14" to convert our multimodal data into its respective embeddings.

2. LanceDB is our vector database that stores the relevant embeddings.

3. GTA-V Image Captioning dataset for our magic box.

## Environment Setup

I'm using a MacBook Air M1, and it's worth mentioning that certain dependencies and configurations might differ based on the system you're using.

Here are the steps to install the relevant dependencies

    # Create a virtual environment
    python3 -m venv env

    # Activate the virtual environment
    source env/bin/activate

    # Upgrade pip in the virtual environment
    pip install --upgrade pip

    # Install required dependencies
    pip3 install lancedb clip torch datasets pillow
    pip3 install git+https://github.com/openai/CLIP.git

And remember to get your access token from the hugging face to download the data.

## Downloading the Data

The dataset can easily be fetched using the datasets library.

    import clip
    import torch
    import os
    from datasets import load_dataset

    ds = load_dataset("vipulmaheshwari/GTA-Image-Captioning-Dataset")
    device = torch.device("mps")
    model, preprocess = clip.load("ViT-L-14", device=device)

Downloading the dataset may require some time, so please take a moment to relax while this process runs. Once the download finishes, you can visualize some sample points like this:

    from textwrap import wrap
    import matplotlib.pyplot as plt
    import numpy as np

    def plot_images(images, captions):
    	plt.figure(figsize=(15, 7))
    	for i in range(len(images)):
        	ax = plt.subplot(1, len(images), i + 1)
        	caption = captions[i]
        	caption = "\n".join(wrap(caption, 12))
        	plt.title(caption)
        	plt.imshow(images[i])
        	plt.axis("off")

    # Assuming ds is a dictionary with "train" key containing a list of samples
    sample_dataset = ds["train"]
    random_indices = np.random.choice(len(sample_dataset), size=2, replace=False)
    random_indices = [index.item() for index in random_indices]

    # Get the random images and their captions
    random_images = [np.array(sample_dataset[index]["image"]) for index in random_indices]
    random_captions = [sample_dataset[index]["text"] for index in random_indices]

    # Plot the random images with their captions
    plot_images(random_images, random_captions)

    # Show the plot
    plt.show()

![](https://lh7-us.googleusercontent.com/oyo2S6RRiYktMoCldnfBRvPY4meYjKS6Lhuj7WbeWUFmG94gPMjykRiz4sxUYY5s_WRD218pM0EnFVXvJEsgIaP4yskTHrwvVuw3FRki3_FD--iZ2G0eeAEiK5oNU6H9DF0BNDhuPebaPAZ3Oq8Y6ao)
## Storing the Embeddings

The dataset consists of two key features: the image and its corresponding descriptive text. Initially, our task is to create a LanceDB table to store the embeddings. This process is straightforward â€“ you only need to define the relevant schema. In our case, the columns include a "vector" for storing the multimodal embeddings, a "text" column for the descriptive text, and a "label" column for the corresponding IDs.

    import pyarrow as pa
    import lancedb
    import tqdm

    db = lancedb.connect('./data/tables')
    schema = pa.schema(
      [
      	pa.field("vector", pa.list_(pa.float32(), 512)),
      	pa.field("text", pa.string()),
      	pa.field("id", pa.int32())
      ])
    tbl = db.create_table("gta_data", schema=schema, mode="overwrite")

## Encode the Images

We'll simply take the images from the dataset, feed them into an encoding function that leverages our Multimodal Embedding model, and generate the corresponding embeddings. These embeddings will then be stored in the database.

    def embed_image(img):
    	processed_image = preprocess(img)
    	unsqueezed_image = processed_image.unsqueeze(0).to(device)
    	embeddings = model.encode_image(unsqueezed_image)

    	# Move to CPU, convert to numpy array, extract element list
    	result = embeddings.detach().cpu().numpy()[0].tolist()
    	return result

So our `embed_image` function takes an input image, preprocesses it through our CLIP model preprocessor, encodes the preprocessed image, and returns a list representing the embeddings of that image. This returned embedding serves as a concise numerical representation, capturing all the key features and patterns within the image for downstream tasks or analysis. Next, call this function for all the images and store the relevant embeddings in the database.

    data = []
    for i in range(len(ds["train"])):
    	img = ds["train"][i]['image']
    	text = ds["train"][i]['text']

    	# Encode the image
    	encoded_img = embed_image(img)
    	data.append({"vector": encoded_img, "text": text, "id" : i})

Here, we're just taking a list and adding the numeric embeddings, reference text, and the current index ID to it. All that's left is to include this list in our LanceDB table. Voila, our data lake for the embeddings is set up and good to go.

    tbl.add(data)
    tbl.to_pandas()

Until now, we've efficiently converted the images into their respective multimodal embeddings and stored them in the LanceDB table. Now, the LanceDB tables offer a convenient feature: adding or removing images is remarkably straightforward. Just encode and add the new image, following the same steps we followed for the previous images.

## Query Search

Our next move is to embed our text query using the same multimodal embedding model we used for our images. Remember that "box" I mentioned earlier? Essentially, we want this box to create embeddings for our images and texts, ensuring that the representation of different types of data happens in the same way. Following this, we just need to initiate a search to find the nearest image embeddings that match our text query.

    def embed_txt(txt):
    	tokenized_text = clip.tokenize([txt]).to(device)
    	embeddings = model.encode_text(tokenized_text)

    	# Move to CPU, convert to numpy array, extract element list
    	result = embeddings.detach().cpu().numpy()[0].tolist()
    	return result

    res = tbl.search(embed_txt("a road with a stop")).limit(3).to_pandas()
    print(res)

Output:

    0 | [0.064575195, .. ] | there is a stop sign...| 569 |    131.995728
    1 | [-0.07989502, .. ] | there is a bus that... | 423 | 135.047852
    2 | [0.06756592, .. ]  | amazing view of a ...    | 30  | 135.309937

Let's slow down a bit and understand what just happened. Simply put, the code snippet executes a search algorithm at its core to pinpoint the most relevant image embedding that aligns with our text query. As showcased above, the resulting output gives us embeddings that closely resemble our text query.  In the result, the second column presents the embedding vector, while the third column contains the image description that closely matches our text query. Essentially, we've determined which image closely corresponds to our text query by examining the embeddings of our text query and the image.

### It's similar to saying, If these numbers represent the word "Cat," I spot an image with a similar set of numbers, so most likely it's a match for an image of a "Cat."

If you are looking for an explanation of how the search happens, I will write a detailed explanation in the coming write-ups because it's so exciting to look under the hood and see how the search happens. There is something called Approximate Nearest Neighbors (ANN), a technique used to find the closest points in high-dimensional spaces efficiently. ANN is extensively used in data mining, machine learning, computer vision, and NLP use cases. So when we passed our embedded text query to the searching algorithm and asked it to give us the closest sample point in the vector space, it used a type of ANN algorithm to get it for us. Specifically, LanceDB utilizes DANN (Deep Approximate Nearest Neighbor) to search the relevant embeddings within its ecosystem.

In our results, we have five columns. The first is the index number, the second is the embedding vector, the third is the image's description matching our text query, and the fourth is the image's label. However, let's focus on the last column â€“ Distance. When I mentioned the ANN algorithm, it simply draws a line between the current data point (in our case, the embedding of our text query) and identifies which data point (image) is closest to it. If you observe that the other data points in the results have a greater distance than the top one, it indicates they are a bit further away or more unrelated to our query. To clarify, the distance calculation is part of the algorithm itself.

## D-DAY

Now that we have all the necessary information, displaying the most relevant image for our query is straightforward. Simply take the relevant label of the top-matched embedding vector and showcase the corresponding image.

    data_id = int(res['id'][0])
    display(ds["train"][data_id]['image'])
    print(ds["train"][data_id]['text'])

![](https://lh7-us.googleusercontent.com/oPR4LeWI4lKRtxVxms0KhWJ4M5SESqAuKe3amG92kXoLWMiEvIYdU6tu01fTWad_M4X4m-cVoPsgLuvwGa5kVT3ttP0Urnf_zhEQmBgMdRUwCFLgg2KCKiUMmFk_lZ9ncNyHtm7LCS_G5pk04zzW17Q)
    there is a truck driving down a street with a stop sign

## Whatâ€™s next?

To make things more interesting, I'm currently working on creating an extensive GTA-V captioning dataset. This dataset will include more images paired with their respective reference text, providing a richer set of queries to explore and experiment with. Nevertheless, there's always room for refining the model. We can explore creating a customized CLIP model adjusting various parameters. Increasing the number of training epochs affords the model more time to grasp the relevance between embeddings.

Additionally, Meta developed an impressive multimodal embedding model known as [ImageBind](https://imagebind.metademolab.com/). We can consider trying ImageBind as an alternative to our current multimodal embedding model and comparing the outcomes. The fundamental concept behind the Multimodal RAG workflow remains consistent with numerous available options.

Explore Google Colab for access to the entire code in one place.
[

Google Colab

![](https://ssl.gstatic.com/colaboratory-static/common/005460c8a91a7de335dec68f82b6f6e5/img/favicon.ico)

![](https://colab.research.google.com/img/colab_favicon_256px.png)
](https://colab.research.google.com/drive/1LM-WrDSBXpiMZ94CtaMCaGHlkxqGR6WK?usp=sharing)![](https://lh7-us.googleusercontent.com/CsmrViKLHisLVUqkTURr2dV6Wxk5_0ENWp3yXScEj9JRGfX5E2o0kGVGEbroz33mzmr3IYBBYXMmlXIUYfMENBUUaZAeBpt0pTeHfqlY7u4K0Su-2jfyko21vC-oNdH-sr9UCOCQ-RPaSevffb4LQCI)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\multitask-embedding-with-lancedb-be18ec397543.md

---

```md
---
title: "Multitask Embedding with LanceDB"
date: 2023-11-14
author: Kaushal Choudhary
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/multitask-embedding-with-lancedb-be18ec397543/preview-image.png
meta_image: /assets/blog/multitask-embedding-with-lancedb-be18ec397543/preview-image.png
description: "Explore about multitask embedding with lancedb. Get practical steps, examples, and best practices you can use now."
---

In this blog, weâ€™ll cover InstructOR embedding model and how it can be used with LanceBD. LanceDB embedding API already supports this model so we can simply use it from the registry.

Link to the [code](https://github.com/lancedb/vectordb-recipes/tree/main/examples/instruct-multitask). Follow along with this [***Colab Notebook***](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/instruct-multitask/main.ipynb).

## Embeddings Recap

Text embedding is a piece of text projected into a high-dimensional latent space. The position of our text in this space is a vector, a long sequence of numbers. Think of the two-dimensional Cartesian coordinates from algebra class, but with more dimensions â€” often 768 or 1536.

For example, hereâ€™s what the [OpenAI text-embedding-ada-002](https://openai.com/blog/new-and-improved-embedding-model) model does with the paragraph above. Each vertical band in this plot represents a value in one of the embedding spaceâ€™s 1536 dimensions.
![](https://miro.medium.com/v2/resize:fit:770/0*36dOYEz3S8q3OJgy)OpenAI Embedding graph
**What is InstructOR embedding model, and how is it different from the otherwise vanilla embedding model?**

The [**InstructOR** ](https://instructor-embedding.github.io/)multitask embedding model, is an embedding method which **computes custom embedding** for the particular text based on the **instruction**(**s**) provided.

Letâ€™s look at some examples

Instruction : Represent the Science Title

    # prepare texts with instructions
    text_instruction_pairs = [
        {"instruction": "Represent the Science title:", "text": "3D ActionSLAM: wearable person tracking in multi-floor environments"}
    ]
    # calculate embeddings
    customized_embeddings = model.encode(texts_with_instructions)

Instruction : Represent the Medicine sentence for clustering

    import sklearn.cluster
    sentences = [['Represent the Medicine sentence for clustering: ','Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity']]
    embeddings = model.encode(sentences)
    clustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=2)
    clustering_model.fit(embeddings)
    cluster_assignment = clustering_model.labels_
    print(cluster_assignment)

This type of embedding is found to be more efficient for specific task based LLMs.

Let us now get into the use cases of this embedding model.

## Semantic Search with Instructor model and LanceDB.

We are using Instruct Embedding model with LanceDB embedding API. Find more [here](https://lancedb.github.io/lancedb/embeddings/).

So, now we make a call to the registry to get our custom embedding model from the API.

Here, we also pass the instruction while calling the embedding model.

    import lancedb
    from lancedb.pydantic import LanceModel, Vector
    from lancedb.embeddings import get_registry
    from lancedb.embeddings import InstructorEmbeddingFunction

    instructor = get_registry().get("instructor").create(
                                source_instruction="represent the document for retreival",
                                query_instruction="represent the document for most similar definition")

Creating a schema for the data to be ingested. We can change the schema according to our need, but we are going to go with this template for now.

We will instantiate the DB and create a table to store the data.

    #custom schema for our model
    class Schema(LanceModel):
        vector: Vector(instructor.ndims()) = instructor.VectorField()
        text: str = instructor.SourceField()

    #intializing the db and creating a table
    db = lancedb.connect("~/.lancedb")
    tbl = db.create_table("intruct-multitask", schema=Schema, mode="overwrite")

Add the data to the table.

    """
    data = [
        {"instruction": "Represent the science title:", "text": "Aspirin is a widely-used over-the-counter medication known for its anti-inflammatory and analgesic properties. It is commonly used to relieve pain, reduce fever, and alleviate minor aches and pains."},
        {"instruction": "Represent the science title:", "text": "Amoxicillin is an antibiotic medication commonly prescribed to treat various bacterial infections, such as respiratory, ear, throat, and urinary tract infections. It belongs to the penicillin class of antibiotics and works by inhibiting bacterial cell wall synthesis."},
        {"instruction": "Represent the science title:", "text": "Atorvastatin is a lipid-lowering medication used to manage high cholesterol levels and reduce the risk of cardiovascular events. It belongs to the statin class of drugs and works by inhibiting an enzyme involved in cholesterol production in the liver."},
    ]

    """
    #we pass the data in this format to lancedb for embedding
    data = [
        {"text": "Aspirin is a widely-used over-the-counter medication known for its anti-inflammatory and analgesic properties. It is commonly used to relieve pain, reduce fever, and alleviate minor aches and pains."},
        {"text": "Amoxicillin is an antibiotic medication commonly prescribed to treat various bacterial infections, such as respiratory, ear, throat, and urinary tract infections. It belongs to the penicillin class of antibiotics and works by inhibiting bacterial cell wall synthesis."},
        {"text": "Atorvastatin is a lipid-lowering medication used to manage high cholesterol levels and reduce the risk of cardiovascular events. It belongs to the statin class of drugs and works by inhibiting an enzyme involved in cholesterol production in the liver."}
    ]

    tbl.add(data)

Did you notice the commented out data variable? Now, if you were not using LanceDBâ€™s embedding API for the Instructor model, you would have to pass the instruction along with the data like shown above. But because we have already defined the instruction while calling the model, we donâ€™t have to pass it again.

Now, we are going use this embedding for **Semantic Search.**

LanceDB facilitates direct text search, obviating the necessity for manual query embedding.

    query = "amoxicillin"
    result = tbl.search(query).limit(1).to_pandas()
    print(result)

Which will give us an output like this:

                                                  vector  \
    0  [-0.024510665, 0.0005563133, 0.0288403, 0.0807...

                                                    text  _distance
    0  Amoxicillin is an antibiotic medication common...   0.163671

## **Same Data with Different Instruction Pair**

To analyze the effect and contrast between different instruction pairs, we shall input the same data but with another pair.

This is the new instruction pair for the same input data as above. For brevity, only the instruction part is shown below.

    instructor = get_registry().get("instructor").create(
                                source_instruction="represent the captions",
                                query_instruction="represent the captions for retrieving duplicate captions"
                                )

From this we will get an output like this:

                                                  vector  \
    0  [-0.024483299, 0.000932854, 0.033273745, 0.077...

                                                    text  _distance
    0  Amoxicillin is an antibiotic medication common...    0.18135

The* **_distance*** value is different for each embedding we calculated on the same data, but with different instruction pairs.

## Question Answering with Instructor model and LanceDB.

You can just call the model from registry, and change the query instruction.

    import lancedb
    from lancedb.pydantic import LanceModel, Vector
    from lancedb.embeddings import get_registry
    from lancedb.embeddings import InstructorEmbeddingFunction

    instructor = get_registry().get("instructor").create(
                                source_instruction="represent the docuement for retreival",
                                query_instruction="Represent the wikipedia question for retreving supporting documnents"
                                )

Creating the schema and the DB.

    class Schema(LanceModel):
        vector: Vector(instructor.ndims()) = instructor.VectorField()
        text: str = instructor.SourceField()

    db = lancedb.connect("~/.lancedb-qa")
    tbl = db.create_table("intruct-multitask-qa", schema=Schema, mode="overwrite")

Letâ€™s add the data.

    data_qa = [ {"text": "A canvas painting is artwork created on a canvas surface using various painting techniques and mediums like oil, acrylic, or watercolor. It is popular in traditional and contemporary art, displayed in galleries, museums, and homes."},
        {"text": "A cinema, also known as a movie theater or movie house, is a venue where films are shown to an audience for entertainment. It typically consists of a large screen, seating arrangements, and audio-visual equipment to project and play movies."},
        {"text": "A pocket watch is a small, portable timekeeping device with a clock face and hands, designed to be carried in a pocket or attached to a chain. It is typically made of materials such as metal, gold, or silver and was popular during the 18th and 19th centuries."},
        {"text": "A laptop is a compact and portable computer with a keyboard and screen, ideal for various tasks on the go. It offers versatility for browsing, word processing, multimedia, gaming, and professional work."}
    ]
    tbl.add(data_qa)

Querying the database using text search.

    query = "what is a cinema"
    result = tbl.search(query).limit(1).to_pandas()
    print(result.text)

                                                  vector  \
    0  [0.021844529, 0.0017777127, 0.022723941, 0.049...

                                                    text  _distance
    0  A cinema, also known as a movie theater or mov...   0.131036

You can find various types of instructions for your use case. Check out the official project page of [Instruct](https://instructor-embedding.github.io/).
![](https://miro.medium.com/v2/resize:fit:770/1*wOgP--ufAxpis3ysuZI_rw.png)Instruction examples
Visit our [GitHub ](https://github.com/lancedb)and if you wish to learn more about LanceDB python and Typescript library
For more such applied GenAI and VectorDB applications, examples and tutorials visit [VectorDB-Recipes.](https://github.com/lancedb/vectordb-recipes/tree/main)

Lastly, for more information and updates, follow our [LinkedIn](https://www.linkedin.com/company/lancedb/) and [Twitter.](https://twitter.com/lancedb)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\ner-powered-semantic-search-using-lancedb-51051dc3e493.md

---

```md
---
title: "NER-Powered Semantic Search Using LanceDB"
date: 2023-11-08
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/ner-powered-semantic-search-using-lancedb-51051dc3e493/preview-image.png
meta_image: /assets/blog/ner-powered-semantic-search-using-lancedb-51051dc3e493/preview-image.png
description: "**NER stands for Named Entity Recognition**, a form of natural language processing (NLP) that involves extracting and identifying essential information from text."
---

**NER stands for Named Entity Recognition**, a form of natural language processing (NLP) that involves extracting and identifying essential information from text. The information that is extracted and categorized is called an **entity**. It can be any word or a series of words that consistently refers to the same thing. Some examples of entities are:

- **Person**: The name of a person, such as John, Mary, or Barack Obama.
- **Organization**: The name of a company, institution, or group, such as Google, NASA, or LanceDB.
- **Location**: The name of a geographical place, such as London, Mount Everest, or Mars.
- **Date/Time**: A specific point or period in time, such as 2023, November 5, or 14:23:31.
- **Numeral**: A numerical value, such as 42, 3.14, or $199.99.

![](https://miro.medium.com/v2/resize:fit:770/0*7tz40y-1soPJspml.png)
In simpler words, if your task is to find out â€˜whereâ€™, â€˜whatâ€™, â€˜whoâ€™, and â€˜whenâ€™ from a sentence, NER is the solution you should opt for.
![](https://miro.medium.com/v2/resize:fit:770/0*YtFAd1gli2RZTFCd)Photo by [Simon Berger](https://unsplash.com/@8moments?utm_source=medium&amp;utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&amp;utm_medium=referral)
## Methods of Named Entity Recognition

1. **Dictionary-Based NER â€” **In this method, a vocabulary-based dictionary is utilized, and basic string-matching algorithms are employed to determine if an entity exists within the provided text by comparing it with the entries in the vocabulary. However, this method is typically avoided due to the necessity of regularly updating and maintaining the dictionary.
2. **Rule-Based NER â€” **This approach relies on employing a predefined collection of rules for extracting information, with these rules being categorized into two types: pattern-based and context-based. Pattern-based rules analyze the morphological patterns of words, while context-based rules examine the context in which the words appear within the text document.
3. **Deep Learning- Based NER** â€” Deep learning NER outperforms all other methods in terms of accuracy due to its ability to construct word associations, leading to a superior understanding of the semantic and syntactic connections among words. Additionally, it possesses the capability to automatically analyze topic-specific and high-level terminology.

In this blog, weâ€™ll see an example of Named Entity Recognition using the Deep Learning approach for which, ***we use a dataset containing ~190K articles scraped from Medium***. We selected 50K articles from the dataset as indexing all the articles may take some time.

To extract named entities, we will use a*** NER model finetuned on a BERT-base model***.

The flow will look like this
![](https://miro.medium.com/v2/resize:fit:1100/1*xicHs6pPkpR9e9gXT-M4gg.png)
In short, we use the NER model to further filter the semantic search results. The predicted named entities are used as â€œpost filtersâ€ to filter the vector search results.

## Implementation

In the Implementation section, we see the step-by-step implementation of NER on the Medium dataset. Starting with the first step of ***loading the dataset from huggingface***.

    from datasets import load_dataset

    # load the dataset and convert to pandas dataframe
    df = load_dataset(
        "fabiochiu/medium-articles",
        data_files="medium_articles.csv",
        split="train"
    ).to_pandas()

Once a dataset is loading, weâ€™ll ***preprocess a loaded dataset ***which will include removing empty rows, taking only 1000 words from the article, etc.

    # drop empty rows and select 20k articles
    df = df.dropna().sample(20000, random_state=32)
    df.head()

    # select first 1000 characters
    df["text"] = df["text"].str[:1000]
    # join article title and the text
    df["title_text"] = df["title"] + ". " + df["text"]

These are basic preprocessing steps that I have performed; you can apply them further according to your datasets.

To extract named entities, we will ***use a NER model finetuned on a BERT-base model***. The model can be loaded from the HuggingFace model hub.

    from transformers import AutoTokenizer, AutoModelForTokenClassification
    from transformers import pipeline

    model_id = "dslim/bert-base-NER"

    # load the tokenizer from huggingface
    tokenizer = AutoTokenizer.from_pretrained(
        model_id
    )
    # load the NER model from huggingface
    model = AutoModelForTokenClassification.from_pretrained(
        model_id
    )
    # load the tokenizer and model into a NER pipeline
    nlp = pipeline(
        "ner",
        model=model,
        tokenizer=tokenizer,
        aggregation_strategy="max",
        device=device
    )

Letâ€™s try to use the NER pipeline to extract named entities from the text

    text = "What are the best Places to visit in London"
    nlp(text)

The output of the pipeline looks like

    [{'entity_group': 'LOC',
      'score': 0.99969244,
      'word': 'London',
      'start': 37,
      'end': 43}]

This shows our NER pipeline is working as expected and accurately extracting entities from the text.

Now we have the NER pipeline, weâ€™ll initialize the Retriever.

A ***Retriever ***model is used to embed passages (article title + first 1000 characters) and queries. It ***creates embeddings such that queries and passages with similar meanings are close in the vector space***. We will use a sentence-transformer model as our retriever. The model can be loaded using the following code.

    from sentence_transformers import SentenceTransformer

    # load the model from huggingface
    retriever = SentenceTransformer(
        'flax-sentence-embeddings/all_datasets_v3_mpnet-base',
        device=device
    )

Now let's ***generate Embeddings and insert them inside LanceDB for querying***

Connect LanceDB

    import lancedb
    db = lancedb.connect("./.lancedb")

Letâ€™s first write a helper function to extract named entities from a batch of text.

    def extract_named_entities(text_batch):
        # extract named entities using the NER pipeline
        extracted_batch = nlp(text_batch)
        entities = []
        # loop through the results and only select the entity names
        for text in extracted_batch:
            ne = [entity["word"] for entity in text]
            entities.append(ne)
        return entities

Now generating embeddings and storing them in the table of LanceDB with their respective metadata

    from tqdm.auto import tqdm
    import warnings
    import pandas as pd
    import numpy as np

    warnings.filterwarnings('ignore', category=UserWarning)

    # we will use batches of 64
    batch_size = 64
    data = []
    from collections import defaultdict
    # table_data = defaultdict(list)

    for i in tqdm(range(0, len(df), batch_size)):
        # find end of batch
        i_end = min(i+batch_size, len(df))
        # extract batch
        batch = df.iloc[i:i_end].copy()
        # generate embeddings for batch
        emb = retriever.encode(batch["title_text"].tolist()).tolist()
        # extract named entities from the batch
        entities = extract_named_entities(batch["title_text"].tolist())
        # remove duplicate entities from each record
        batch["named_entities"] = [list(set(entity)) for entity in entities]
        batch = batch.drop('title_text', axis=1)
        # get metadata
        meta = batch.to_dict(orient="records")
        # create unique IDs
        ids = [f"{idx}" for idx in range(i, i_end)]
        # add all to upsert list
        to_upsert = list(zip(ids, emb, meta,batch["named_entities"]))
        for id, emb, meta, entity in to_upsert:
          temp = {}
          temp['vector'] = np.array(emb)
          temp['metadata'] = meta
          temp['named_entities'] = entity
          data.append(temp)
    #create table using above data
    tbl = db.create_table("tw", data)
    # check table
    tbl.head()

Now we have all the embeddings with their metadata in a table, we can start querying.

Letâ€™s first write a helper function to extract the named entity, encode text into retriever format, and filter the query results.

    def search_lancedb(query):
        # extract named entities from the query
        ne = extract_named_entities([query])[0]
        # create embeddings for the query
        xq = retriever.encode(query).tolist()
        # query the lancedb table while applying named entity filter
        xc = tbl.search(xq).to_list()
        # extract article titles from the search result
        r = [x["metadata"]["title"] for x in xc for i in x["metadata"]["named_entities"]  if i in ne]
        return pprint({"Extracted Named Entities": ne, "Result": r})

Now Let's try Querying

    query = "How Data is changing world?"
    search_lancedb(query)

Output looks like

    {'Extracted Named Entities': ['Data'],
     'Result': ['Data Science is all about making the right choices']}

Another Query

    query = "Why does SpaceX want to build a city on Mars?"
    search_lancedb(query)

Output looks like

    {'Extracted Named Entities': ['SpaceX', 'Mars'],
     'Result': ['Mars Habitat: NASA 3D Printed Habitat Challenge',
                'Reusable rockets and the robots at sea: The SpaceX story',
                'Reusable rockets and the robots at sea: The SpaceX story',
                'Colonising Planets Beyond Mars',
                'Colonising Planets Beyond Mars',
                'Musk Explained: The Musk approach to marketing',
                'How Weâ€™ll Access the Water on Mars',
                'Chasing Immortality',
                'Mission Possible: How Space Exploration Can Deliver Sustainable '
                'Development']}

For quickstart, You can try out [Google Colab](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/tutorials/NER-powered-Semantic-Search-with-LanceDB/NER_powered_Semantic_Search_with_LanceDB.ipynb?source=post_page-----51051dc3e493--------------------------------), which has all this code.

[NER is widely used in various NLP tasks. These include:](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/tutorials/NER-powered-Semantic-Search-with-LanceDB/NER_powered_Semantic_Search_with_LanceDB.ipynb?source=post_page-----51051dc3e493--------------------------------)

- Efficient search algorithms
- Content recommendations
- Customer support
- Healthcare
- Summarizing resumes
- Content classification for news channels.

## Conclusion

Named entity recognition primarily uses NLP and Deep Learning. ***With NLP, a NER model studies the structure and rules of language*** and tries to form intelligent systems that are capable of deriving meaning from text and speech. Meanwhile, ***Deep Learning algorithms help an NER model to learn and improve over time***.

Using these two technologies, the model detects a word or string of words as an entity and classifies it into a category it belongs to. Harnessing the advanced vector search functionalities provided by LanceDB enhances its overall utility.

**Visit the **[**LanceDB **](https://www.linkedin.com/feed/?trk=sem-ga_campid.14650114791_asid.148989926143_crid.662526548043_kw.www+linkedin_d.c_tid.kwd-296759856208_n.g_mt.p_geo.9062111#)**repo to learn more about lanceDB python and Typescript library**

**To discover more such applied GenAI and vectorDB applications, examples, and tutorials visit **[**VectorDB-Recipes**](https://github.com/lancedb/vectordb-recipes)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\openais-new-embeddings-with-lancedb-embeddings-api-a9d109f59305.md

---

```md
---
title: "Benchmarking New OpenAI Embedding Models with LanceDB"
date: 2024-02-13
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/openais-new-embeddings-with-lancedb-embeddings-api-a9d109f59305/preview-image.png
meta_image: /assets/blog/openais-new-embeddings-with-lancedb-embeddings-api-a9d109f59305/preview-image.png
description: "A couple of weeks ago, OpenAI launched their new and most performant embedding models with higher multilingual performance and new parameters to control the overall size, updated moderation models, API usage management t."
---

A couple of weeks ago, OpenAI launched their new and most performant embedding models with higher multilingual performance and new parameters to control the overall size, updated moderation models, API usage management tools, and reduced pricing for GPT-3.5 Turbo.

Embeddings are numeric representations of content, such as text or code. This helps machine learning models understand semantic relationships with data points and perform similarity-based retrieval. Embeddings are used in knowledge retrieval tasks, especially in Retrieval Augmented Generation (RAG) for systems like ChatGPT, Bard, and other related tasks.
![](https://miro.medium.com/v2/resize:fit:700/1*9R8vHBGUhSstHB4uCFEGEw.gif)Credits: OpenAI
The newly launched OpenAI's Embedding models `text-embedding-3-small` and `text-embedding-3-large,` have generated considerable interest due to their higher multilingual performance and new parameter `dimensions` to control the overall size of embeddings. `text-embedding-3-small` returns embeddings of 1536-dimensional vector size same as older embedding model `text-embedding-ada-002` and `text-embedding-3-large` returns embeddings of 3072-dimensional vector size.
Weâ€™ll see performance improvement with and without dimensionality reduction features.

The latest embedding model `text-embedding-3-small`is highly efficient and represents a significant improvement compared to its predecessor, the `text-embedding-ada-002` model released in December 2022.

## Performance Increase

While Comparing `text-embedding-ada-002` to `text-embedding-3-small`, the average score on a commonly used benchmark for English tasks ([MTEB](https://github.com/embeddings-benchmark/mteb)) using the *StackOverflowDupQuestions* task shows an increase from 50.5% to 51.4%.

## Native support for Dimensionality Reduction

Using larger embeddings and storing them in a vector store for retrieval generally costs more and consumes more computing, memory, and storage than smaller embeddings.

Length of embeddings (by omitting specific numbers from the end of the sequence) without compromising their ability to represent concepts. This can be performed by utilizing the `dimensions` API parameter. In the MTEB benchmark, an `text-embedding-3-large` embedding can be reduced to a size of 256 and still exhibit performance compared to an `text-embedding-ada-002` embedding with a size of 1536.

Here are the Results of the MTEB benchmark on the ***StackOverflowDupQuestions*** task.
![](https://miro.medium.com/v2/resize:fit:567/1*JkCenLsWfEwx45NrewdcLw.png)MTEB Benchmark mAP scores
## **OpenAI Embedding Models with LanceDB**

![](https://miro.medium.com/v2/resize:fit:408/1*jv8RjdLjF8wD_1OZbuFStg.png)
The new OpenAI embedding models work well with LanceDB. Combining these developer-friendly embedding models with LanceDBâ€™s vector database is excellent for tasks like Semantic Search, Knowledge Retrieval, and many other applications.

OpenAI embeddings function can be called via LanceDB while creating a table that implicitly converts text into OpenAI embeddings and adds them in the table in the following way using Python, which gives the flexibility to use the vectorized text across all sessions.

    import lancedb
    from lancedb.embeddings import EmbeddingFunctionRegistry

    db = lancedb.connect("/tmp/db")
    registry = EmbeddingFunctionRegistry.get_instance(name="text-embedding-3-small")
    func = registry.get("openai").create()

    class Words(LanceModel):
        text: str = func.SourceField()
        vector: Vector(func.ndims()) = func.VectorField()

    table = db.create_table("words", schema=Words)
    table.add(
        [
            {"text": "hello world"}
            {"text": "goodbye world"}
        ]
        )

    query = "greetings"
    actual = table.search(query).limit(1).to_pydantic(Words)[0]
    print(actual.text)
    The OpenAI embedding function is already ingested in the table. It doesnâ€™t require loading it again while adding text to the table.
    table = db.open_table("words")

    # Automatically vectorized text across all sessions
    table.add(
        [{"text": "changing world"}])

    query = "world"
    result = table.search(query).limit(1).to_pydantic(Words)[0]
    print(result.text)

## Conclusion

This was a short intro to new OpenAI embedding models and how LanceDBâ€™s Embedding API simplifies working with embedding functions. Check out other interesting [blogs](__GHOST_URL__/) and solutions on [**vectordb-recipes**](https://github.com/lancedb/vectordb-recipes).
```

---

## ðŸ“„ æ–‡ä»¶: drafts\optimizing-llms-a-step-by-step-guide-to-fine-tuning-with-peft-and-qlora-22eddd13d25b.md

---

```md
---
title: "Optimizing LLMs: a Step-by-Step Guide to Fine-Tuning with PEFT and QLoRA"
date: 2023-09-28
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/optimizing-llms-a-step-by-step-guide-to-fine-tuning-with-peft-and-qlora-22eddd13d25b/preview-image.png
meta_image: /assets/blog/optimizing-llms-a-step-by-step-guide-to-fine-tuning-with-peft-and-qlora-22eddd13d25b/preview-image.png
description: "Understand about optimizing llms: a step-by-step guide to fine-tuning with peft and qlora. Get practical steps, examples, and best practices you can use now."
---

**A Practical Guide to Fine-Tuning LLM using QLora**

Conducting inference with large language models (LLMs) demands significant GPU power and memory resources, which can be prohibitively expensive. To enhance inference performance and speed, it is imperative to explore lightweight LLM models. Researchers have developed a few techniques. In this blog, weâ€™ll delve into these essential concepts that enable cost-effective and resource-efficient deployment of LLMs.

## What is Instruction Fine-Tuning?

Instruction fine-tuning is a critical technique that empowers large language models (LLMs) to follow specific instructions effectively. When we begin with a base model, pre-trained on an immense corpus of worldly knowledge, it boasts extensive knowledge but might not always comprehend and respond to specific prompts or queries. In essence, it requires fine-tuning to tailor its behavior.

**When Does Instruction Fine-Tuning Work**?

Instruction fine-tuning shines in specific scenarios:

1. Precision Tasks: When precision in responses is paramount, such as classifying, summarizing, or translating content, instruction fine-tuning significantly enhances accuracy.
2. Complex Tasks: For intricate tasks involving multiple steps or nuanced understanding, instruction fine-tuning equips the model to generate meaningful outputs.
3. Domain-Specific Tasks: In specialized domains, instruction fine-tuning enables the model to adapt to unique language and context.
4. Tasks Requiring Improved Accuracy: When base model responses require refinement for higher accuracy, instruction fine-tuning becomes invaluable.

**When Might Instruction Fine-Tuning Fall Short?**

Despite its advantages, instruction fine-tuning may face challenges in specific situations:

1. Smaller Models: Instruction fine-tuning can be tough for smaller LLMs with fewer parameters, impacting performance.
2. Limited Space in Prompts: Long examples or instructions in prompts may reduce space for essential context.
3. High Memory and Compute Demands: Full fine-tuning, and updating all model weights, needs significant memory and computation, which may not work in resource-limited setups.

**Mitigating Catastrophic Forgetting in Fine-Tuning**

While instruction fine-tuning is a potent tool for boosting LLM performance, itâ€™s crucial to address a potential challenge called catastrophic forgetting. This issue can affect the modelâ€™s ability to generalize to other tasks, necessitating strategies to minimize its impact.

**When Does Catastrophic Forgetting Occur?**

Catastrophic forgetting usually arises during fine-tuning when an LLM is optimized for a single task, potentially erasing or degrading its prior capabilities. For example, fine-tuning a sentiment analysis task may lead the model to struggle with tasks it previously excelled at, like named entity recognition.

Options to Address Catastrophic Forgetting: To mitigate or prevent catastrophic forgetting, consider these approaches:

**1. Task-Specific Multitask Fine-Tuning**: When aiming to preserve a modelâ€™s multitask capabilities and prevent catastrophic forgetting, you can opt for task-specific multitask fine-tuning. This approach involves fine-tuning multiple tasks simultaneously, ensuring the model maintains its versatility. However, it comes with a requirement for a substantial dataset containing examples across various tasks.

i) **Single Task Fine-Tuning Example**: Imagine you have a base model and a specific task, such as sentiment analysis. Single-task fine-tuning involves optimizing the model exclusively for this task, resulting in improved performance in sentiment analysis. However, this process may lead to forgetting other tasks it previously excelled in.

ii)**Multitask Fine-Tuning Example** â€” FLAN-T5: FLAN-T5 serves as an excellent example of multitask fine-tuning. Itâ€™s a multitask fine-tuned version of the T5 foundation model. FLAN-T5 has been trained on a diverse range of datasets and tasks, encompassing 473 datasets across 146 task categories. This extensive multitask fine-tuning equips FLAN-T5 to excel in various tasks simultaneously, making it a versatile and capable model.

**2. Parameter Efficient Fine-Tuning (PEFT)**: PEFT offers a more memory-efficient alternative to full fine-tuning. It preserves most of the original LLMâ€™s weights while training only a small number of task-specific adapter layers and parameters. Weâ€™ll explore PEFT further in this blog series.

## Parameter Efficient Fine-Tuning (PEFT): Making LLMs More Efficient

Training large language models (LLMs) is a computational behemoth. Full fine-tuning, where all model weights are updated during supervised learning, demands immense memory capacity, including storage for model weights, optimizer states, gradients, forward activations, and temporary memory throughout the training process. This memory load can swiftly surpass whatâ€™s feasible on consumer hardware.

In contrast, parameter-efficient fine-tuning (PEFT) methods offer an elegant solution. PEFT strategically targets only a fraction of the modelâ€™s parameters for modification, significantly reducing memory requirements. Hereâ€™s why PEFT matters:

1. Focused Parameter Updates: PEFT targets specific model parameters, reducing memory load.
2. Memory Efficiency: PEFT keeps most LLM weights frozen, using only a fraction of the original modelâ€™s parameters, making it suitable for limited hardware.
3. Catastrophic Forgetting Mitigation: PEFT minimizes the risk of catastrophic forgetting.
4. Adaptation to Multiple Tasks: PEFT efficiently adapts to various tasks without significant storage demands.

**PEFT Methods:**

- **Selective Methods**: Fine-tune a subset of LLM parameters, offering a balance between parameter efficiency and computational cost.
- **Reparameterization Methods**: Reduce trainable parameters by creating new low-rank transformations of existing LLM weights, we can use LoRA, QLora methods
- **Additive Methods**: Keep original LLM weights frozen and introduce new trainable components, such as adapter layers or [soft prompt methods.](https://arxiv.org/pdf/2306.04933.pdf)

Stay tuned as we explore specific PEFT techniques like prompt tuning and LoRA to understand how they reduce memory requirements during LLM fine-tuning.

Now weâ€™ll delve into specific PEFT techniques QLora, a deeper understanding of how these methods reduce memory requirements during LLM fine-tuning

## LoRA (Low-rank Adaptation): Reducing Memory for LLM Fine-Tuning

Low-rank Adaptation, or LoRA, is a parameter-efficient fine-tuning technique categorized under re-parameterization methods. LoRA aims to drastically cut down the number of trainable parameters while fine-tuning large language models (LLMs). Hereâ€™s a closer look at how LoRA works:
![](https://miro.medium.com/v2/resize:fit:770/1*iGTsCbqOE_RuaqE_mBYapg.png)
Understanding the Transformer Architecture: To appreciate LoRA, letâ€™s revisit the fundamental architecture of a transformer model. The transformer architecture consists of an encoder and/or decoder part, each containing self-attention and feedforward networks. These components have weights that are initially learned during pre-training.

**Reducing Parameters with LoRA**: LoRA employs a smart strategy â€” that freezes all the original model parameters and introduces a pair of rank decomposition matrices alongside the existing weights. The key steps in LoRA are as follows:
![](https://miro.medium.com/v2/resize:fit:660/1*U1pDAeIrj9Hxh4DnFZMsAw.gif)
1. Low-Rank Matrices: LoRA introduces two low-rank matrices, Matrix A and Matrix B, alongside the original LLM weights.
2. Matrix Dimensions: The dimensions of these smaller matrices are carefully set so that their product results in a matrix of the same dimensions as the weights theyâ€™re modifying.
3. Training Low-Rank Matrices: During fine-tuning, you keep the original LLM weights frozen while training Matrix A and Matrix B using supervised learning, a process youâ€™re already familiar with.
4. Inference: For inference, you multiply the two low-rank matrices together to create a matrix with the same dimensions as the frozen weights. You then add this new matrix to the original weights and replace them in the model.
5. Significant Parameter Reduction: One of LoRAâ€™s remarkable features is its ability to substantially reduce the number of trainable parameters. To put this into perspective, letâ€™s consider an example based on the transformer architectureâ€™s dimensions. A typical weights matrix in a transformer has 32,768 trainable parameters. If you apply LoRA with a rank of eight, you will train two small rank decomposition matrices. Matrix A with dimensions 8 by 64 results in 512 trainable parameters, while Matrix B with dimensions 512 by 8 amounts to 4,096 trainable parameters. In total, youâ€™re training just 4,608 parameters, which is an 86% reduction compared to the original.

Due to its parameter efficiency, LoRA can often be executed on a single GPU, eliminating the need for an extensive distributed GPU cluster. The memory required to store LoRA matrices is minimal, enabling fine-tuning for numerous tasks without the burden of storing multiple full-size LLM versions.

**LoRA Performance**: LoRAâ€™s efficiency doesnâ€™t come at the cost of performance. While the reduction in parameters might lead to slightly lower performance gains compared to full fine-tuning, LoRA still delivers impressive results, especially when compared to the base LLM model.

In practice, LoRA is an invaluable tool for efficiently fine-tuning LLMs and adapting them to specific tasks without overwhelming computational and memory resources. It strikes a balance between parameter efficiency and performance, making it a go-to technique for many natural language processing applications.

But wait, thereâ€™s a game-changer on the horizon â€” QLoRA.

## What Is QLoRA?

[QLoRA](https://arxiv.org/pdf/2305.14314.pdf), which stands for Quantized Low-rank Adaptation, takes fine-tuning to the next level. It empowers you to fine-tune LLMs on a single GPU, pushing the boundaries of whatâ€™s possible. How does QLoRA differ from LoRA?

The paper introduces QLoRA, an efficient fine-tuning method that enables the training of a 65-billion-parameter language model on a single 48GB GPU while maintaining good performance. QLoRA leverages 4-bit quantization and Low-Rank Adapters (LoRA) to achieve this. The authorsâ€™ best model family, named Guanaco, outperforms previously released models on the Vicuna benchmark, reaching 99.3% of ChatGPTâ€™s performance with just 24 hours of fine-tuning on a single GPU.

Key innovations in QLoRA include the use of a 4-bit NormalFloat (NF4) data type for normally distributed weights, double quantization to reduce memory usage, and paged optimizers to manage memory spikes. They fine-tune over 1,000 models using QLoRA and analyze instruction following and chatbot performance across various datasets, model types (LLaMA, T5), and scales (33B and 65B parameters).

Results show that QLoRA fine-tuning with a small high-quality dataset achieves state-of-the-art results, even with smaller models compared to previous state-of-the-art models. The paper also discusses chatbot performance, highlighting that GPT-4 evaluations can be a cost-effective alternative to human evaluation. Additionally, the authors question the reliability of current chatbot benchmarks for evaluating chatbot performance and present a comparative analysis between Guanaco and ChatGPT.

The authors have made their [models and code](https://github.com/artidoro/qlora), including CUDA kernels for 4-bit training, available to the public.

**The QLoRA Advantage**

QLoRA introduces innovative techniques that set it apart: Certainly, hereâ€™s a concise summary of QLORAâ€™s key innovations:

QLoraâ€™s Memory-Efficient Innovations:

1. **4-bit NormalFloat**: QLORA introduces a quantization data type optimized for normally distributed data, achieving efficient compression with minimal information loss.
2. **Double Quantization**: This technique quantizes the quantization constants, saving an average of about 0.37 bits per parameter, leading to significant memory savings in large models.
3. **Paged Optimizers**: QLORA uses NVIDIA unified memory to tackle memory spikes during training, especially when processing long sequences, making it feasible to train large models without running into memory limitations.

These innovations collectively enable more efficient and memory-friendly training of large-scale language models, making QLORA a groundbreaking approach for AI research and development

## Getting Hands-On with QLORA: Implementation and Fine-Tuning

Enough theory; itâ€™s time to roll up our sleeves and dive into the exciting world of QLORA. In this hands-on session, weâ€™ll walk through the steps to fine-tune a model using QLORA and save it in a quantized form. To achieve this, weâ€™ll rely on two powerful libraries: Transformers and Bits & Bytes. These libraries are the cornerstone of implementing QLoRA, a remarkable evolution of the Low-Rank Adapter (LoRA) technique, supercharged with quantization.

But before we embark on this coding adventure, letâ€™s make sure we have all the essential libraries in place. So, fasten your seatbelts as we take the first steps towards unleashing the full potential of QLORA in your AI projects.â€

**Library Setup:**

To work with QLORA, weâ€™ll use two essential libraries:

1. Transformers: This library is crucial for handling pre-trained language models, including QLORA, and facilitates fine-tuning and deployment.
2. Bits & Bytes: This is the core of QLORAâ€™s functionality. It seamlessly integrates QLORA with Transformers, simplifying the process.

Additionally, weâ€™ll harness the power of Hugging Face Accelerate, a library that optimizes training for large language models, ensuring faster and more efficient results.

    !pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git
    !pip install -q datasets bitsandbytes einops wandb
    !pip install -Uqqq pip --progress-bar off
    !pip install -qqq bitsandbytes==0.39.0 --progress-bar off
    !pip install -qqq torch==2.0.1 --progress-bar off
    !pip install -qqq -U git+https://github.com/huggingface/transformers.git@e03a9cc --progress-bar off
    !pip install -qqq -U git+https://github.com/huggingface/peft.git@42a184f --progress-bar off
    !pip install -qqq -U git+https://github.com/huggingface/accelerate.git@c9fbb71 --progress-bar off
    !pip install -qqq datasets==2.12.0 --progress-bar off
    !pip install -qqq loralib==0.1.1 --progress-bar off
    !pip install -qqq einops==0.6.1 --progress-bar off

    import json
    import os
    from pprint import pprint
    import bitsandbytes as bnb
    import pandas as pd
    import torch
    import torch.nn as nn
    import transformers
    from datasets import load_dataset
    from huggingface_hub import notebook_login
    from peft import (
        LoraConfig,
        PeftConfig,
        PeftModel,
        get_peft_model,
        prepare_model_for_kbit_training,
    )
    from transformers import (
        AutoConfig,
        AutoModelForCausalLM,
        AutoTokenizer,
        BitsAndBytesConfig,
    )

We must include our Hugging Face token, ensure that itâ€™s in write mode, and ultimately, use it to upload our model weights to the Hugging Face platform.

    notebook_login()

Download the sample dataset of e-commerce faq & we will finetune using this dataset

    !gdown 1tiAscG941evQS8RzjznoPu8meu4unw5A

Check the few samples of data. we are using [pprint](https://docs.python.org/3/library/pprint.html) library

    pprint(data["questions"][0], sort_dicts=False)

below is a data format of our FAQ-based dataset.
This structured dataset provides answers to common queries, ensuring a seamless shopping experience for our valued customers. & we are fine-tuning the model based on this data.
![](https://miro.medium.com/v2/resize:fit:770/1*Q8V61c3byNfGdT4cp3QqDg.png)
    with open("dataset.json", "w") as f:
        json.dump(data["questions"], f)

**Model selection**

In our current approach, we have implemented a sharded model [**TinyPixel/Llama-2â€“7B-bf16-sharded**](https://huggingface.co/TinyPixel/Llama-2-7B-bf16-sharded) which involves dividing a large neural network model into multiple smaller pieces, typically more than 14 pieces in our case. This sharding strategy has proven to be highly beneficial when combined with the â€˜accelerateâ€™ framework

When a model is sharded, each shard represents a portion of the overall modelâ€™s parameters. Accelerate can then efficiently manage these shards by distributing them across various parts of the memory, including GPU memory and CPU memory. This dynamic allocation of shards allows us to work with very large models without requiring an excessive amount of memory

    MODEL_NAME = "TinyPixel/Llama-2-7B-bf16-sharded"

Now, letâ€™s explore the advanced usage of 4-bit quantization, a technique that can further optimize our model. Before diving in, letâ€™s understand the key parameters and how to use them.

In our code, we make use of the `BitsAndBytesConfig` from the 'transformers' library and pass it as an argument to the `quantization_config` when calling `from_pretrained`. Don't forget to set `load_in_4bit = True` when using `BitsAndBytesConfig`.

Compute Data Type: By default, the compute data type used during computation is `float32`. However, you can change it to `bf16` (bfloat16) for faster processing. For example, you might want to use `bf16` it for computations while keeping hidden states in `float32`.

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,  # Change this dtype for speedups.
    )

The 4-bit integration supports two quantization types: **FP4 **and** NF4.** FP4 stands for Fixed Point 4, while NF4 stands for Normal Float 4. The latter is introduced in the QLoRA paper. You can switch between these two types using the `bnb_4bit_quant_type` parameter. By default, FP4 quantization is used.

Finally, we load our model and tokenizer with these configurations:

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        quantization_config=bnb_config,
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token
    def print_trainable_parameters(model):
        """
        Prints the number of trainable parameters in the model.
        """
        trainable_params = 0
        all_param = 0
        for _, param in model.named_parameters():
            all_param += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
        print(
            f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
        )

This code snippet enables gradient checkpointing to reduce memory usage during training and then preparing the model for quantization

    model.gradient_checkpointing_enable()
    model = prepare_model_for_kbit_training(model)

[**LoraConfig**](https://huggingface.co/docs/peft/conceptual_guides/lora) allows you to control how LoRA is applied to the base model through the following parameters:

    from peft import LoraConfig, get_peft_model

    lora_alpha = 16
    lora_dropout = 0.1
    lora_r = 64

    config = LoraConfig(
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        r=lora_r,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, config)
    print_trainable_parameters(model)

below are the hyperparameters we can choose; you can always do the experiments & select the best

    generation_config = model.generation_config
    generation_config.max_new_tokens = 80   # maxium no of token in output will get
    generation_config.temperature = 0.7
    generation_config.top_p = 0.7
    generation_config.num_return_sequences = 1
    generation_config.pad_token_id = tokenizer.eos_token_id
    generation_config.eos_token_id = tokenizer.eos_token_id

    %%time
    # Specify the target device for model execution, typically a GPU.
    device = "cuda:0"

    # Tokenize the input prompt and move it to the specified device.
    encoding = tokenizer(prompt, return_tensors="pt").to(device)

    # Run model inference in evaluation mode (inference_mode) for efficiency.
    with torch.inference_mode():
        outputs = model.generate(
            input_ids=encoding.input_ids,
            attention_mask=encoding.attention_mask,
            generation_config=generation_config,
        )

    # Decode the generated output and print it, excluding special tokens.
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

Now we can prepare prompts for text-generation tasks

    def generate_prompt(data_point):
        return f"""
    : {data_point["question"]}
    : {data_point["answer"]}
    """.strip()

    def generate_and_tokenize_prompt(data_point):
        full_prompt = generate_prompt(data_point)
        tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)
        return tokenized_full_prompt

    data = data["train"].shuffle().map(generate_and_tokenize_prompt)

create an output folder for saving all experiments

    OUTPUT_DIR = "experiments"

we are using TensorBoard for tracking our experiments.

    %load_ext tensorboard
    %tensorboard --logdir experiments/runs

Below are several training parameters. To explore all of them, please refer to the [BitsAndBytesConfig](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig)

    training_args = transformers.TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        num_train_epochs=1,
        learning_rate=2e-4,
        fp16=True,
        save_total_limit=3,
        logging_steps=1,
        output_dir=OUTPUT_DIR,
        max_steps=80,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        warmup_ratio=0.05,
        report_to="tensorboard",
    )

    trainer = transformers.Trainer(
        model=model,
        train_dataset=data,
        args=training_args,
        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
    )
    model.config.use_cache = False
    trainer.train()

![](https://miro.medium.com/v2/resize:fit:770/1*Kjbl5JFQ6k2sXyluT3KXCQ.png)
Trained model for a few epochs. now save the trained model

    model.save_pretrained("trained-lama_model")

now we are pushing our weights to Huggingface hub, so later we can use these weights for fine-tuning use your own directory name to push it into your HF repo.

    model.push_to_hub(
        "akashAD/Llama2-7b-qlora-chat-support-bot-faq", use_auth_token=True
    )

load the trained model

    PEFT_MODEL = "akashAD/Llama2-7b-qlora-chat-support-bot-faq"

    config = PeftConfig.from_pretrained(PEFT_MODEL)
    model = AutoModelForCausalLM.from_pretrained(
        config.base_model_name_or_path,
        return_dict=True,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
    tokenizer.pad_token = tokenizer.eos_token

    model = PeftModel.from_pretrained(model, PEFT_MODEL)

below are some experimental hyperparameters you can play with it to get the best results

    generation_config = model.generation_config
    generation_config.max_new_tokens = 50
    generation_config.temperature = 0.3
    generation_config.top_p = 0.7
    generation_config.num_return_sequences = 1
    generation_config.pad_token_id = tokenizer.eos_token_id
    generation_config.eos_token_id = tokenizer.eos_token_id

    %%time
    prompt = f"""
    : How can I create an account?
    :
    """.strip()

    encoding = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    with torch.inference_mode():
        outputs = model.generate(
            input_ids=encoding.input_ids,
            attention_mask=encoding.attention_mask,
            generation_config=generation_config,

        )
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

The below code utilizes our model to generate text responses from user questions, streamlining conversational AI interactions

    def generate_response(question: str) -> str:
        prompt = f"""
    : {question}
    :
    """.strip()
        encoding = tokenizer(prompt, return_tensors="pt").to(DEVICE)
        with torch.inference_mode():
            outputs = model.generate(
                input_ids=encoding.input_ids,
                attention_mask=encoding.attention_mask,
                generation_config=generation_config,
            )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        assistant_start = ":"
        response_start = response.find(assistant_start)
        return response[response_start + len(assistant_start) :].strip()

now let us ask questions to our model

    prompt = "Can I return a product if it was a clearance or final sale item?"
    print(generate_response(prompt))

    prompt = "What happens when I return a clearance item?"
    print(generate_response(prompt))

That's it; you can try to play with these hyperparameters to achieve better results.

Feel free to explore the power of LLMs on your own data with this[ **Colab notebook**](https://colab.research.google.com/drive/1cDGWbgnkTaGwF-HMWCewoqBM_Ir2ywFf?usp=sharing)

## Summary

This blog explores instruction fine-tuning and mitigating catastrophic forgetting in large language models (LLMs). It covers how instruction fine-tuning improves LLMs for precision tasks but may face challenges with smaller models and resource constraints.

To tackle catastrophic forgetting, strategies like task-specific multitask fine-tuning and parameter-efficient fine-tuning (PEFT) are discussed, with a focus on PEFTâ€™s memory efficiency.

The blog introduces LoRA (Low-rank Adaptation), a parameter-efficient fine-tuning technique, and QLoRA (Quantized Low-rank Adaptation), which enhances LoRA. It explains their benefits and differences.

A hands-on QLoRA implementation is demonstrated using Transformers and Bits & Bytes libraries, including model selection and training. Lastly, we covered saving and sharing trained models on the Hugging Face Hub and provided instructions for model loading and text generation tasks.

Stay tuned for upcoming blogs as we delve deeper into the world of Large Language Models (LLMs). Your support is greatly appreciated â€” leave a like if you found our exploration enlightening!

For a deeper dive into cutting-edge technology, explore the** **[**vector-recipes**](https://github.com/lancedb/vectordb-recipes) repository, brimming with real-world examples, use cases, and recipes to ignite your next project. We trust you found this journey informative and inspiring. Cheers!
```

---

## ðŸ“„ æ–‡ä»¶: drafts\s3-backed-full-text-search-with-tantivy-part-1-ac653017068b.md

---

```md
---
title: "S3 Backed Full-Text Search with Tantivy (Part 1)"
date: 2023-08-14
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/s3-backed-full-text-search-with-tantivy-part-1-ac653017068b/preview-image.png
meta_image: /assets/blog/s3-backed-full-text-search-with-tantivy-part-1-ac653017068b/preview-image.png
description: "Discover about s3 backed full-text search with tantivy (part 1). Get practical steps, examples, and best practices you can use now."
---

[Tantivy](https://github.com/quickwit-oss/tantivy) is a highly performant full text search library written in Rust. The python version of tantivy powers the full-text search (FTS) feature in [LanceDB](https://github.com/lancedb/lancedb).

However, vanilla tantivy only supports building and searching index against a local directory. This is a problem for [LanceDB Cloud](https://noteforms.com/forms/lancedb-mailing-list-cloud-kty1o5?notionforms=1&amp;utm_source=notionforms), because we want to store data in and serve data from S3. In this article we will walk through the engineering challenges we encountered in making tantivy s3-compatible.

The work described in this article has been made available as OSS in [this repo](https://github.com/lancedb/tantivy-object-store).

Note: this implementation depends on certain implementation details of tantivy, which could limit upgrade compatibility in the future.

## Engineering Challenges

We ran into quite a few challenges during implementation, the most notable ones are:

1. How can we ensure a distributed reader doesnâ€™t read a dirty `meta.json` file?
2. Index build became incredibly slow with our first naive implementation.
3. Index search also became slow with the naive implementation.

## Implementing Copy on Write (CoW) for Tantivy

[!!!WARNING!!!]: While tantivy is unlikely to make major changes to its `meta.json` file or committing strategy anytime soon, this implementation relies on the fact that `meta.json` is the single source of truth for an indexâ€™s content. If `meta.json` no longer is the source of truth to an index, this implement would break and potentially cause data corruption.

The goal is to enable multiple distributed readers to read from the same index while making sure a writer can update the index without interrupptin the readers. This would allow us to run multiple LanceDB Cloud query node instances against the same FTS index.
![](https://miro.medium.com/v2/resize:fit:770/1*cj41cmBTBz0QaT9h3e6loQ.png)
Since there is only one `meta.json` file, the following could happen
![](https://miro.medium.com/v2/resize:fit:770/1*w45DjlH0YzqnrITLLueRyg.png)
To avoid readers from reading partially written `meta.json` file, we decided to implement a CoW `meta.json` file. What does that mean? See this diagram.
![](https://miro.medium.com/v2/resize:fit:770/1*ReeLhxHyF4zWF1phSvl4xQ.png)
## Read Path

![](https://miro.medium.com/v2/resize:fit:770/1*RuOecvFfhr7dvSh7vD9gAg.png)
On read path, we asynchronously check the latest version of the index. In the diagram above, we use a S3 file, updated by writer, to indicate the latest version of the an index. When a version change is detected, the reader switches to the new version.

## Write Path

![](https://miro.medium.com/v2/resize:fit:770/1*gw3qLodnW_VwK8wrvAc-1A.png)
On the write path, we read from `meta.json.{read}`before the first write happens. This is because `meta.json.{write}` doesnâ€™t exist yet. However, after the write, reads are routed to `meta.json.{write}` , as we have copied the file on write. Finally, when index update is complete, the writer updates the version file in s3.

With the above mechanism, we are able to guarantee that distributed readers can will only see immutable and complete `meta.json` files.

We will cover challenges with indexing and search latency in the next part of this series.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\sentiment-analysis-using-lancedb-2da3cb1e3fa6.md

---

```md
---
title: "Sentiment Analysis Powered by Semantic Search Using LanceDB"
date: 2023-11-19
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/sentiment-analysis-using-lancedb-2da3cb1e3fa6/preview-image.png
meta_image: /assets/blog/sentiment-analysis-using-lancedb-2da3cb1e3fa6/preview-image.png
description: "**Sentiment Analysis** also known as Opinion Mining is a NLP technique that is used **to analyze texts for polarity, from positive to negative**."
---

**Sentiment Analysis** also known as Opinion Mining is a NLP technique that is used **to analyze texts for polarity, from positive to negative**. It helps businesses to monitor customer feedback, product reputation, and social media sentiment.

***For example***, â€œI love this productâ€ would have a high positive score, while â€œ*This is the worst service ever*â€ would have a* low negative score*. Sentiment analysis can also detect emotions, sarcasm, and context in text, making it more accurate and near to human intelligence.

## Methods of Sentiment Analysis

- **Rule-based:** This method performs sentiment analysis ***based on a set of manually crafted rules***.
- **Automatic:** This method relies on ***ML/NLP techniques to learn from data***.
- **Hybrid:** This approach combines both rule-based and automatic approaches.

In this Blog, Weâ€™ll see an example to *analyze sentiments toward the Hotel Industry and understand Customer Perception and Potential areas that need improvement*. To do this, we will:

1. ***Generate Sentiment labels and scores using BERT models ***based on customer reviews.

2.*** Store them in a LanceDB table with metadata ***and their embeddings of customer reviews.

3.*** Query the LanceDB table on selected areas and understand customer opinions.***

You can check out the attached [Google Colab](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/tutorials/Sentiment-Analysis-Analyse-Hotel-Reviews/Sentiment-Analysis-using-LanceDB.ipynb?source=post_page-----2da3cb1e3fa6--------------------------------) where you can find all codes and easy-to-quick-start steps.
![](https://miro.medium.com/v2/resize:fit:770/1*9DUEPiPEd5sdZ1YKQFiy0g.png)**Flowchart of Implementation**
## Implementation

In the Implementation section, we see the step-by-step implementation of NER on the Medium dataset. We are starting with the first step of ***loading the hotel reviews dataset from huggingface*.**

    from datasets import load_dataset

    # load the dataset and convert to pandas dataframe
    df = load_dataset(
        "ashraq/hotel-reviews",
        split="train"
    ).to_pandas()

Once a dataset is loading, weâ€™ll ***preprocess a loaded dataset, which will take*** only 1000 characters from the reviews.

    # keep only the first 1000 characters from the reviews
    df["review"] = df["review"].str[:1000]
    # glimpse the dataset
    df.head()

These are basic preprocessing steps that I have performed, you can apply them further according to your datasets.

Now, weâ€™ll initialize the Sentiment Analysis Model, weâ€™ll use a ***RoBERTa or DistilBERT model fine-tuned for sentiment analysis ***to analyze the hotel reviews from the HuggingFace model hub

    # set device to GPU if available
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # @title Select Sentiment Analysis Model and run this cell

    model_id = "cardiffnlp/twitter-roberta-base-sentiment" # @param {type:"string"}
    select_model = 'cardiffnlp/twitter-roberta-base-sentiment' # @param ["cardiffnlp/twitter-roberta-base-sentiment", "lxyuan/distilbert-base-multilingual-cased-sentiments-student"]
    model_id = select_model
    print("Selected Model: ", model_id)

This code will create a dropdown in Google Colab from there y***ou can select RoBERT or DistilBERT which sentiment analysis model*** you want to use for creating a sentiment analysis pipeline.

    from transformers import (
        pipeline,
        AutoTokenizer,
        AutoModelForSequenceClassification
        )

    # load the model from huggingface
    model = AutoModelForSequenceClassification.from_pretrained(
        model_id,
        num_labels=3
    )

    # load the tokenizer from huggingface
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # load the tokenizer and model into a sentiment analysis pipeline
    nlp = pipeline(
        "sentiment-analysis",
        model=model,
        tokenizer=tokenizer,
        device=device
        )

The sentiment analysis model returns `LABEL_0` for negative, `LABEL_1` for neutral and `LABEL_2` for positive labels. We can add them to a dictionary to easily access them when showing the results.

    labels = {
            "LABEL_0": "negative",
            "LABEL_1": "neutral",
            "LABEL_2": "positive"
        }

Now weâ€™ll test the sentiment analysis pipeline that we created using the BERT model

    # view review number 241
    test_review = df["review"][241]
    test_review

This will output a review

     Room was small for a superior room and poorly lit especially as
    it was an inside room and overlooked the inside wall of the hotel
    No view therefore needed better lighting within Restaurant tables
    were not well laid and had to go searching for cutlery at breakfast

    # get the sentiment label and score for review number 241
    nlp(test_review)

This is how weâ€™ll get results from the analysis pipeline

    [{'label': 'LABEL_0', 'score': 0.7736574411392212}]

Now, we have created an analysis Pipeline; the Next step is to ***Initialize the Retriever***

A ***Retriever model is used to embed passages and queries, and it creates embeddings such that queries and passages with similar meanings are close in the vector space***.

We will use a sentence-transformer model as our retriever. The model can be loaded as follows:

    from sentence_transformers import SentenceTransformer

    # load the model from huggingface
    retriever = SentenceTransformer(
        'sentence-transformers/all-MiniLM-L6-v2',
        device=device
    )

Now letâ€™s ***generate Embeddings and insert them inside LanceDB for querying***

Connect LanceDB

    import lancedb
    db = lancedb.connect("./.lancedb")

Letâ€™s first write a helper function to generate sentiment labels and scores for a batch of reviews.

    def get_sentiment(reviews):
        # pass the reviews through sentiment analysis pipeline
        sentiments = nlp(reviews)
        # extract only the label and score from the result
        l = [labels[x["label"]] for x in sentiments]
        s = [x["score"] for x in sentiments]
        return l, s

Letâ€™s write another helper function to convert dates to timestamps.

    import dateutil.parser

    # convert date to timestamp
    def get_timestamp(dates):
        timestamps = [dateutil.parser.parse(d).timestamp() for d in dates]
        return timestamps

Now generating embeddings and storing them in the table of LanceDB with their respective metadata

    from tqdm.auto import tqdm

    # we will use batches of 64
    batch_size = 64
    data = []
    for i in tqdm(range(0, len(df), batch_size)):
        # find end of batch
        i_end = min(i+batch_size, len(df))
        # extract batch
        batch = df.iloc[i:i_end]
        # generate embeddings for batch
        emb = retriever.encode(batch["review"].tolist()).tolist()
        # convert review_date to timestamp to enable period filters
        timestamp = get_timestamp(batch["review_date"].tolist())
        batch["timestamp"] = timestamp
        # get sentiment label and score for reviews in the batch
        label, score = get_sentiment(batch["review"].tolist())
        batch["label"] = label
        batch["score"] = score
        # get metadata
        meta = batch.to_dict(orient="records")
        # create unique IDs
        ids = [f"{idx}" for idx in range(i, i_end)]
        # add all to upsert list
        to_insert = list(zip(ids, emb, meta))
        for id, emb, meta in to_insert:
            temp = {}
            temp['vector'] = emb
            for k,v in meta.items():
                temp[k] = v
            data.append(temp)

    # create and insert these records to lancedb table
    tbl = db.create_table("tbl", data, mode= "overwrite")

We have successfully inserted all customer reviews and relevant metadata. We can move on to analyzing sentiments.

Now that we have all the customer reviews inserted, we will search for a few areas that customers usually consider when staying at a hotel and ***analyze the general opinion of the customers***.

The LanceDB vector database makes it very flexible and faster to do this as we can easily search for any topic and get customer reviews relevant to the search query along with sentiment labels as metadata.

Letâ€™s write another helper function to store the count of sentiment labels.

    def count_sentiment(result):
        # store count of sentiment labels
        sentiments = {
            "negative": 0,
            "neutral": 0,
            "positive": 0,
        }
        # iterate through search results
        for r in result:
            # extract the sentiment label and increase its count
            sentiments[r["label"]] += 1
        return sentiments

We will start with a general question about the room sizes of hotels in London and return the top 100 reviews to analyze the overall customer sentiment.

**Analyzing Room Size Reviews in London**

    query = "What is customers opinion about room sizes in London?"
    # generate dense vector embeddings for the query
    xq = retriever.encode(query).tolist()

    # query lancedb
    metadata = ["hotel_name", "label", "review", "review_date", "timestamp"]
    result = tbl.search(xq).select(metadata).limit(100).to_list()
    sentiment = count_sentiment(result)

Plotting the results for better analysis

    import seaborn as sns

    # plot a barchart using seaborn
    sns.barplot(x=list(sentiment.keys()), y = list(sentiment.values()))

The resultant graph looks like this
![](https://miro.medium.com/v2/resize:fit:597/1*GVb41Yp9y7lOemhJtbMtaQ.png)**Room Size Reviews in London**
***The customers are generally satisfied with the room sizes, although many are still neutral and positive*.**

**Analyzing Room Size Reviews in a Specific Period**

    # generate timestamps for start and end time of the period
    start_time = get_timestamp(["2015-01-01"])[0]
    end_time = get_timestamp(["2017-12-31"])[0]

    query = "are the customers satisified with the room sizes of hotels in London?"
    # generate query embeddings
    xq = retriever.encode(query).tolist()

    # query lancedb with query embeddings and appling prefiltering for given time period
    where_filter = f"{start_time} <= timestamp <= {end_time}"
    result = tbl.search(xq).where(where_filter, prefilter=True).limit(10).to_list()

    # get an overall count of customer sentiment
    sentiment = count_sentiment(result)
    # plot a barchart using seaborn
    sns.barplot(x=list(sentiment.keys()), y = list(sentiment.values()))

The Resultant graph shows
![](https://miro.medium.com/v2/resize:fit:587/1*gtosgyDd2neMimWBNi3wZQ.png)**Room Size Reviews in a Specific Period**
***We have a slightly different result now. Almost the same number of customers had either a neutral or positive view of the room size during the selected period.***

Further Basic Analysis is done in the attached Google Colab notebook at the end of this blog, which you can refer to.

**Analyzing Multiple hotels at Multiple Grounds**

Now Letâ€™s multiple hotels together on multiple grounds to better understand the variation in reviews.

We have selected 5 Hotels for them we are analyzing 5 grounds which are Room Size, Cleanliness, Room Service, Food Service, and Ventilation.

    #selected hotels
    hotels =[
        "Strand Palace Hotel",
        "St James Court A Taj Hotel London",
        "Grand Royale London Hyde Park",
        "Hotel Da Vinci",
        "Intercontinental London The O2",
    ]

    queries = {
        "room size": "are customers happy with the given room sizes?",
        "cleanliness": "are customers satisfied with the cleanliness of the rooms?",
        "room service": "did the customers like how they were treated by the staff while room service?",
        "food service": "did the customers enjoy the food and servicing?",
        "ventilation": "customer opinion on the air passage and AC"
    }

Now, We need to iterate through all the hotels and run these queries for each hotel. This would give us customer reviews relevant to the selected hotel areas. After that, we count the sentiment labels and plot results for each hotel.

    import matplotlib.pyplot as plt
    import pandas as pd

    hotel_sentiments = []

    # iterate through the hotels
    for hotel in hotels:
        final_result = []

        # iterate through the keys and values in the queries dict
        for area, query in queries.items():
            # generate query embeddings
            xq = retriever.encode(query).tolist()

            # query lancedb with query embeddings and appling prefiltering for hotel names
            where_filter = f"hotel_name= '{hotel}'"
            result = tbl.search(xq).where(where_filter, prefilter=True).limit(100).to_list()

            # get an overall count of customer sentiment
            sentiment = count_sentiment(result)
            # sort the sentiment to show area and each value side by side
            for k, v in sentiment.items():
                analysis_data = {
                    "area": area,
                    "label": k,
                    "value": v
                }
                # add the data to final_result list
                final_result.append(analysis_data)

        #dataframe of final results
        df = pd.DataFrame(final_result)
        # mapping dataframe records with hotel
        hotel_sentiments.append({"hotel": hotel, "df": df})

We may now plot the final data to make inferences.

    # create the figure and axes to plot barchart for all hotels
    fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(25, 4.5))
    plt.subplots_adjust(hspace=0.25)

    counter = 0
    # iterate through each hotel in the list and plot a barchart
    for d, ax in zip(hotel_sentiments, axs.ravel()):
        # plot barchart for each hotel
        sns.barplot(x="label", y="value", hue="area", data=d["df"], ax=ax, palette="Spectral")
        # display the hotel names
        ax.set_title(d["hotel"])
        # remove x labels
        ax.set_xlabel("")
        # remove legend from all charts except for the first one
        counter += 1
        if counter != 1: ax.get_legend().remove()
    # display the full figure
    plt.show()

This will plot results for all 5 hotels on all 5 grounds from which weâ€™ll analyze it.
![](https://miro.medium.com/v2/resize:fit:770/1*DKa8gF4gMj3SYS-EBE1PGQ.png)**Analyzing Multiple hotels at Multiple Grounds**
This can be better seen on Google Colab, which is attached at the end of this blog.

The following observations can be made for the hotels based on the sentiment analysis:

1. **Strand Palace Hotel:** The majority of customers were pleased with the cleanliness, room service, and food service of the rooms, while a considerable number of them were not very satisfied with the room sizes and the ventilation in rooms.
2. **St James Court A Taj Hotel London:** The majority of the customers were really happy with the selected five areas
3. **Grand Royale London Hyde Park**: The majority of the customers were not satisfied with the room size, while a good number of them were pretty satisfied with the cleanliness, room service, food service, and ventilation of the rooms.
4. **Hotel Da Vinci**: The majority of customers are happy with room sizes, cleanliness, food service, and ventilation but a considerable about of them are not satisfied with room service
5. **Intercontinental London The O2**: the majority of the customers were really happy with the selected five areas, making this hotel the best among the selected hotels.

Although we have experimented with a few selected areas and hotels, you can get creative with your queries and get the sentiment around your area of interest with different sets of hotels immediately.

For quick and further analysis, You can try out [Google Colab](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/tutorials/Sentiment-Analysis-Analyse-Hotel-Reviews/Sentiment-Analysis-using-LanceDB.ipynb?source=post_page-----2da3cb1e3fa6--------------------------------), which has all these codes

Some of the benefits of Hotel Review Analysis are:

- Monitor and respond to customer feedback and address any issues or complaints that may arise.
- Measure and improve the quality of their Services, Facilities, Amenities, and Staff, and Identify the areas that need more attention or improvement.
- Understand the needs and expectations of their Target market, and tailor their offerings and marketing strategies accordingly.

## Conclusion

Sentiment Analysis is primarily using NLP and Deep Learning techniques. ***BERT is used in the Sentiment Analysis Pipeline ***and predicts Sentimens for reviews.

Using these two technologies, the model detects a sentiment of reviews. Harnessing the advanced vector search functionalities provided by LanceDB enhances its overall utility in analyzing the sentiments of reviews of different hotels on different grounds.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\simplest-method-to-improve-rag-pipeline-re-ranking-cf6eaec6d544.md

---

```md
---
title: "Simplest Method to Improve RAG Pipeline: Re-Ranking"
date: 2023-11-21
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/simplest-method-to-improve-rag-pipeline-re-ranking-cf6eaec6d544/preview-image.png
meta_image: /assets/blog/simplest-method-to-improve-rag-pipeline-re-ranking-cf6eaec6d544/preview-image.png
description: "Unlock about simplest method to improve rag pipeline: re-ranking. Get practical steps, examples, and best practices you can use now."
---

## Problem Statement:

In a typical RAG pipeline, LLM Context window is limited so for a hypothetical 10000 pages document, we need to chunk the document. For any incoming user query, we need to fetch `Top-N` related chunks and because neither our Embedding are 100% accurate nor search algo is perfect, it could give us unrelated results too. This is a flaw in RAG pipeline. How can you deal with it? If you fetch Top-1 and the context is different then itâ€™s a sure bad answer. On the other hand, if you fetch more chunks and pass to LLM, itâ€™ll get confused and with higher number, itâ€™ll go out of context.

## Whatâ€™s the remedy?

Out of all the methods available, Re-ranking is the simplest. Idea is pretty simple.

1. You assume that Embedding + Search algo are not 100% precise so you use Recall to your advantage and get similar high `N` (say 25) number of related chunks from corpus.

2. Second step is to use a powerful model to increase the Precision. You re-rank above `N` queries again so that you can change the relative ordering and now select Top `K` queries (say 3) to pass as a context where `K` < `N` thus increasing the Precision.

## Why canâ€™t you use the bigger model in the first place?

Would your search results be better if you were searching in 100 vs 100000 documents? Yes, so no matter how big of a model you use, youâ€™ll always have some irrelevent results because of the huge domain.

Smaller model with efficient searching algo does the work of searching in a bigger domain to get more number of elements while the larger model is precise and because it just works on `K`, there is a bit more overhead but improved relevancy.
![](https://miro.medium.com/v2/resize:fit:369/1*GHOR7e1JM0GkUBRdGGr9cA.jpeg)All Fluff No Code
Follow along with this [colab](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/RAG_re_ranking/main.ipynb?source=post_page-----cf6eaec6d544--------------------------------)

    !pip install -U lancedb transformers datasets FlagEmbedding unstructured -qq

    # NOTE: If there is an import error, restart and run the notebook again

    from FlagEmbedding import LLMEmbedder, FlagReranker
    # Al document present here https://github.com/FlagOpen/FlagEmbedding/tree/master
    import os
    import lancedb
    import re
    import pandas as pd
    import random

    from datasets import load_dataset

    import torch
    import gc

    import lance
    from lancedb.embeddings import with_embeddings

    task = "qa" # Encode for a specific task (qa, icl, chat, lrlm, tool, convsearch)
    embed_model = LLMEmbedder('BAAI/llm-embedder', use_fp16=False) # Load model (automatically use GPUs)

    reranker_model = FlagReranker('BAAI/bge-reranker-base', use_fp16=True) # use_fp16 speeds up computation with a slight performance degradation

Load [BeIR Dataset](https://huggingface.co/datasets/BeIR/scidocs). This is a dataset built specially for retrieval tasks to see how good your search is working

    queries = load_dataset("BeIR/scidocs", "queries")["queries"].to_pandas()
    docs = load_dataset('BeIR/scidocs', 'corpus')["corpus"].to_pandas().dropna(subset = "text").sample(10000) # just random samples for faster embed demo
    docs.sample(3)

Get embedding using [LLM embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder) and create Database using [***LanceDB***](https://github.com/lancedb/lancedb)

    def embed_documents(batch):
        """
        Function to embed the whole text data
        """
        return embed_model.encode_keys(batch, task=task) # Encode data or 'keys'

    db = lancedb.connect("./db") # Connect Local DB
    if "doc_embed" in db.table_names():
      table = db.open_table("doc_embed") # Open Table
    else:
      # Use the train text chunk data to save embed in the DB
      data = with_embeddings(embed_documents, docs, column = 'text',show_progress = True, batch_size = 128)
      table = db.create_table("doc_embed", data=data) # create Table

Search from a random Text

    def search(query, top_k = 10):
      """
      Search a query from the table
      """
      query_vector = embed_model.encode_queries(query, task=task) # Encode the QUERY (it is done differently than the 'key')
      search_results = table.search(query_vector).limit(top_k)
      return search_results

    query = random.choice(queries["text"])
    print("QUERY:-> ", query)

    # get top_k search results
    search_results = search("what is mitochondria?", top_k = 10).to_pandas().dropna(subset = "text").reset_index(drop = True)

    search_results

Re-rank Search Results using Re-ranker from [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)

Pass all the results to a stronger model to give them the similarity ranking

    def rerank(query, search_results):
      search_results["old_similarity_rank"] = search_results.index+1 # Old ranks

      torch.cuda.empty_cache()
      gc.collect()

      search_results["new_scores"] = reranker_model.compute_score([[query,chunk] for chunk in search_results["text"]]) # Re compute ranks
      return search_results.sort_values(by = "new_scores", ascending = False).reset_index(drop = True)

    print("QUERY:-> ", query)

    rerank(query, search_results)

Visit the[ LanceDB (YC W22) ](https://github.com/lancedb/lancedb)repo to learn more about lanceDB python and Typescript library

To discover more applied GenAI and vectorDB applications, examples, & tutorials, visit[ vectordb-recipes](https://github.com/lancedb/vectordb-recipes)

Adios Amigos! Until next time â€¦â€¦.
![](https://miro.medium.com/v2/resize:fit:548/1*S6wSYo7DdFRWF0ZKvqn4jg.gif)Adios Amigos
```

---

## ðŸ“„ æ–‡ä»¶: drafts\splitting-scheduling-from-decoding.md

---

```md
---
title: "Columnar File Readers in Depth: Scheduling vs Decoding"
date: 2024-05-22
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/splitting-scheduling-from-decoding/preview-image.png
meta_image: /assets/blog/splitting-scheduling-from-decoding/preview-image.png
description: "We've been working on readers / writers for our recently announced and are posting in-depth articles about writing a high performance file reader.  In the first article I talked about how we obtain ."
---

We've been working on readers / writers for our recently announced [Lance v2 file format](__GHOST_URL__/lance-v2/) and are posting in-depth articles about writing a high performance file reader.  In the first article I talked about how we obtain [parallelism without row groups](__GHOST_URL__/file-readers-in-depth-parallelism-without-row-groups/).  Today, I want to explain how, and why, we separate scheduling from decoding.

## What are Scheduling & Decoding?

Scheduling is the act of figuring out what I/O requests need to be performed to read the requested ranges (potentially with a filter).  Decoding is the act of taking the loaded I/O and converting it into the desired encoding.  Both tasks are synchronous CPU tasks however there is I/O that happens in between them.  For example, if we were asked to load a range of rows from a flat encoded array we might have something that looks like this:

    async def load_float_range(f: BinaryIO, row_start: int, row_end: int):
      # Scheduling
      bytes_start = row_start * 4
      num_bytes = (row_end - row_start) * 4
      # I/O (we are pretending we have asynchronous python I/O here)
      await f.seek(bytes_start)
      bytes = await f.read(num_bytes);
      # Decoding
      return [                             \
        struct.unpack('f', bytes[i:i+4])   \
          for i in range(0, len(bytes), 4) \
      ]
    }

Scheduling for a flat encoding is very simple.  We multiply the row range by the # of bytes per value to get the byte range.  Decoding is also pretty simple.  We need to reinterpret the bytes as the desired data type.  Both steps can get much more complicated.

## Separating the Steps

When we talk about separating these two steps we mean that we are putting each responsibility into its own interface (trait, abstract base class, etc.)  In other words, instead of a single interface like this:

    class Decoder(ABC):
      def decode(f: BinaryIO, row_start: int, row_end: int) -> pa.Array:
        pass

We have two interfaces:

    class Scheduler(ABC):
      def schedule(row_start: int, row_end: int, io_reqs, decoders) -> None:
        pass

    class Decoder(ABC):
      def decode(data: bytes) -> pa.Array:
        pass

ðŸ’¡

`io_reqs` and `decoders` aren't defined. These are channels for threaded communication. More details below. Also, these interfaces are a bit simpler than what we actually have in practice (e.g. we've omitted filters).

## Why Separate?

Now that we understand what this separation is, the next obvious question is why we would want to do this.  There are a few different reasons:

### Performance

In theory, it is possible to achieve good performance without splitting these two steps, but in practice I believe this to be incredibly hard to achieve.  For example, consider the variable length list encoding.  This typically leads to a situation (described further below) called "indirect I/O" where some scheduling has to happen after some decoding.  In most readers I've seen, this is tackled by pausing the scheduling process and this leads to gaps where I/O could be performed but isn't (something that should be avoided at all costs in a reader).  In fact, this is generally inevitable in combined readers, though it is rather tricky to explain and involves the way we apply back pressure (read ahead limits must be tied to decoding and I/O but shouldn't be tied to scheduling).
![](__GHOST_URL__/content/images/2024/05/Split-Scheduler-Back-Pressure-1-.png)Splitting Scheduling and Decoding Leads to Better I/O Control
A second factor in performance, which will be discussed in much more depth in a future post, is thread locality.  By moving the decoding steps we make it possible to do decode-based parallelism.  If we combine decode-based parallelism with our query engine we can "fuse" the decode with our query pipeline.  This prevents a copy of our data to/from main memory by preserving CPU cache locality.

### Extensibility and Decoupling

This next argument is about design.  Breaking up a complex task into two composable tasks generally reduces the complexity (provided each task stands on its own).  A good example of this is the list decoder which has very complex scheduling logic and non-trivial decoding logic as well.

We've also found that it makes categorization of encodings more sophisticated.  For example, pushdown statistics are typically viewed by a file reader as a specialized structure.  Under the lens of scheduling and decoding we can view pushdown statistics as just another encoding, one that is purely a scheduler and has no decoding component.
![](__GHOST_URL__/content/images/2024/05/Pushdown-as-a-scheduler-1-.png)Pushdown projection refines the range that we are scheduling
### Schedule-Only Tasks (e.g. cost / cardinality estimation)

Finally, by breaking scheduling out into its own phase, we can choose to run just the scheduling phase.  This is important for tasks like cost estimation.  By running the scheduling phase by itself and accumulating the I/O requests we can quickly and cheaply determine how much I/O a given request might take.  Note: we can also calculate other statistics such as expected cardinality (e.g. using a histogram) even if they don't affect the I/O.

## Implementation

Hopefully we've convinced you that separating these two concepts is important.  Implementation of such a reader is tricky but not impossible.  To do this in Lance we separate the work into three different loops, the scheduler loop, the decoding loop, and the I/O loop.  These pieces run in parallel and communicate with each other as the file is read.
![](__GHOST_URL__/content/images/2024/05/Scheduler-Decoder-Threads-1-.png)The three loops involved in reading a file
The **scheduler loop **walks through the file metadata and, for each page included in the requested range, issues I/O requests and then describes the range that was scheduled to the decoder loop.  The scheduler loop is run today on a single thread task launched for the purpose.  The scheduler loop is a synchronous task, quite short (well under 10% of the total runtime in large files, even with fast I/O) and never blocks.

The **I/O scheduler** receives I/O requests from the scheduler and throttles them to achieve a configurable I/O depth.  The I/O scheduler is a single thread task that blocks frequently, whenever the I/O depth has been reached, or when it is waiting for input from the scheduler.

The **decoder loop** first waits for scheduled ranges, and then waits for enough I/O to complete in order to decode a batch.  There is no dedicated task for the decoder loop.  Instead, it is run by caller threads.  Each call to decode another batch becomes a thread task of sorts.  That will be described in a dedicated blog post soon.

## On No!  Indirect I/O!

I mentioned earlier that scheduling lists was quite complicated, since some of the scheduling work needs to be done after decoding.  This is a problem for our design since our scheduler loop never blocks.  To give an example, consider we are asked to read row 17 from an array of type `list<int32>`.  First, we must read how long list 17 is.  We schedule an I/O request for the byte range containing the list length.  Then (assuming list 17 isn't empty) we must read the actual list data.  We don't know the byte range containing the actual data (it depends on the length).

We call this problem "indirect I/O" and our solution is to launch a temporary, nested, scheduler loop.  This scheduler loop is launched immediately and waits for the offsets.  Once the offsets arrive it decodes the offsets and runs scheduling for the list data.  This needs to be a "full scheduler loop" because we can have quite a complicated situation like `list<struct<list<struct<...>>>`.  In fact, each layer of `list` adds another layer of indirect I/O and another scheduler loop.
![](__GHOST_URL__/content/images/2024/05/Indirect-Scheduling.png)
This isn't great for performance but it is inevitable with the typical encoding used for variable length lists.  Most existing readers that I've seen either don't address this (because they don't support reading ranges and thus force a complete page load of the list and all its list items) or do this by blocking the scheduler (note that an async / await is still "blocking the scheduler" if you have a batch read ahead limit).

There are also ways to avoid this indirect I/O.  The most common is when you are doing full scans and you are asking for a complete page of list offsets.  In this case the indirect I/O can be skipped (we know we need **all** of the list items referenced by the page).  Another potential shortcut we would like to investigate is to use "skip tables" for list lengths.  These tables would be optional column metadata which, if present, can be used to quickly determine a rough set of item ranges needed, without doing any I/O.  However, it is important for us that our reader be able to handle the indirect case well.  Also note that future encodings such as run-end encoding may also need indirect I/O.

### I/O Priority

Since we do not block the scheduler while waiting for list offset data it means that the scheduler continues and submits more I/O requests for later and later portions of data.  This causes an I/O priority problem.  Once we have the list offsets loaded we will want to run the indirect scheduler loop.  It will also submit I/O requests.  We don't want to place these requests at the end of the queue.

To solve this our I/O loop actually has a priority queue that is used once we've hit the max I/O depth.  The logic for this queue is fairly simple.  Every I/O request is associated with some top-level row number.  The lower the row number the higher the priority of the I/O.
![](__GHOST_URL__/content/images/2024/05/I-O-Priority.png)Prioritizing I/O in this fashion ensures our decoder waits as little as possible
## Results

The end result of all of this is that we can read files quickly and with *very consistent* I/O scheduling.  For an example we have done some profiling of a full read of one file of the [NYC taxi data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).  In parquet this file is about ~420MB.  In Lance this file occupies ~2.3GB because we don't yet have any compressive encodings.  Even though the data is 5 times larger, we've found that Lance only lags behind pyarrow's parquet reader by about 2x (and uses about 75% as many CPU operations because it isn't dealing with compression) because Lance manages to achieve much better parallelism.  The gap is quickly closing ðŸŽï¸.

To show the effects of indirect I/O and priority we profiled a version of Lance that doesn't use the "avoid indirect I/O on full page load" shortcut.  We also have traces both with and without prioritized I/O.
![](__GHOST_URL__/content/images/2024/05/image-3.png)Without prioritized I/O the decode is delayed
In both traces the pink tasks represent our I/O and there are very few gaps in the I/O.  The brown represents the decode tasks.  Without prioritized I/O the I/O and decode don't overlap much because the decode is waiting on I/O that gets put at the end of the queue.
![](__GHOST_URL__/content/images/2024/05/image-4.png)With prioritized I/O the delay is much smaller
With prioritized I/O we can clearly see the delay is much smaller and the decode is able to run in parallel with the I/O.  At the same time we are still able to feed the I/O scheduler.

Finally, note that metadata loading and scheduling are **very fast**.  This is partly because we are doing a full scan (instead of a point lookup) but I was still not expecting it to be this fast.  We have to really zoom in to see these tasks:
![](__GHOST_URL__/content/images/2024/05/image-5.png)If we zoom in enough we can see the main scheduler loop (we don't yet trace indirect scheduler tasks)
In both cold (no data in page cache) and warm reads the metadata loading, metadata decoding, and scheduling take less than 0.5% of the total read time.  This is primarily because lance has very large pages and fairly sparse metadata.  However, this can still be a significant amount of time for point lookups as we will see in future blog posts.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\streamlining-our-sdks.md

---

```md
---
title: "Streamlining Our SDKs"
date: 2024-07-16
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/streamlining-our-sdks/preview-image.png
meta_image: /assets/blog/streamlining-our-sdks/preview-image.png
description: "In the fast-evolving landscape of software development, maintaining multiple SDKs in various languages can be a challenging endeavor, especially for a small startup!  Each bespoke implementation comes with its own set of."
---

In the fast-evolving landscape of software development, maintaining multiple SDKs in various languages can be a challenging endeavor, especially for a small startup! Each bespoke implementation comes with its own set of intricacies, making updates and bug fixes a time-consuming process. To address these challenges, we've embarked on a journey to rewrite our SDKs in Python and JavaScript as thin wrappers around our Rust SDK.

## Why Refactor?

Over time, our SDKs have grown in complexity, with each language implementation diverging from the others. This has made it difficult to maintain feature parity across all SDKs. By rewriting our SDKs to be thin wrappers over our Rust SDK, we can consolidate our codebase and ensure that all SDKs are in sync with the latest features and bug fixes.

Moreover, this refactor significantly simplifies the process of adding support for new languages. As a highlight, we are excited to announce that we are actively working on new Java bindings. This would not have been practical with the previous architecture, but now it's within reach, thanks to the unified Rust core.

While refactoring may seem like a technical detail, it has significant implications for our users and the future of our SDKs. Unifying our SDKs around a core Rust implementation allows us to streamline current operations and ensure consistent feature development across all supported languages.

One of the key features we can now implement is full-text search, which has been limited to Python only. Centralizing our code in Rust makes implementing and optimizing full-text search much easier. We are moving this functionality into the Rust core, so it can be exposed across all clients.

In essence, this strategic shift is not just about simplifying our code base today; it's about unlocking new possibilities and delivering more powerful features in the future.

## What impact does this have on existing users?

### TypeScript Users

Existing users of the npm package `vectordb` will need to migrate to the new `@lancedb/lancedb`*(npm)* package. We've made efforts to keep the public APIs as similar as possible to minimize friction during this transition. While the old SDK will continue to receive bug fixes and support for now, it will eventually be phased out as we focus our efforts on the new one.

### Python Users

Python users will not see much of a public change as we are continuing to use the `lancedb` PyPI package. However, there will be some internal refactors. The `sync` API is currently built using our old architecture. Most of our efforts going forward will bring the `async` API to feature parity with the `sync` API. Eventually, the `sync` API will become a wrapper over the `async` API. At that point, Python users will be free to choose the `sync` or `async` API without having to worry about differences in feature support.

### Rust Users

Rust users should use the `lancedb` crate. The previous `vectordb` crate was removed from support in February 2024.

## Need Help?

If you have any issues or questions during the migration, please reach out to us on [Discord](https://discord.com/invite/zMM32dvNtd) or [GitHub](https://github.com/lancedb). We're here to help and ensure a smooth transition.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\substrait-powered-filter-pushdown-in-lance-3d204355365c.md

---

```md
---
title: "Substrait Powered Filter Pushdown in Lance"
date: 2024-01-31
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/substrait-powered-filter-pushdown-in-lance-3d204355365c/preview-image.png
meta_image: /assets/blog/substrait-powered-filter-pushdown-in-lance-3d204355365c/preview-image.png
description: "Build about substrait powered filter pushdown in lance. Get practical steps, examples, and best practices you can use now."
---

by Weston Pace

Filter pushdown is one of the more fundamental optimizations in any data engineering pipeline. The premise is simple: the earlier you filter data in your pipeline the less work you have to do overall.
![A diagram showing how filters are best applied before loading from I/O or sending across a network.](https://miro.medium.com/v2/resize:fit:770/1*xR-_L5RYsY-Px_ONFditXQ.png)Filtering earlier in the pipeline can avoid disk or network I/O
Unfortunately, as data solutions become more decentralized, this optimization has become more difficult because it requires many different components to agree on what a â€œfilter expressionâ€ is. Here at LanceDb we want Lance to serve as an efficient data source in as many situations as possible. This means weâ€™ve run head first into this problem.
![A diagram showing how user input (in polars or duckdb format) gets transformed before reach datafusion (via lance) and datafusion is unable to recognize it.](https://miro.medium.com/v2/resize:fit:770/1*K0ocmQTdSz99VGIYqfu6Mg.png)Using Lance via DuckDb or Polars (via Pyarrow compute) can make pushdown difficult
Between Datafusion, Lance, DuckDb, Pyarrow, and Polars we have four different representations of filters (not five, since Lance uses Datafusion internally)! We talked about this problem briefly in the past when discussing our Polars integration. Weâ€™ve also been maintaining a lot of custom code to handle this scenario in our DuckDb integration. If our solution to this problem is to simply add conversion methods between the different libraries then weâ€™re going to need O(nÂ²) different converters and weâ€™re just getting started!

## Substrait to the Rescue

Fortunately, using [Substrait](https://substrait.io/), weâ€™ve managed to find a more maintainable solution. Substrait is an open standard for query plans and, since expressions are a part of query plans, it gives us an open standard for compute expressions too. Support for Substrait has been growing over past year and both pyarrow and Datafusion now support converting expressions to and from Substrait. In a recent release of Lance we added support for accepting filter expressions as Substrait and itâ€™s now used internally to support our pushdown from pyarrow, duckdb, and polars, which allows us to retire our custom conversion logic.
![If each system can consume/produce Substrait then we donâ€™t need all NÂ² conversion between systems.](https://miro.medium.com/v2/resize:fit:770/1*HpT662Epn8FQ2K1VSzixXA.png)Substrait can server as a common interchange between the different libraries
## Whatâ€™s Next?

By simplifying our code this feature is a win on its own. However, we believe the future of Substrait is bright and its potential has just begun to show. Let me describe a few of the potential developments that weâ€™ve got our eye on.
![](https://miro.medium.com/v2/resize:fit:770/1*ldcaGpLJuG9qbsSICbe7sw.png)With Substrait we can simplify Lance integration
## More Complex Pushdown

Filter pushdown has been historically limited to some very basic compute operations such as <, â‰¤, >, â‰¥, ==, â€¦ Thereâ€™s even a special word, â€œsargableâ€, used to refer to these expressions. This limitation originates from the fact that these are the only expressions that are easily pushed down into classic database indices (e.g. btrees).

Unfortunately, this limitation is becoming too restrictive. Pushdown needs to be able to cross many different boundaries, and the underlying systems can sometimes support much richer and more complex functions. For example, [Skyhook](https://arrow.apache.org/blog/2022/01/31/skyhook-bringing-computation-to-storage-with-apache-arrow/) is a compelling demonstration where arrow-cpp (with all arrow compute functions) is installed in the storage nodes of a Ceph cluster. Imagine a cloud storage API like [S3-select](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html) except it can apply rich complex filters before sending Arrow data outside the cloud.

## Alignment on Function Behavior

Substrait integrations have so far been focused on properly mapping â€œfunction namesâ€. For example, as shown above, Pyarrow has a scalar function â€œinvertâ€ which, in Datafusion, is not a scalar function but actually a syntax node â€œnotâ€. In Substrait these are both a scalar function named â€œnotâ€ and so, by using Substrait, the two libraries can find common ground. This is kind of mapping is required but also fairly basic.

A more complicated challenge is aligning function behavior between different libraries, especially when it comes to corner cases. For example, should integer division by zero yield NULL or raise an error? What should happen if there is an arithmetic overflow with unsigned integers? How should negative numbers be handled in the modulus operation? In Substrait, we know that forcing libraries to adopt one single behavior isnâ€™t going to work.
![https://xkcd.com/927/](https://miro.medium.com/v2/resize:fit:770/1*9mYmSFArZPnDooxgK8Y6KQ.png)[https://xkcd.com/927/](https://xkcd.com/927)
Instead, the preferred approach is to list all of the different behaviors, typically using â€œfunction optionsâ€. Adoption of these options has been slow so far. Most Substrait consumers simply ignore them. However, efforts such as the [BFT](https://voltrondata.github.io/bft/index.html) (which aims to document all these subtle differences) should at least make it possible for users to know when (and how) a query will behave differently as the backend changes and this can eventually drive adoption of the feature in plan consumers.

## â€œPlan Pushdownâ€, Going Beyond Filters

Earlier we pointed out that data endpoints can handle complex expressions. It turns out that some, such as Lance, can handle pushing down even more complicated plan optimizations that donâ€™t fit the template of â€œfilter pushdownâ€. One of Lanceâ€™s most powerful features is its secondary vector indices. Applying a vector index is more than a simple filter pushdown.
![A diagram showing how a query plan with 4 nodes (scan/project/sort/limit) can be simplified into a query plan with 2 nodes (index search/take)](https://miro.medium.com/v2/resize:fit:770/1*lOvjAeLN6JUdTnhM8lMVLQ.png)One Example of a Vector Index Optimization
How should DuckDb or Polars push something like this down? Currently, the Lance integration with these libraries is limited to pushing down filters. If you want to perform vector searches then you need to use Lance directly (for now). We could invent a new concept for â€œvector search pushdownâ€ or, using Substrait, we could simply pass the entire query plan down to the data source and let it optimize the query however it sees fit.

Admittedly, the ecosystem isnâ€™t quite ready for this feature yet. Consumers would have to considerable investment to involve something like this in their planning process. However, it is an exciting look at the kinds of things that could become possible as Substrait adoption grows and itâ€™s one of the reasons that Lance is excited to add these Substrait integrations whenever possible.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\using-column-statistics-to-make-lance-scans-30x-faster-86402ca541ee.md

---

```md
---
title: "Using Column Statistics to Make Lance Scans 30x Faster"
date: 2023-12-20
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/using-column-statistics-to-make-lance-scans-30x-faster-86402ca541ee/preview-image.png
meta_image: /assets/blog/using-column-statistics-to-make-lance-scans-30x-faster-86402ca541ee/preview-image.png
description: "Unlock about using column statistics to make lance scans 30x faster. Get practical steps, examples, and best practices you can use now."
---

In Lance v0.8.21, we introduced column statistics and statistics-based page pruning. This enhancement reduces the number of IO calls needed for scanning with a filter, making scans 30x faster in some cases. This is a common optimization in analytics formats like Parquet. However, combined with Lanceâ€™s superior handling of vector and unstructured data, we get 3x faster scans with predicates for vector data versus Parquet.

Today these statistics are used when scanning datasets or deleting data based on a predicate. This is the first of several improvements weâ€™ll be making to filtering, so expect to see similar improvements for vector search pre-filtering.

## Performance improvements for scans

How much does this feature improve performance? As a simple example, we created a test dataset with 1 million rows. The data had the schema:

- **id**: an int64 incrementing id
- **score**: a random float32
- **category**: a string column with random letter of the alphabet
- **embedding**: a 1536-dimensional vector

To maximize the impact of statistics-based pruning, we ran the generated dataset through Delta Lakeâ€™s Z-order to cluster on the `score` and `category` columns. We wrote this out as a Parquet dataset (using PyArrow 14.0.1) and Lance dataset. On disk Parquet took 7.1 GB and Lance took 6.5 GB.

We ran two queries with the same filter but different projections. One we call â€œscalarâ€ as it projects `id` and `score`, two analytical columns. Another we call â€œvectorâ€, as it projects `id` and `embedding`, the latter being a vector column. The filter predicate in both cases is `score > 0.8 AND category IN (â€˜Aâ€™, â€˜B', â€˜C')`.
![A bar chart compares the query time to scan data in Lance versus Parquet. This shows two situations: scalar and vector projection. In the scalar case, Lance shows 14 milliseconds while Parquet shows 12 milliseconds. In the vector case, Lance shows 61 milliseconds while Parquet shows 181 milliseconds.](https://miro.medium.com/v2/resize:fit:626/1*7tXr8Z4-O2Xj0i4Nrk69Dw.png)Query time measured for Lance versus Parquet (best of 10). The scalar query was â€œSELECT id, score FROM data WHERE score > 0.8 AND category IN (â€˜Aâ€™, â€˜Bâ€™, â€˜Câ€™)â€ while the vector query was â€œSELECT id, embedding FROM data WHERE score > 0.8 AND category IN (â€˜Aâ€™, â€˜Bâ€™, â€˜Câ€™)â€. Lance is much faster in the vector case because it uses more efficient vector representation and late materialization for vector columns. Lance was only 2 milliseconds slower in the scalar case.
When filtering on `score` and `category` and projecting the `id` and `embedding` columns, Lance achieved a significant performance improvement. It was 30 times faster than before and surpassed the performance of Parquet scan by a factor of 2.8. This improvement is attributed to the new scanner that reduced the amount of data that needed to be loaded. In earlier versions of Lance, running this query required 10,248 IO calls, but with stats-based pruning, only 538 IO calls were needed. Thatâ€™s a 94% reduction in IO!
![](https://miro.medium.com/v2/resize:fit:770/1*M0NV2WemCVfpZyEDdPeaQA.png)
As expected, Lance outperforms Parquet when selecting vector columns. However, what about scalar columns? We also executed the same query, but only projected `id` and `score`. In this case, Lance's query time improved by 3x compared to previous versions, and it was only about 3 ms slower than Parquet. While Parquet excels in analytics data, we are confident that we can make additional optimizations to match or even surpass Parquet's performance.

## How we prune pages

Lance uses an expression simplification approach to statistics based page pruning. This is different from other systems which can only prune entire sets of rows. There are two additional things we can do:

1. Eliminate reading some filter columns, if they are no longer needed
2. Create residual expressions, which is the remaining predicate that must be evaluated given what we know in the statistics.

To see how this works, we can image an example. Say we have two columns: `score`, a float column, and `category`, a string column. For simplicity, letâ€™s say these are both not nullable. We want to filter for where `score > 0.5 AND category IN ('A', 'B', 'C')`. Data is organized into groups of rows, where each group has 1 page for each of the columns. As we are scanning, we have statistics about `min` and `max` bounds for both `score` and `category`.

In the first page, `max(score) = 0.4` we know for sure that this group cannot satisfy the `score > 0.5`. Anything ANDâ€™ed with `false` is going to be `false`, so this filter isnâ€™t satisfiable by this entire group. We can skip it.

In the second page, `min(score) = 0.6` so we know for certain all rows will satisfy the `score > 0.5`. Any expression ANDâ€™ed with `true` is the same as the original expression, so we can simplify the predicate to `category IN ('A', 'B', 'C')`. This predicate doesnâ€™t involve the `score` column, so we can skip reading that column and just read `category` in order to evaluate this expression.

In the third page, `min(category) = 'D'`, so we know `category IN (...)` is always `false`. Similar to the first page, this eliminates the entire group.

In the fourth and final page, `min(category) = 'C'`. This doesnâ€™t eliminate the `category` filter, but it does allow us to simplify it to the residual expression `category = 'C'`, since `'A'` and `'B'` are no longer possibilities in this page.
![](https://miro.medium.com/v2/resize:fit:770/1*j_KLVrtfn0CJh0kBTn2P1Q.png)
By performing these simplifications, we only needed to read 3 of the 8 pages of data. Additionally, in one of the cases we were able to simplify the filter to an equality check rather than set containment check, which is generally cheaper.

## Implementing with DataFusion

Lanceâ€™s scanner uses DataFusion, a toolbox for creating query engines. We utilize this library for SQL parsing, query planning, expression optimization, and execution. Implementing this feature required two steps:

1. Rewriting expressions based on statistics. For example, if the earlier expression was `score > 0.5 AND category IN ('A', 'B', 'C')` and the statistic was `max(score) = 0.4`, it would be rewritten as `false AND category IN ('A', 'B', 'C')`.
2. Simplifying expressions. In this case, `false AND category IN ('A', 'B', 'C')` would be simplified to `false`. These simplifications are based on the expression itself and general rules.

We contributed the first step to DataFusion ([PR](https://github.com/apache/arrow-datafusion/pull/7467)). Right now this handles inequalities, null checks (`IS [NOT] NULL`), and `IS IN`, but could be extended to support more expression types in the future. The second step already had a high-quality implementation from other members of the DataFusion community.

Overall, our experience with DataFusion has been great. Many of the advanced features we look for have already been implemented, and those that we needed to implement got quick feedback and approval.

## Whatâ€™s next

These statistics are only useful to the extent the rows in your dataset are clustered for the columns are you are filtering on. Some tables will have a natural clustering based on insertion order. For example, if there is a insertion timestamp column or an incrementing id, those will already be ordered well and filtering on them will be very effective. In other cases, your data may need to be rearranged to be optimally clustered. We already have a compaction operation that requires moving your data, so during that phase we could optionally cluster data along columns of your choice. This is the first enhancement that is on our roadmap.

In addition, most table formats have some statistics at the file level in addition to the page-level. This allows skipping entire files that donâ€™t have relevant data. This is especially helpful for cold queries where the statistics havenâ€™t already been cached. This feature is also on our roadmap.

Finally, there are several other query types where we can integrate statistics. For example, they can be used during the pre-filtering stage of vector search to improve the performance of ANN queries that have metadata filters.

Statistics unlocks a variety of scenarios where performance can be significantly improved. All of these improvements are on our roadmap and will help make Lance the best format for AI data lakes.
```

---

## ðŸ“„ æ–‡ä»¶: drafts\using-lancedb-with-azure-blob-everything-you-need-to-know-to-start-building-2.md

---

```md
---
title: "Speaker Identification and Transcription Mapping Using Vector Search"
date: 2025-02-26
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/using-lancedb-with-azure-blob-everything-you-need-to-know-to-start-building-2/preview-image.png
meta_image: /assets/blog/using-lancedb-with-azure-blob-everything-you-need-to-know-to-start-building-2/preview-image.png
description: "Master about speaker identification and transcription mapping using vector search. Get practical steps, examples, and best practices you can use now."
---

ðŸ’¡

This is community post by Shresth Shukla

I'm sure you all have used LanceDB on Colab a lot, building complex RAG applications and proofs of concept. But often, when it comes to building these applications in production, we might need to store these embeddings somewhere on the cloud for data security and faster performance. I have worked with many clients, and the majority of them preferred Azure services over AWS, especially enterprise clients. That's when I thought, why not use LanceDB with Azure instead of building it locally? And how about building something combining this feature?

In this blog, we'll build an interesting application to map speakers based on their voices using vector similarity and store voice embeddings in an Azure Blob Storage. We'll develop an application that generates speaker-based transcriptions, mapping each speaker to their correct label from a known database. To be more specific, we will create a speaker-mapped RTTM file, which can be used to generate more accurate transcripts.

We'll go step by step, starting with creating audio embeddings, building a database of known speakers, and then moving towards Whisper and Nemo-MSDD diarization to identify speakers and map them accurately. Let's go!

### How to use Azure Blob as an object store for LanceDB?

Before we move on to building our application, I think it'd be helpful to understand how to connect LanceDB with Azure blob. It might seem a bit detailed, but it's essential for building our speaker-mapped transcription.

Currently, LanceDB supports integration with all major cloud storage providers as object stores, including AWS, Azure, and Google Cloud. It also has its own cloud version in beta and offers an enterprise version for full control over data and format. Depending on your use case, you can choose the most suitable option.
![](__GHOST_URL__/content/images/2025/02/image-10.png)
You can opt for other cloud storage solutions if you want to use the OSS version. There is detailed [documentation](https://lancedb.github.io/lancedb/concepts/storage/#1-s3-gcs-azure-blob-storage) available on choosing the best storage options for your LanceDB setup.

Today, we'll focus on using the OSS version with Azure Blob Storage, as it is widely used among enterprises. I wanted to write about using LanceDB with Azure since there is very little content available on this topic on the internet.

When connecting with Azure Blob, most of the code remains similar to what you would use when building on Colab. However, now we need to connect our container to the storage account of Azure Blob. Storing embeddings on cloud storage after creation is essential to reduce computation overhead during development and production.

Let's do this step by step. The first step is to define the following details in your environment variables. If you are working on Azure and using it in production, Azure App Services allows you to store environment variables there. If you are working in a notebook, say Colab, which I'll also be using for this demo, you can use the following code to save your storage account details there.

    import os
    os.environ["AZURE_STORAGE_ACCOUNT_NAME"] = <your storage account name>
    os.environ["AZURE_STORAGE_ACCOUNT_KEY"] = <your storage account key>

It is crucial to do this as your very first step because if you donâ€™t, thereâ€™s a possibility that LanceDB wonâ€™t get the account credentials properly, even if you pass them inside the `"storage_options"` parameter as shown in the documentation. Adding these details with the correct parameter names is a must for establishing a connection with Azure Blob. (you don't need to pass *storage_options if using env variables*)

Once you do this, the next step is to connect to the container. You need to pass the container name as a parameter and use the following code to establish the connectionâ€“

    # installing ncessary libraries
    !pip install adlfs lancedb

    import lancedb

    AZURE_BLOB_CONTAINER = "externaldata" #name of the container

    # Define the LanceDB path in Azure Blob. You can pass this variable as path.
    lance_db_path = f"abfs://{AZURE_BLOB_CONTAINER}/lancedb/" #we didn't use this path but you can replace it in next line.

    # Connect to LanceDB with Azure Blob Storage as Object Store. Async Method
    db = await lancedb.connect_async(f"az://{AZURE_BLOB_CONTAINER}/lancedb/")

    #Sync Connection - Both method works. You can test it on both.
    # db = lancedb.connect(f"az://{AZURE_BLOB_CONTAINER}/lancedb/")

    # Check connection
    print("Connected to LanceDB on Azure Blob Storage!")

Note that when connecting to Azure, LanceDB provides both synchronous and asynchronous API connections. You need to use different methods to validate whether everything is working or not.

Once you complete the above step using either the synchronous or asynchronous method, you are now ready to create your table there. Let's create a simple table to see if everything is working correctly. (I'm assuming an asynchronous API connection for now.)

    import pandas as pd

    table_name = "testing"

    df = pd.DataFrame({
        "text": "Hello, my name is Shresth",  #this is just for example
        "vector": [[23, 45, 6, 7, 8, 8, 8, 923, 3, 3, 3, 3]]  # Nested list structure to represent vector values correctly.
    })

    db.create_table(table_name, data=df, mode="overwrite")
    # The table with the same name will be overwritten when we rerun the query with another text.

If there are no restrictions on your blob account, you might be able to perform these basic operations on the blob directly. If you get any read/write error, you might need to check the permissions of your blob storage in this section. Make sure it is either enabled for all networks or that your IP is configured correctly, especially when using colab for development purposes â€“
![](__GHOST_URL__/content/images/2025/02/image-27.png)
The above code will help you create a "*lancedb*" folder in your blob storage account, and inside that folder, you can see the `.lance` file we just created.
![](__GHOST_URL__/content/images/2025/02/image-1.png)folder structure on blob![](__GHOST_URL__/content/images/2025/02/image-3.png)table inside that folder![](__GHOST_URL__/content/images/2025/02/image-7.png)sub-components inside lance file
Note that some improvements are still being made to establish Azure connections with LanceDB. However, once you set up the connection as shown in the code above, you'll be able to read and write your vectors to your blob storage. Also, note that once created, they will remain there inside the container, and you can connect to them later to query anytime.

You can use the following code to see the results. In the case of an asynchronous connection, you might need to convert your output to either PyArrow or Pandas format to visualize the content. When using a synchronous connection, a simple `to_list()` the method also works.

    #to print result of asysn connection

    table = await db.open_table("eng_chunks")
    # Finally, print out the data in the table

    print(table)

    arrow_table = await table.to_pandas()
    print(arrow_table)

    #to print result of sync connnection, you need to use the search method. the following code needs to be used.

    print(table.search().to_list())

![](__GHOST_URL__/content/images/2025/02/image-6.png)sample screenshot to show you the output from Async connection
And yes, not just on Colabâ€”if you're taking this to production, you can do similar testing on VS Code to check if the connection works properly.

You need to follow the same steps and load the environment variables before making a connection. (You don't need to pass *storage_options* as a parameter)
![](__GHOST_URL__/content/images/2025/02/image-11.png)if no dataset is present with that name, it'll be created.![](__GHOST_URL__/content/images/2025/02/image-13.png)terminal output of above code in visual studio
### Building Speaker Mapped Transcription using Vector Search

Now that we know how to connect LanceDB with Azure, let's build an interesting application by combining this feature. These days, we see different models being used for transcription. A common use case is generating a transcription of a meeting recording. Companies often have internal meetings and create minutes of the meeting afterward. Don't you think we can automate this?

Well, people have already started doing it. The main challenge is getting the transcription right. While transcription itself has been automated, there's a catchâ€”most meeting transcriptions donâ€™t include the speaker's name, just the timestamp (unless you partner with the tool providers, which obviously many don't xd).
![](__GHOST_URL__/content/images/2025/02/image-14.png)blurry meme for a blurry transcript xd
Generally, models provide a transcript, but we never know who is actually speaking those words and sentences. Thatâ€™s where diarization comes in. ***Speaker diarization*** is the process of separating an audio signal into different segments based on who is speaking at any given time.

It helps identify the number of speakers in the audio (in simple terms). So, if your meeting has three members, diarization models can map transcriptions to these speakers. But there's a small problemâ€”while diarization can detect the probable number of speakers, it can't tell who is speaking unless we provide that information externally. This is where vector search and vector databases come in.

You can have a database of known speakers, typically employees in a company, and use it to compare voices in a meeting to accurately map each speaker with their name. There's no need to train a separate speaker identification model for this. Instead, you can simply create embeddings and use them to find the correct match. The best part? You donâ€™t need to generate embeddings multiple timesâ€”only when adding new speakers to the database (for example when a new candidate joins the company). This can be managed through a separate pipeline.

Ready to test this? Here's what we'll do to validate our approachâ€“
![](__GHOST_URL__/content/images/2025/02/image-24.png)
We'll take 3-4 voice samples from different speakers, create their embeddings, and store them in our LanceDB vector database with Azure Blob as Object Store. For this, we'll use an open-source audio embedding model by *SpeechBrain*. It offers three major models, and ECAPA-TDNN might be the best fit for our needs in this case. There are also paid models available like of [TwelveLabs](https://docs.twelvelabs.io/docs/create-audio-embeddings).
![](__GHOST_URL__/content/images/2025/02/image-15.png)Embedding models provided by SpeechBrain
Here's the code on how to do this â€“

    #!pip install torchaudio speechbrain lancedb numpy

    import torchaudio
    import speechbrain
    from speechbrain.inference import SpeakerRecognition
    import lancedb
    import numpy as np
    import pandas as pd
    import pyarrow as pa

    import os
    os.environ["AZURE_STORAGE_ACCOUNT_NAME"] = "<your_storage_account_name>"
    os.environ["AZURE_STORAGE_ACCOUNT_KEY"] = "<your_account_access_key>"
    #note that you can add other parameters supported in similar manner as env variables.

    # Load the Speaker Recognition model
    model = SpeakerRecognition.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb",
                                            savedir="tmp_model")

    def get_embedding(audio_path):
        """Extracts speaker embedding from an audio file"""
        signal, fs = torchaudio.load(audio_path)

        # Convert stereo to mono (if needed)
        if signal.shape[0] > 1:
            signal = torch.mean(signal, dim=0, keepdim=True)  # Average both channels

        embedding = model.encode_batch(signal).squeeze().detach().cpu().numpy()
        return embedding.flatten().tolist()  # Convert to list for Lancedb storage

    # Initialize LanceDB with Azure blob
    AZURE_BLOB_CONTAINER = "externaldata"
    db = lancedb.connect(f"az://{AZURE_BLOB_CONTAINER}/lancedb/")
    # Check connection
    print("Connected to LanceDB on Azure Blob Storage!")

    # Define schema with audio storage
    schema = pa.schema([
        ("name", pa.string()),
        ("embedding", pa.list_(pa.float32(), 192)),  # 192-dimensional embedding
        ("audio", pa.binary())  # Store raw audio bytes
    ])

    def load_audio_as_bytes(audio_path):
        """Reads an audio file and converts it to bytes"""
        with open(audio_path, "rb") as f:
            return f.read()

    # Create table with the correct schema
    table = db.create_table("speakers", schema=schema, mode="overwrite")

    # Sample known speakers
    known_speakers = {
        "Shresth": "/content/input_audio_shresth.m4a",
        "Rahul": "/content/input_audio_rahul.m4a",
        "Arjun" : "/content/input_audio_arjun.mp3"}

    # Store known speaker embeddings in LanceDB
    data = []
    for name, file in known_speakers.items():
        embedding = get_embedding(file)
        audio_bytes = load_audio_as_bytes(file)  # Convert audio to bytes. you can convert it into byte64 string as well

        data.append({
                "name": name,
                "embedding": embedding,
                "audio": audio_bytes  # Store audio in LanceDB
            })

    table.add(data)

If you run this code on Colab, you'll see a screen like this and can validate whether the connections are correct or notâ€“
![](__GHOST_URL__/content/images/2025/02/image-30.png)
On your Azure Blob Storage, you can see a new Lance file named "speakers" that we just created â€“
![](__GHOST_URL__/content/images/2025/02/image-29.png)
Since we can store both audio and its embedding in LanceDB, we have designed our schema accordingly. If you have prepared this dataset in the cloud, you can also pass blob URL here as metadata if you don't want to store the full audio in the LanceDB. The models we used generate an embedding vector of size 192 in this case.
![](__GHOST_URL__/content/images/2025/02/image-18.png)length of embedding vector created
Now we have completed the major part. The next step is to query this database using an audio input to identify speakers. This is when you should think about your use case again.

For building a meeting transcription tool that converts audio or video recordings into a well-structured transcript with accurate speaker names. Can you think of how youâ€™d approach this? Plenty of tools and techniques are available for different stages of the process. For example:

- **Whisper** is great for transcription.
- **Demucs** help separate speech from background music and noise, improving transcription and diarization accuracy.
- **WhisperX** also helps with forced alignment to generate precise timestamps.
- **NeMo MSDD** performs best for speaker diarization.
- **Punctuation-based realignment** improves readability and structure.
- And many more depending on your use case and requirements.

I have found a resource that can help you with transcription, diarization, and related processing tasks.
[

Google Colab

![](__GHOST_URL__/content/images/icon/favicon-25.ico)

![](__GHOST_URL__/content/images/thumbnail/colab_favicon_256px-25.png)
](https://colab.research.google.com/drive/1kJlAhfi0N6Djo1AQTc5HEVZZXgSSucsL?usp=sharing)
Here's what a general flow would look like for our current application â€“ We'll load our audio file, which could be a meeting recording or any audio for which you want a transcription. We transcribe this using OpenAI's Whisper model. After the transcription, we convert the audio to mono format and pass it to Nemo MSDD's diarization model to create a speaker-timestamp distribution. This can then be used to map words and sentences to each speaker based on these timestamps.
![](__GHOST_URL__/content/images/2025/02/image-19.png)
At which step do you think we can include this vector search capability to map speakers with the correct names? Right after we get the diarization output. NeMo MSDD provides speaker mappings in a format like this. It generates an RTTM(*Rich Transcription Time Marked*) file with speaker predictions along with timestamps.
![](__GHOST_URL__/content/images/2025/02/image-23.png)
You can get this diarized output simply by passing your audio file in mono WAV format to Nemo MSDD's `NeuralDiarizer()` function.

Here's a code snippet to use Nemo-MSDD for diarization â€“

    #this is part of code and does not run independently. please refer to colab for complete code.

    #converting to mono for nemo compatibility.

    ROOT = os.getcwd()
    temp_path = os.path.join(ROOT, "temp_outputs")
    os.makedirs(temp_path, exist_ok=True)
    torchaudio.save(
        os.path.join(temp_path, "mono_file.wav"),
        audio_waveform.cpu().unsqueeze(0).float(),
        16000,
        channels_first=True)

    # Initialize NeMo MSDD diarization model. #Once you run this code, you'll get your RTTM file in the temp_outputs folder.
    #It is a time taking step and can be speed up with better infra if you are taking this for production. Or you can chose to use APIs for this.

    temp_path = "temp_outputs"
    msdd_model = NeuralDiarizer(cfg=create_config(temp_path)) #.to("cuda")
    msdd_model.diarize()

    del msdd_model
    torch.cuda.empty_cache()

After word and speaker mapping and alignment, the final output looks like this (without speaker names). We can replace speaker IDs with their correct names at this stage(after RTTM) or after obtaining the final transcriptâ€”both approaches work, but using the RTTM file is the preferred choice.
![](__GHOST_URL__/content/images/2025/02/image-20.png)
We can see that it works perfectly, but we donâ€™t just need these speaker IDsâ€”we want our transcript to have the correct names of the speakers, right?

We can extract audio for different speakers and use it to compare with our vector database. How do we do that? It's simple. You can extract audio for a particular speaker from the original audio using the diarization output (RTTM file) and compare it with known speakers. You don't need the full audioâ€”just a 10-second clip per speaker is enough for the search to work. Now our flow would look something like this â€“
![](__GHOST_URL__/content/images/2025/02/image-26.png)
You can extract the first 10 seconds for each speaker from your RTTM file using this code:

    #refer to colab for complete code

    from pydub import AudioSegment

    # Path to input audio
    audio_path = audio_path

    # Load audio file
    audio = AudioSegment.from_file(audio_path)

    # Read RTTM file and extract timestamps
    rttm_file = "/content/temp_outputs/pred_rttms/mono_file.rttm"

    speaker_segments = {}

    with open(rttm_file, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 8:
                speaker = parts[7]  # Speaker ID (e.g., spk_0)
                start_time = float(parts[3]) * 1000  # Convert sec â†’ milliseconds
                duration = float(parts[4]) * 1000  # Convert sec â†’ milliseconds
                end_time = start_time + duration

                # Store segments for each speaker
                if speaker not in speaker_segments:
                    speaker_segments[speaker] = []

                speaker_segments[speaker].append((start_time, end_time))

    # Process first 10 seconds for each speaker
    for speaker, segments in speaker_segments.items():
        speaker_audio = AudioSegment.silent(duration=0)  # Empty audio segment
        total_duration = 0

        for start_time, end_time in segments:
            segment_duration = min(end_time - start_time, 10_000 - total_duration)  # Limit to 10 sec
            speaker_audio += audio[start_time:start_time + segment_duration]
            total_duration += segment_duration
            if total_duration >= 10_000:  # Stop at 10 seconds
                break

        # Save speaker's first 10 seconds
        if total_duration > 0:
            output_filename = f"{speaker}_first_10s.wav"
            speaker_audio.export(output_filename, format="wav")
            print(f"Saved {output_filename}")

Once you get the audio clips for all the speakers that the model has predicted, you can use the following code to compare the extracted audio with known speakers. Here, I tested this approach on my audio, which is different from the one used to create the knowledge base.

    # Given a new speaker audio sample from the complete audio
    query_embedding = get_embedding("/content/testing_audio_shresth.m4a")

    # Search in LanceDB and retrieve similarity scores
    results = table.search(query_embedding).metric('cosine').limit(1).to_pandas()

    # Get the closest match and its similarity score
    if not results.empty:
        identified_speaker = results.iloc[0]["name"]
        similarity_score = 1-results.iloc[0]["_distance"]  # Lower distance = better match
        if similarity_score < 0.5:
            identified_speaker = "Unknown"
            print(identified_speaker,"Speaker not found. Similarity score in current dataset - ", similarity_score)

        else:
          print(f"Identified Speaker: {identified_speaker}, Similarity Score: {similarity_score}")

And damn, it works perfectly.
![](__GHOST_URL__/content/images/2025/02/image-22.png)
You need to perform this step for all the speakers extracted from the audio and create a mapping of speaker IDs to their actual names. Once your mapping is ready, all you need to do is replace the speaker labels with their correct names. That means replacing "speaker_0" with "Shresth," "speaker_1" with "Arjun," and so on for all the speakers identified in the transcript.

Here's how you can do this with your RTTM file:

    #refer to colab for complete code
    #assuming you have used above code in loop to create mapping like this after querying from the database.
    # You can also hose to create transcription as it is and then replace speakers in final transcript created.

    # Define the speaker mapping
    speaker_mapping = {
        "speaker_0": "Shresth",
        "speaker_1": "Arjun",
        "speaker_2": "Hamdeep",
        # Add more mappings as needed
    }

    # Load the RTTM file
    rttm_file_path = "/content/temp_outputs/pred_rttms/mono_file.rttm"
    output_file_path = "/content/temp_outputs/pred_rttms/updated_mono_file.rttm"

    # Read and modify the file line by line
    with open(rttm_file_path, "r") as file:
        lines = file.readlines()

    # Replace speaker labels while preserving spacing
    with open(output_file_path, "w") as file:
        for line in lines:
            parts = line.strip().split()  # Split by whitespace
            if len(parts) > 7 and parts[7] in speaker_mapping:  # Check if column 8 (index 7) is in mapping
                parts[7] = speaker_mapping[parts[7]]  # Replace speaker label
            file.write(" ".join(parts) + "\n")  # Preserve original spacing

    print("Modified RTTM file saved successfully!")

This step creates an updated RTTM file, which can be used to align the transcription with speakers and timestamps from Whisper and forced alignment. The notebook contains code for these next steps.

I hope you got the idea of how to proceed with this and build an application that not only performs transcription but also generates a more accurate speaker-mapped transcript while storing audio embeddings in an Azure Blob Storage account. This transcript can then be used by LLMs to create more context-aware notes and summaries. Vector search definitely proves to be the best approach in such cases.

Refer to this notebook for complete code implementation
[

Google Colab

![](__GHOST_URL__/content/images/icon/favicon-26.ico)

![](__GHOST_URL__/content/images/thumbnail/colab_favicon_256px-26.png)
](https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Speaker_Mapped_Transcription/Speaker_Mapping.ipynb)
Star ðŸŒŸ LanceDB recipes to keep yourself updated -
[

GitHub - lancedb/vectordb-recipes: High quality resources & applications for LLMs, multi-modal models and VectorDBs

High quality resources &amp; applications for LLMs, multi-modal models and VectorDBs - GitHub - lancedb/vectordb-recipes: High quality resources &amp; applications for LLMs, multi-modal models andâ€¦

![](__GHOST_URL__/content/images/icon/pinned-octocat-093da3e6fa40-7.svg)GitHublancedb

![](__GHOST_URL__/content/images/thumbnail/vectordb-recipes-2)
](https://github.com/lancedb/vectordb-recipes/)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\using-prediction-guard-and-lancedb-to-prevent-medical-hallucinations-201e57883750.md

---

```md
---
title: "Using Prediction Guard and LanceDB to Prevent Medical Hallucinations"
date: 2024-01-02
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/using-prediction-guard-and-lancedb-to-prevent-medical-hallucinations-201e57883750/preview-image.png
meta_image: /assets/blog/using-prediction-guard-and-lancedb-to-prevent-medical-hallucinations-201e57883750/preview-image.png
description: "Understand about using prediction guard and lancedb to prevent medical hallucinations. Get practical steps, examples, and best practices you can use now."
---

This is a collective work of the following authors:

[*Sharan Shirodkar (Prediction Guard) *](https://www.linkedin.com/in/sharanshirodkar7/)[*Daniel Whitenack (Prediction Guard)*](https://www.linkedin.com/in/danielwhitenack/)

[*Bingyang (Icy) Wang (Emory University)*](https://www.linkedin.com/in/bingyang-icy-wang/)* *[*Guangming (Dola) Qiu (Emory University) *](https://www.linkedin.com/in/dolaqiu/)[*Jerry (Yuzhong) M. (Emory University)*](https://www.linkedin.com/in/jerryyzmei/)* *[*Yige Wang (Emory University)*](https://www.linkedin.com/in/yigewang65/)

## Introduction

In spite of their eloquence and popularity, Large Language Models (LLMs) often struggle to generate output that is factually accurate. In particular, they can hallucinate information due to a limited grounding in reality. This problem is magnified in high-stakes domains like healthcare, where any faulty information could lead to critical errors or patient harm. Even minor inaccuracies cannot be tolerated when lives are at stake.

At the same time, generative AI systems are poised to create widespread, positive changes in healthcare. Imagine a world where healthcare professionals and volunteers are free to devote their full attention to their patients, unburdened by labor-intensive paperwork. Envisage a future where AI can streamline administrative processes, freeing up valuable time and resources.

In this post, we present innovative work from a team of Emory University students that leveraged LanceDB and [Prediction Guard](https://www.predictionguard.com/) to prevent hallucinations in an LLM-based system for processing medical transcriptions. This work was part of the recent [Data4Good Case Competition](https://business.purdue.edu/events/data4good/) sponsored by Microsoft, Prediction Guard, and Purdue University.

## The Data4Good Competition

The Data4Good Competition concluded on December 2nd, 2023, bringing together undergraduate and graduate students from various regions to showcase their critical thinking, data analysis, and artificial intelligence skills. Co-sponsored by Mitchell E. Daniels, Jr. School of Business masterâ€™s programs, Microsoft, INFORMS, Prediction Guard, and Certiport, this prestigious case competition offered a total prize pool of $45,000. Teams of 3â€“4 students competed in regional rounds, and the top-performing teams advanced to the final competition held at Purdue University in West Lafayette, IN.

The competition addressed the pressing issue of automating medical form filling, recognizing the labor-intensive nature of this process within the healthcare and hospice sectors. Participants were challenged to leverage open-source LLMs (like Llama 2 and WizardCoder) to extract, rephrase, summarize, and validate information needed for medical forms based on simulated medical transcription (e.g., patient-doctor conversations or doctor dictations). These open-source models were accessed via [Prediction Guard](https://www.predictionguard.com/), a company developing private LLM APIs with features focusing on safety and trust (PII and prompt injection filtering, factuality checking, toxicity checks, etc.). The competition provided (as input data) synthesized transcriptions of medical conversations similar to:

> D: What brings you in?
>
> P: Hi, Iâ€™ve Iâ€™ve had this pain on the outside of my uh right elbow now itâ€™s it I first started knowing, noticing it several months ago, but recently itâ€™s just been more painful.
>
> D: OK, so you said several months ago. Um did anything happen several months ago? Was there any sort of trigger, trauma, anything like that to that area?
>
> P: No, there wasnâ€™t any any trauma or any triggers that that I noticed, I was just um feeling it, uh, a bit more at the end of of work. Um yeah, I was just having it uh, feeling the pain a bit more at the end of work.
>
> D: OK, does uh anything make it better or worse, the pain?
>
> P: Um yeah, if I, really if Iâ€™m just resting the elbow um it makes it better, and Iâ€™ve tried uh things like ibuprofen um which has helped with the pain, Iâ€™ll Iâ€™ll do that for um hoping I can get through work sometimes if the pain is bad enough.
>
> D: Right, OK. Um and if you were to describe the quality of the pain, is it sharp, throbby, achy?
>
> P: Uh itâ€™s um kind of uh, well, itâ€™s achy and then sometimes depending on the movement it can get, it can be sharp as well.
>
> D: It can be sharp, OK. OK, um and what sorts of movements make it worse?
>
> P: Um, so like, really itâ€™s mostly the movements at my wrist, if Iâ€™m bending my wrist down, uh I can I can feel it, or um if Iâ€™m having to pick things up or hold heavy objects at work, I do a lot of repetitive uh things at at work, I work on a line.
>
> D: OK, OK. And 1 to 10, 10 being the worst pain youâ€™ve ever felt, how bad is this pain?
>
> P: It is about a four.
>
> D: About a four, OK. And have you ever experienced this type of pain before?
>
> P: etcâ€¦

Based on these kinds of transcriptions, teams needed to create an LLM-based system to answer specific medical form questions like:

- **What symptoms is the patient experiencing? **Pain on the outside of their right elbow
- **When did the symptoms start? **3 months ago
- **What medicine did the doctor prescribe?** Tylenol
- etcâ€¦

Obviously, hallucinating the answers to such questions could have serious consequences for patient care. Thatâ€™s where Emoryâ€™s unique retrieval-based system utilizing LanceDB and Prediction Guard could step in to help!

## Mitigating Hallucinations via Retrieval Augmentation

Although every medical transcription will be unique, much external information exists about real-world medical conditions, symptoms, medicines, etc. To ground the information extraction capabilities of their solution, the Emory students employ a hybrid, two-step retrieval augmentation methodology.

First, the solution utilizes non-vector fuzzy matching to crawl information about certain trusted sources of medical information, such as those from JHU Medicine, by crawling the site map under www.hopkinsmedicine.org/health/conditions-and-diseases/<name of conditions>. The crawled information includes extracted URLs (e.g., of pages discussing medical conditions and treatments) along with corresponding metadata (title, page description, etc.). When processing a new medical transcription, an LLM (using a few shot prompt) extracts a guess for the symptoms and condition represented in the transcription. The LLM might suggest, for example, that the patient is experiencing a â€œfungal infectionâ€ with symptoms including â€œWeight loss, Chest pain, and Itchy or scaly skin.â€ This guess information is then fuzzy matched (via Levenshtein Distance and a collection of common condition acronyms) to relevant reference URLs via the crawled metadata.
![](https://miro.medium.com/v2/resize:fit:1400/1*WiOEI3O57b0fatQ20kxNAw.png)
If the content from the matched reference URLs has not been downloaded previously, it is requested and parsed programmatically. The content is chunked, embedded, and loaded into LanceDB on the fly in sections such as introductions, symptoms, and treatments. Sentence Transformers is used to generate embeddings via the open-access embedding model â€œall-MiniLM-L12-v2â€. LanceDB was used partially because of the ease with which the API could be integrated into the processing code. However, the on-disk and serverless nature of LanceDB make it additionally appealing for this use case because healthcare clients will often need to self-host databases and/or keep data within their own networks.
![](https://miro.medium.com/v2/resize:fit:1400/1*DQnznxG2Sy_bsMq-31gLFw.png)
In the second stage of retrieval, the transcription is matched to relevant chunks from the retrieved medical documents via vector search. These chunks of the documents are injected into a template for a retrieval augmented prompt and a second LLM call. The second LLM call benefits from the reference medical information, improving the results and reducing the likelihood of hallucinating irrelevant symptoms, treatments, or condition names.
![](https://miro.medium.com/v2/resize:fit:1400/1*TJsoYdLFw8nDqyjSaDtjxA.png)
This two-step, hybrid methodology, which the Emory students call â€œcondition-based RAGâ€, has the following benefits:

1. Reduction in the burden to embed a vast knowledge base up-front. The fuzzy matching to relevant URLs is extremely efficient and allows relevant documents to gradually be parsed, chunked, embedded, and added to LanceDB.
2. Integration of new data sources over time without any model re-training or batch updates to the vector database. New URLs can be scraped and added to the â€œstableâ€ of reference documents over time, and they will be parsed and embedded as they are needed.

In the end, the above-described system resulted in a top 5 score in the Kaggle leaderboard for the Data4Good competition, with a Word Error Rate (WER) of around 0.5. The students estimate that at this error rate, such a system could save hours of administrative work every week for healthcare workers. This translates to freeing up more physician availability for clinically important activities like examining patients, making diagnoses, formulating treatment plans, etc.

They envision applying this in a human-in-the-loop manner. Specifically, the team envisions combining the retrieval augmented output with a [factual consistency score](https://docs.predictionguard.com/reference/factuality) from Prediction Guard. This is calculated by comparing a reference text (e.g., one retrieved from LanceDB) with the generated form field to check for inaccuracies. Low-scoring results can be prioritized for human review, such that human annotators can focus their efforts on suspect answers from the LLM-based system.

## Key Takeaways

Retrieval has proved to be [one of the most applied mechanisms](https://blog.langchain.dev/langchain-state-of-ai-2023/) for those building and deploying LLM-based applications. It provides a way for you to combine your data with LLMs (something that almost everyone wants to do) and build cool â€œchat-with-your-dataâ€ bots. However, the work by this team from Emory demonstrates that retrieval can also be used in a variety of interesting ways in the context of domain-specific information retrieval.

If you are trying to extract domain-specific information from unstructured data, consider leveraging external data sources that provide expertly curated information about the domain. You can match queries to this external data to provide context within your information extraction pipelines. In cases where data privacy is a concern, self-hosting a vector database can be easy using LanceDB as an embedded option with on-disk storage.

Also, think about combining the outputs of these pipelines with other kinds of evaluations (e.g., factual consistency) that are grounded in the retrieved information. The built-in factual consistency check in Prediction Guard provides an easy-to-access and scalable solution that is already integrated with a privacy-conserving LLM API.

Learn more here:

- Prediction Guard: [https://www.predictionguard.com/](https://www.predictionguard.com/)
- RAG starter tutorial with Prediction Guard: [https://docs.predictionguard.com/usingllms/augmentation#retrieval-augmentated-generation-rag](https://docs.predictionguard.com/usingllms/augmentation#retrieval-augmentated-generation-rag)
- Data4Good: [https://business.purdue.edu/events/data4good/](https://business.purdue.edu/events/data4good/)
- LanceDB: [https://lancedb.com/](https://lancedb.com/)

[https://blog.lancedb.com/?source=post_page-----201e57883750--------------------------------](https://blog.lancedb.com/?source=post_page-----201e57883750--------------------------------)
```

---

## ðŸ“„ æ–‡ä»¶: drafts\vector-arithmetic-with-lancedb-an-intro-to-vector-embeddings.md

---

```md
---
title: "Vector Arithmetic with LanceDB: an Intro to Vector Embeddings"
date: 2024-07-02
author: LanceDB
categories: ["Community"]
draft: true
featured: false
image: /assets/blog/vector-arithmetic-with-lancedb-an-intro-to-vector-embeddings/preview-image.png
meta_image: /assets/blog/vector-arithmetic-with-lancedb-an-intro-to-vector-embeddings/preview-image.png
description: "Build about vector arithmetic with lancedb: an intro to vector embeddings. Get practical steps, examples, and best practices you can use now."
---

ðŸ’¡

This is a community post by "Artitra Roy Gosthipaty"

## Introduction

Imagine living your life chained in a cave, facing a blank wall. The only things you see are shadows projected onto the wall by objects passing in front of a fire behind you.
![](__GHOST_URL__/content/images/2024/06/plato.webp)Plato's Allegory of the Cave ([Source](https://www.thoughtco.com/the-allegory-of-the-cave-120330))
This imagery is derived from [Plato's Allegory of the Cave](https://en.wikipedia.org/wiki/Allegory_of_the_cave), a profound philosophical story. Though it may seem dark and confined, it carries a deeper meaning. The shadows you see are mere abstractions, simplified versions of objects that are far richer and more complex in reality.

In a similar way, **embeddings** can be thought of as entities that provide a rich representation of the data we encounter in our everyday lives. They transform *complex*, *high-dimensional* data into a **simpler**, more **meaningful** form, allowing us to understand and manipulate it more effectively.

In the later sections of this tutorial, we'll conduct exciting experiments to demonstrate the power of embeddings. We'll start with semantic searches within our dataset, showcasing how to retrieve relevant results effortlessly.

    query = "people smiling"

![](__GHOST_URL__/content/images/2024/06/data-src-image-16b71471-0f63-4851-a700-cb4cc2b50e01.png)
    query = "cars on street"

![](__GHOST_URL__/content/images/2024/06/data-src-image-04168333-dd70-407e-b0a2-70cea0bb1c72.png)
Additionally, we'll create a prompt, performing vector arithmetic to subtract an object from the prompt and add a different object in its place, and explore the fascinating outcomes in the embedding space. These experiments will highlight the versatility and strength of embeddings in various practical applications.

    query = "many people happy"
    sub_query = "happy"
    rep_query = "sad"

![](__GHOST_URL__/content/images/2024/06/data-src-image-e89cc7c7-e6e9-46c2-8eb3-89896184d319.png)
    query = "many people happy"
    sub_query = "many people"
    rep_query = "single man"

![](__GHOST_URL__/content/images/2024/06/data-src-image-24441821-28ca-4dba-8802-ab6dad217d55.png)
In this tutorial, we will cover the following:

1. **What** are embeddings?
2. **How** do I store embeddings?
3. **Why** are they important?

## What are embeddings?

Embeddings are numerical representations (vectors or tensors) of data, often used in machine learning and deep learning to transform complex, high-dimensional information into a more *manageable* form.
![](__GHOST_URL__/content/images/2024/06/meme.jpg)Vectors or Tensors?
In deep learning (DL), embeddings are typically learned by neural networks, capturing the underlying relationships and patterns within the data. For example, in natural language processing (NLP), **word embeddings** convert words into dense vectors where similar words have similar vector representations.

This allows the model to understand semantic relationships between words, such as "king" and "queen" being related to royalty, or "Paris" and "France" being connected by geography.

Similarly, in the world of images, embeddings can represent images in a way that captures visual similarity, enabling tasks like *image search* or *clustering* similar images together.

### Text Embedding

Let's understand embeddings with code. For this tutorial we will be using the [SentenceTransformers](https://sbert.net/) library. It provides us with a number of embedding models. Luckily, LanceDB has an integration with SentenceTransformers, which makes our lives easier.

We first install and import our necessary libraries.

    $ pip install -U -q sentence-transformers
    $ pip install -U -q lancedb

Install the necessary packages

    import torch
    from lancedb.embeddings import get_registry

    DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

Import libraries and set device

Here we build a utility function `get_embedding`. The function accepts text, and outputs the embedding of the text.

    def get_embedding(text):
        embedding = embedding_model.embedding_model.encode(
            text, convert_to_tensor=True
        ).cpu().numpy()
        return embedding

We have our setup ready. The one thing that we are missing is our embedding model. We will grab our model from the LanceDB integration. We will be using the `all-mpnet-base-v2` model for our use case, feel free to use any other model that you like.

> To know more about LanceDB and embedding refer to their [official documentation](https://lancedb.github.io/lancedb/embeddings/embedding_functions/).

    embedding_model = (
        get_registry()
        .get("sentence-transformers")
        .create(name="all-mpnet-base-v2", device="cuda:0")
    )

Let's take our embedding model for a ride, shall we? We will take three texts and compute the semantic distance between them.

> Semantic meaning changes with the task and dataset with which the model was trained. You might find some texts to be semantically similar which are not supposed to be. Now you know why.

    text1 = "king"
    text2 = "queen"
    text3 = "apple"

    embedding1 = get_embedding(text1)
    embedding2 = get_embedding(text2)
    embedding3 = get_embedding(text3)

    print(f"Similarity between `{text1}` and `{text2}`: {embedding1.dot(embedding2):.2f}")
    print(f"Similarity between `{text1}` and `{text3}`: {embedding1.dot(embedding3):.2f}")

Similarity between `king` and `queen`: 0.63 Similarity between `king` and `apple`: 0.21

## Ingest embeddings in LanceDB

Storing vector embeddings efficiently is crucial for leveraging their full potential in various applications. To **manage**, **query**, and **retrieve** embeddings effectively, especially when dealing with large-scale and multi-modal data, you need a robust solution.

LanceDB is an open-source vector database specifically designed for this purpose. It allows you to store, manage, and query embeddings, making it easier to integrate them with your data.

We will be taking LanceDB for a spin on our multi-modal dataset, and understand how to perform vector arithmetic with the embeddings.

    import os
    import io
    import lancedb
    import pandas as pd
    import pyarrow as pa
    from PIL import Image
    from matplotlib import pyplot as plt
    from lancedb.pydantic import LanceModel, Vector

## Multi-Modal Dataset

We will be working with the [Flickr-8k](https://paperswithcode.com/dataset/flickr-8k) dataset. It is a multi modal dataset, which consists of images and their corresponding captions.

I have used the [Kaggle dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k) rather than downloading from source as I found it to be programatically more feasible. Just follow along to download the dataset.

    $ kaggle datasets download -d adityajn105/flickr8k
    $ unzip -q flickr8k.zip

> If you face any problem with the Kaggle API please refer to their [official documentation](https://www.kaggle.com/docs/api).

    IMAGE_DIR = "/content/Images"
    ANNOTATION_FILE = "/content/captions.txt"

Detour: The Flickr-8k dataset consists of `5` captions per image. I found it to be more efficient to group all the captions together for an image. This decreases the number of rows in the dataset. You can skip this section, and follow along with the original dataset too.

    df = pd.read_csv(ANNOTATION_FILE)
    df = df.groupby("image")["caption"].apply(list).reset_index(name="caption")
    df["caption"] = df["caption"].apply(lambda x: " ".join(x))

## Understanding the Schema

In any database, a schema defines the **structure** and **organization** of the data, specifying how data is stored, accessed, and managed. In the context of a vector database, the schema becomes particularly important as it needs to accommodate not just traditional data types but also complex structures like vector embeddings.

Consider the following code example, which defines a schema for storing image embeddings along with related data:

    class Schema(LanceModel):
        vector: Vector(embedding_model.ndims()) = embedding_model.VectorField()
        image_id: str
        image: bytes
        captions: str = embedding_model.SourceField()

Here, the schema is defined using a custom model class `Schema`. The `pa_schema` specifies the data types for each field:

- `vector`: A list of `768` float32 values representing the embedding vector.
- `image_id`: A string identifier for the image.
- `image`: Binary data representing the image itself.
- `captions`: A string containing descriptive text about the image.

The custom `Schema` class, inheriting from `LanceModel`, further organizes these fields. It uses a `Vector` type for the embedding vector, ensuring it aligns with the dimensions of the model's output. The `image_id` is a string, `image` is stored as bytes, and `captions` are linked to the source field of the embedding model.

## Ingesting Data into the Database

This section of the code is responsible for processing and ingesting the dataset into LanceDB. Let's break down the code and understand this approach:

    def process_dataset(dataset):
        for idx, (image_id, caption) in enumerate(dataset.values):
            try:
                with open(os.path.join(IMAGE_DIR, image_id), "rb") as image:
                    binary_image = image.read()

            except FileNotFoundError:
                print(f"image_id '{image_id}' not found in the folder, skipping.")
                continue

            image_id = pa.array([image_id], type=pa.string())
            image = pa.array([binary_image], type=pa.binary())
            caption = pa.array([caption], type=pa.string())

            yield pa.RecordBatch.from_arrays(
                    [image_id, image, caption],
                    ["image_id", "image", "captions"]
                )

This function, `process_dataset`, iterates through each entry in the dataset, reads the corresponding image file, and converts it into a **binary** format. If the image file is not found, it skips that entry.

It then creates arrays for the `image_id`, `image`, and `caption` fields and yields a `RecordBatch` containing these arrays. This batch processing is efficient for ingesting large datasets without running out of memory or creating many versions of dataset.

    db = lancedb.connect("embedding_dataset")
    tbl = db.create_table("table", schema=Schema, mode="overwrite")
    tbl.add(process_dataset(df))

Here, the code connects to or create a local LanceDB database connection named `embedding_dataset` and creates a table with the defined Schema. The `mode="overwrite"` ensures that any existing table with the same name is replaced. The `tbl.add(process_dataset(df))` line then adds the processed dataset to the table.

## Why This is a Good Practice

1. **Centralized Storage for all your data, not just vectors**: Everythingâ€”images, text, and embeddingsâ€”is stored in one place, with a defined schema, making it easier to manage and query. This centralized approach simplifies data handling and enhances efficiency.
2. **Automated Encoding**: LanceDB embedding API handles the encoding of data, eliminating the need for manual encoding. This reduces the chances of errors and ensures consistency across the dataset.
3. **Efficiency**: LanceDB consumes data as `RecordBatch` iterators in one go without creating multiple versions. It is recommended for ingesting large datasets, specifically larger-than-memory datasets.

## Semantic Search Made Simple

Semantic search is a powerful technique that allows us to **retrieve** data based on the meaning and context of the search query, rather than relying solely on keyword matching. This is particularly useful in scenarios where we want to find images, documents, or other data types that are semantically similar to a given query.

Manually implementing semantic search would be a complex and daunting task. It would require extensive work to encode both the search query and the dataset into a comparable format, and then efficiently compare them to find the most relevant results. This involves dealing with high-dimensional vector operations, managing large datasets, and ensuring that the comparisons are both accurate and performant.

With LanceDB, performing semantic search is pretty straightforward. The API design is intuitive.  Let's take a look at the following example:

    def show_image(image):
        stream = io.BytesIO(image)
        plt.imshow(Image.open(stream))
        plt.axis("off")
        plt.show()

Utility for images

    query = "dog running through sand"

    hit_lists = tbl.search(query) \
        .metric("cosine") \
        .limit(2) \
        .to_list()

    for hit in hit_lists:
        show_image(hit["image"])

![](__GHOST_URL__/content/images/2024/06/data-src-image-81dc5ee5-6b4f-47bb-aa31-234be2d6c18a.png)![](__GHOST_URL__/content/images/2024/06/data-src-image-3e82d66b-554e-4caa-831d-472ef22c6a7d.png)
In this code, we perform a semantic search for the query `"dog running through sand"`. The search function leverages LanceDB's capability to handle embeddings efficiently:

- **Encoding the Text**: Only the text query needs to be encoded. The embedding API takes care of converting it into the appropriate vector representation.
- **Performing the Search**: The `tbl.search(query)` method initiates the search, and by chaining methods like `.metric("cosine")` and `.limit(2)`, we specify the search criteria. Cosine similarity is used as the metric to find the most similar embeddings, and the results are limited to the top 2 hits.
- **Retrieving and Displaying Results**: The results are then converted to a list with `.to_list()`, and we loop through the hits to display the corresponding images.

This process is almost **instantaneous**, demonstrating the efficiency and elegance of LanceDB. The seamless integration of search capabilities with data storage ensures that you can quickly and effectively retrieve the most relevant data based on semantic meaning. LanceDB's design abstracts away the complexities, providing a user-friendly interface to perform powerful semantic searches with ease.

## Vector Arithmetic in Action

Get ready to be amazed! We're taking semantic search to the next level by performing vector arithmetic on embeddings. This approach allows us to manipulate the embeddings in ways that capture complex relationships and nuances in the data, bringing some truly exciting possibilities to life.

Let's dive into the code and see this in action:

    query = "dog running through sand"
    sub_query = "dog"
    rep_query = "human"

    query_emb = get_embedding(query)
    sub_query_emb = get_embedding(sub_query)
    rep_query_emb = get_embedding(rep_query)

    emb = query_emb - sub_query_emb + rep_query_emb

    hit_lists = tbl.search(emb) \
        .metric("cosine") \
        .limit(6) \
        .to_list()

    for hit in hit_lists:
        show_image(hit["image"])

![](__GHOST_URL__/content/images/2024/06/data-src-image-b785a4d5-51a5-4008-9c9a-61fe4fced8e9.png)

![](__GHOST_URL__/content/images/2024/06/data-src-image-7dd39e33-cd00-4a25-a1ab-aaae6f7372ec.png)

![](__GHOST_URL__/content/images/2024/06/data-src-image-e9ce45a7-7889-4f48-987c-08b3b71e858e.png)

![](__GHOST_URL__/content/images/2024/06/data-src-image-3275a95d-30dd-4851-852b-d55a7487e8f4.png)

![](__GHOST_URL__/content/images/2024/06/data-src-image-f2241563-1c7e-4cf6-8cfb-4faa76763304.png)

![](__GHOST_URL__/content/images/2024/06/data-src-image-800fb529-fd54-494d-9850-d47506c29e74.png)

    query = "many people happy"
    sub_query = "many people"
    rep_query = "single man"

    query_emb = get_embedding(query)
    sub_query_emb = get_embedding(sub_query)
    rep_query_emb = get_embedding(rep_query)

    emb = query_emb - sub_query_emb + rep_query_emb

    hit_lists = tbl.search(emb) \
        .metric("cosine") \
        .limit(6) \
        .to_list()

    for hit in hit_lists:
        show_image(hit["image"])

![](__GHOST_URL__/content/images/2024/06/data-src-image-fcfe6ce6-eb78-4ee9-aac7-92a8eb0b652c.png)

![](__GHOST_URL__/content/images/2024/06/data-src-image-15ec8c4c-781c-4da6-b26a-76d4725c118c.png)

![](__GHOST_URL__/content/images/2024/06/data-src-image-5f388a46-0b55-4b31-b121-ab086abb7b70.png)

![](__GHOST_URL__/content/images/2024/06/data-src-image-1a071274-a6cd-467d-ba93-55e0c8a8c138.png)

![](__GHOST_URL__/content/images/2024/06/data-src-image-7ef5f6f9-1e9f-45c3-92b5-bd70ed1058f4.png)

![](__GHOST_URL__/content/images/2024/06/data-src-image-aedccc6e-65a7-4f35-857b-fe208a0a397a.png)

In this example, we start with a query, "dog running through sand", and create embeddings for this query, a sub-query "dog", and a replacement query "human".

By performing the arithmetic operation `query_emb - sub_query_emb + rep_query_emb`, we effectively transform our original query into a new one. This operation intuitively means: replace "dog" with "human".

## Conclusion

Congratulations on making it through this tutorial on vector arithmetic using text embeddings! We've explored the fascinating world of embeddings, understanding their significance in deep learning and how they transform complex data into meaningful vectors.

We've seen how LanceDB, with its powerful and elegant design, simplifies the process of storing, managing, and querying these embeddings, making sophisticated tasks like semantic search and vector arithmetic both accessible and efficient.

Thank you for following along, and happy embedding!
```

---

## ðŸ“„ æ–‡ä»¶: drafts\why-dataframe-libraries-need-to-understand-vector-embeddings-291343efd5c8.md

---

```md
---
title: "Open Vector Data Lakes"
date: 2023-05-21
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/why-dataframe-libraries-need-to-understand-vector-embeddings-291343efd5c8/preview-image.png
meta_image: /assets/blog/why-dataframe-libraries-need-to-understand-vector-embeddings-291343efd5c8/preview-image.png
description: "Discover about open vector data lakes. Get practical steps, examples, and best practices you can use now."
---


This post is an expanded version of the upcoming joint talk [Quokka](https://github.com/marsupialtail/quokka) + [LanceDB](https://github.com/lancedb/lancedb) talk at the [Data+AI Summit](https://www.databricks.com/dataaisummit/) this June.

## Vector embeddings are here to stay

Vector embeddings are here to stay. It is hard to conceive of constructing recommendation system, search engine, or LLM-based app in this day and age without using vector embeddings and approximate/exact nearest neighbor search. Vector embeddings are the easiest way to do retrieval on unstructured data formats like text and images, and there are countless ways to generate them that get better each day.

## Current approaches wonâ€™t scale

There are three main reasons that I believe the incumbent vector databases canâ€™t succeed in the long-term.

First and foremost is cost â€” vector databases today resemble OLTP databases with strong focus on ingest speed, write consistency and point query latency. However, when their size starts to blow up, so do their costs. Just take a look at pricing for Pinecone.

Second is vendor lock-in â€” with the exception of LanceDB, the long term storage format for vector databases are closed to all analytics tools other than that vector database. Do you really want to lock up such an important part of your data in this way? Or have to store a separate copy?

Third is flexibility â€” In the structured data world, *data lakes* have become popular as a long-term storage for OLTP stores like Postgres or MySQL. The data lake has much worse transactional performance for online workloads, but support cheap long term storage and relatively efficient querying. Most importantly, the long term storage is in an open format decoupled from the OLTP store, allowing different tools to compete and excel at different tasks, like dashboard reporting or machine learning training. None of this would be possible if your S3 was full of MySQL pages or Postgres WALs.

## We need better vector embedding tools

Itâ€™s clear from the get-go that vector embeddings are a whole new data type, and significant changes to current data systems are needed to support them well. Hundreds of millions of investment dollars have poured into making a new generation of databases that are optimized around vectors. Existing SQL/noSQL players like Redis, Elastic, Postgres, Clickhouse, DuckDB have all built extensions that support vector operations. It is certainly a very active space.

However, I want to focus this post on DataFrames, and why theyâ€™re currently so bad at handling vector data. It used to be that DataFrame libraries lead databases in features (Python UDFs). However, in the case of vector embeddings, I believe they are falling behind.

## Storage

For starters, thereâ€™s no agreement on what the type of vector embeddings should be as a column in a DataFrame. In Pandas, they get cast into an â€œobjectâ€ datatype, which is opaque and not amenable to optimizations. Apache Arrow probably has the best idea, representing the vector embeddings as a FixedSizeList datatype (this is what LanceDB uses). Recently it has also introduced the â€œTensorâ€ datatype in Release 12.0.0. Unfortunately most people use Polars to operate on Arrow data, and Polars does not support FixedSizeList or Tensor, only List, though there is an ongoing draft [PR](https://github.com/pola-rs/polars/pull/8342) to address this. In Spark we probably will use the ArrayType, which doesnâ€™t enforce length. Concretely this also means that files written by some systems will be unreadable by others, and itâ€™s a nightmare to convert between different dataframe libraries.

## Compute

Once the storage/memory type is settled, we should allow DataFrame-native computations on the vector embedding column. Most people currently just do .to_numpy() on that column from the dataframe and start using ad-hoc numpy/faiss code. Then the resulting numpy array is stitched with other metadata back into a dataframe to continue processing in the relational world.

This is the only option today, but it is quite suboptimal. Imagine having to convert a numerical column to numpy every time you want to do a filter operation. At what point do you ditch the dataframe library altogether and start doing everything in numpy? Of course, .to_numpy() only works on single-machine libraries like Polars and Pandas. If you are using Spark, good luck. Maybe write a UDF or something?

## Bottom line

DataFrame engines should support native operations on vector embedding columns, such as exact/approximate nearest neighbor searches or range searches. Recently, a new format [Lance](https://github.com/eto-ai/lance) has come out as strong alternatives to Parquet that has native support for vector indices. That means any Arrow compatible DataFrame engine can immediately gain vector search capabilities if it was able to push the right syntax down to Lance.

## Quokkaâ€™s Vector Operations

As a proof of concept and hopefully an example for other DataFrame engines, I have started implementing vector-embedding-native operations in Quokka.

## What is Quokka

For those unfamiliar, Quokka is a distributed DataFrame system currently largely supporting the Polars API, with an aspiring [SQL](https://github.com/marsupialtail/quokka/blob/master/pyquokka/sql.py) interface. It is fault tolerant and usually much faster than SparkSQL if data fits in memory. You can also use Quokka locally, just do `pip3 install pyquokka` and familiarize yourself with the API [here](https://marsupialtail.github.io/quokka/simple/). Similar to Spark and Polars, Quokka has a lazy API so it can optimize a user's query before running it by pushing down filters, selecting columns early and reordering joins.

Since Quokka is very much based on Polars, the data type for embeddings is currently a Polars List (FixedSizeList is forthcoming). If it encounters data with other formats, it will try to convert them under the hood.

## Lance IO

Quokka supports ingest from the [Lance](https://github.com/eto-ai/lance) format. Lance is an on-disk alternative to Parquet specifically optimized for AI data with an optional vector index built on the vector embedding column. If you are working with vector embedding data, you should strongly consider using Lance.

To read a Lance dataset into a Quokka DataStream, just do `qc.read_lance("vec_data.lance")`. You can also read many Lance datasets on object storage into the same Quokka DataStream: `qc.read_lance("s3://lance_data/*")`.

## Embeddings-related Compute in Quokka

Quokka currently supports just one operation `vector_nn_join` on vector embedding data. You can perform a `vec_nn_join` between a Quokka DataStream and a Polars DataFrame: `a.vec_nn_join(df, vec_column = "vec", k = 1, probe_side = "left")`, assuming `a["vec"]` and `df["vec"]` are vectors. If the `probe_side` is left, this will for every row in the Quokka DataStream find `k` nearest neighbors in the Polars DataFrame based on the vector columns. If the `probe_side` is right, this will find `k` nearest neighbors in the Quokka DataStream for every row in the DataFrame.

But hey! Why canâ€™t we join a DataStream against a DataStream? In Quokka, DataStreams are reserved for very large data sources that donâ€™t fit in memory. A `vector_nn_join` between DataStreams would be extremely computationally expensive even with indexes.

## Push it down!

If the source of the DataStream has indices (Lance), the `vector_nn_join` will be pushed down to be an approximate nearest neighbor search to the source. Otherwise, it will be an exact nearest-neighbor search with the good old `numpy.dot`.

The vector embedding API is very much a work in progress. If people are interested, future APIs under consideration are `vector_range_join` and `vector_groupby` based on clustering. Check out the code examples [here](https://github.com/marsupialtail/quokka/blob/master/apps/vectors/do_lance.py). Contributions welcome!

## What I hope this enables: open vector data lakes

So what would a real vector data lake look like? Vector embeddings should be stored in Parquet, or Lance, as a native data type. Metadata formats such as Delta Lake or Iceberg should support rich indices to be added by the user, and support versioning on these indexes. Query engines such as Trino and SparkSQL should be able to do nearest neighbor search on the vector data, just like how they are able to filter or join relational data.

Of course, vector databases are still needed to provide operational access to the latest data, just like Oracle/Postgres/MySQL. However, old data should be periodically ETLâ€™ed out of those systems to the data lake. Data engineering teams are already experts at doing it.

Quokka is the first system that tries to allow people to do something like this, but I donâ€™t think it will be the last, or the best. Executing on this vision needs collaboration from open data lake formats Iceberg and Delta, file formats like Parquet and query engines like Quokka, Trino and SparkSQL. But the open data community moves fast, and I have high hopes for the future!
```

---

## ðŸ“„ æ–‡ä»¶: drafts\you-can-now-delete-rows-in-lance-and-lancedb-8200d885d1cb.md

---

```md
---
title: "You Can Now Delete Rows in Lance and LanceDB!"
date: 2023-06-28
author: LanceDB
categories: ["Engineering"]
draft: true
featured: false
image: /assets/blog/you-can-now-delete-rows-in-lance-and-lancedb-8200d885d1cb/preview-image.png
meta_image: /assets/blog/you-can-now-delete-rows-in-lance-and-lancedb-8200d885d1cb/preview-image.png
description: "Explore about you can now delete rows in lance and lancedb!. Get practical steps, examples, and best practices you can use now."
---

by Will Jones

Starting in Lance 0.5.0, you can delete rows in Lance datasets. Up until now, weâ€™ve only supported adding new data (new rows with append or newStarting in Lance 0.5.0, you can delete rows in Lance datasets. Up until now, weâ€™ve only supported adding new data (new rows with append or new columns with [merge](https://lancedb.github.io/lance/read_and_write.html#adding-new-columns)) or overwriting the whole dataset. Now we support deleting individual rows or fragments. This is one of the first in a series of features we are releasing to make it easier to manage large scale AI datasets.

The basics of delete are very simple: given a dataset, you can specify a SQL predicate and all of those rows will be deleted.

    >>> import lance
    >>> import pandas as pd
    >>> data = pd.DataFrame({
    ...     'x': [1, 2, 3],
    ...     'y': ['a', 'b', 'c']
    ... })
    >>> dataset = lance.write_dataset(data, 'my_dataset', mode="overwrite")
    >>> dataset.to_table().to_pandas()
       x  y
    0  1  a
    1  2  b
    2  3  c
    >>> dataset.delete("x = 2")
    >>> dataset.to_table().to_pandas()
       x  y
    0  1  a
    1  3  c

Deletion in Lance is non-destructive. We keep a snapshot of each version of the dataset, so you can always go back to previous versions.

    >>> old_dataset = lance.dataset("my_dataset", version=dataset.version - 1)
    >>> old_dataset.to_table().to_pandas()
       x  y
    0  1  a
    1  2  b
    2  3  c

Lance uses SQL expressions, which allows for complex predicates during deletion.

    >>> dataset.delete("(x = 1) OR (y in ('a', 'b', 'e'))")
    >>> dataset.to_table().to_pandas()
       x  y
    0  3  c

See [our documentation](https://lancedb.github.io/lance/read_and_write.html#filter-push-down) for a full list of supported SQL in predicates.

## Implementation and expected performance

How does Lance implement deletion? When we delete rows, we save a new file called a deletion file for each affected fragment. The file contains the row ids of all deleted rows.

By writing these deletion files, we avoid rewriting data files, which is expensive on its own but also has special ramifications for Lance. If a data file is rewritten, the row ids are changed, invalidating the ANN indices for those files. So if we rewrote the data files, weâ€™d also have to update the indices.

To see which fragments in the datasets have deletion files, use the **get_fragments()** method. From our previous example, if we compare the fragments in the **old_dataset** versus the new **dataset**, weâ€™ll see the new dataset now has an additional **deletion_file** but the **data_files** remains the same.

    >>> old_dataset.get_fragments()
    [LanceFileFragment(id=0,
                       data_files=['605be8b8-8b01-47b3-b4a8-45af4b2acdea.lance'])]
    >>> dataset.get_fragments()
    [LanceFileFragment(id=0,
                       data_files=['605be8b8-8b01-47b3-b4a8-45af4b2acdea.lance'],
                       deletion_file='_deletions/0-14-16447660214542675320.arrow')]

Reading these new deletion files can add a small amount of latency to queries, so we cache this metadata. This means that tables that have deletions will have a little bit of cold start latency. Weâ€™ll be benchmarking and improving this in the near future.

In cases where a predicate aligns with the write pattern, we can avoid writing deletion files entirely; instead just eliminating an entire fragment. For example, imagine there is a sensor dataset that dumps data with a timestamp every hour. If we pass a deletion predicate on the timestamp column, we can just eliminate the particular data fragments that are part of the dump. The dataset might be written like:

    >>> from datetime import datetime, timedelta
    >>> import shutil
    >>> import pyarrow as pa

    >>> def new_data(timestamp: datetime) -> pa.Table:
    ...     nrows = 10_000
    ...     return pa.table({
    ...         "insertion_timestamp": pa.array([timestamp] * nrows),
    ...         "locxal_id": pa.array(range(nrows), pa.uint64()),
    ...     })>>> for hours in range(10):
    ...     timestamp = datetime(2023, 1, 1) + timedelta(hours=hours)
    ...     data = new_data(timestamp)
    ...     dataset = lance.write_dataset(data, 'time_series', mode="append")
    ...>>> dataset.head(9).to_pandas()
      insertion_timestamp  locxal_id
    0          2023-01-01          0
    1          2023-01-01          1
    2          2023-01-01          2
    3          2023-01-01          3
    4          2023-01-01          4
    5          2023-01-01          5
    6          2023-01-01          6
    7          2023-01-01          7
    8          2023-01-01          8

Initially, there will be 10 fragments, once for each hour:

    >>> dataset.get_fragments()
    [LanceFileFragment(id=0, data_files=['7e2d7a80-7545-4c97-9c4c-a83795177c51.lance']),
     LanceFileFragment(id=1, data_files=['049d47dc-9757-4188-bdc2-0a3f95124ef6.lance']),
     LanceFileFragment(id=2, data_files=['3c7f9a43-f709-474a-92e5-fa79d21d54c8.lance']),
     LanceFileFragment(id=3, data_files=['5b529fc4-2bd9-4d9b-a5e9-f0ea0e422384.lance']),
     LanceFileFragment(id=4, data_files=['c6b8f9e6-3146-44fe-a637-40c4d08aeb36.lance']),
     LanceFileFragment(id=5, data_files=['989292c8-0d40-4c35-aea1-c1e4b00138f5.lance']),
     LanceFileFragment(id=6, data_files=['48bf4832-5d67-4d46-99ae-53d9c0be2514.lance']),
     LanceFileFragment(id=7, data_files=['7fbfdca7-31f7-43e0-aa8e-f186df92652a.lance']),
     LanceFileFragment(id=8, data_files=['ee85d31c-19e1-418b-b6d7-9b930ae75ec4.lance']),
     LanceFileFragment(id=9, data_files=['ee93cb15-c0fe-47d6-8cb1-870c0d400ccf.lance'])]

Then after deleting the first 5 hours, there will only be 5 fragments remaining. Notice none of them have any deletion files.

    >>> dataset.delete("insertion_timestamp < cast('2023-01-01 05:00:00' as timestamp)")
    >>> dataset.get_fragments()
    [LanceFileFragment(id=5, data_files=['989292c8-0d40-4c35-aea1-c1e4b00138f5.lance']),
     LanceFileFragment(id=6, data_files=['48bf4832-5d67-4d46-99ae-53d9c0be2514.lance']),
     LanceFileFragment(id=7, data_files=['7fbfdca7-31f7-43e0-aa8e-f186df92652a.lance']),
     LanceFileFragment(id=8, data_files=['ee85d31c-19e1-418b-b6d7-9b930ae75ec4.lance']),
     LanceFileFragment(id=9, data_files=['ee93cb15-c0fe-47d6-8cb1-870c0d400ccf.lance'])]

By deleting whole fragments, we avoided the need for any deletion files. This workflow can avoid any latency hit but works only for particular workloads.

Admittedly, without partition values, itâ€™s hard to see what columns the fragment boundaries align with. Once partitioning is added to Lance datasets, such a query could be written without having to know the details of the write pattern, making it much easier to work with.

## Future directions

Next on our roadmap, weâ€™ll be implementing partitioning and updates.

Partitioning will make many write patterns easier to implement, such as deleting or overwriting partitions. It will also provide faster queries and scans with predicates, as we can eliminate whole partitions based on partition values.

Updates will allow changing the value of a subset of a few rows. This will pave the way for more complex ETL patterns such as upserts and merge operations columns with [merge](https://lancedb.github.io/lance/read_and_write.html#adding-new-columns)) or overwriting the whole dataset. Now we support deleting individual rows or fragments. This is one of the first in a series of features we are releasing to make it easier to manage large scale AI datasets.

The basics of delete are very simple: given a dataset, you can specify a SQL predicate and all of those rows will be deleted.

    >>> import lance
    >>> import pandas as pd
    >>> data = pd.DataFrame({
    ...     'x': [1, 2, 3],
    ...     'y': ['a', 'b', 'c']
    ... })
    >>> dataset = lance.write_dataset(data, 'my_dataset', mode="overwrite")
    >>> dataset.to_table().to_pandas()
       x  y
    0  1  a
    1  2  b
    2  3  c
    >>> dataset.delete("x = 2")
    >>> dataset.to_table().to_pandas()
       x  y
    0  1  a
    1  3  c

Deletion in Lance is non-destructive. We keep a snapshot of each version of the dataset, so you can always go back to previous versions.

    >>> old_dataset = lance.dataset("my_dataset", version=dataset.version - 1)
    >>> old_dataset.to_table().to_pandas()
       x  y
    0  1  a
    1  2  b
    2  3  c

Lance uses SQL expressions, which allows for complex predicates during deletion.

    >>> dataset.delete("(x = 1) OR (y in ('a', 'b', 'e'))")
    >>> dataset.to_table().to_pandas()
       x  y
    0  3  c

See [our documentation](https://lancedb.github.io/lance/read_and_write.html#filter-push-down) for a full list of supported SQL in predicates.

## Implementation and expected performance

How does Lance implement deletion? When we delete rows, we save a new file called a deletion file for each affected fragment. The file contains the row ids of all deleted rows.

By writing these deletion files, we avoid rewriting data files, which is expensive on its own but also has special ramifications for Lance. If a data files is rewritten, the row ids are changes, invalidating the ANN indices for those files. So if we rewrote the data files, weâ€™d also have to update the indices.

To see which fragments in the datasets have deletion files, use the **get_fragments()** method. From our previous example, if we compare the fragments in the **old_dataset** versus the new **dataset**, weâ€™ll see the new dataset now has an additional **deletion_file** but the **data_files** remains the same.

    >>> old_dataset.get_fragments()
    [LanceFileFragment(id=0,
                       data_files=['605be8b8-8b01-47b3-b4a8-45af4b2acdea.lance'])]
    >>> dataset.get_fragments()
    [LanceFileFragment(id=0,
                       data_files=['605be8b8-8b01-47b3-b4a8-45af4b2acdea.lance'],
                       deletion_file='_deletions/0-14-16447660214542675320.arrow')]

Reading these new deletion files can add a small amount of latency to queries, so we cache this metadata. This means that tables that have deletions will have a little bit of cold start latency. Weâ€™ll be benchmarking and improving this in the near future.

In cases where a predicate aligns with the write pattern, we can avoid writing deletion files entirely; instead just eliminating an entire fragment. For example, imagine there is a sensor dataset that dumps data with a timestamp every hour. If we pass a deletion predicate on the timestamp column, we can just eliminate the particular data fragments that are part of the dump. The dataset might be written like:

    >>> from datetime import datetime, timedelta
    >>> import shutil
    >>> import pyarrow as pa

    >>> def new_data(timestamp: datetime) -> pa.Table:
    ...     nrows = 10_000
    ...     return pa.table({
    ...         "insertion_timestamp": pa.array([timestamp] * nrows),
    ...         "locxal_id": pa.array(range(nrows), pa.uint64()),
    ...     })

    >>> for hours in range(10):
    ...     timestamp = datetime(2023, 1, 1) + timedelta(hours=hours)
    ...     data = new_data(timestamp)
    ...     dataset = lance.write_dataset(data, 'time_series', mode="append")
    ...

    >>> dataset.head(9).to_pandas()
      insertion_timestamp  locxal_id
    0          2023-01-01          0
    1          2023-01-01          1
    2          2023-01-01          2
    3          2023-01-01          3
    4          2023-01-01          4
    5          2023-01-01          5
    6          2023-01-01          6
    7          2023-01-01          7
    8          2023-01-01          8

Initially, there will be 10 fragments, once for each hour:

    >>> dataset.get_fragments()
    [LanceFileFragment(id=0, data_files=['7e2d7a80-7545-4c97-9c4c-a83795177c51.lance']),
     LanceFileFragment(id=1, data_files=['049d47dc-9757-4188-bdc2-0a3f95124ef6.lance']),
     LanceFileFragment(id=2, data_files=['3c7f9a43-f709-474a-92e5-fa79d21d54c8.lance']),
     LanceFileFragment(id=3, data_files=['5b529fc4-2bd9-4d9b-a5e9-f0ea0e422384.lance']),
     LanceFileFragment(id=4, data_files=['c6b8f9e6-3146-44fe-a637-40c4d08aeb36.lance']),
     LanceFileFragment(id=5, data_files=['989292c8-0d40-4c35-aea1-c1e4b00138f5.lance']),
     LanceFileFragment(id=6, data_files=['48bf4832-5d67-4d46-99ae-53d9c0be2514.lance']),
     LanceFileFragment(id=7, data_files=['7fbfdca7-31f7-43e0-aa8e-f186df92652a.lance']),
     LanceFileFragment(id=8, data_files=['ee85d31c-19e1-418b-b6d7-9b930ae75ec4.lance']),
     LanceFileFragment(id=9, data_files=['ee93cb15-c0fe-47d6-8cb1-870c0d400ccf.lance'])]

Then after deleting the first 5 hours, there will only be 5 fragments remaining. Notice none of them have any deletion files.

    >>> dataset.delete("insertion_timestamp < cast('2023-01-01 05:00:00' as timestamp)")
    >>> dataset.get_fragments()
    [LanceFileFragment(id=5, data_files=['989292c8-0d40-4c35-aea1-c1e4b00138f5.lance']),
     LanceFileFragment(id=6, data_files=['48bf4832-5d67-4d46-99ae-53d9c0be2514.lance']),
     LanceFileFragment(id=7, data_files=['7fbfdca7-31f7-43e0-aa8e-f186df92652a.lance']),
     LanceFileFragment(id=8, data_files=['ee85d31c-19e1-418b-b6d7-9b930ae75ec4.lance']),
     LanceFileFragment(id=9, data_files=['ee93cb15-c0fe-47d6-8cb1-870c0d400ccf.lance'])]

By deleting whole fragments, we avoided the need for any deletion files. This workflow can avoid any latency hit, but works only for particular workloads.

Admittedly, without partition values, itâ€™s hard to see what columns the fragment boundaries align with. Once partitioning is added to Lance datasets, such a query could be written without having to know the details of the write pattern, making it much easier to work with.

## Future directions

Next on our roadmap, weâ€™ll be implementing partitioning and updates.

Partitioning will make many write patterns easier to implement, such as deleting or overwriting partitions. It will also provide faster queries and scans with predicates, as we can eliminate whole partitions based on partition values.

Updates will allow changing the value of a subset of a few rows. This will pave the way for more complex ETL patterns such as upserts and merge operations.
```